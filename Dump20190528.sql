-- MySQL dump 10.13  Distrib 8.0.15, for Win64 (x86_64)
--
-- Host: localhost    Database: reportengine
-- ------------------------------------------------------
-- Server version	8.0.15

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
 SET NAMES utf8 ;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `admins`
--

DROP TABLE IF EXISTS `admins`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
 SET character_set_client = utf8mb4 ;
CREATE TABLE `admins` (
  `adminname` varchar(255) NOT NULL,
  `adminpassword` varchar(255) NOT NULL,
  PRIMARY KEY (`adminname`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `admins`
--

LOCK TABLES `admins` WRITE;
/*!40000 ALTER TABLE `admins` DISABLE KEYS */;
INSERT INTO `admins` VALUES ('admin','netas**');
/*!40000 ALTER TABLE `admins` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `fieldcontents`
--

DROP TABLE IF EXISTS `fieldcontents`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
 SET character_set_client = utf8mb4 ;
CREATE TABLE `fieldcontents` (
  `pname` varchar(255) NOT NULL,
  `field1` varchar(255) DEFAULT NULL,
  `field2` varchar(255) DEFAULT NULL,
  `field3` varchar(255) DEFAULT NULL,
  `field4` varchar(255) DEFAULT NULL,
  `field5` varchar(255) DEFAULT NULL,
  `field6` varchar(255) DEFAULT NULL,
  `field7` varchar(255) DEFAULT NULL,
  `field8` varchar(255) DEFAULT NULL,
  `field9` varchar(255) DEFAULT NULL,
  `caseid` varchar(255) DEFAULT NULL,
  KEY `pname` (`pname`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `fieldcontents`
--

LOCK TABLES `fieldcontents` WRITE;
/*!40000 ALTER TABLE `fieldcontents` DISABLE KEYS */;
INSERT INTO `fieldcontents` VALUES ('hardware','546','78678678','      ','','','','','','',NULL),('p2','34','','      ','','','','','','',NULL),('hardware','2','34','      ','','','','','','',NULL),('p2','last','','      ','','','','','','',NULL),('upgrade','6','8','      ','','','','','','',NULL),('upgrade','6','8','      ','','','','','','',NULL),('hardware','4','7','      ','','','','','','',NULL),('hardware','1','19','      ','','','','','','','86'),('p2','','','      ','','','','','','','ewr3423242'),('upgrade','78','90','      ','','','','','','','9087-564545'),('p2','567','','      ','','','','','','','78-78'),('p2','5454543','','      ','','','','','','','56'),('hardware','1','2','      ','','','','','','','89-89'),('upgrade','3','5','      ','','','','','','','34'),('hardware','6','10','      ','','','','','','','9000-56'),('hardware','4','7','      ','','','','','','','45'),('upgrade','59','16','      ','','','','','','','98'),('hardware','5','8','      ','','','','','','','90'),('p2','rewrewrewrw','','      ','','','','','','','34324'),('hardware','rewrewrewrw','56','      ','','','','','','','34324'),('upgrade','rewrewrewrw','56','      ','','','','','','','34324'),('upgrade','rewrewrewrw','56','      ','','','','','','','34324'),('upgrade','rewrewrewrw','56','      ','','','','','','','34324'),('p2','rewrewrewrw','56','      ','','','','','','','34324'),('hardware','4','78','      ','','','','','','','5656565'),('upgrade','werew','ewrewr','      ','','','','','','','wewrwr'),('upgrade','rewr','wrwer','      ','','','','','','','werwe'),('p2','ewrewrewr','','      ','','','','','','','345345345'),('Software','67','2006','2015      ','Netas','customer','12.00','','','','12'),('hardware','','','      ','','','','','','','re'),('Software','344','342324','24242      ','42342342','23424','234243242','','','','334'),('hardware','wd','asdasd','      ','','','','','','','4546456'),('upgrade','54545','5654dfgdf','      ','','','','','','','454444444444444'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','wdwdwdsa'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','wdwdwdsa'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','wdwdwdsa'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','wdwdwdsa'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','wdwdwdsa'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','123456'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','123456'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','1234567'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','1234567'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','12365'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','654413'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','654413'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','654413435'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','654413435'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','654413435'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','654413435'),('upgrade','gtyhgf','reterdfgdf','      ','','','','','','','654413435'),('hardware','6','8','      ','','','','','','','123654'),('hardware','34','54','      ','','','','','','','dffds'),('hardware','56','67','      ','','','','','','','4545345'),('hardware','56','67','      ','','','','','','','4545345'),('hardware','1','23','      ','','','','','','','2500'),('hardware','1','23','      ','','','','','','','25001'),('hardware','1','23','      ','','','','','','','2512'),('hardware','1','23','      ','','','','','','','251223'),('hardware','1','23','      ','','','','','','','3343'),('upgrade','134','2343','      ','','','','','','','33434'),('upgrade','5','1','      ','','','','','','','345423'),('p2','78','','      ','','','','','','','9087'),('hardware','67','76','      ','','','','','','','12'),('hardware','67','76','      ','','','','','','','123'),('upgrade','45','4532','      ','','','','','','','43432'),('hardware','1','2','      ','','','','','','','4578'),('hardware','1','345','      ','','','','','','','78946554'),('hardware','45','44','      ','','','','','','','3411112'),('hardware','45','44','      ','','','','','','','3411112q'),('hardware','45','44','      ','','','','','','','3411112q'),('hardware','45','44','      ','','','','','','','34111'),('hardware','45','44','      ','','','','','','','34123'),('p2','Pegasus2','','      ','','','','','','','12321133'),('hardware','232','3432','      ','','','','','','','34344534'),('hardware','342','67878','      ','','','','','','','4324324'),('hardware','342','67878','      ','','','','','','','4324324'),('hardware','342','67878','      ','','','','','','','4324324'),('hardware','13','123','      ','','','','','','','1222254'),('upgrade','MCP_18.0.32.0','MCP_19.0.21.0','      ','','','','','','','183452-324955'),('hardware','34','56','      ','','','','','','','23344-453453'),('hardware','1','45','      ','','','','','','','898989'),('hardware','34','45','      ','','','','','','','456789124'),('hardware','34','45','      ','','','','','','','4567891'),('upgrade','14.1.0.13','14.1.15.3 ','      ','','','','','','','193424-324953'),('hardware','fr','ttt','      ','','','','','','','13234'),('hardware','fr','ttt','      ','','','','','','','13232'),('hardware','fr','ttt','      ','','','','','','','1323232123'),('hardware','fr','ttt','      ','','','','','','','1323232123'),('hardware','fr','ttt','      ','','','','','','','fgfg'),('hardware','fr','ttt','      ','','','','','','','fgfgds'),('hardware','localllllll1','localiki','üç','','','','','','','1236365888514656553232'),('hardware','fr','ttt','      ','','','','','','','123636588851465632'),('hardware','fr','ttt','      ','','','','','','','12363658865632'),('hardware','456','275','      ','','','','','','','453445dsa'),('hardware','kilimcinin','köroğlu','      ','','','','','','','454532'),('local','local','local ','her yer','','','','','','','efdew'),('local','local','local ','her yer','','','','','','','efdew'),('local','local','local ','her yer','','','','','','','efdew'),('local','local','local ','her yer','','','','','','','efdew'),('hardware','rt','454','      ','','','','','','','566464324'),('hardware','yep','yepo','      ','','','','','','','675756534'),('hardware','yepas','yepow','      ','','','','','','','675756dsa'),('hardware','yepas','yepoww','      ','','','','','','','675756'),('hardware','yepaswd','yepoww','      ','','','','','','','6757asad'),('hardware','yepaswd','yepoww','      ','','','','','','','6757as324'),('hardware','yepaswd','yepoww','      ','','','','','','','6757as324'),('hardware','yepaswd','yepoww','      ','','','','','','','67524'),('hardware','yepaswd','yepoww','      ','','','','','','','6752'),('hardware','yepaswd','yepoww','      ','','','','','','','675223'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','6734242'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','673424243'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','424243'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','424243'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','424243'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','42dsada'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','42dsada'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','42dsadawe'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','42dsadawe'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','42dsadawasde'),('local','ne ','çok','local','','','','','','','42dsadawasd'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','34242342'),('hardware','yepaswd32423','yepoww23','      ','','','','','','','3424234223'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','1','23','      ','','','','','','','2342'),('local','local','local ','her yer','','','','','','','efdew'),('local','final','second','third','','','','','','','3423123123'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','rtgfdgdf','rgfdgdfgd','      ','','','','','','','wdasdas'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','wdsa','asdas','      ','','','','','','','dsadas'),('hardware','wadsada','wdsadasd','      ','','','','','','','wdadas'),('hardware','ttr','tyty','      ','','','','','','','32432432423'),('hardware','wdsada','wdsadas','      ','','','','','','','dsadas'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('upgrade','asdas','assdada','      ','','','','','','','wdsadaswds'),('upgrade','asdas','assdada','      ','','','','','','','wdsada323swds'),('hardware','f','t','      ','','','','','','','wadasd'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('local','sdasdas','adsadasd','asdasdasda      ','','','','','','','dsadas'),('hardware','asdas','asdasda','      ','','','','','','','dsada'),('hardware','dsadas','assdasdas','      ','','','','','','','324324324'),('hardware','wrewrw','5435353453','      ','','','','','','','dsadasd'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('hardware','wdsadas','wdasdas','      ','','','','','','','wdasdas'),('hardware','wdsadas','wdasdasd','      ','','','','','','','dsadas'),('hardware','dasdas','wdasdasd','      ','','','','','','','dsadas'),('hardware','dsadas','wdsadas','      ','','','','','','','dsadas'),('hardware','wdasdas','dasdasdas','      ','','','','','','','dsadwd'),('p2','uhhuhj','2','asdasd','asdas','asdas','','','','','test1'),('hardware','adasdkmsd','mbmnmnmnm','asdkadkas','','','','','','','wdsadas'),('elmaupgrade','ewf','sada','dad','null','null','null','','','','1995');
/*!40000 ALTER TABLE `fieldcontents` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `generalinfo`
--

DROP TABLE IF EXISTS `generalinfo`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
 SET character_set_client = utf8mb4 ;
CREATE TABLE `generalinfo` (
  `generalinfo_id` int(11) NOT NULL AUTO_INCREMENT,
  `fromcompletedby` varchar(255) NOT NULL,
  `gpsproduct` varchar(255) NOT NULL,
  `startdate` date DEFAULT NULL,
  `caseid` varchar(255) DEFAULT NULL,
  `customername` varchar(255) DEFAULT NULL,
  `description` text,
  `pname` varchar(255) NOT NULL,
  PRIMARY KEY (`generalinfo_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1256 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `generalinfo`
--

LOCK TABLES `generalinfo` WRITE;
/*!40000 ALTER TABLE `generalinfo` DISABLE KEYS */;
INSERT INTO `generalinfo` VALUES (1,'Kaan BEREKETLI (NETAS External)','AS-OAM','2019-05-07','190506-247314      ','Liberty Global','Customer Load: 17.0.22\n\nMark Zattiero from ER pages us mentioning that Liberty Global is not able to provision through OSSGATE. Also Ogulcan from CMT team was involved in the process.\n\nThere was a huge delay when they run \"qsip \" and a complete fail when they tried to add or remove lines. \nWe checked if the PROVs and SMs were up and they were. We checked PROV Client if it\'s slow or delaying or not. It turned out PROV Client was working just fine. We were able to add or get any user in a moment.\nWe opened up the PROV debug logs in order to check what\'s happening on the PROV side. We couldn\'t see any issue or request coming from CMT.\n\nThen Ogulcan from CMT renewed their certificates and restarted the SESM on that side. After that the problem was resolved and we dropped from the call.','null'),(2,'YUKSEK, Mustafa','AS-OAM','2019-05-01','190416-182643      ','TSTT','Thomas Godwin who is from ER paged me and reported that the customer and Bill were not able to reach to remote control for the blade 3 (mas blade) due to java issue. They were trying to perform migration for MAS blades from old chassis to the new chases but now the blades were not reachable from network. The hardware type of the blade is IBM-HS21. \n\nTherefore, we wanted from customer to access to the blade via KVM . He was able to connect and login prompt appeared. We tried to pinging IP of the  MAS blade from EM server but no luck.  We asked from customer to ping the gateway IP of this blade from console but It was not pingable and MAS servers seemed as grey out.\n\nWe connected to MM. The MAS blade was seen up and running as green. I/O modules were up and running. Then we asked from the customer please un-plug the EMS modules  (power modules) and replugged then customer reseat and plugged all 4 modules at the same time and server is up again however the blades weren\'t able to connect network. \n\nSo, we strongly believed this is an issue about customer network configuration that causes the blade not to be able to connect the service network.  We checked ESM module configs and we compare all the configs with our running lab and that seems ok. We performed power cycle to MAS blade and I/O modules and MM itself but did not solve our issue. \n\nFor this reason, we stated that we needed a view of the design team to understand what kind of parameters should be checked after the BCT chassis Itself replaced and other old blades migrated to it. Then we will continue investigating the issue with our working time that starts at 8 am GMT+3 and customer accepted,it then we drooped from the call.','null'),(3,'YUKSEK, Mustafa','AS-OAM','2019-04-30','190416-182643      ','TSTT','Tom Draper who is from ER called to me and reported that the customer wasn\'t able to reach Management Module after replacing the faulty BCT chassis with new one. In this respect, we\'ve connected the site with GTS VM and checked the situation and all reported things were like as they said.\n\nThere wasn\'t any access to Management module and we joined to Bridge with the customer who are Von and Alex. Then we asked If there is any specific action on the site before, they stated that BC-T_00 has been replaced on two weeks ago, and needs to get the new management module configured then they faced the problem. All the MAS blades was on this chassis and no service on the MAS side.\n\nSo, we suspected about cables were plugged in a proper way (with a true configuration) or not and we shared an ss which was taken from our labs to show cabling. Customer stated that cabling was correct.\nAlso, we\'ve used configuration wizard but the result didn\'t change.\n\nAfter that, we wanted to reseat the ESM cards, two on each end of the BC-T shelf but Von wasn\'t familiar with the shelf.\nThen we provided related document \"BladeCenter T Type 8720 and 8730 Hardware Maintenance Manual and Troubleshooting Guide\" for guiding the customer to be sure they configured correctly but the customer said they don\'t know anything about that.\n\nDuring all these operations the customer disappeared from the bridge much time and we needed to wait for them. This happened for a few times.\n\nAfter all these actions we wanted to ERS team for checking the ports and they reported that there isn\'t any issue about ERS ports.\nThen, we checked all related documents then we said to the customer please follow the provided documents with mentioned titles but the customer wanted to us join the process from step1 and complete it. Then a Bomgar session was set for this operation then our cooperation friend Bill Joined to us for helping the customer during following the documents. Bill noticed they did not plugged the 8600 to BCT chassis and after plugged it the problem of entering MM was resolved.  then we dropped from the call.','null'),(4,'Emre OVA (NETAS External)','AS-OAM','2019-04-23','TBD      ','TRICOM','Upgrade Path: 18.0.31.0  19.0.20.X\n\nDonnell Williamson from SWD paged me out to report that platform patching of the primary host servers failed in Tricoms AS Upgrade. I have connected to site and checked the status of upgrade wizard. Primary host server network communication was failed after the patching operation is done on HOS1S1. Server was not accessible and pingable. It was a SB-ATCA Emerson 7370 open slot blade. I requested customer specbook and Donnall sent it to me via e-mail.\n\nAfter I checked the specbook, I found that problematic server is located on 0 0 5 0     number, so I recommended them to run following scripts through NDM interface.\n\nha app-blade deactivate 0 0 5 0\nha app-blade offline 0 0 5 0\nha app-blade online 0 0 5 0\nha app-blade activate 0 0 5 0\n\nAfter the connection of HOS1S2 came back, Donnall retried UW and problematic screen passed successfully. SWD will create the follow-up case and assign to AS OAM queue for further investigation. I collected necessary logs in RCA analysis. Donnall will attach those log files into case files of newly created case.','null'),(5,'Emre OVA (NETAS External)','AS-OAM','2019-04-22','190422-183328      ','HAWAIIAN TELECOM, INC','Jim Shelby from ER paged me out to report that Hawaii Telecom have provisioning outage. He provided me site access via GTS VM servers and I connected to site for further investigation. Site was running on HP-3310 servers at MCP 14.0.16.3 load.\n\r\nPROV1_0 was running as Configured  Up  Active. I killed the process ID of PROV1-0 and re-deployed the instance but deploying/starting actions were taking a long time. Then I realized that running CPU rate is around %98 levels on both EMServers. Both of EMServers had critical CPU usage alarms. Both of those servers were running for 498 days. In order to speed up the working of tasks on servers, I recommended customer to reboot both EMServers in sequence. (First hot standby one, then active one).\n\r\nAfter customer took our corrective actions, all issues resolved and provisioning attempts were successful.\n\r\nSince customer system is working on a retired MCP release, we did not provide any RCA.','null'),(6,'Emre OVA (NETAS External)','AS-OAM','2019-04-22','190422-183254      ','ONECONNECT SERVICES INC','Tom Draper from ER paged out AS OAM GPS pager for reporting an E2 outage by One Connect system. Omer responded Toms call and summarized me the outage details. I joined the GTS VM and worked with Tom and Omer. SM-1 was in Configured  Up  Active status, connection to DB instance 1 was looking down, and customer was not able to change voice mail password through PROV GUI.\n\nAs a first action, I swacted SM units to recover system from E2 and SM 0 got into Active position again. It was working as Online  Up -Active. Then, I restarted NED connection of EMS2 (SM1) and re-deployed SM1 instance to EMS2. We could fix rogue instance failure in this way. SM0 and SM1 was working as Online Up Active and Online Up Hotstandby. DB connection failures were resolved as well.\n\nThen, customer reported that there is a DBCOMM alarm on BCP1_0 unit and I thought that it might be a stuck alarm and can be cleared via BCP instance restart. After I received customer approval, I restarted BCP1-0 and alarm had been cleared on BCP1-0.\n\nAs a last customer question, customer was not able to change Voice mail password via A2 PROV GUI. GUI was responding with the following error:\n\n\"Unable to process request at this time. Please make sure there is no problem with Media Application Server (MAS) connection.\"\n\nI checked the accessibility of 4 MAS Servers but all of those were looking as UP and running. At this point, I realized that IMMServers have SVR 101 (server down) alarms. We requested customer to bring the connection of IMMServers back, we provided them related Ips and hostnames. Customer could not find the location of servers during approximately 30 mins. After customer rebooted related IMM/MAS servers, issue has been resolved. \n\nAfter all alarms cleared on customer AS system, we dropped from bridge.\n\nSince the issue reported from an EOL release (17.0.22.11/ AS 10.4), no RCA provided.','null'),(7,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-04-12','190413-182231','IPC SYSTEMS','Customer Load(if it is upgrade upgrade path): 19.0.4.2/19.0.21.1 \r\nProblem Description \r\n==================== \r\nUnstable prov during upgrade - stuck upgrade \n\r\nActions taken \r\n================= \r\nBrent Combs from ER paged me that IPC system had the problem during upgrade. He \'s told me He had performed undeploy/deploy but it didnt work. \r\nI have connected to site via Webex and i have also joined to bridge.  \r\nAfter i have connected to problematic server via ssh, i have checked PROV work logs under the directory /var/mcp/run/MCP_XX/work/PROVx.log to check the exceptions.  \r\nI have seen jboss exceptions on work logs and run \"mcpRelease.pl\", \"showversion.pl\" to see the hardware type and levels of the platform. I have compared the levels with release notes.  \r\nThe hardware type was set as Other so i have asked the customer what is the hardware type of this server. Laurent confirmed me that it is DELL PowerEdge R630. \r\nAfter that we have looked at the oss logs and have seen the error below: \n\r\nCaused by: java.lang.Exception: Port 8083 already in use. \r\n at org.jboss.web.WebServer.start(WebServer.java:233) \r\n       at org.jboss.web.WebService.startService(WebService.java:322) \r\n       at org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.java:376) \r\n       at org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupport.java:322) \r\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \n\r\nSince we have seen the port in the use error and exceptions from oss logs, we run \"netstat -a | grep *8083*\" and \"netstat -a\" to see the ports in use.  \r\nThere is no 8083 in use. Then, we have run \"neinit -p\" and have seen that PA and PROV were deployed in the same server. \r\nOnly the SM and PROV can be on the same server, because PROV and PA utilize the same port 8083 for jboss. The customer undeployed PA Server and deployed it to another VM(PA server guest) which was deployed in same HostServer. \r\nIn this way, PROV would able to take the Active Role and upgrade step was passed successfully.  \r\nBecause of the problem solved, i have dropped the call. Laurent will perform the other steps. \n\n\r\nAny Follow-up Case \r\n=================== \r\nNone','null'),(8,'Joyce Lyon','AS-OAM','2019-04-10','None','','Testing errors.','null'),(9,'Mustafa YUKSEK (NETAS External)','AS-OAM','2019-04-02','190402-180207','JET INFOSYSTEMS','Customer Load ==> 17.0.32.0\n\r\nProblem Description\r\n======================================\r\nI\'ve been paged by Rodney Neese and he stated that the customer was making to migration operation for their system from HP3310 to MA-MRS however when they tried to apply new license key for the new hardware, they faced \"invalid license key\" error and there was a critical license key alarm on active SM unit after the half migration of it completed. Also, when they tried to restart SM_1 which was running on HP3310, they\'ve experienced \"no boot db instance available error \"\n\n\r\nActions taken\r\n=======================================\r\nI connected the site and checked the status of the SM instances;\r\nSM_0 was active and was migrated from HP3310 to MA-RMS, \r\nSM_1 was configured/down/unavailable and it was on HP3310,\r\nIn this respect, I tried to apply the license key however I faced the mentioned \"invalid license key\" error and I queried that does the license key belong the MA-RMS or HP3310?\n\r\nFirst, he said it was old, then he said it was for new hardware, then they made sure that it was for the HP3310 and they agreed. For this reason, I mentioned that in the followed document on \"Precautions and Preparations\" section is stated that the customer should have a valid license key for new hardware which is provided by KRS.\r\nHowever, the customer didn\'t have it and they stated that on that time they weren\'t able to take a new one from KRS and they will take it on tomorrow.For this reason they wanted to roll back their primary units to legacy HP3310.\n\r\nThen,when we tried to change the state of the SM_1 as hot standby the following no boot db instance available error was returned;\n\r\n.com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionPoolException: No boot db instance available at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.createBootDBInstance(DBConnectionManagerBase.java:320) at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.bootstrap(DBConnectionManagerBase.java:216) at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initDBLayer(InstallationUtilitiesBase.java:158) at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initPersistenceFramework(InstallationUtilitiesBase.java:109) at .\n\r\nThen, we identified that SM_1/data/neprops.txt was populated with incorrect DB host IP. After re-populating the db.host value with running DB host IP, SM1 was able to be initialized and we could bring the system to the old legacy server. After we recovered the system from E1 condition, I dropped from the bridge. If they have a correct license key then they will be able to complete the whole procedure.','null'),(10,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2019-03-23','190323-178905','AXTEL S. A. B. DE C. V.','Customer Load --> 17.0.32.0\n\r\nProblem Description\r\n====================\r\nI have been paged by Bob Johnson and he stated that FPM1 was unavailable while doing the upgrade.\n\r\nActions taken\r\n=================\r\nI connected the site and checked the status of the FPM1, it was online/down/unavailable. I tried to kill/start but it did not help, FPM instance was dropping unavailable from initializing. When I checked the work log of the problematic FPM, there were some exceptions like \"Unable to initialize NetworkInterfaceManager\", \"Failed to parse interface information string\" and \"UnknownHostExeption\". After that, I checked the Multinetting Parameter of the problematic FPM from MCP GUI and noticed that Multinetting Status set as \"false\". I make the status of the FPM as offline/down/unavailable by killing the process and set this Multinetting parameter as \"true\" and start the FPM instance again. At this point, the FPM instance became online/up/hot standby. Since the problem has been solved I dropped from the call.\n\r\nAny Follow-up Case\r\n===================\r\n190323-178905','null'),(11,'Omer KIRCALI','AS-OAM','2019-03-14','190314-177643','University of Texas - Austin','Customer Load(If an upgrade problem upgrade path): MCP_19.0.21.1\n\r\nProblem Description:\r\n==============================\r\nMartha Foster paged me about they cannot access the PROV GUI after entering the user/password. Its become Timeout. Also they could not send OPI request to PROV via GVPP.\n\r\nActions Taken:\r\n====================\r\nI connected the VM and check the issue. Both PROV1 and PROV2 was ONLINE UP ACTIVE. I tried another browser but the result was the same. I asked from Martha to restart the PROV2 first and then PROV1. After she restarted both PROV Instance , then problem resolved. I asked from martha to collect oss logs with ssh to Actice EM from below directories and after customer confirm that there was no provisioning problem exit , I dropped from call.\n\r\n/var/mcp/oss/log/SM/all/MCP/PROV_x/ \r\n/var/mcp/oss/log/SM/all/MCP/SM_x/ \n\r\nAny followup case:\r\n=======================\r\nNone','null'),(12,'Omer KIRCALI','AS-OAM','2019-03-12','190312-176992','FRONTIER COMMUNICATIONS','Customer Load(If an upgrade problem upgrade path): Upgrade path is --> 14.1.0.13 / 14.1.15.3\n\r\nProblem Description:\r\n==============================\r\nJames Wilkerson paged me about failure on the DB mock screen. I have connected the site and check the issue. On the wizard , Step 14 of Upgrade Wizard: DB mock screen was failed and the below error was exist;\n\r\nUpgrade path is --> 14.1.0.13 / 14.1.15.3\n\r\nChecking db server 113.47.22.18 disk space to perform dry run upgrade ...\r\nDB server 113.47.22.18 total tablespace size is 7157760 KB\r\nDeploying DB run time scripts necessary for the Dry-run DB upgrade ...\r\nPerforming DB Upgrade Dry-run, please be patient.\r\nGetting DB Upgrade Dry-run result ...\r\nFails to create duplicated schema from currently deployed schema.\r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/dryRunDBUpgrade/ut_dryRunDBUpgrade.EMS1_07305f6e-2bee-1b21-a83e-000e0cb7c3dc_ut_dryRunDBUpgrade.pl_DB_MOCK_UPG_0.20190312_000258.log\n\r\nSchema Duplication Error\n\r\nI have checked the OracleMethod_upgrade_dry_run.log and I saw the below error;\n\r\nORA-39126: Worker unexpected fatal error in KUPW$WORKER.PUT_DDLS [PLUGTS_BLK]\r\nORA-01403: no data found\n\r\nORA-06512: at \"SYS.DBMS_SYS_ERROR\", line 95\r\nORA-06512: at \"SYS.KUPW$WORKER\", line 6345\n\r\n----- PL/SQL Call Stack -----\r\nobject line object\r\nhandle number name\r\n0x70632650 15032 package body SYS.KUPW$WORKER\r\n0x70632650 6372 package body SYS.KUPW$WORKER\r\n0x70632650 12391 package body SYS.KUPW$WORKER\r\n0x70632650 3346 package body SYS.KUPW$WORKER\r\n0x70632650 6972 package body SYS.KUPW$WORKER\r\n0x70632650 1314 package body SYS.KUPW$WORKER\r\n0x70447990 2 anonymous block\n\r\nJob \"SYSTEM\".\"SYS_IMPORT_TRANSPORTABLE_01\" stopped due to fatal error at 00:13:52\n\n\n\r\nDatabase impdp fatal error at /usr/lib/perl5/site_perl/DB/OraDataPump.pm line 826\r\nDB::OraDataPump::checkFatalError(\'DBCommands=HASH(0xa2fbea0)\') called at /usr/lib/perl5/site_perl/DB/OraDataPump.pm line 96\r\nDB::OraDataPump::impExpData(\'DBCommands=HASH(0xa2fbea0)\') called at /usr/lib/perl5/site_perl/DB/OraDataPump.pm line 215\r\nDB::OraDataPump::importData(\'DBCommands=HASH(0xa2fbea0)\') called at /var/mcp/run/MCP_14.1/mcpdb_1/bin/base/DryRunUpgrade.pm line 867\r\nDryRunUpgrade::impTbs(\'DBCommands=HASH(0xa2fbea0)\',\'/var/mcp/db/backup/dryRunUpgradeBackup.tar.gz\',\'MCPUSER\',\'dryRunUser\',\'ARRAY(0xa2e4504)\',\'ARRAY(0xa2de5cc)\') called at /var/mcp/run/MCP_14.1/mcpdb_1/bin/base/DryRunUpgrade.pm line 180\r\nDryRunUpgrade::create_dry_run_schema(\'DBCommands=HASH(0xa2fbea0)\',\'dryRunUser\') called at /var/mcp/run/MCP_14.1/mcpdb_1/bin/base/DryRunUpgrade.pm line 111\r\nDryRunUpgrade::upgrade_dry_run(\'DBCommands=HASH(0xa2fbea0)\',\'MCP_14.1.15.3_2015-12-18-1735\') called at /usr/lib/perl5/site_perl/DB/OraMethod.pm line 66\r\nDB::OraMethod::start(\'DBCommands=HASH(0xa2fbea0)\') called at /usr/lib/perl5/site_perl/mcsBase/BaseOperation.pm line 561\r\nmcsBase::BaseOperation::main(\'DBCommands=HASH(0xa2fbea0)\') called at ../bin/dbCommands.pl line 55\n\r\nActions Taken:\r\n====================\r\nThis is a known oracle issue and we have performed this procedure onto the so many sites as seen in the below cases;\n\r\n190305-175915\r\n180217-686755\r\n150706-538174\r\n140915-493577\n\r\nThen I applied the procedure on SECONDARY DB since the wizard performs dry-run upgrade on SECONDARY DB;\n\r\nSolution description:\r\n1. Login Oracle 10g Database as sysdba user\r\n[ntdbadm@vcarsSMLt0 ntdbadm]$ sqlplus\r\nSQL*Plus: Release 10.2.0.4.0 - Production on Wed Aug 18 13:40:30 2010\r\nCopyright (c) 1982, 2007, Oracle. All Rights Reserved.\r\nEnter user-name: sys as sysdba\r\nEnter password:\r\nConnected to:\r\nOracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production\r\nWith the Partitioning, OLAP, Data Mining and Real Application Testing options\r\nSQL>\r\n2. SET ESCAPE OFF on SQL*Plus\r\nSQL> set escape off\r\n3. Catdph.sql will Re-Install DataPump types and views\r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdph.sql\r\n4. prvtdtde.plb will Re-Install tde_library packages\r\nSQL >@ $ORACLE_HOME/rdbms/admin/prvtdtde.plb\r\n5. Catdpb.sql will Re-Install DataPump packages\r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdpb.sql\r\n6.Dbmspump.sql will Re-Install DBMS DataPump objects\r\nSQL >@ $ORACLE_HOME/rdbms/admin/dbmspump.sql\r\n7. To recompile invalid objects, if any\r\nSQL >@ $ORACLE_HOME/rdbms/admin/utlrp.sql\n\r\nAfter Retried the screen It was passed. Then I dropped from call.\n\r\nAny followup case:\r\n=======================\r\nWe are not givin RCA since the MCP release is retired.','null'),(13,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-03-09','190306-176264','U.S. Air Force Falcon AFB','Customer Load:MCP_18.0.28_X\r\nProblem Description\r\n====================\r\nHeap exhaustion alarm, PROV was bouncing, DB Connection Failure Alarm\n\r\nActions taken\r\n=================\r\nJohn Shamer from ER had paged me about the Blacbox issue 190306-176264. Bob Vosberg and Jerry Lalk have called in to report that PROV1 is bouncing. They see a Heap exhaustion alarm, PROV1 drops, and they get a no comms to DB alarm till PROV1 recovers. It is unknown if anyone is running the script detailed in the case.\r\nThere were no connection to site directly, The Alarm was diseapered after i have joined to bridge. \r\nI have talked with John Kisner,Bob Vosverg , Jerry Lalkthe and Vikram.\r\nThe customer told us that they run the script on 10 intances to get the userdata in the previous day. \r\nMost probably, execulation PROV memory turned Down and DB connection Failure alarm occured. \r\nDue to the alarm was not ON the system recently, we are not able to give RCA. \r\nFor investigate this issue reason further, i gave detailed procedure for log and data collection. \n\r\nJerry and Bob will collect the requested data collection and attach them to the case(You can see the procedures of requested data in case notes of the 190306-176264.)\r\nJohn will dispatch the case to GPS OAM queue to investigate the issue further.\n\r\nAfter 30 minutes passed, the alarm didnt occur.\r\nAccording to this, i have told ER to collect Heap Dump data when the alarm occurs and if they see any PROV down-DBConnection Failure call back.\r\nWe will continue this case with 190306-176264.\n\n\r\nAny Follow-up Case\r\n===================\r\nnone','null'),(14,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-03-06','190306-176113','Frontier COMMUNICATIONS','Upgrade Path(From/to): 14.1.0.13  / 14.1.15.3\n\r\nProblem Description\r\n====================\r\nDB SM upgrade screen was failed.\n\r\nActions taken\r\n=================\r\nJames Will from Sofware Delivery team paged me about having issues while upgrading on SM/DB screen. I have connected to site with James on RTPTECHOPSVM15.   \r\nI have asked James to connect both EM Servers via cli. After we have connected to EM Servers , i have checked the upgrade logs and seen that dbuserdata was not exist on the /var/mcp/install directory.  \r\nI have copied missing dbuserdata file from /mcpdb_0/work to missing /var/mcp/install directory, compared their md5sums and moved failed state files which located under upgrade work logs. After performing Retry, the upgrade wizard failed again due to \" Failed to obtain the required DB information. \" I have performed Save&Exit . It was a generic error and difficult to identify the reason for what cant being obtained. There was no any permission denied, missing file or such kind of error in details.   \r\nIn order to identify that whether the root cause is associated with current DB status or not, we have launched the MCP GUI and checked the database server alarms but monitoring was off and DB status was grey out in Logical View. After we started monitoring the database servers , we havent seen any connection failure alarm. Also we have connected to database and checked the loads of database servers.   \r\nAfter this, we have seen that primary EM Server installprops.txt file had the to load which is 14.1.15.3 instead of from load 14.1.0.13 . Weve performed mcpInstallFirstLoad.pl and choose the from load in case of not causing any problem and performed SM double Swact.   \r\nThen, we have taken some Java Cache error messages while trying to launch Upgrade Wizard because of computer Java version and I have cleaned the Java Cache from computer. It took a few minutes.. (longer than expected).  When we re-launched Upgrade Wizard, we have received same error message. We decided to run manual upgrade script to see the error specifically. (mcpUpgrade.pl) So, I have connected to Em server via cli again and run mcpUpgrade.pl and seen the permissions error. This time returned error message was more clear and saying us that the permission of uploaded .dbuserdata is not valid. We compared the permissions with our local GPS labs and gave the correct permissions to directories and files. We clicked Retry again the error logs were passed and database server was upgraded. While we are waiting for the completion of running script, we have received no such file or directory error for dbsep8.ora file. It was a known issue and while performing an upgrade from 10g to 11g , these missing dbsepX.ora file error was seen. There are related software fixes in the upper versions so we moved the dbora8.ora file in the directory that software was looking for. After this, retry button was clicked again and during SM upgrade I have received OMI web Service fail error in upgrade wizard. When we quickly checked the returned error on closed JIRAs and cases, we found that its a known bug of UW for current customer load and there are permanent fixes in latest MRs of 14.1.10..  Since, I have checked the SM upgrade was successfully completed on smUpgrade logs, I clicked Save and Exit button and  re-launch Wizard again.  \r\nAt the end, the next button was enabled and SM&DB upgrade was successfully completed. After passing the screen successfully, we dropped from the GTS VM and James will continue to perform rest of the upgrade steps.    \n\n\r\nAny Follow-up Case\r\n=================\r\nNone','null'),(15,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2019-03-05','190305-175915','FRONTIER COMMUNICATIONS','Upgrade Path--> 14.1.0.13 - 14.1.15.3\n\r\nI have been paged by James from SWD and he reported that DBMock has failed with \"Fails to create duplicated schema from currently deployed schema.\" When I checked the logs, there were the following errors;\n\r\nORA-39126: Worker unexpected fatal error in KUPW$WORKER.PUT_DDLS [PLUGTS_BLK] \r\nORA-01403: no data found\n\r\nORA-06512: at \"SYS.DBMS_SYS_ERROR\", line 95\r\nORA-06512: at \"SYS.KUPW$WORKER\", line 6345,\n\r\nI searched and found similar cases with identical errors caused during data pump import:\n\r\n180217-686755\r\n150706-538174\r\n140915-493577\n\r\nHere is the recovery action that I performed;\n\n\r\n1. Login Oracle 10g Database as sysdba user \r\n[ntdbadm@vcarsSMLt0 ntdbadm]$ sqlplus \r\nSQL*Plus: Release 10.2.0.4.0 - Production on Wed Aug 18 13:40:30 2010 \r\nCopyright (c) 1982, 2007, Oracle. All Rights Reserved. \r\nEnter user-name: sys as sysdba \r\nEnter password: \r\nConnected to: \r\nOracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production \r\nWith the Partitioning, OLAP, Data Mining and Real Application Testing options \r\nSQL> \r\n2. SET ESCAPE OFF on SQL*Plus \r\nSQL> set escape off \r\n3. Catdph.sql will Re-Install DataPump types and views \r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdph.sql \r\n4. prvtdtde.plb will Re-Install tde_library packages \r\nSQL >@ $ORACLE_HOME/rdbms/admin/prvtdtde.plb \r\n5. Catdpb.sql will Re-Install DataPump packages \r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdpb.sql \r\n6.Dbmspump.sql will Re-Install DBMS DataPump objects \r\nSQL >@ $ORACLE_HOME/rdbms/admin/dbmspump.sql \r\n7. To recompile invalid objects, if any \r\nSQL >@ $ORACLE_HOME/rdbms/admin/utlrp.sql\n\r\nThe first four scripts are re-creation and re-grant for the objects such as package, library, tables, and procedures for the data pump.  The last one stands for schema re-compilation. \n\r\nAfter performing the above procedure, we click the retry button on the wizard and the screen has passed successfully.','null'),(16,'Bill Picardi','AS-OAM','2019-02-27','190227-175111','VIDEOTRON LTD.','ER called for assistance with Upgrade Wizard failure at step 19, prepareDB for a MCP 18.0.xx load.\r\n  The Databases monitor are in a \"STOPPED\" state and we don\'t know how to get them running to allow the wizard to continue.  \r\n  The monitors could not be started because the database was quiesced, due to steps within the wizard upgrade procedure.  We activated the database, started the monitors and retried, and the wizard failed again.  We restarted services (SM, ned, ORACLE, SMMP, CRON) activated the database and restarted the monitors again, then retried the wizard, and it failed again.  We ran a database backup and tried to perform a resync, and the resync failed.  The cause of the failures is seen during the cleanup replication procedure within the wizard and within the resync, returning the error \"ORA-00060\" such as:\n\r\nDROP REPSITE \r\nTRUNCATE REP QUEUE \r\nDROP REPGROUP MCSDBSCHEMAREPGROUP \r\n   DECLARE \r\n* \r\nERROR at line 1: \r\nORA-00060: deadlock detected while waiting for resource \r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6470 \r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6016 \r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 7027 \r\nORA-06512: at \"SYS.DBMS_REPCAT_MAS\", line 2695 \r\nORA-06512: at \"SYS.DBMS_REPCAT\", line 635 \r\nORA-06512: at line 70 \n\r\n  The GPS team collaborated, and attempted to try to resolve by recompiling the sys tables in oracle with a script as sysdba, from AAK-53419.  A follow-up attempt to cleanup replication failed again, and the Wizard would also fail.  We rebooted each DB servers, then ran the cleanup replication script, and it finally was successful.  Following this, the customer retried the wizard and step 19 completed, and they could continue the upgrade.  After this we dropped the call.','null'),(17,'Mustafa YUKSEK (NETAS External)','AS-OAM','2019-02-27','190227-175116','SINGTEL OPTUS PTY LTD','Customer Load:19.0.1.0\r\n======================\n\r\nProblem Description;\r\n=======================\r\nMark Zattiero who is from ER paged to us and reported that they were having a problem while trying to start the  FPM1_0 and FPM1_1 instances, they were /offline/down/unavailable/ and they didn\'t know after which action they faced the problem. Also, he said for solving the problem they applied a reboot action for the FPM servers however the problem continued.\n\n\r\nActions Taken;\r\n==============\r\nIn this respect, we connected the site and checked all mentioned points and everything like as reported. Also, FPM1 was managing 30 ME and FPM2 was managing 10 ME in the system. When we checked up the work logs of the instances, we realized below explained error;\n\r\n         java.lang.OutOfMemoryError: Java heap space \r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.MemFile.grow(MemFile.java:80) \r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.MemFile.(MemFile.java:65) \r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.loadMemFiles(OMServer.java:82) \r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.(OMServer.java:38) \r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.getInstance(OMServer.java:45) \r\n        at com.nortelnetworks.mcp.ne.share.omserver.runtime.OMFileTask.handleStartEvent(OMFileTask.java:71) \r\n        at com.nortelnetworks.mcp.ne.share.omserver.runtime.StartEvent.handle(StartEvent.java:24) \r\n        at com.nortelnetworks.mcp.base.task.SimpleTask.handle(SimpleTask.java:26) \r\n        at com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:709) \r\n        at com.nortelnetworks.mcp.base.task.Task.run(Task.java:543) \r\n        at com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:152) \r\n        at java.lang.Thread.run(Unknown Source) \n\n\r\nand when we controlled the output of \"top\" script for the servers and we saw CPU usage of the instances were reaching %140-%160 ranges while trying to start the instances;\n\r\n  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                           \r\n32681 ntappsw   20   0  437m 127m  14m S 167.8  3.3   0:07.03 java                                                             \r\n 1726 root      20   0  4060  552  468 S  0.3  0.0   0:03.73 sftpPullrd                                                        \r\n32412 root      20   0 15024 1296  980 R  0.3  0.0   0:00.03 top                                                               \r\n    1 root      20   0 19360 1528 1220 S  0.0  0.0   0:00.88 init                                                              \r\n    2 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kthreadd                                                          \r\n    3 root      RT   0     0    0    0 S  0.0  0.0   0:00.18 migration/0\n\r\nSo, we suspected the memory capacity of the instances, then we explored memory of the instances were set 256m and engineering parameters of the servers were like below explained ;\n\r\nEmservers are A2_Virtualized_16GBCore_DB_Large,\r\nFPM servers are A2_Virtualized_4GB2Core,\n\r\nFinally, for the EMservers of the customer was large then we\'vecided to increase the memory of the FPM instances as 512m.So,we reconfigured the jvm.cfg file of them and changed the memory value.Then we started the instances successfully.\n\r\nAll of the above actions the problem resolved then we dropped from the call.\n\r\nAny Follow-up case\r\n190227-175132','null'),(18,'Kaan BEREKETLI (NETAS External)','AS-OAM','2019-02-26','190226-174910','VIDEOTRON LTD.','Rodney from ER paged us about an issue on Videotron, they were having troubles with starting their primary SM instance. It was on \"initializing\" state when I connected to the site. Rodney told me that it has been like this for  a while, so I checked the work logs of the particular SM, noticed that the logs also had gotten stuck. I suspected that the services on the server might be corrupted and checked the up-time. Turned out that primary EM had been up for 3 years. This isn\'t recommended.\n\r\nWe asked the customer if it was possible to reboot the server. After the approval, the server was rebooted and primary SM was able to initialize into Hot-Standby state.\n\r\nThe customer was also worried about the threshold alarm on their secondary DB server.  I checked the partitions on it and saw that /opt was almost full. I cleaned some temporary files one by one and got rid of that alarm.\n\r\nAfter Rodney had approved that the result was fine, I dropped from the GTS VM.','null'),(19,'Mustafa YUKSEK (NETAS External)','AS-OAM','2019-02-25','190225-174633','LIBERTY GLOBAL SERVICES BV','Vernon Sauls who is from ER paged me and reported that the customer which on is 17.0.22.20 release has applied a reboot action for SESMServer15 which is SESM8_0 instance was deployed on it. After the reboot action, the SESM8_0 instance passed configured/down/unavailable state.\n\r\nIn this respect, I connected the site and when I checked the instance, it seemed like he mentioned and I tried to establish a connection to SESMServer15 but I wasn\'t able to it and I experienced  \"Connection timed out\" error then I tried to ping IP of the server however it wasn\'t pingable. \n\r\nSo, I recommended to the customer, please apply a power cycle action to the server and  I waited for the response of the customer over an hour but they didn\'t respond and It was unclear when this would happen then I dropped from the call.','null'),(20,'Emre OVA (NETAS External)','AS-OAM','2019-02-22','190221-174198','SINGTEL OPTUS PTY LTD','Current MCP Load: 19.0.1.1\n\r\nSabri Karakas from AS CallP informed me that PROV1_0 instance was stucked in Activating state during approximately 20 minutes and there were lots of Data Out of Sync alarm on active SM unit.\n\r\nAs a first action, I have killed the PROV instance which is hunged in Activating stuck via CLI as follows:\n\r\nneinit -autorestart =off\r\nkill -9 \r\nneinit restart\r\nneinit -autorestart=on\n\r\nThen, I performed a SM swact and all alarms cleared on SM units.\n\r\nThen I have started the PROV1_0 unit on MCP GUI and it\'s activating duration takes 5 mins. After the initialization process is completed, both SM units and PROV units were able to get into UP and running status as stable.\n\r\nCurrently system is working and all alarms on SM and PROVs were cleared via taken actions.','null'),(21,'Mustafa YUKSEK (NETAS External)','AS-OAM','2019-02-19','180830-148677','LIBERTY GLOBAL SERVICES BV','Rodney Neese from ER called me for planned MW action of the Liberty Global\'s system. The customer is on 17.0.22.20 release and has some BCP problems which are all BCP instances weren\'t able to send the Spool files to the OSS directory, as a result of that capacity of spool directory was filled and their monitorings were seeming Grey out.\r\nNevertheless, when we run \"./getInstancestate.pl\" script for these instances, I saw they were active and were able to handle the calls.\n\r\nFor this problem, we\'ve set another MWs before and in this time the design team prepared a DebugJAR for tonight and we tried to apply it to BCP10_0 instance and collected the related logs. We believed thanks to these logs we will see all details about the problem and detecting to root cause.\r\nWhile applying it I followed the below-explained steps;\n\r\n  -We\'ve connected the BCP10 server and copied the prepared jar which is 17.0.22.20_AAK-58767_190208.jar to under \r\n   /var/mcp/run/MCP_17.0/BCP-10_0/jars directory.\n\r\n  -We went to /var/mcp/run/MCP_17.0/BCP-10_0/bin directory and added the DebugJar in \"builJVMCfg.sh\" file with \r\n   below explianed lines;\n\r\n   echo \"OPTION_T=-classpath ../jars/17.0.22.20_AAK- \r\n   58767_190208.jar:../jars/delta_mcp.jar:../jars/mcp.jar:../jars/jhall.jar:../jars/3rdParty.jar:../jars/bcprov- \r\n   jdk16-140.jar\" >> $JVMCFG\n\r\n - Restarted BCP-10_0 instance,\n\r\n - Applied SM double swact \n\r\n - As a result of these actions I collected; *BCP10_0 work log,\r\n                                             *BCP10_0 Spool files,\r\n                                             *SM_0/1 spool files and OSS logs,\n\r\nI provided all of the logs to the design team and they will investigate them deeply. Also, we recommended to the customer for performing a reboot action to the SESM servers because they were up nearly 950 days. During this duration, the customer completed this.\n\r\nSo, the DebugJAR was applied successfully to the BCP10_0 and were collected the related logs which will forward us to the root cause of the problem and solution of that. Finally, we checked the state of all instances and wanted to the customer applying power cycle for down servers but he wasn\'t on the site then we dropped from the call.','null'),(22,'Mustafa YUKSEK (NETAS External)','AS-OAM','2019-02-18','190215-173426','LIBERTY GLOBAL SERVICES BV','Tom Draper paged me out from ER and stated that ER called to GTS but she told she will leave in 15 minutes so did not help. Then ER paged GPS to go on with procedure even there is no error case.\n\r\nIn this respect,ER reported that the customer which is on MCP_17.0.22 MR release, restored the SESM6_0 server with 17.0.7 and now wanted to restore as 17.0.10 but although I provided to related release note which is explained all prerequisite and although  I\'ve mentioned that the customer should install the server with 17.0.10 iso file, they installed it with 17.0.7.\n\r\nThen I connected the site and tried to connect the server but there wasn\'t anybody know passwords of the server then I changed the password of users as they wanted. Also, when I checked the state of the server, the customer  completed until the restore operation steps and didn\'t deploy SESM6_0 so I said first of all the customer should complete the remaining steps by following the System Recovery MOP and then they should have 17.0.10 patch iso file and move it to the under /var/mcp/media/patch directory then they should run \"patchPlatform.pl\" script. However, the customer didn\'t want to continue without any reason then Tom discussed with them and didn\'t take any response and they didn\'t want to go any further. So, we stayed there and I provided above explained action plans then dropped the call.','null'),(23,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-02-15','190215-173426','Liberty Global','Customer Load:17.0.22.x\n\r\nProblem Description\r\n====================\r\nCustomer requested documentations and assistance for fresh installation of SESM6_0.\n\n\r\nActions taken\r\n=================\r\nToday Brent from ER called me about having E2 issue which is about SESM 6 - 0 is showing Down Unavailable that Liberty Global faced.\r\nI asked ER when this happened. Then, Brent told me that \"They were making rollback and SESM was unavailable\" \r\nI asked again to Brent about rollback and I also said that customer makes Rollback only if some problem occurs during upgrade, what was the issue. \r\nBrent told me that they dont know but they have site access.\r\nHe gave me the previous case which has the ID  181016-156216, customer confirmed that they are making \"FRESH INSTALL\"\r\nI have connected to site and check the cases. \r\nThe case was on GTS queue and Brent told me \"Nur requested that I engage GPS... case is 190215-173426 which we opened as an E2 ...which is referenced previous ticket... 181016-156216\"\r\nThen Gary came to take this issue (he is the day shift). Brent lastly told to Gary and me that there is not a customer facing document for the install.  I inform ER that we provided hardware replacement procedure with all documents and informed customer about which OS load should they use on 181016-156216. I also offer to supply them the MOP as well (I attached it to ER case 190215-173426)\r\nI also state that If there is any problem customer faced currently during the installation, I can join to help them. \r\nSo that was an ongoing case for 3 months waiting for customer to replace their EOL HW HP 3310. And as GPS we had provide them all the documentation and steps to follow. Also, the platform installation can be done by customer since it is HW level connection. At the end we joined to the customer bridge and customer told they do not have any on site person to perform the steps.\r\nWe again highlighted the documentation since there is a preparation before doing the install on SESM server such as making all loads prepared.\r\nSince customer does not have any of them, dropped the bridge. And ER told customer local RTS team can help them for assistance.\n\n\n\r\nAny Follow-up Case\r\n===================\r\nNone.','null'),(24,'Emre OVA (NETAS External)','AS-OAM','2019-01-22','190122-169800','VTR.COM SPA','Customers MCP release: AS 8.0 BRC (MCP 14.0.10.x)\n\r\nTom Draper from ER paged me out for an E1 case. VTR SPAs SIP calls were failing and MCP GUI connection were intermittently loosing. \n\r\nTom provided site access via GTS VM servers but MCP GUI was not launching on that desktop. Tom and Jose (GTS) told me that MCP GUI is only accessible on customers desktop. For this reason, we had arranged a Bomgar session with customers PC through connected VM and started to work on issue.\n\r\nIn order to check the system status, I launched the MCP GUI and observed that there was only one working SM instance. SM 0 was Active and SM 1 was down. All other instances were down as well. In other saying, system was completely down. Active SM instance was bouncing between Active  Unavailable  Active. It was not working as stable. There was a heap exhaustion memory alarm on Active SM unit. Additionally, a critical alarm was observed on primary EM server. (/var/mcp partition usage reached to %95 level.)\n\r\nFirstly, I started to work by cleaning up the primary EM server. .hprof and .pid files were generated under /var/mcp/run/MCP_14.0/SM_0/work/. Those files were occupying approximately 20GB disk space. These files are producing when a SM memory issue occurs/ total oldGen usage reaches to 100% level. I removed those files manually and total disk usage decreased from 95% to 60% levels. SM units were not still working as stable and keep bouncing. After resolving disk usage and memory issues on System Manager, I undeployed/deployed both SM units then both units started to work as Active/Standby. Those instances were running as stable. \n\r\nAfter SM stability is provided, I focused on SIP call failure and observed that SESM units were down as I mentioned above. There was totally one SESM pair in the system. I killed and restarted SESM1_0 and SESM1_1. Those SESM units started to work as Active/Standby. I requested customer to make a call test but calls were still failing in spite of restarting the SESM units. Then, I monitored the status of SESM units and seen that there are critical alarms on Active and Standby SESM instances. (Above threshold for spool files).\n\r\nI needed to connect SESM servers via SSH as root for cleaning up the spool files. In this regard, I requested Tom and Jose for connecting to SESM servers as root, but root passwords was unknown by customer. (ntsysadm permission was not sufficient). Customer was not able to find the correct root password approximately during 20-30 mins, then I asked their approval to reset root password. They approved that I can reset the password and I changed the root password through NED. \n\r\nAfter accessing to SESM servers as root, I counted total spool file number on SESMServer1 and SESMServer2. There were 500K spools on SESMServer1 and 200K spools on SESMServer2. I started to execute the following query for cleaning up the spools on SESMServers but completing this script took approximately 30-40 minutes due to the total spool file number. (thousands of spools were exist on SESM servers)\n\r\nfind /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f -exec rm -f {} \\; ; find /var/mcp/spool/tmom -type f -exec rm -f {} \\; ; echo DONE\n\r\nAfter clean up operation completed, I restarted both SESM units and requested customer to make call tests again. This time both incoming and outgoing calls were successfull. Customer performed provisioning, registration and call tests during 30 mins and finally they confirmed that all issues are resolved and system is working as stable.\n\r\nSince the customers current MCP release is end-of-life and not supported for RCA (8.0 BRC), there is no any follow-up case of this outage.','null'),(25,'Emre OVA (NETAS External)','AS-OAM','2019-01-21','190121-169536','LIBERTY GLOBAL SERVICES BV','Customer Load: 17.0.22.20\n\r\nThomas Godwin from ER paged out GPS and reported that MCP GUI could not be launched properly. \n\r\nThomas provided site access via GTS VM servers and we could connected to customer site with his assistance.\n\r\nFirst, we have tried to launch MCP GUI but an Oracle exception was returned and blocking to launch MCP GUI. (ORA-0600)\n\r\nSince we observed the Oracle and DB-related exceptions, we have checked the status of DB and observed that /opt partition was near to full capacity. (%99)\n\r\nDue to the fact that it\'s a known issue and fixed in latest MRs and patches, we have manually deleted old alert,trace and listener logs and /opt directory usage has decreased from %99 to %74.\n\r\nThen, we retried to launch GUI but the result was same. In order to see the impacts of taken actions, we recommended to restart database by running the following commands on primary DB.\n\r\n/etc/init.d/dbora stop\r\n/etc/init.d/dbora start\n\r\nWhen we recommended these actions, customer was not in maintenance window and request to schedule a MW. We agreed with customer and dropped from bridge.\n\r\nAfter 1 hour passed, customer informed us and stated that recommended actions taken on site and it resolved the issue.\n\r\nAfter the all issues resolved, I informed customer with MR/patch versions which include the permanent fix in 10.4, 11.2, and 12.0.\n\r\n10.4 ==> 17.0.32.0 MR\r\n11.2 ==> 18.0.30 MR\r\n12.0 ==> 19.0.4 MR\n\r\nThen, customer confirmed that everything is working stable and we dropped from the customer site.','null'),(26,'Mustafa YUKSEK (NETAS External)','AS-OAM','2019-01-17','180830-148677','LIBERTY GLOBAL SERVICES BV','Rodney Neese from ER Called me for planned MW action of the Liberty Global\'s system. The customer is on 17.0.22.20 release and has some BCP problems which are all BCP instances weren\'t able to send the Spool files to the OSS directory, as a result of that capacity of the spool directory was filled and their monitorings were seeming Grey Out. Nevertheless, when we run \"./getInstancestate.pl\" script for these instances, I saw they were active and were able to handle the calls.\n\n\r\n In this respect, the customer has been experiencing these problems for a long time, so the design team prepared a below explained action plans for tonight and we applied them to the system.\n\r\n1- First of all, we applied the prepared debug JAR by AS design. It will enable us to work on more comprehensive BCP logs for identifying the root cause. It applied on just one BCP instance on the purpose of log collection and I undeployed the applied JAR after log collection is completed.\n\r\n2- Since SM has capacity issues and currently manage more than 40 MEs (there are totally 50 MEs managed by SM in customer system), I temporarily stopped the monitoring of all BCP and SESM Servers, as a result of that number of monitoring MEs decreased under the 40, then  double swatted SMs and proceed with the next actions (Due to the fact that Managed Element number of SM will be decreased from 50 to 35 levels, we expect that BCP instances will be manageable this time): \n\r\na- Stoped one cluster of the BCP. Cleaned up the spool files by running the following commands:\n\r\nfind /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f -exec rm -f {} \\; ; find /var/mcp/spool/tmom -type f -exec rm -f {} \\; ; echo DONE\n\r\nb - Undeployed the BCP instances which work on one cluster via MCP GUI. (For example, firstly BCP01, 02 and 03 will be undeployed for HELMCluster-0)\r\nc - Connected the BCP server via SSH as root, changed directory to /var/mcp/run/ and delete MCP_17.0 folder by running rm -rf MCP_17.0\r\nd - Rebooted the BCP servers located on just one cluster.\r\ne - Deployed BCP instances via MCP GUI.\r\nf - Started  BCP instances running on that cluster. (All BCP instances in a cluster should be started at the same time.)\r\ng - Checked for spool logs of these BCPs if they are being transferred to SM. On the other hand, to be sure that restarted BCPs are working fine and their status are ACTIVE. (Even if their status seem Offline  Down  Unavailable on MCP GUI, you can check the real status of instances by running ./getInstanceState.pl (under /var/mcp/run/MCP_17.0/BCP-0X_0/bin/), neinit -p and mptool -l commands on BCP servers as root.\n\r\nAfter performed the recommended actions above, we applied the same actions for BCP instances located on other clusters such as Cluster-2, 3 and 4. \r\nThen we started the monitoring of SESM and BCP servers again. \n\r\nAfter completed the explained procedure, we faced the same problem unfortunately and we needed design input for further actions. Then Ferit from AS design joined us. As a recommendation of Ferit, we restarted NED connection for all BCP instances and we\'ve set all family logs of BCP0 as verbose then applied Undeployed/deployed/started to the BCP0 and collected work and oss logs. In this respect, Ferit realized some exceptions and wanted to investigate them in Source Code. Then we explained that to the customer and we said: \"as a result of an investigation of the design team, I will update the case with our last findings\".Finally, we checked the state of all instances and the system was delivered to the customer as stable like as at the start point of the MW  and we dropped from the Pager.','null'),(27,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-01-13','190113-168359','SINGTEL OPTUS PTY LTD','Customer Load:18.0.0.x\n\r\nProblem Description\r\n====================\r\nUnable to launch MCP GUI \r\nSM_0(HotStanby SM)  was not sshable just pingable.\n\r\nActions taken\r\n=================\r\nToday Brent Combs from ER paged me about that SM_0 server was down(not sshable ) but it is pingable and the customer was not able to open GUI. \r\nI have connected to site and seen that the SM_1 was up for 535 days and even SM_1 is active the MCP GUI connection is not opening due to time-out.\r\n I recommended to Brent to make Power Cycling to Sm_0. Acoording to this, they said to customer that they need to send tech to site to perform hard reboot. \r\n The customer tech was not familar with site or the SSL equipment and Looked at node names and spaec book and \"believe\" that the Tech has found the SSLL rack and identified SM 0. \r\n We waited till they finished to power cycling. After 4-5 hours ER paged me again that SM_0 has been power cycled. The ER reported me that it seems like database was down after reboot. \r\n I have connected the site again and perform database restart(/etc/init.d/dbora stop & /etc/init.d/dbora start ) and opened monitoring the mcpdb_0. SM\'s turned stable and they were green on MCP GUI.\r\n ER told me that the customer wants to turn SM_0 to active. I suggested to reboot to SM_1 due to it was up over than 500 days and not answering quickly while we were trying to open MCP GUI. \r\n After i received approval, i rebooted the SM_1 but it didnt come back for half an hour. We wanted to customer to power cycling also to SM_1. Since SM_1 turned to active it was Offine/up/hotstanby. \r\n I performed kill/start to SM_1. The System turned stable. Now SM\'s are working Up/Hotstanby.The all alarms were gone. \n\n\r\nAny Follow-up Case\r\n===================\r\nNone. The ER didnt open a follow-up case.','null'),(28,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-01-11','190109-167884','SKY UK','Customer LOAD:19.0.4.0\n\r\nProblem Description:\r\n==========================\r\nQuiesced Database\r\nSKY UK made ./cleanupDBReplication.pl and ./setupDBReplication.pl for the case 190108-167663 & 190109-167884 to get rid of Oracle Link Replication alarm. While they are proceeding the steps, they received error and fails and they paged us. \r\nWhen i have connected to site, i have seen that the database is not writable.\n\r\nActions taken:\r\n=================\r\nSince they paged us that their alarm is still on the site, i did some tests and realise that the database stucked in quiesced mode. \r\n-I run ./activateRepDB.pl on primary database. It failed due to some Oracle errors. \r\n-After this, i performed database restart action (/etc/init.d/dbora stop & /etc/init.d/dbora start). \r\n i run ./activateRepDB.pl again and it failed with same error. \r\nWe checked the customer backups and saw that they are taking Daily backup. We suggest to customer to make database restore with their daily backups. \r\nIt was the only way to save the database from read-only mode. \r\nAfter Tony gave the approval (due to the database stucked and read-only mode, we needed to perform it quickly ) ,\r\n I performed ./dbRestore.pl . \n\r\nAfter database turned normal with previous day backup(performed check on MCP GUI) ,\r\n I performed ./resync.pl from primary database to secondary database. \r\n According to all this actions, resync completed successfully and the system turned stable.\n\n\r\nAny followup case:\r\n=====================\r\nNone','null'),(29,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-01-08','190108-167658','SKY UK','Customer Load: 19.0.4.0 \n\r\n Problem Description \r\n ================================== \r\n PROV2_0 and AccountManager_1 were down.  \n\n\r\n Actions taken \r\n =========================== \r\nTom Draper from Ribbon ER reported that PROV_2 and AccountManager_1 stayed in Down after taken actions for the E2 case 190108-167613. He reported also they receive some Oracle Link Replication Error alarms.  \r\nI wanted him to open Major cases to investigate further.  \n\r\nI have connected the site again and performed kill/start on MCP GUI for both instances and they turned to Active/Online/Up. System turned to stable.  \r\nAfter this, i have connected to EM Server2(Which was deployed AM and PROV) and run \"neinit autorestart \"  \r\nAs per as i see from there, it was stayed in OFF. It means that after rebooting the server, the intances couldnt come UP automatically.  \r\nSince i have seen what caused this problem, i run\"neinit autorestart=on\" and i fixed the problem. Customer will not see the same problem till the autorestart stays on.  \n\r\nFor the Oracle Link Replication alarms , i worked on the case and solved the alarm. You can reach the  case for updates by following ID (190108-167663)\n\n\n\n\r\n Any followup case \r\n ========================= \r\n None','null'),(30,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2019-01-08','190108-167613','SKY UK','Customer Load: 19.0.4.0\n\r\n Problem Description\r\n ==================================\r\n SM_1 is bouncing from Warm Standby to Synchronizing\r\n MCP GUI is not opening\n\n\r\n Actions taken\r\n ===========================\r\n Tom Draper from ER paged me out and reported that hotstandby SM was bouncing Warm Standby, Synchronizing and Down. They were E2 situation and had a orginal case 190107-167446. But after this problem, they received another problem by opening MCP GUI.\n\r\nIn order to identify the root cause, I have connected to customer site via GTS VMs and reviewed the case notes of created BC case (190103-167116).\n\r\nFirst of all, I have connected to EM servers via cli and check the status of SM\'s by using the command  ./getInstanceState.pl on both EM Server. SM_0 was Active and SM_1 was Synchronizing. It was shown me that even SM\'s are up the MCP GUI was not responsing. \n\r\nWhile I was looking for the reasons for why the problem occurs when both SM\'s up, i have performed kill/undeploy/deploy/start via ssh but SM was not able to start and logs were not written. I \'ve runned \"uptime\" on EM Server2 and  seen that the server was up for 350-400 days. I recommended the customer to reboot the Host2. While we were working on this issue, it was not MW so we performed all steps without causing any provisioning or call process failure. The customer approve reboot to Host2 and after reboot , we have seen that ./smStart.pl was not working due to time out errors. \n\r\nAccording to time-out errors, we have connected to primary database to check the DB connection. We have run some queries on database and seen that the database was reachable but responsed back very late. We have checked to ssh to primary database server which is also EM Server( to jump from other servers) and it was slow than usual. \n\r\nConsidering this slowless, we have thought that the problem is Primary database and wanted from customer to check provisioning by making some tests. But PROV GUI was not able to open (couldnt be loginned)because of time-out. \n\r\nSince we cant connect to MCP GUI/PROV GUI, we have verified that primary database was up but not quickly response. We recommended to customer to restart the database (etc/init.d/dbora stop & etc/init.d/dbora start).\n\n\r\nIn accordance with database restart, we suggested to perform double swact to make both SM units to be able to connect fresh restarted database regulary and turn stable. \n\n\r\nAs a result, the system turned stable, GUI\'s are reachable and both SM\'s are working Active_Hotstandby recently.\n\n\n\r\n Any followup case\r\n =========================\r\n 190108-167632 Followup:E2:A2: SM.1: Operational State: Warm Standby -> Synchronizing (Down)','null'),(31,'Ken Johnson','AS-OAM','2018-12-20','181221-166172','Kandy EU  (Internal)','Following a configuration change to the AS\'s Journal engineering parameter the SESM failed to initialize. This prevents subscribers from registering via the Kandy SPiDR, resulting in a voice and video call outage for Kandy KCS users. \n\r\nAfter entering the initializing state, the SESM became stuck and was unable to complete/fail. This continued after reverting the configuration change.  At this point both SESMs were down and a full sync from the Oracle DB was needed to initialize them.  Suspected that the 5700mb Java virtual machine memory limit was insufficient and it was almost at this limit before the config change.  After changing the jvm.cfg file to allow for more RAM, the SESM was able to startup and come in service.  The mate unit continued to fail (stuck in init) till its memory was also increased to 6400mb.\n\r\nFollow up case to track the reEngineering of these SESM from Small to Large, and cleanup of any test users/domains.\n\r\nIMPACT: This issue was reported by internal Kandy monitoring tools and occurred during the site\'s maintenance window.  No external customers have reported a problem.\n\r\nJVM Heap change:\r\n================\r\n[root@kaganvm51 data]# pwd\r\n/var/mcp/run/MCP_17.0/SESM1_1/data\r\n[root@kaganvm51 data]# diff jvm.cfg_20181221_old jvm.cfg\r\n38,39c38,39\r\n> OPTION=-Xms6400m\r\n> OPTION=-Xmx6400m\r\n---\r\n< OPTION=-Xms5700m\r\n< OPTION=-Xmx5700m','null'),(32,'Emre OVA (NETAS External)','AS-OAM','2018-12-19','181219-165781','HONG KONG BROADBAND NETWORK LTD.','Tianjun Guo from Ribbon GTS reported that registration problems occured after the 19.0.21 MR upgrade of HKBN. After the half upgrade of AS upgrade completed, while customer make a registration test, its seen that users can not register when SM 0 is active with new load. When we switched SM units back to SM 1 (19.0.20.8), registrations was working successfully. In order to identify the reason for why the problem occurs only when SM 0 is active, we had collected SM, PROV/PA and SESM debug logs and found the root cause.\n\r\nOn the basis of taken logs, I observed that PA login was failing when a user who located in sub-domain tries to log in Personal Agent. \n\r\nWhen we take a look at the PROV OPI debug logs, its seen that system only accepts login attempts in username@rootdomain format.\n\r\nPROV1_0 DALOG 2 INFO DEC19 13:41:34:392 MCP_19.0.21.0\r\n  [** DALOG **]=> Default string = An error occured. Please contact your next level of support.\n\r\nPROV1_0 DALOG 2 INFO DEC19 13:41:34:392 MCP_19.0.21.0\r\n  [** DALOG **]=> Translated string = Login failed. Please try to login with \'username@rootdomain\' format.\n\r\nDue to the fact that all users of HKBN located under sub-domains instead of root domains, all registration attempts were failing. In oder to detect the root cause, I investigated the returned exceptions and found that this behavior changed in 19.0.20.9 and upper releases. In other saying, users were able to register in both root and sub-domain formats before 19.0.20.9 but a design change were blocking it on new load. That was the reason for why the registration attemps fail on 19.0.21.0 whereas the same tests pass on old load (19.0.20.8). At this point, we contacted to A2 design support team and identified that where it fails on source code side. In this regard, DS team developed a new corrective fix JAR and it had been applied on SM and PROV units of customer in 19.0.21. MR load. After the prepared JAR applied on customer system, we completed our tests and verified that fix is working as expected. In this way, HKBN could complete full AS upgrade without needing to any roll back action. Now, HKBN system is working on 19.0.21 MR load without any issue.','null'),(33,'Joyce Lyon','AS-OAM','2018-12-18','TBD','GPS UC','Testing the Pager Report System','null'),(34,'Mustafa YUKSEK (NETAS External)','AS-OAM','2018-12-15','181212-164967','BAHAMAS TELECOMMUNICATIONS COMPANY','Kevin Gaines paged me out and mentioned that \"Kevin Kaczmarski who is from A2 CALLP GPS called me and said this cases issue should be with A2 OAM GPS, this is why I called you.\"In this respect, they described the problem like that After reviewing the site state, there is a SESM service unit down making it an E2 (outage), AS OAM GPS will be needed for the recovery.\r\nFor this reason, I\'ve connected the Bahama Communıcatıons Company\'s site and I saw the customer is on 17.0.4.3 release, SESM1_1 confıgured/up/active and SESM1_0 was confıgured/down/unavailable and wasn\'t able to\r\nreach ssh connection. Also, all running instances were working as Confıgured/up/active. In order to make some operation I waited for approval from the customer a long time then I tried to kill/deploy/start operatıons for SESM1_0 from MCP GUI but it wasn\'t allowable and faced maintenance operation in progress error. In the meantime, we recovered the PROV instances from \"confıgured\" to \"online\" state with kill/deploy/start operation.\r\nand I tried to connect the PA servers and waited for the right password but the customer wasn\'t able to find it so we couldn\'t connect them.\r\nAlso, I tried to establish an ssh connection to EMservers however the provided password was wrong I\'ve waited for the right password. Then connected the EMservers successful with right password and checked oss and work logs and everything seems good. When I connected the SESM1_1, I saw the server was up nearly 470 days and I confirmed that the customer never applied any reboot action for SESM1_1 and SESM1_0 until now. So the SESM1_0 was up with the same days. For his reason, they didn\'t work stable we need to apply reboot action for SESM1_0.In this time, Kevin Gaines changed the shift with Brent Combs and we continued with him. So, we told our action plan to him as you can see below;\n\r\nha app-blade deactivate  \r\nha app-blade offline  \r\nha app-blade online \r\nha app-blade activate \n\r\n He tried to find blade number which belongs the SESM1_0 and waited for customer approval long time. Then the customer mentioned \r\nthat the SESM1_0 situation is known hardware issue and they were planning to make hardware replacement.\n\r\nThen we attend the Bridge at the request of the customer they asked that why we would not be able to register new lines on this SESM is this related the Raid nosync issue or what?\n\r\nThen we investigated SESM1_1 callp trace and checked states of SM_0 and SM_1. The SM_1 was configured/up/active and SM_0 was online/up/hot standby. In this respect, we\'ve killed/started SM_1 and SM_0 then the system worked stable and\r\nnew line was able to register. As a result of this action, SM_0 passed active mode instead of SM_1.\r\nFinally, the problems solved with all of these actions, then we dropped from the bridge and pager.','null'),(35,'Emre OVA (NETAS External)','AS-OAM','2018-12-07','181206-164184','BT COMMUNICATIONS IRELAND LIMITED','Upgrade Path (FROM/TO LOAD): 19.0.3.0 - 19.0.20.8\n\r\nProblem Description:\n\r\nMeraz Aziz (SWD) paged me out to report that DB MOCK step failed. The following error was returned on Upgrade Wizard:\n\r\nThe upgrade scripts in load MCP_19.0.20.8_2018-06-06-1655 \r\nwas tested against the current database, the test result is \r\nFAILED!!!! \r\nthe reason for the failure is: \r\nError when run the upgrade scripts against the current database. \r\nPlease review the result file: \r\n/var/mcp/upgrade_tools/work/dryRunDBUpgrade/DBUpgradeDryRunResult_MCP_19.0.20.8_2018-06-06-1655_20181206_222224.zip \r\nat EM/SM server(10.144.98.27). \n\r\nSince the DB MOCK is one of the Pre-Upgrade steps, I recommended to keep working on the issue during our business hours and agreed with SWD. As soon as arriving to office this morning, I prioritized and identified the issue. You can find how we resolved the problem in solution details part.\n\r\nSolution Details:\n\r\n- Firstly, I checked the dryRunUpgrade logs and identified where the problem occurs. Accordingly, I have observed the following exceptions:\n\r\nUNNING SCRIPT 170321-623937_clicktocall_assign_sapbxuser.sql  \r\n****************************************************  \r\n        INSERT INTO SVC_USERTYPE (TYPE_ID, SERVICE_ID)  \r\n*  \r\nERROR at line 1:  \r\nORA-00001: unique constraint (DRYRUNUSER.PK_SVC_USERTYPE) violated  \n\n\r\n   exec MCSDB_UTL.SET_NOTIFY_FILE(\'OracleMethod_upgrade_dry_run_notice\');  \r\n   exec MCSDB_COMMON.DEBUG(TRUE,\'OracleMethod_upgrade_dry_run_debug.log\');  \r\n@/var/mcp/run/MCP_19.0/mcpdb_1/data/upgrade/rel19.0_prov_part_0.sql;  \r\n   exec MCSDB_COMMON.DEBUG(FALSE);  \n\r\n   exec MCSDB_UTL.CLOSE_NOTIFY_FILE;  \n\n\r\n************* -S fails at /usr/share/perl5/DB/OraSQLWrapper.pm line 673  \r\n        DB::OraSQLWrapper::sqlCmd(\'AppSqlWrapper=HASH(0x291e3b8)\', \'sys\', \'-S\', \'   exec MCSDB_UTL.SET_NOTIFY_FILE(\\\'OracleMethod_upgrade_dry_...\') called at /usr/share/perl5/DB/OraSQLWrapper.pm line 579  \r\n        DB::OraSQLWrapper::sqlSysCmd(\'AppSqlWrapper=HASH(0x291e3b8)\', \'-S\', \'   exec MCSDB_UTL.SET_NOTIFY_FILE(\\\'OracleMethod_upgrade_dry_...\') called at /var/mcp/run/MCP_19.0/mcpdb_1/bin/../data/upgrade/Upgrades.pm line 11  \n\r\n- Problem was happening while running clicktocall_assign_sapbxuser.sql due to a unique constraint failure.\n\r\n- Moreover, failure was occuring while trying to add an entry into SVC_USERTYPE table. Since BT has already done a manual change on their DB and entry which is trying to be added is already exist on the database, this SQL script was failing. \n\r\n- In order to enable this script to work properly, we have deleted the entries from Secondary DB manually and re-run the ./ut_dryRunDBUpgrade.pl script. This time the result was successfull and DryRunUpgrade on secondary DB had been completed successfully.\n\r\n- Since the same entries which cause the failure are exist on primary DB, we have to ask customer for approval to delete those entries from primary DB before running DB upgrade, otherwise we can encounter the same problem while upgrading the primary database. \n\r\n- If customer approve to take these actions on primary DB, we will delete those entries by running the following script on primary DB:\n\r\n* DELETE FROM SVC_USERTYPE WHERE TYPE_ID=(SELECT TYPE_ID FROM USER_TYPES WHERE NAME=\'sasippbxext\') and SERVICE_ID=(SELECT SERVICE_ID FROM SYSTEM_SERVICES WHERE SERVICE_NAME=\'clicktocall\' AND VENDOR=\'nortel\');\n\r\n* commit; \"\n\r\nAs I explained above, the issue that we faced in Dry Run upgrade resolved by taking these actions. So, customer approval is waiting for case closure.','null'),(36,'Mustafa YUKSEK (NETAS External)','AS-OAM','2018-11-25','181125-162302','SWISSCOM AG','Tom Draper paged me about the Prov exceptions noticed after some provisioning tests after Upgrade. According to Tom after Swisscom upgrade from 19.0.40.0 to 19.0.20.9,\r\nthey started PROV tests and while making add, delete and update testing they faced the SWERR 799 alarms and when I checked  the attached case logs I realized 4 exceptions ;\n\r\n1- 	PROV1_0 SWERR 799 ALERT NOV25 05:29:32:122 MCP_19.0.20.9\n\r\n  java.lang.NullPointerException\r\n              at com.nortelnetworks.ims.app.prov.ws.services.sippbx.server. SipPbxAdminValidatorUtil.validateTgrpAndSASipPbxSessionsCount(SipPbxAdminValidatorUtil.java:762)\r\n              at com.nortelnetworks.ims.app.prov.ws.services.sippbx.server.SASipPbxModifyValidator.validate(SASipPbxModifyValidator.java:25)\r\n              at com.nortelnetworks.ims.base.prov.ws.fw.common.service.ServiceRegistry.validate(ServiceRegistry.java:166)\r\n              ...\r\nfor this exception AAK-57910 JIRA opened and fixed on 13.0 release for this reason we need a new case and will past patch it to last patch of 12.1.\n\n\r\n2- 	PROV2_0 SWERR 799 ALERT NOV25 05:44:32:851 MCP_19.0.20.9\r\n  Exception caught in notifyEventListeners: \r\n  java.lang.NullPointerException\r\n              at com.nortelnetworks.ims.mw.imdb.tables.IMSipPbxExtensionLinkTable.handleCommonDatasync(IMSipPbxExtensionLinkTable.java:259)\r\n              at com.nortelnetworks.ims.mw.imdb.tables.IMSipPbxExtensionLinkTable.receiveDataSyncEvent(IMSipPbxExtensionLinkTable.java:301)\n\r\nFor this exception, AAK-57639  JIRA opened and fixed for 13.0 release so we need a new case and will past patch it to  last patch of 12.1.\n\r\n3-	PROV1_0 SWERR 799 ALERT NOV25 05:34:05:386 MCP_19.0.20.9\r\n  Observer exception.\r\n  [Observer: com.nortelnetworks.mcp.ne.share.security.DoSMgr@79753b2]\r\n  [Arg: EEChangeAction, deleteStandAloneSipPbx: af41f2, a_999922_gate_swisscom_ch, true, false, true, null, dIP:null, dPort:0, uuid:null, , true, 20, true, false]\r\n  com.nortelnetworks.mcp.base.collections.NotFoundException: key does not exist:\r\n  af41f2\r\n              at com.nortelnetworks.mcp.base.collections.HashMap.removeElement(HashMap.java:866)\r\n              at com.nortelnetworks.mcp.base.collections.NoSyncHashMap.removeElement(NoSyncHashMap.java:469)\n\r\n and for this exception, AAK-30707  JIRA opened for 10.2 release however it didn\'t fixed because they weren\'t able to reproduce this issue again.\r\n We will contınue with new case and new jıra for this issue.\n\r\n And last exception is ;\r\n 	PROV1_0 SWERR 799 ALERT NOV25 05:32:13:623 MCP_19.0.20.9\n\r\n  java.lang.ClassCastException: [Lcom.nortelnetworks.ims.app.prov.ws.services.sippbx.shared.TrunkGroupDO; cannot be cast to com.nortelnetworks.ims.app.prov.ws.services.sippbx.shared.TrunkGroupDO\r\n              at com.nortelnetworks.ims.app.prov.ws.services.sippbx.server.SippbxTgrpDeleteMethodValidator.validate(SippbxTgrpDeleteMethodValidator.java:33)\r\n              at com.nortelnetworks.ims.base.prov.ws.fw.common.service.ServiceRegistry.validate(ServiceRegistry.java:166)\r\n              at com.nortelnetworks.ims.base.prov.ws.fw.common.handler.InputValidationHandler.doMethodValidation(InputValidationHandler.java:94)\r\n and there isn2t any case or JIRA for this exceptioin and we will contınue with new jıra and new case on this issue.\n\r\nFinally, I was informed TOM about the exceptions are not service impacting and I need to investigate the exceptions with DS team. Then I dropped from the call.','null'),(37,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2018-11-16','181116-161206','Verizon Communications','John Kisner paged the AS OAM GPS pager. While Verizon Communications was making upgrade, they reported that SESM1_1 have stayed in Online/Down/Unavaiable in Step 33- Patching Secondary Instances.\r\nI have asked to him if SESM is reachable. John confirmed that SESM is not accessable (couldnt connect via ssh). Acorrding to this, i offered them to the power cycling . \r\nAfter this action, while we were waiting to reboot(NO ETA on site access for reboot), the customer said to ER that they have a problem in opening Wizard. When they tried to opened Upgrade Wzard in debug mode, they realised that it turned back to Welcome Screen. \r\nI looked at the screenshots that they attached and i saw that they Forced Out the user who makes upgrade while they were opening the Wizard in Debug Mode. \r\nER(Bob Johnson) has connected the SM server via ssh with GTS VM servers and i have connected the database to see the wizard state. As i see from the database, the Wizard state seems like in Pre-Upgrade Welcome Screen. \r\nCustomer said to ER that they want to continue to upgrade without SESM1_1 because of MW will end soon and after fix SESM problem. \r\nDue to this, i have connected the database again and change the Wizard_State as Step 33-Upgrade-Patching secondary instances screen.\n\n\r\nAfter all of this action, customer tried to connect the Wizard again but Wizard couldnt open due to some Java Cache alarms. Because of this, i performed the procedure below:\n\r\n1.Please check if SM_0 is active \r\n2.Clean the Java Cache in the local computer\r\n3.Run ./mcpInstallFirstLoad.pl\n\r\nThe other step was double swact to SM server but the customer didnt want to double swact because of the secondary instances were different loads.( Primay instances were patched but secondary was in previous (From) load.)\r\nBecause of We can not able to open the Wizard, We suggested the customer to continue the upgrade manually. \r\nWhe the customer agreed with us about manual upgrade, we run the commands below:\n\r\n1.Patch Secondary Oracle\r\n2.Upgrade Secondary NE Instances\r\n3.SetUpDBReplication\r\n4.TruncateWizardState\r\n5.Take PostUpgradeBackup DB\r\n6.MCPPrepareLoad\r\n7.McpInstallFiles\n\r\nWithin this commands, we finished the upgrade successfully. For SESM1_1 problem, hard reboot didnt work and they made BIOS setting chance. \r\nAccordingly, they changed the faulty disks and they redeployed the SESM1_1 server.\r\nOn the other words, the problem were fixed after this action plans.','null'),(38,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2018-11-16','181116-161198','Telefonica Moviles','Hi all,\r\nThe problem was that SESM1_0 in continuously bouncin between Up/Initializing and Down/Unavailable after SESM server replacement. The replacement was because of the faulthy disk problem in HS20. They replace it with HS21 by using SystemRecovery MOP. \r\nI have connected the site with Bomgar and i killed/started the instance , it didnt work. After this i have tried reboot/power on-off/deploy/undeploy but the result was the same. \r\nI have seen this error below:\r\nAfter i try to reboot the system and i have seen pool connected successfully\r\nUnable to initialize NetworkInterfaceManager.\r\njava.lang.NullPointerException\r\n	at com.nortelnetworks.mcp.base.io.nif.fw.LogicalInterfaceUtils.getNetworkInterfaceData(LogicalInterfaceUtils.java:134)\r\n	at com.nortelnetworks.mcp.base.io.nif.fw.InterfaceOwnerFactory.create(InterfaceOwnerFactory.java:46)\r\n	at com.nortelnetworks.mcp.base.io.nif.fw.NetworkInterfaceManager.(NetworkInterfaceManager.java:267)\r\n	at com.nortelnetworks.mcp.base.io.nif.fw.NetworkInterfaceManager.createInstance(NetworkInterfaceManager.java:113)\r\n	at com.nortelnetworks.mcp.ne.base.system.ft.nif.SystemNifManager.(SystemNifManager.java:138)\r\n	at com.nortelnetworks.mcp.ne.base.system.ft.nif.NetworkInterfaceSubsystemManager.initialize(NetworkInterfaceSubsystemManager.java:80)\r\n	at com.nortelnetworks.mcp.ne.base.system.main.NEBootstrapper.initSingleNifManager(NEBootstrapper.java:891)\r\n	at com.nortelnetworks.mcp.ne.base.system.main.NEBootstrapper.bootstrap(NEBootstrapper.java:1127)\r\n	at com.nortelnetworks.mcp.ne.base.system.main.NEBootstrapper.start(NEBootstrapper.java:1206)\r\n	at com.nortelnetworks.ims.app.sessmgr.system.main.SessMgrBootstrapper.main(SessMgrBootstrapper.java:40)\r\n[Fri Nov 16 06:44:09 2018] Starting SESM1...\n\r\nDue to this, i wanted oss , work logs , ifconfig and userinfo.txt files, showversion.pl command results for both SESM server to see if there is any error to configuration parameteres. There were no configuration error. \n\r\nAdditionally, After we receive that error on work logs, we have checked the Multinetting Parm. Multinetting Parameter were set false on MCP GUI. we did to kill/undeploy before changing the parm as true, then we performed deploy/start. It worked.','null'),(39,'Bill Picardi','AS-OAM','2018-11-03','181103-159328','KBS / CONVERGE DISTRIBUTION LIMITED','ER paged out to assist with unmanageable System Mangers within the Application Server\'s MCP GUI, as both services had a status of Online/Unavailable and could not be stopped or started.  \n\r\n  As seen with commands from the CLI, SM1 was active and SM0 was hot standby. We killed SM0 from the MCP GUI to place it in a Offline / Unavailable state, then stopped the SM1 from the CLI.  We started SM0 from the CLI and started SM1 from the MCP GUI, which allow management of the SM from the MCP GUI again, with both services in good status.\n\r\n  The Session manager also had the same issue.  We checked which was active and standby from the CLI and killed the hot standby, SESM1_1, to correct its status.  Immediately restarting the SESM did allow it to start and come online, but the MCP GUI did not display the service as started.  We stopped the service from the CLI and kept it offline for several minutes, and then restarted the SESM, which allowed networking issues to timeout and clear, allowing the SESM1_1 service to come back online as hot standby.\r\n  The SESM1_0 servers had more that 18000 files in /var/mcp/spool.  We killed the service then cleared the files, and restarted the service.  We found that it also could not come online if you restarted it immediately.  The service needed a 10 minute delay to allow the MCP GUI to start it and have its status displayed correctly within the MCP GUI.  Once the SM an SESM services were online with good status in the MCP GUI NE Maintenance, and the alarms status of each was back to green, we ended the call.\r\n-Bill','null'),(40,'Bill Picardi','AS-OAM','2018-10-29','181026-158125','Vodafone','ER paged GPS to report problems after the prior day\'s work to clear issues with SESM1 and DB.  The services display alarms and some had rogue status and replication link alarms.  The SM GUI displayed the active SM as configured/online and the database had increasing deferred transactions.  We expect the SM element manager is faulty, the GUI output could be corrupt, or not updating properly and may not have the current alarms, so we decided to stop and restart the SM and DB on both EMS servers.  No Issues are reported from status checks of the SM and DB from the CLI.  The SM GUI reports no critical or major alarms or issue for the SM or SESM or DB.  The PROV still had alarms, and it was killed, redeployed, and restarted to clear these.\r\n  Another status check revealed broken jobs and deferred transactions on DB1, and DB1 was restarted again to clear these.  All services have good status and no alarms.  We recommend monitoring the DB for increasing deferred transaction for the next hour.  The SESM occasionally flashes an alarm about high SIP count, but this clears on its own. Mentioned to the customer that they may decide to restart the SESM unit at some point.\r\n-Bill P.','null'),(41,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-10-28','181026-158125','VODAFONE NEW ZEALAND LIMITED','I have been paged by John Kisner from ER and he reported that SESM1_0 had become unresponsive and had to be power cycled to recover it. Once power cycled, SESM1_0 came up Hot Standby but Admin state stayed \"CONFIGURED\" instead of \"ONLINE\". \n\r\nI connected to the site and killed/re-deployed the problematic SESM instance. Then SESM became ONLINE/Hot Standby as expected.','null'),(42,'Mustafa YUKSEK (NETAS External)','AS-OAM','2018-10-25','181024-157574','VODAFONE NEW ZEALAND LIMITED','I have been paged by Roberto Garcia and explained that there were THLD 401,JMXM 701, RTA101 alarms on PROV2_0 and two of them were critical alarms.\n\r\nThe customer, to clear these alarms applied killing process for PROV2_0 and it was stuck in killing. Then When I connected the site I realized the EMServer2 was not reachable and EMServer1 was working nearly  469 days. In this respect, I\'ve given \"reboot \" action for establishing ssh connection to  EMServer2  that is PROV2_0  deployed on it and to clear the alarms.\n\r\nNearly 3 hours later on site engineer rebooted the EMserver2  and the alarms were cleared. Then I dropped the call.','null'),(43,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-10-24','181023-157273','DEPARTMENT OF TRANSPORTATION (DOT)','Upgrade Path: 17.0.32.0 --> 19.0.20.8\n\r\nI have been paged by Caleb Coleman from SWD. He reported that provisioning client an personal agent were inaccessible after the upgrade and he could not pull OMI certs because of this issue. I connected to site and checked the status of the PROV and PA via ./getInstance.pl and noticed that both of them were active. However, PROV1 and PA2 were not accessible from MCP GUI. First, I restarted the PROV1. When PROV1 became online/up/active I clicked to PROV GUI button and I was able to connect to PROV GUI after the restart. After that, I checked the PA2 from MCP GUI and noticed that old load has been deployed to PA2 instance. I did undeploy and deployed the correct load. When PA2 became online/up/active, I was able to connect to PA2 GUI from MCP. Then, I dropped the call.','null'),(44,'Omer KIRCALI','AS-OAM','2018-10-20','181020-157015','DEPARTMENT OF TRANSPORTATION (DOT)','Caleb Coleman paged me about the Dbserver1 is stucked on the %28 while primary patching process of the Upgrade. \n\r\nUpgrade path: 17.0.32.0 to 19.0.20.8.\n\r\nI connect to the DB server1 and run showversion.pl. I notice that the platform is patched successfully.\n\r\n--------------------------------------------------------------------------------\n\r\nMCP ShowVersion script\n\r\nThis script will read versions of software on this system and generate\r\n a report to /var/mcp/os/reports/showVersion.2018-10-20T1-44-4.txt\n\r\n--------------------------------------------------------------------------------\n\r\nSystem Type:                  mcp_core_linux_ple4\n\r\nMCP Platform Release Level :  19.0.11 (via patching)\n\r\nMCP Platform Hardware Type :  RedHat-KVM\n\r\nNED Version:                  Version: 19.0.2 (Built on Dec  9 2015 15:15:00)\n\r\nJRE Version:                  1.6.0_161\n\r\nLinux Kernel Version:         2.6.32-696.10.3.el6.x86_64\n\n\r\nNIF Version:                  mcp-nif-fw-utils-19.0-5\r\nJBoss Version:                mcp-version=/main/mcp_install_12.0_int/mcp_install_12.0_sp_int/5\n\n\r\nThe report is written to /var/mcp/os/reports/showVersion.2018-10-20T1-44-4.txt\n\r\nPlease check /var/mcp/os/logs/showVersion.2018-10-20T1-44-4.log for any error reports\n\n\r\nI asked from Caleb to press Save&Exit and re-open the Wizard. After he performed this action, patching step is passed successfully then I dropped from call.','null'),(45,'Emre OVA (NETAS External)','AS-OAM','2018-10-12','180830-148677','LIBERTY GLOBAL SERVICES BV','There was an ongoing BCP spooling issue in customer system. Kyle Mawst (ER) paged me out to perform suggested action plan by A2 GPS.. Since ER/customer are not familiar with these actions and it\'s a little bit complex from customer/ER point of view, GPS assistance was requested. In this respect, I have been paged out to perform the following actions:\n\r\n1) Add a new FPM on MCP GUI.\r\n2) An FPM instance will be deployed on a hotstandby SESM server temporarily. In the case of making this deployment on SESM server, it will need to be decommissioned.\r\n3) Start FPM instance that you deployed on hotstandby SESM server and make sure that its running properly.\r\n4) Stop monitoring of the BCPServer\r\n5) Stop one cluster of the BCP. Clean up the spool files.\r\n6) Change FPM value of BCP servers and instances FPM from SM to FPM on MCP GUI.\r\n7) Star the BCPs on that cluster.\r\n8) Check for spool logs of these BCPs if they are being transferred to that new FPM or not.\r\n9) If spool files are being transferred to the FPM but still grey out at SM, then swact SM instance as well.\n\r\nAs a first action, I was going to deploy a Fault Performance Manager but I got the following error after my attempt:\n\r\n\"Add operation failed: Runtime Validation Failed: Not enough licenses for this Network Element (fpm), check license key.\"\n\r\nUnder these circumstances, since the Fault Performance Manager is not licensed in customer\'s existing license key, we could not go forward and perform rest of the action plan steps.\n\r\nWe will continue to work on new actions in collaboration with A2 designers and determine our next road map within a short span of time. Accordingly, customer will be informed about next actions.','null'),(46,'Buket Nazmiye KUCUKER (NETAS External)','AS-OAM','2018-10-11','181011-155443','Vodafone Fiji','Vodafone Fiji was upgrading their systems from 19.0.4.2 to 19.0.20.9. \r\nThere is a criticial alarm on SM1: \n\r\nAlarmName: LKEY_HARDWARE_MISMATCH_ERROR_CRITICAL_754  \r\nTimeStamp: Thu Oct 04 16:17:18 ADT 2018  \r\nFaultNumber: 754  \r\nShortFamilyName: LKEY  \r\nLongFamilyName: LKEYSYS  \r\nSeverity: CRITICAL  \r\nProbableCause: file error  \r\nDescription: License key problem detected: License key does not match hardware.  \r\nCorrective Action: Install a valid license key. \n\r\nLicence Key was correct and customer said that they havent changed any hardware recently. \n\n\r\nSolition: Double swact to SM cleared the all alarms.','null'),(47,'Emre OVA (NETAS External)','AS-OAM','2018-10-11','181011-155361','VODAFONE UK LTD','Upgrade Path: 18.0.30.X - 19.0.4.X\n\r\nMeraz Aziz from SWD paged me out to report that SESM2_0 stucked in Synchronizing state while upgrading the primary network elements.\n\r\nI connected the site via GTS VM servers and investigated the issue.\n\r\nTotally, there were 3 SESM Pairs in the system.. SESM1_0 and SESM3_0 was upgraded to new load successfully and working fine. We were observing the problem just for SESM2_0.\n\r\nAs a first action, I tried to kill, undeploy, deploy and start SESM2_0 but the result was same. Instance was not able to get into hotstandby status. \n\r\nWhen I take a look at the SESM2_0 oss logs, I had observed the following DBCOMM alarms..\n\r\nSESM2_0 DBCOMM 704 ALERT OCT10 23:38:21:175 MCP_19.0.4.0\r\n  Connection to DB Instance with configured release 18.0.30.0 from an NE instance running incompatible release 19.0.4.0 is prohibited\n\r\nDespite of the fact that SM/DB upgrade is completed without any issue, these DBCOMM alarms were generating.. \n\r\nIn order to identify that whether these alarms are specifically raising on this instance or not, I checked the oss logs of SESM1_0 and SESM3_0.. The same alarms were appeared on these instances but it was not blocking to bring SESM instance to hotstandby status.\n\r\nIn parallel, I had connected the DB via DB Server\'s SQL CLI.. it was looking that DB is working in new load. (19.0.4)\n\r\nOther than this, I checked the neprops.txt file under /var/mcp/run/MCP_19.0/SESM2_0/data and confirmed that this file is populated with correct DB IPs.\n\r\nUnder these circumstances, I decided that this problem is CallP related and CallP GPS should be paged out to proceed with further investigation.\n\r\nAt this point, SWD paged out CallP GPS team and continued to work with Berat.\n\r\nIn order to resolve the issue, we stopped the active SESM instance and this time SESM2_0 was able to get into active status with new load. After that, we performed another swact to bring SESM 2_0 instance to hotstandby status again. \n\r\nSince the issue is resolved after performing the actions that I mentioned above, CallP GPS collected the necessary logs for RCA and we dropped from the bridge.\n\r\nRCA analysis will be provided with the following case:\n\r\n181011-155361 - GSD: C20-A2: 19.0.4.0 : SESM stuck in synchronizing state','null'),(48,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-10-04','TBD','Smart City','Upgrade Path:19.0.4.2 -19.0.20.8\n\r\nI have been paged by James Wilkerson from SWD. He reported that secondary SESM instance was stuck at configured/down/unavailable after it upgraded. I connected to the site and tried to kill the process of the SESM but its process id was bouncing. Because of this reason, I closed the ned auto restart via \"neinit -autorestart=off\" command. After that, I clicked to the deploy button on MCP GUI and SESM instance became offline/down/unavailable then I started to SESM instance. After performing these, the problem has been resolved.','null'),(49,'Omer KIRCALI','AS-OAM','2018-09-28','180928-153436','RAZORLINE, LLC','Gary Norwood contacted me about MCP GUI couldn\'t be opened by customer after upgrade.\n\r\nThe Upgrade path is 17.0.32.0 >> 19.0.20.8\n\r\nThese are the exception that customer have while opening the MCP GUI;\n\r\njavax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure\r\n	at sun.security.ssl.Alerts.getSSLException(Unknown Source)\r\n	at sun.security.ssl.Alerts.getSSLException(Unknown Source)\r\n	at sun.security.ssl.SSLSocketImpl.recvAlert(Unknown Source)\r\n	at sun.security.ssl.SSLSocketImpl.readRecord(Unknown Source)\r\n	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(Unknown Source)\r\n	at sun.security.ssl.SSLSocketImpl.startHandshake(Unknown Source)\r\n	at sun.security.ssl.SSLSocketImpl.startHandshake(Unknown Source)\r\n	at sun.net.www.protocol.https.HttpsClient.afterConnect(Unknown Source)\r\n	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(Unknown Source)\r\n	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(Unknown Source)\r\n	at sun.net.www.protocol.http.HttpURLConnection.access$200(Unknown Source)\r\n	at sun.net.www.protocol.http.HttpURLConnection$9.run(Unknown Source)\r\n	at sun.net.www.protocol.http.HttpURLConnection$9.run(Unknown Source)\r\n	at java.security.AccessController.doPrivileged(Native Method)\r\n	at java.security.AccessController.doPrivilegedWithCombiner(Unknown Source)\r\n	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown Source)\r\n	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(Unknown Source)\r\n	at com.sun.deploy.net.HttpUtils.followRedirects(Unknown Source)\r\n	at com.sun.deploy.net.BasicHttpRequest.doRequest(Unknown Source)\r\n	at com.sun.deploy.net.BasicHttpRequest.doRequest(Unknown Source)\r\n	at com.sun.deploy.net.BasicHttpRequest.doGetRequest(Unknown Source)\r\n	at com.sun.javaws.jnl.LaunchDescFactory._buildDescriptor(Unknown Source)\r\n	at com.sun.javaws.jnl.LaunchDescFactory.buildDescriptor(Unknown Source)\r\n	at com.sun.javaws.jnl.LaunchDescFactory.buildDescriptor(Unknown Source)\r\n	at com.sun.javaws.Main.launchApp(Unknown Source)\r\n	at com.sun.javaws.Main.continueInSecureThread(Unknown Source)\r\n	at com.sun.javaws.Main.access$000(Unknown Source)\r\n	at com.sun.javaws.Main$1.run(Unknown Source)\r\n	at java.lang.Thread.run(Unknown Source)\n\r\nAfter I connected to GTSVM that Gary connected to site , MCP GUI was working properly. \n\r\nThen customer agree with the problem was on their PC I dropped from call.','null'),(50,'Omer KIRCALI','AS-OAM','2018-10-01','181001-153614','SINGTEL OPTUS PTY LTD','Mark Zattiero paged me about the boot error faced after discarding the sdb of the SESM server.\n\r\nBefore sda discarding;\n\r\n                       GNU GRUB  version 0.97  (624K lower / 2089064K upper memory)                \n\n\n\r\n                    ***************************************************************************    \n\r\n                    * Red Hat Enterprise Linux Server (2.6.18-416.el5)                        *    \n\r\n                    *                                                                         *    \n\r\n                    *                                                                         *                                   *    \n\r\n                    *                                                                         *    \n\r\n                    ***************************************************************************    \n\r\n                         Use the * and * keys to select which entry is highlighted.                \n\r\n                         Press enter to boot the selected OS or \'p\' to enter a                     \n\r\n                         password to unlock the next set of features.                              \n\n\n\r\n                   root (hd1,1)   \n\r\n				   root (hd1,1)        \n\r\n                   Error 25: Disk read error                                                       \n\r\n                   Press any key to continue...\n\r\nThe disk type is;\r\n[ntsysadm@O2OUSSLM01 ~]$ mcpRelease.pl\n\n\n\r\n                           *** MCP Platform Release ***\n\n\n\r\n                   System Type:     mcp_core_linux_ple2\n\r\n                   Release Level:   18.0.10 (via patching)\n\r\n                   Hardware Env:    Intel-TIGH2U\n\r\nThe Customer release is 18.0.28.1				   \n\r\nI investigated the issue on the site and tried the combinations as below. Results are here;\n\r\nOnly sda in slot 0 ..  Error 21: Selected disk does not exist\r\nOnly sda in slot-1    System trying to boot from network.  \n\r\n                  Reboot and Select proper Boot device                                             \n\r\n                  or Insert Boot Media in selected Boot device and press a key  \n\r\nOnly sdb in slot 0  System trying to boot from network.  \n\r\n                  Reboot and Select proper Boot device                                             \n\r\n                  or Insert Boot Media in selected Boot device and press a key  \n\r\nOnly sdb in slot-1 Error 21: Selected disk does not exist  \n\r\nSince the both disks were not booted , I recommended OS installation to sda, and recommended disk replacement to sdb since there was HWER 708 alarm exist on it.\n\r\nI recommended to follow \" Installing Operating System for HT-Langley\" on the SystemRecoveryMop. Customer is on 18.0.28 release and server is ple2. So customer will user A2J0M180 order code ESD. I dropped from call after providing the action plan.','null'),(51,'Omer KIRCALI','AS-OAM','2018-09-28','180928-153394','RAZORLINE, LLC','Donnell Williamson paged me about voicemail issue that is faced on the half-upgrade of Razorline.\n\r\nThe Upgrade path is 17.0.32.0 >> 19.0.20.8\n\r\nThe issue is when they set the charge id as public the issue is not seen. When its setted as private , the issue is occured.\n\r\nDonnel stated that there is two jar has been given for 17.0.32.0 which are in order (17.0.32.0_AAK-56947_Jun21.jar and 17.0.32.0_AAK-56946_Jun21.jar)\n\r\nThere is also two JAR for 19.0.20.8 (19.0.20.8_AAK-58003_Sep17.jars & 19.0.20.8_AAK-58004_Sep17.jars). He stated that firstly he applied first AAK-58003 after AAK-58004 and issue is occured.\n\r\nHe notice that he should swap it as AAK-58004 first and after AAK-58003. After swapping the JAR files the issue is resolved and I dropped from call.','null'),(52,'Omer KIRCALI','AS-OAM','2018-09-27','180927-153306','CHUNGHWA TELECOM CO. LTD.','Peter meloney paged me about the upgrade failure on the 38 th step (Upgrading Secondary Element Instances) due to AM_1 that is stucked on the configured up Hotstandby.\n\r\nUPGRADE PATH is; 16.0.2.8 -> 18.0.31.0\n\r\nI connected to the site and ssh to EMServer2 which the AM_1 deployed onto it. The AM_1 cannot be killed , stoped from the MCP GUI. That\'s why, I follow the below sequence with ssh to EMserver2;\n\r\nneinit -autorestart=off\r\nneinit -p\r\nkill -9 \r\nneinit restart\r\nneinit -autorestart=on\n\r\nAfter that I deployed it and remained as offline down unavailable stage.\n\r\nAfter retry the wizard this step is passed. After problem is resolved I dropped from call.','null'),(53,'Omer KIRCALI','AS-OAM','2018-09-26','180926-152823','CHUNGHWA TELECOM CO. LTD.','Kenny Chi paged me about MCP GUI issue that it is could not opening on a specific PC.\n\r\nCustomer is on 11.2.\n\r\nMCP GUI was not opening and giving the below exceptions and errors;\n\r\njava.lang.ClassNotFoundException: sun.security.pkcs11.SunPKCS11\r\nat java.net.URLClassLoader$1.run(Unknown Source)\r\nat java.net.URLClassLoader$1.run(Unknown Source)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat java.net.URLClassLoader.findClass(Unknown Source)\r\nat com.sun.jnlp.JNLPClassLoader.findClass(Unknown Source)\r\nat java.lang.ClassLoader.loadClass(Unknown Source)\r\nat java.lang.ClassLoader.loadClass(Unknown Source)\r\nat com.nortelnetworks.mcp.client.smgui.main.MCMain.setupKeyStore(MCMain.java:354)\r\nat com.nortelnetworks.mcp.client.smgui.main.MCMain.setupSocketSecurity(MCMain.java:289)\r\nat com.nortelnetworks.mcp.client.smgui.main.MCMain.init(MCMain.java:127)\r\nat com.nortelnetworks.mcp.client.smgui.main.MCMain.(MCMain.java:84)\r\nat com.nortelnetworks.mcp.client.smgui.main.MCMain.main(MCMain.java:752)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\nat java.lang.reflect.Method.invoke(Unknown Source)\r\nat com.sun.javaws.Launcher.executeApplication(Unknown Source)\r\nat com.sun.javaws.Launcher.executeMainClass(Unknown Source)\r\nat com.sun.javaws.Launcher.doLaunchApp(Unknown Source)\r\nat com.sun.javaws.Launcher.run(Unknown Source)\r\nat java.lang.Thread.run(Unknown Source)\n\r\nand on the launch bar;\n\n\n\n\nMCP Mgmt Console - 172.31.252.8\nGenband\n\nMCP System Management Console\nMgmtConsole\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12120\n12121\n\n\n\n\r\nAs referance from \"AAK-45845\" the problem is resolved after copy the \"SunPKCS11.jar\" file under C:\\Program Files \\Java\\jre7\\lib\\ext from C:\\Program Files (x86)\\Java\\jre6\\lib\\ext\n\r\nAfter the problem is resolved I dropped from call.','null'),(54,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-09-17','180917-151243','Kandy Business Solutions','Upgrade Path: 18.0.28.2 - 19.0.20.4\n\r\nI have been paged by Chris Henwood from SWD about script distribution failure during the upgrade. I connected to site and performed the ned restart but it did not help. Since the script distribution screen is one of the prep steps of the upgrade and it is not in the pager support scope we agreed to proceed as a case work.','null'),(55,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-09-20','180919-151733','VODAFONE UK LTD','UPGRADE PATH : 18.0.30/19.0.4.0 \n\r\nI have been paged by Meraz Aziz. He said that host server was not responding after OS patching reboot. Once Meraz provide a blade number, we performed to following procedure over NDM to recover the host server;\n\r\nha app-blade deactivate 0 0 7 0\r\nha app-blade offline 0 0 7 0\r\nha app-blade online 0 0 7 0\r\nha app-blade activate 0 0 7 0 \n\r\nAfter performing the above procedure, the problem has been resolved and upgrade continued.','null'),(56,'Omer KIRCALI','AS-OAM','2018-09-19','180919-151647','CHUNGHWA TELECOM CO. LTD.','Hello,\n\r\nPeter Meloney paged me about SM_1 was stucked on configured unavailable after performing a reboot to EMserver2. Customer is on 16.0.2.8.\n\r\nUpgrade path is AS 9.1 ->11.2 (16.0.2.8 -> 18.0.31.0)\n\r\nHe state that the the upgrade was failed on the Extract Upgrade Tools. He patched the host server as recommended on the 180914-151018. After patching they were failed again. So he decided to reboot to both EM servers. SM_1 is became active but SM_1 not\n\r\nWe wait some time to customer supply site access via bomgar. After customer solve the VPN issue I connect to site.\n\r\nAfter I connected to site checked the status of the SM_1. It was down configured ,unavailable stage.. In order to bring SM 1 and other units back, I have tried to restart SM 1 by running ./smStop.pl and ./smStart.pl but it did not work and make the SM 1 hotstandby. I tried also kill/start from GUI and kill/ned restart/start from clie but did not recover. It remains offline HOTSTANBY with a critical alarm (LKEY/LKEYSYS 754 CRITICAL) on the SM_1. Then, when I take a look at the SM 1 work logs, I observed the following critical license key alarms:\n\r\n[Thu Sep 20 00:33:43 2018] Starting SM...\r\n[Thu Sep 20 00:33:43 2018] Process argument list:\r\n[Thu Sep 20 00:33:43 2018] Arg[0] = /opt/mcp/java/MCP_16.0/jre/bin/java\r\n[Thu Sep 20 00:33:43 2018] Arg[1] = -server\r\n[Thu Sep 20 00:33:43 2018] Arg[2] = -XX:NewSize=32m\r\n[Thu Sep 20 00:33:43 2018] Arg[3] = -XX:MaxNewSize=32m\r\n[Thu Sep 20 00:33:43 2018] Arg[4] = -XX:MaxTenuringThreshold=0\r\n[Thu Sep 20 00:33:43 2018] Arg[5] = -Xms512m\r\n[Thu Sep 20 00:33:43 2018] Arg[6] = -Xmx512m\r\n[Thu Sep 20 00:33:43 2018] Arg[7] = -XX:+UseConcMarkSweepGC\r\n[Thu Sep 20 00:33:43 2018] Arg[8] = -XX:+UseParNewGC\r\n[Thu Sep 20 00:33:43 2018] Arg[9] = -XX:+PrintClassHistogram\r\n[Thu Sep 20 00:33:43 2018] Arg[10] = -XX:CMSInitiatingOccupancyFraction=68\r\n[Thu Sep 20 00:33:43 2018] Arg[11] = -XX:+UseCMSInitiatingOccupancyOnly\r\n[Thu Sep 20 00:33:43 2018] Arg[12] = -XX:+UseMembar\r\n[Thu Sep 20 00:33:43 2018] Arg[13] = -Dfile.encoding=UTF8\r\n[Thu Sep 20 00:33:43 2018] Arg[14] = -classpath ../jars/delta_mcp.jar:../jars/mcp.jar:../jars/jhall.jar:../jars/3rdParty.jar:../jars/tomcat.jar:../jars/axis.jar:../jars/OMIServer.jar:../jars/bcprov-jdk16-140.jar:../jars/jsch-0.1.42.jar:../jars/JPam-1.1.jar\r\n[Thu Sep 20 00:33:43 2018] Arg[15] = -DpropFile=../data/neprops.txt\r\n[Thu Sep 20 00:33:43 2018] Arg[16] = com.nortelnetworks.mcp.ne.sm.system.main.SMBootstrapper\r\n[Thu Sep 20 00:33:43 2018]\r\nSYS/SYSTEM 801 INFO  SEP20 00:33:45\r\nState: BOOTSTRAPPING\n\r\nSYS/SYSTEM 831 INFO  SEP20 00:33:45\r\nNE running in non-FIPS mode\n\r\nJMXM/JMXM 201 INFO  SEP20 00:33:49\r\nJVM System Startup Report:\r\n[Linux][ARCH: i386][VER: 2.6.32-220.13.1.el6.x86_64][PROCESSORS: 2]\r\n[PROCESS: 5964@CHTIA2EM2][VM: Java HotSpot(TM) Server VM:Sun Microsystems Inc.:20.1-b02]\r\n[SPEC: Java Virtual Machine Specification:Sun Microsystems Inc.:1.0]\r\n[THREAD: [CONTENTION MONITORING: Y:N][CPU TIME: Y:Y][MONITOR USAGE: Y][SYNC USAGE: Y]]\r\n[STARTTIME: Thu Sep 20 00:33:43 CST 2018]\n\r\nTimer expiry event posted to inactive mailbox com.nortelnetworks.mcp.base.task.TaskMailbox@1a998c7\r\nMailbox owner: com.nortelnetworks.mcp.base.task.TaskScheduler$AuditTask@6b93c5\r\nExpiry event: com.nortelnetworks.mcp.base.timer.TimerExpiryEvent[source=Task: com.nortelnetworks.mcp.base.task.TaskScheduler$AuditTask@6b93c5\r\nState: ina]\r\nLKEY/LKEYSYS 734 ALERT  SEP20 00:36:55\r\nError occured validating license key. License key is not valid on this hardware. Error Code: 2113\n\r\nLKEY/LKEYSYS 754 CRITICAL  SEP20 00:36:55\r\nLicense key problem detected: License key does not match hardware.\n\r\n[root@CHTIA2EM2 work]#\n\n\r\nThen I recommend to ask a licance key from KRS and apply it with below procedure;\n\r\n1- Once you have the new license key, launch the MCP GUI \r\n2- Click on ,  from the left navigation panel. \r\n3- Click on -/+ sign to edit existing license key \r\n4- Locate the new license key on your desktop and click Open \r\n5 Then click apply. \r\n6- The license key is applied to your system. You can perform an SM swact after applying the new license key.\n\r\nPeter try to download the licance key with MAC adress of the EM servers. He downloaded a licance key and I apply the procedure and restart the SM_1. But the alarm was not cleared. I asked Peter to contanct with KRS team. He contacted with mike beall from KRS.\n\r\nPeter state that the \"/usr/local/bin/getMAC.sh\" and \"/usr/local/bin/getHostMAC\" scripts is not working on the EM server 2. I also observe that the EM server 2 was not able to ssh to Host2 which it is deployed on it. I ask from peter to contine the issue as case work since there is no full outage on the system. I dropped from call as agreement.','null'),(57,'Omer KIRCALI','AS-OAM','2018-09-08','180831-148926','AXTEL','Hello,\n\r\nJoyce Lyon is paged me about to multiple registration was detected on the AXTEL site and they want to assign multiple registration service to all users on the domain that the issue is occurs.\n\r\nAXTEL is on MCP_17.0.22 MR.\n\r\nShe asked me the OPI request that is used on multiple registration service assignment.\n\r\nI performed a test on the lab and detect the OPI request from oss logs and shared the wsdl and requests for assigning the multiple registration service as below;\n\r\nThere is two step to assign the multiple register service to a user. One is assigning the service to a user and second is assigning the system profile of the service.\n\r\n**The wsld of assigning the service to the user is UserServiceAdmin and the request is setServicesForUser. Here is my test result;\n\r\ncommon;ids=omer1@nl15ldap.com&nortel&&nortel&&multipleregister&&presence&;fromdomain=UserServiceAdmin;fdns=setServicesForUser\n\r\n**The wsdl of assigning the system profile of the service is MLRSPAdminService and the request is setSystemProfileForUser. here is my test result;\n\r\nIn sendLocalDataSync: common;ids=omer1@nl15ldap.com&_system_default_;fromdomain=MLRSPAdminService;fdns=setSystemProfileForUser\n\r\nIn order to perform a multiple assignment of multiple register service to all users , pro version of the SOAP UI can be used or and xml request can be created.\n\r\nAfter shared the OPI request I dropped from call.','null'),(58,'Omer KIRCALI','AS-OAM','2018-09-04','180831-148867','FRONTIER COMMUNICATIONS',' Robert Starling paged me due to the booting problem after change the disk sda. The customer load is MCP_14.1.0. \n\r\n Initially, I asked the reason of the changing the disk sda. Robert inform me about there was an HWER 708 alarm on it and customer decided to change the disk and insert a new disk sda.\n\r\n SNMNCAXPPSA  ** HWER708 AUG24 07:28:53 2039 FLT  HWER Fault \r\n         Location: 113.47.22.19-A2E-Mgr-SM_1;113.47.22.19 \r\n         Notification Id: 100000688 \r\n         State: Raised \r\n         Category: processingError \r\n         Cause: inputOutputDeviceError \r\n         Time: Aug 24 07:28:53 2018 \r\n         Component Id: Server=SESS6;Software=HWER \r\n         Specific Problem: HWER;708 \r\n         Description: Listed partition(s) is/are faulty. \r\n         md5 (Failed: [sda7]) \r\n         md7 (Failed: [sda5]) \r\n         md6 (Failed: [sda8]) \r\n         md4 (Failed: [sda9]) \r\n>        md0 (Failed: [sda1])\n\r\n The console logs shown when the new sda disk inserted is below;\n\r\nEntering boot selection menu...\n\r\n    GRUB  version 0.93  (619K lower / 3931072K upper memory)\n\r\n [ Minimal BASH-like line editing is supported.  For the first word, TAB\r\n   lists possible command completions.  Anywhere else TAB lists the possible\r\n   completions of a device/filename.]\n\r\ngrub>\n\r\nsetkey [TO_KEY FROM_KEY]               setup [--prefix=DIR] [--stage2=STAGE2_\r\nsplashimage FILE                       terminal [--dumb] [--no-echo] [--no-ed\r\nterminfo [--name=NAME --cursor-address testvbe MODE\r\nunhide PARTITION                       uppermem KBYTES\r\nvbeprobe [MODE]\n\r\ngrub> boot\n\r\nError 8: Kernel must be loaded before booting\n\r\ngrub>\n\r\ngrub>\n\r\ngrub>\n\r\ngrub>\n\r\ngrub>\n\r\ngrub>\n\r\ngrub>\n\r\ngrub>\n\r\n After that I recommand to try a boot with only sdb, It did not work. Then I learned that the new sda has an other OS on it and after insert it and mirroring with sdb, the file system is corrupt on the sdb as well.\n\r\n I asked to insert the old sda in order to mirror the file system that is not corrupted but It did not worked as well and conclude with same console output.\n\r\n As per tries , we needed a new OS installation. I recommand to re-install the OS with SystemRecoverMop and attached to the case. Robert told me the document is a bit complicated, and I recommand him to read the instructions carefully and If faced with and error ping me. We had taken the userinfo.txt with \"cat (HD0,0)/admin/userinfo.txt\" and customer was insert 2 spare disk.\n\r\n The customer was follow the instructions and state that there was an error booting after re-installation , but however , It reached to login prompt. The error was below;\n\r\nUse of uninitialized value in concatenation  or string at /usr/local/bin/CommonInstallUtilsV2.pm line 780. \r\n                      Error-calcBcast-1: Malformed ip address \"\". \r\n                                                                 .................................... \n\r\n                 Performing Hardening and Security ......................................grep: /admin/userinfo.txt: No such file or directory \r\n                                                       grep: /admin/userinfo.txt: No such file or directory \r\n                     .chmod: failed to get attributes of `/admin/userinfo.txt\': No such file or directory \r\n                   .\n\r\n They tried to enter as root with root/password but It did not work. Also there was an error about ping the server\'s machine IP from another server. They stated me they type install, instead of install-kvm. I told them to follow the constructions only but Robert stated me that they tried using install-kvm for hours on the previous issue case 180625-138227 and it would not work. Also he stated that this customer he was working with is the one that worked that issue so he was quite familiar with that previous issue and the problems they had. So , agreed with continue on the install.\n\r\n After reboot again in order to re-install , they could not saw the installation screen and tried again with pressing cd button on the server. Then they change the boot options and installation screen is came again.\n\r\n Customer continue with writing install on the first screen and follow the other steps. I recommand also using the IP\'s that written on the userinfo.txt. In the end , result was the same. The IP was not pingable and the default password (root/password) was not working.\n\r\n I recommanded Robert to type install-kvm which is written on the SystemRecoverMop instructions , and also do not copy-paste the IP\'s, type them after pressing the manual installation. Robert stated me the on-site has to leave so we will continue this tomorrow afternoon.','null'),(59,'Bill Picardi','AS-OAM','2018-08-22','180822-147284','Optus','GPS OAM asked to assist in recovering BCP servers within a cluster that were stuck in a restart loop.  \r\n    Started investigation and research for issues at site by reviewing service logs, running commands on the console for server status, and checking status in MCP GUI. Several BCP servers were in this state, but remaining servers in the cluster were handling calls properly, but alarming with high usage.\r\n    Errors included \"HA Layer has signaled EXITING for the following reason: Unable to communicate with AISExec\", and \"WARNING: HaLayer shutting down (Unable to communicate with AISExec.)...\"\r\n    Stopping and restarting the individual problematic BCP server in the cluster via the MCP GUI did not successfully place the BCP server online.  We stopped all BCP servers within the cluster simultaneously, then restarted them all, and the servers came online.  The MCP GUI revealed that BCP services are no longer in a restart loop.  We performed additional status checks upon the console to validate running processes and that services are running properly.\r\n    The customer will perform similar actions at a later time to recover additional clusters that have BCP servers stuck in a restart loop, within a faulted state.','null'),(60,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-07-27','180727-143212','CABLE ONDA SA','I have been paged by James Wilkerson from SWD about upgrade failure on SM&DB upgrade screen. He stated that he got \"Script hasn\'t completed in predefined time interval\" failure then we suggest trying retry. After clicking retry, the screen has failed with \"Fail: Primary SM should be ONLINE -HOT STANDBY\" at this time. We stopped the primary SM instance to make the secondary SM instance active. After secondary SM instance became active screen has passed successfully.','null'),(61,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-07-28','TBD','U.S. Air Force Falcon AFB','I have been paged by CALLP GPS manager Recep Topcu about black box outage. He stated that, although the problem seems like callp specific, there was a huge amount of CPU utilization on the problematic SESM server and this CPU utilization might cause the problem. In order to investigate this CPU issue we asked below from the customer; \n\r\n1- mcpRelease.pl \r\n2- cat /proc/cpuinfo\r\n3- cat /proc/meminfo\r\n4- vmstat\r\n5- top\r\n6- ps -ef\r\n7- uptime\r\n8- sar -dp 1 1\r\n9- Historical OM Browser output for investigating the maxCPUandMemory\n\r\nIt became clear that the problem was related with callp before we could reach the requested logs.','null'),(62,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-07-24','180724-142464','BT MSL','Dean Gilbert from ER paged me out and reported that admin state of the SESM3_0 remained stuck in configured after performing deactivate/activate action. We connected to site and ssh to EM Server first. After that, we jumped to SESM3_0 and stopped the instance from CLI by running neStop.pl command. Then restarted the ned connection and finally deployed the instance on the MCP GUI again. The customer confirmed that SESM 3 0 is now online/hot-standby. Since the issue has been resolved, we dropped the call.','null'),(63,'Mustafa YUKSEK (NETAS External)','AS-OAM','2018-07-20','180621-137800','VERIZON BUSINESS PURCHASING LLC','We have been paged by John from ER stating that SESM server is not being booted up.\r\nThere was no console access or any access to the server. And the customer is not aware how to set up console connection to that server.\n\r\nWe advised them, they can get info from 3310 specs since it is an HW specific configuration.\n\r\nCustomer and ER told they can be available after 4 hours with an experienced site engineer.\n\r\nSo we provided the below information:\n\r\n1) If there is nothing written on the console logs after the power cycle, you should test the server via replacing a new disk. If there are no console logs written again after disk replacement(with power cycle), it means the server is faulty and you should replace the server and perform a re-installation(via system recovery mop).\r\n2) If you can see the console logs are written after replacing the disk, you should perform a re-installation with the new disk(via system recovery mop).  \r\n-I attach the system recovery mop and you can see all steps for recovering your HP3310 server via following the instructions.\n\r\nAnd the load for the reinstallation should be ready before starting the action plan. It is a very old load for HP 3310 server.\r\nSo please be prepared with the load file for 3310 server','null'),(64,'Mustafa YUKSEK (NETAS External)','AS-OAM','2018-07-18','180718-141616','MIDCONTINENT COMMUNICATIONS','Upgrade path: 19.0.4.2 to 19.0.20.8\n\r\nSWD paged out stating that Primary HOST OS patching screen failed with the below error message: Unable to connect to daemon: This might be due to a server or network problem. Please ensure that the server is running and reachable\n\r\nHOST1S1 was not reachable due to a network error. The customer was doing a fiber work at the same time and a fiber cut occurred. That cut caused server drop from the network and not reachable.\r\nIt took 2 hours customer to restore fiber cut. After that, we waited for the customer to let us know which blade AS is deployed.\n\r\nAfter it is confirmed 0 0 5 0 we provided below action plan to be applied on NDM blade:\n\r\nha app-blade deactivate 0 0 5 0\r\nha app-blade offline  0 0 5 0\r\nha app-blade online  0 0 5 0\r\nha app-blade activate 0 0 5 0\n\r\nThen HOST1S1 was reachable and screen passed successfully.\n\r\nLet the SWD complete until Half Pause Point, and then complete second side of the system upgrade on tonights mtc window.','null'),(65,'Emre OVA (NETAS External)','AS-OAM','2018-07-04','180704-139602','HONG KONG BROADBAND NETWORK LTD.','Kevin Gaines paged out GPS and requested an assistance for recovering SESM2_0 unit. I\'ve connected the site and checked the status of SESM2 server. Accordingly, this server was not reachable and I could not login to that server. \n\r\nSince it\'s a virtualized environment and SESM2 guest VM located on HKBNHost2Server1 host server, I\'ve tried to check the status of problematic guest VM by connecting the related host server but I was not able to connect the host server via SSH. \n\r\nWhen we take a look at the host server\'s console logs, the following I/O errors and disk failures were observed.\n\r\nEXT3-fs error (device dm-2): read_inode_bitmap: Cannot read inode bitmap - block_group = 0, inode_bitmap = 34\r\nEXT3-fs (dm-2): I/O error while writing superblock\r\nEXT3-fs (dm-2): error in ext3_new_inode: IO failure\r\nEXT3-fs (dm-2): I/O error while writing superblock\r\nmd: super_written gets error=-19, uptodate=0\r\nmd: super_written gets error=-19, uptodate=0\r\nmd: super_written gets error=-19, uptodate=0\n\r\nDue to the fact that only SESM2 guest VM is deployed on this host server, I\'ve tried to deactivate and activate the host server on NDM by executing the following commands but it did not work:\n\r\nha app-blade deactivate     \r\nha app-blade offline     \r\nha app-blade online    \r\nha app-blade activate    \n\r\nUnder these circumstances, since there are disk failures on this host, I suggested to re-seat the RTM card of SB-ATCA blade and customer performed this action.\n\r\nAfter re-seating the RTM card and performing one more deact/act on NDM, host server and SESM2 guest VM was recovered.\n\r\nIn order to provide a root cause analysis, customer opened a follow-up case. Due to the fact that its an hardware-related issue and application was not reachable because of this reason, follow up-case had been routed to Genius team\'s queue.','null'),(66,'Emre OVA (NETAS External)','AS-OAM','2018-07-03','180703-139354','Verizon Communications','Customer Release Level: MCP_14.1.10\n\r\nTom Draper from ER paged me out and reported that provisioning fails and customer is not able to make any provision due to the fact that primary DB look down.. \n\r\nI\'ve connected the site and confirmed that primary DB is down.. All other NEs were up and running properly.\n\r\nIn order to bring the primary database back, I tried to run /etc/init.d/dbora stop and /etc/init.d/dbora start but DB start was failing.. \n\r\nApart from this, I\'ve tried to connect the primary database from CLI but the following error was returned:\n\r\nORA-01034: ORACLE not available\r\nORA-27101: shared memory realm does not exist\r\nLinux Error: 2: No such file or directory\n\r\nWhen I checked the /opt/mcp/db/logs/startDatabase.log file, it\'s seen that some control files were broken and it was blocking the start DB.\n\r\nORA-00214: control file \'/opt/mcp/db/oradata/mcpdb/control01.ctl\' version \r\ninconsistent with file \'/var/mcp/db/data/mcpdb/control02.ctl\' version \n\r\nstart db fails at /usr/lib/perl5/site_perl/mcsBase/SysUtl.pm line 400 \r\nmcsBase::SysUtl::pipedCmd(\'mcsBase::SysUtl=HASH(0x8a5d930)\',\'sqlplus \r\n-S\',\'ARRAY(0x8bce094)\',\'backtic\',900,\'start db\',\'undef\') called at \n\r\nstart db fails \n\r\nFollowing actions taken on primary DB server for recovering the issue.\n\r\n- Connect the primary database server.\r\n- Run oracleUninstall.pl under /var/mcp/install as ntappadm\r\n- Run oracleInstall.pl -primary with correct oracle installer version.\r\n- Exit from primary DB server\r\n- Connect the secondary database server.\r\n- Run dbInstall.pl -fo, deploy the required files into primary DB server (Yes)\r\n- Run resync.pl from secondary to primary.\n\r\nDue to the customer\'s retired MCP release level, RCA was not provided on this case.','null'),(67,'Emre OVA (NETAS External)','AS-OAM','2018-07-02','180629-139139','U.T Austin','Glen Anderson paged me out to report that available authorization codes could not be pointed a system profile pool successfully. The following error was returning while trying to modify a system profile:\n\r\n\"Failed to modify authorization codes system profile. The field(s): System Service Profile ID, Authorization Code Value must be unique.\"\n\r\nI could identify the root cause of the problem and fixed it. The reason for why we cant assign available authorization codes to a system profile was a uniqueness check. \n\r\nAccording to the current authorization code working mechanism, while modifying a system profile, system expects that each authorization code should have a unique code name and code value in the pool, but there is no the same check for \"adding authorization code\" operation. In other words, most probably there is a software bug here.  \n\r\nSince there are approximately 5,000 authorization code and we cant identify that which authorization code values are same, Ive connected the database and run the following SQL queries:\n\r\n1-  select authcode_value, count(*) from authcodes group by authcode_value having count(*) > 1 \r\n2- select * from authcodes where authcode_value=491793;\n\r\nThe above SQL queries returns the authcode names which has the same authcode id and violate the uniqueness check:\n\r\nOutput:\n\r\nAuthcode_id    Authcode_name    Authcode_value\n\r\n59885	        poehlmeA	491793\r\n59884	        poehlme	        491793\n\r\nAfter determining that which IDs are same (not unique), Ive launched the System Profile page on PROV GUI and copied all authorization codes to pool (right side) except poehlmeA and saved it. This time, all authorization codes profile modifications had been successfully saved. In other saying, due to the fact that poehlmeA and poehlme has the same authcode value (491793), setsystemprofile operation was failing when these are on the pool at the same time. Due to the fact that these two ha the same authcode, excluding one of them helped us to add all other auth codes into a system profile successfully.\n\r\nIve created the following JIRA and assigned to A2 design support team for providing a permanent fix if it\'s possible: AAK-57137\n\r\nDesign investigation will be ongoing for providing a proper solution permanently.','null'),(68,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-06-29','180625-138227','FRONTIER COMMUNICATIONS','We have been paged by Kyle Mawst about dbInstall script failure. When we connected the site and checked the current situation, we noticed that this script was run on primary EM/DB server despite the System Recovery MOP clearly states that the dbInstall.pl fo script should be run on the secondary EM/DB server. After running this script on Secondary EM/DB server, the script has completed successfully.  \r\nThen we followed the MOP with the next steps and tried to performing SM deploy operation on primary server but it has failed due to missing \".dbuserdata\" file under \"/var/mcp/install\" directory. We compared the checksum of the \".dbuserdata\" files on Secondary EM server under \"/var/mcp/install\", \"/var/mcp/run/MCP_14.1/mcpdb_1/data\" and /var/mcp/run/MCP_14.1/SM_1/data/ and verified that three of them have the same checksum. In this respect, we copied the \".dbuserdata\" file from /var/mcp/run/MCP_14.1/mcpdb_0 and pasted it under /var/mcp/install/ directory of the primary server. After that, we corrected the permission and user group of the \".dbuserdata\"  according to the secondary EM on same directory and re-run the deploy script. Finally, the script has been completed successfully and SM0 became hot standby.   \r\nThe only action that left is deploy and start PROV2_0 on EM1 server. We could not perform this action because we had no MCP GUI access.','null'),(69,'Omer KIRCALI','AS-OAM','2018-06-28','180628-138773','VODAFONE NEW ZEALAND LIMITED','Customer is on MCP_14.1.10.2.\n\r\nGary Norwood contacted me about performing reboot action to the SESM1Server1.\n\r\nThe problem was SESM1_0 is on unavailable stage (stucked on starting) and SESM1_1 is on hotstandby.\n\r\nAfter peforming the reboot SESM1_1 is became active. After starting the SESM1_0 the unit is became offline HOTSTANDBY. \n\r\nI offered killing the unit and start again.\n\r\nAfter starting the unit SESM1_1 is became online HOTSTANDBY. There was an alarm exist on this unit as JMXM701. It is cleared also.\n\r\nI droped from the site since there is no problem on the all SESM units.\n\r\nThanks\r\nÖmer','null'),(70,'Omer KIRCALI','AS-OAM','2018-06-28','180628-138773','VODAFONE NEW ZEALAND LIMITED','Customer is on MCP_14.1.10.2.\n\r\nDean Gilbert contacted me about the SESM1_0 is on unavailable stage and SESM1_1 is on hotstandby.\n\r\nI connected the site and checked the SESM1Server2 and SESM1_1 is on hotstandby as well.\n\r\nWhen I tried to connect the SESM1Server1, I could ssh but could not login with any user in a healty way.\n\r\nCustomer stated that there is no call impact occurs and all calls are good.\n\r\nI suggested to performing a reboot to SESM1Server1 in meintenance window. It will be done on 03:00 pm GMT +3.\n\r\nThanks\r\nÖmer','null'),(71,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-06-27','180625-138227','Frontier Communications','Customer load --> 14.1.10\r\nHardware type --> HP3310\n\r\nI have been paged by Jim Shelby from ER. He said that there was no database in the server after hardware replacement and operating system installation of the new hardware. The first thing we asked was whether he used the System Recovery MOP or not and he said he used the System Recovery MOP and followed the steps properly. We suspected that MOP has not been followed properly and oracle installation has not been completed after ple1 installation. While we starting the investigation he said that chassis has a broken latch and the CMOS battery is dead and they decided re-install again. In this respect, we told him which chapter should be followed also we gave the required actions after ple1 installation. However, he wrote again via IM after ple1 installation and he stated that none of the commands were working. We asked him again whether he followed the MOP properly and he said yes. Also, he said he is not familiar with this MOP and he asked to us to connect the site and complete the rest of the steps and we told MOP all explains every detail with very basic steps so he can continue and page us if any failure happens.\n\r\nHowever, he did not perform the required steps so we had to connect to the site. We asked him to establish the ssh connection to the problematic server but he even was not aware the IP addresses of the server or which one is Primary and which one is Secondary server. Finally, we were able to connect to the server and started to the investigation. We noticed that the MOP was never followed. There was no oracle installer under /var/mcp/media directory while it is very clear on the documentation to make the loads available before starting. Also, platform level of this server is not proper. When we told that is not proper platform load, they said they are aware and only had this one so decided to use it on their own. So now both EM servers has different platform levels which is not a supported configuration.\r\nWe asked him to find the oracle installer but he was not able to find it. Since the following process, is not the procedure that we described and we could not go on without finding the required mediums, we dropped the pager and told them how to find correct Oracle installer load. Also warned the team to find correct version of the Oracle otherwise system will not work. And since that is the primary DB, secondary DB is only read-only so he cannot do any provisioning until primary DB fully recovered.','null'),(72,'Omer KIRCALI','AS-OAM','2018-06-26','180626-138515','YORK UNIVERSITY','Customer is on MCP_19.0.20.2\n\r\nTom Draper contacted me about SM_1 was stucked on the configured and unavailable stage, Sesm1_1 stucked on the killing stage.\n\r\nThey stated that they faced with this error while swacting the Sesm1_1. \n\r\nI connected to the site and ssh to EMserver2 that SM_1 and SESM1_1 is deployed onto it (Also PROV2_0 and AM1_1 is deployed onto it) is kicked me out after some time. I offered to perform a reboot to EMserver2.\n\r\nAfter rebooting SESM1_1 was on the offline stage. SM_1 was still on the configured and unavailable stage. I performed kill and start to SESM1_1 and it became HOTSTANDBY without and problem.\n\r\nI performed a kill to SM_1 from site and run smStart.pl but it became hotstandby with configured stage. I killed it from MCP GUI but did not allow me to start again or undeploy from GUI. I checked the jars if exist. There were no jar file. I decided to deploy and start operation. After I performed the action SM_1 is become HOTSTANDBY without any problem.\n\r\nTom opened a follow-up case and assign to me. I droped the call after the issue is resolved.\n\r\nThanks\r\nÖmer','null'),(73,'Omer KIRCALI','AS-OAM','2018-06-20','180620-137494','BT TELECOMUNICACIONES S A','MAS_16.0.0.861\n\r\nDavid Paniagua paged me about the critical alarms on both secondary MAS(MAS2 and MAS 4) after MAS migration of BT spain.\n\r\nMAS1(Primary),MAS3(secondary) is in cluster and MAS2(Primary),MAS4(Secondary) is in cluster.\n\r\nI connected to server to check the status of MAS servers. MAS 366 error was exist on the MAS2 and MAS4.\n\r\nAlarm: Cluster replication account login has failed.\r\nFault Number:366\r\nSeverity: Critical.\n\r\nCustomer try a reboot to MAS2 and MAS4 servers and the alarms are cleared. I drop the call after the alarms are cleared.\n\r\nThanks\r\nÖmer','null'),(74,'Omer KIRCALI','AS-OAM','2018-06-19','180618-137110','Verizon Communications','Mark Zattiero paged me about the partial outage on the Verizon communications. The customer is on the MCP_18.0.28.1\n\r\n After login the site, I noticed the Sesm2_0 NE was on the unavailable stage. Sesm2_0 is deployed onto the NEserver3 and It is VM of the Host2Server1 in the virtulized system.\n\r\n Stop/start and kill/start operation was not resolve the issue that is done by customer before paging me. \n\r\n Performed deactivate / activate the actual openslot card from NDM but the problem was not resolved.\n\r\n A fresh install to the NEserver3 is decided and will be performed on the meintenance window on @0001 GMT -4 06/20.\n\r\nThanks\r\nÖmer','null'),(75,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-06-03','180603-134695','Guyana Telephone and Telegraph Company','I have been paged by David Hedge again from ER. They stated that threshold value was increased to %85 and also he was unable to connect to active SESM server. First, we deleted the old log files from the server and /var/mcp directory decreased to %55. Apart from this, we deleted the SESM IP from SM\'s known_host file and resolved the other problem. Then we dropped.','null'),(76,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-06-03','180603-134695','Guyana Telephone and Telegraph Company','I have been paged by David Hedge from ER. He said that there was a critical AboveTHLD alarm on MCP GUI and customer was curious about it. We saw the situation was much worse when we connected and checked. There were 81 alarms on MCP GUI and /var/mcp directory of the active SM was %100. SESM1_1 was producing too many logs and that was causing the threshold alarms. First, we removed the old loads from SM0 and SM1 simultaneously. After that, we restarted the SESM instance and made the problematic instance hot standby. Then, we removed the spools from this server. Apart from this, we removed the old files under /var/mcp/oss/log/SM/all/STD/SESM... and /var/mcp/oss/log/SM/all/MCP/SESM... from SM server. After these actions, var/mcp directory\'s size decreased to %75. However, SESM was still producing too many logs, because of this reason we suggested to the customer to paged Callp GPS and we dropped.','null'),(77,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-06-01','180601-134505','TELEFONICA MOVILES','UPGRADE PATH : 17.0.7.4 (10.2) TO 17.0.22.20 (10.4)\n\r\nI have been paged by Meraz Aziz from SWD and he stated that upgrade has failed during setup db replication screen. When we checked the wizard logs we saw that db replication has already been completed successfully with below log;\n\r\n 03:17:03 /opt/mcp/ned/bin/nedclient 10.132.44.212 4890 2>&1 << EOF\r\nconfig 3 dummyRel.resync_delete ROGUE_NE 0 root:root\r\nrun /bin/rm true root:root 400 -rf /var/mcp/run/dummyRel.resync /var/mcp/run/dumm\r\nmyRel.resync_state /var/mcp/run/dummyRel.resync_delete\r\nexit\r\nEOF\r\n 03:17:05 Running ned cmd: /bin/rm -rf /var/mcp/run/dummyRel.resync\r\n 03:17:07 : Resynchronization of the databases is completed successfully\r\n 03:17:07 : Resync is completed!\n\r\nAlthough this script seems completed successfully, upgrade wizard was giving below error; \n\r\nconnection loss\'\' script hasn\'t given a response till a predefined time interval, Resync failed\r\npossible reasons: -  network speed\r\n-there is not enough network bandwidth \r\n-script is stuck due to unexpected scenario\n\n\r\nWhen we faced with these errors on upgrade wizard, we suspected that there might be a ned specific problem and tried to ned restart but its failed.\n\r\nSince there was no alarm on SM GUI and we confirmed that db replication script was completed successfully, we run the wizard with debug mode and finally we were able to pass the screen.','null'),(78,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-06-01','180601-134521','GTD TELEDUCTOS S.A.','Upgrade Path: 17.0.7.14 to 17.0.32\n\r\nI have been paged by James Wilkerson about upgrade wizard failure on \"Patching Operating Systems of the Secondary Servers\" screen. He stated that screen has stuck on the EMServer2 reboot. When we try to ping the EMServer2, its failed. Since we could not reach to EMS2, we requested to performing a power cycle on EMS2. After the power cycle operation, the screen has passed successfully.','null'),(79,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-05-31','180531-134299 ','University of Alabama BIRMINGHAM','UPGRADE PATH :17.0.31.5 --> 17.0.32.0\n\r\nI\'ve been paged by Chris Henwood from SWD. He said that upgrade had failed during the Prepare DB step. When we checked the upgrade wizard logs, we saw the \"*** Cleanup Replication FAILED *** \" error. In the light of this, we run the cleanupReplication.pl manually. After cleanupReplication script has completed successfully, we run the upgrade wizard and screen has passed.','null'),(80,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-05-30','180530-134081','TELEFONICA MOVILES','UPGRADE PATH : 17.0.7.4 (10.2) TO 17.0.22.20 (10.4)\n\r\nI have been paged by Meraz Aziz from SWD about Telefonica AS upgrade failure on Wizard step 25 - Patching/Migrating Oracle database on the primary database server. When we checked the upgrade logs we saw the below failure;\n\r\n> run output mkdir /opt/mcp/db/oradata: No space left on device \r\n> run output\r\n> run output  ERROR: oraUpgradeTool.pl Terminated at  =>  Wed May 30 01:12:57 200\r\n18\r\nFails to check the current db\n\r\n/opt partition size had reached the %100. The customer asked to rollback but\r\nthere was no 10g oracle load which is necessary for the rollback operation.\r\nIn addition, we suspected that if we try to perform the rollback, it will also be failed because of the disk space issue. In the lights of that, we decide to proceed our procedure to recover this with the customer\'s confirmation.\n\r\nIn addition to this procedure, we tried to remove the unnecessary files under /opt directory to reduce possible errors for the next steps. For instance, there was an \"openv\" file which occupies 1.1G under /opt directory.\n\r\nHere is the procedure; \n\r\n1-	Uninstall oracle from primary\r\n2-	Install Oracle to primary DB\r\n3-	Restore from the latest DB backup taken prior to the upgrade\n\r\nSince the wizard state became PREPARE_DB after performing the restore operation we connected the database and changed the wizard state as UPGRADE_DB_SM manually. Our next step was to continue with the UPGRADE_DB_SM but Jullian said that he needs to Susana\'s confirmation for this action. We will continue after confirmation and I will keep the customer updated via follow up case:180530-134081.','null'),(81,'Omer KIRCALI','AS-OAM','2018-05-19','180519-132503','Kandy Business Solutions',' Peter Meloney paged me about call park issue on the pause point stage of the upgrade.\n\r\n The upgrade is from 18.0.28.2 to 19.0.20.4\n\r\n I informed him that the issue is callp releated. \n\r\n After callp team is involved , I dropped from the call.','null'),(82,'Omer KIRCALI','AS-OAM','2018-05-15','180515-131518','Alphawest Services P/L (Carr)','Leo Liao contacted with IMM GPS due to the outage on the IMM server. IMM is running on an ple4 host server. \n\r\nAfter they noticed that also the ple4 host server was unreachable(cannot reach with ssh) they contacted with us(OAM GPS).\n\r\nWe connected to the site and after investigated the console logs we noticed a kernel panic that is;\n\r\niTCO_wdt: unable to reset NO_REBOOT flag, device disabled by hardware/BIOS \r\nACPI: I/O resource 0000:00:1f.3 [0x2000-0x201f] conflicts with ACPI region SMBI [0x2000-0x200f] \r\nBUG: unable to handle kernel NULL pointer dereference at 0000000000000003 \r\nKernel panic - not syncing: Fatal exception in interrupt\n\r\nWe recommanded an hard reboot to host server to customer. After that the host and IMM servers became reachable. \n\r\nWe droped from the issue as OAM GPS due to host server was became up.\n\r\nIMM GPS continued the investigation.','null'),(83,'Feridun Bircan SUBASI (NETAS External)','AS-OAM','2018-05-04','180227-110867','U.S. Air Force Falcon AFB','John Kisner paged me about Avaya phone failing after JAR application. When I checked the JAR files I realized that these JAR files related to the call processing and routed him to callp team.','null'),(84,'Omer KIRCALI','AS-OAM','2018-04-20','180420-127599','University of Texas - Austin','Description;\n\r\nUpgrade path >> 17.0.32.0 -->> 19.0.20.2\n\n\r\nPeter Meloney paged me about they are performing an upgrade with above path and while they ore on the Pause Point stage , They faced with consultative transfer one way voice problem.\n\r\nThe scenario is;\n\r\n-A calls B and call is established.\n\r\n-B calls C and call is established without problem.\n\r\n-B consult transfer the C to A. After transferring A cannot hear any voice.\n\r\nI stated that the problem is Callp releated and I asked him to collect Sesm trace from sesm server and shared the procedure.\n\r\nAfter callp team is connect the site , I dropped from the site.\n\r\nThe upgrade was completed successfully.\n\r\nInvestigation is going on with 180420-127599.\n\r\nRegards\r\nÖmer Kırcalı\r\nA2 OAM GPS','null'),(85,'Omer KIRCALI','AS-OAM','2018-04-18','180416-126798','NEW YORK PRESBYTERIAN','Description \n\r\n Mark Zattiero from ER send me instant message from trillian about there was outage occurs on the HOST server 2 when eth0 is enabled.\n\r\n He stated that ,eth0 is disabled and there was no service impact. Only redundacy lost.\n\r\nActions:\n\r\n I looked at the server logs and and also ifconfig outputs for both Host servers (1-2).\n\r\n There was no issue seen from AS perspective and I recommended ER to ask the issue to ERS team to cover. After that I disconnected from the site.','null'),(86,'Omer KIRCALI','AS-OAM','2018-04-18','180418-127138','','Customer Load: MCP_19.0.4.0\n\r\nDescription:\n\r\n Mark Zattiero from ER paged me about SESM3_0 unit is stucked on the synchronizing stage. Mark also state that not only SESM3_0 is affected but also SESM4_1.\n\r\n Mark also stated that customer apperently had some provisioning corruption, so they needed re-sync. After the restart it stuck on the synchronizing stage.\n\r\nActions:\n\r\n I killed the SESM3_0 while it is on the synchronizing stage and started again. There was no change.\n\r\n I worked with Callp team and saw that this issue is in their scope. They are continuing with the investigation.','null'),(87,'Emre OVA (NETAS External)','AS-OAM','2018-04-15','180415-126645','SONUS NETWORKS','Cesar Martinez from the Nuvia Team had called ER to request IMM assistance for the Culpepper site last night. IMM2 was down and not in service at Culpepper site. In this regard, IMM GPS Sevgi found that it is a IA/RMS host server issue and needs to be investigated by OAM GPS. In other words, due to the fact that IMM Host Server is down, IMM2 guest was not in service. \n\r\nAt this point, I had been paged out by ER (John Kisner) and connected the site for investigation. Customer stated that IA-RMS host server restarted itself unexpectedly and couldnt be booted properly. In order to investigate the console logs, I had requested to perform one more power-cycle on this IA-RMS and share related console logs. As I requested, customer performed power-cycle and attached the server console logs into the case. (180415-126645)\n\r\nAccording to the console log investigation, its found that the server booting process is being stucked due to the following fatal exceptions:\n\r\niTCO_wdt: unable to reset NO_REBOOT flag, device disabled by hardware/BIOS\r\nACPI: I/O resource 0000:00:1f.3 [0x2000-0x201f] conflicts with ACPI region SMBI [0x2000-0x200f]\r\nBUG: unable to handle kernel NULL pointer dereference at 0000000000000003\r\nKernel panic - not syncing: Fatal exception in interrupt\n\r\nApart from this, several reboot attempt had been tried but booting process was staying stucked at the same fatal exceptions.\n\r\nIn this respect, as a first action I suggested to check the current BIOS configurations of this IA-RMS and shared related system recovery mop with customer and ER. In this way, some incorrect BIOS configurations had been detected. Even though the fact that we corrected them and rebooted the server again, we were still receiving the same failures on the console logs. In parallel, I was investigating the reason for why these fatal exceptions are raising on server console logs. Accordingly, after I took a look at the opened tickets in official redhat customer portal website, I found that hardware issues cause this type of problems (such as a failed RAM cell, damaged processor, etc.) \n\r\nAs a result, when we consider all of these investigation results, we suggested to replace this problematic IA-RMS hardware and agreed with customer. So, when the problematic hardware is replaced, ple4 OS installation will be required for the replaced host server. I provided the required system recovery mop and referred the chapter names which are required to be followed for completing this operation. After the OS installation is completed on this hardware, IMM guest creation and installation/restoration will be required again. So, customer stated that this replacement will be done probably tonight. Accordingly, ER updated the case and stated that if any assistance is needed that ER can be contacted, ER will engage GPS as needed.','null'),(88,'Emre OVA (NETAS External)','AS-OAM','2018-04-14','180414-126619','BT TELECOMUNICACIONES S A','Upgrade Path: FROM 17.0.31.5 TO 19.0.4.1\n\r\nDavid Bartlett from Software Delivery paged me out in order to report that Step 46 Post System Setup screen is failed due to the fact that one of the BCP blades are not reachable. \n\r\nI connected the site via GTS VM servers and started to investigate the issue. Accordingly, one of the BCP blades were failed and the following error was observed on the upgrade wizard: \"Unable to connect daemon.\" \n\r\nOnly one BCP instance was deployed on this blade. Additionally, SWD stated that the same issue had been observed during the Step 39 Upgrading Secondary Network Element Instances and he skipped this screen in debug mode due to the fact that all other NEs were successfully upgraded to 19.0.4.1 load... I checked the current status of the BCP5_0 on the MCP GUI and it was in Configured - Down - Unavailable status. \n\r\nApart from this, in spite of the unable to connect daemon error, I was able to connect this BCP blade via SSH. On the other hand, I was able to ping this blade from other BCP blades successfully..\n\r\n[root@NMAD01BCT02-bcp4 ~]# ping 10.143.64.23\r\nPING 10.143.64.23 (10.143.64.23) 56(84) bytes of data.\r\n64 bytes from 10.143.64.23: icmp_seq=1 ttl=64 time=0.020 ms\r\n64 bytes from 10.143.64.23: icmp_seq=2 ttl=64 time=0.021 ms\r\n64 bytes from 10.143.64.23: icmp_seq=3 ttl=64 time=0.018 ms\r\n64 bytes from 10.143.64.23: icmp_seq=4 ttl=64 time=0.014 ms\r\n64 bytes from 10.143.64.23: icmp_seq=5 ttl=64 time=0.016 ms\n\r\nOther than these, when I want to run \"neinit -p\" on this blade, the following error was returned:\n\r\n[root@NMAD01BCT02-bcp5 patch_logs]# neinit -p\r\nnedclient: Error! Could not connect to localhost:4891 or localhost:4890: Connection refused\r\n/opt/mcp/ned/bin/nedquery.pl: Error connecting to localhost:4890.\n\r\nAs a next action, I tried to restart ned by running \"neinit restart\" command and restarted wizard but it didn\'t work.\n\r\nThen, I tried to reboot this blade but it didn\'t help to resolve the issue.\n\r\nIn parallel, I checked the current platform level of this blade and confirmed that this blade had been successfully patched to latest platform level. (19.0.6)\n\r\n[root@NMAD01BCT02-bcp5 patch_logs]# mcpRelease.pl\n\r\n        *** MCP Platform Release ***\n\r\nSystem Type:     mcp_bcp_linux_ple3\r\nRelease Level:   19.0.6 (via patching)\r\nHardware Env:    IBM-HS21\n\r\nI suspected that a hardware specific error might be occured after that platform patching is completed and server reboot operation.. For this reason, I started to investigate server logs .. \n\r\nFirst of all, I checked the patch platform script and confirmed that this scipt had been completed at 23:19\n\r\nApplying patch 19.0.5.p-1.\n\r\n 23:19:17\r\n   Script 19.0.5.p-1 completed successfully\n\r\nPatch files applied successfully.\r\nReboot required flag is found.\r\n 23:19:23 Sending reboot commmand...\n\r\nIn parallel, I checked the server logs (var/log/messages) and seen the following critical hardware issues at 23:19:\n\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: PE ERROR:  Internal error: port 45998 on IP 62.7.41.235 already in use when adding checkpointed connection\n\n\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: sd 0:0:1:0: SCSI error: return code = 0x08000002\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: sdb: Current: sense key: Hardware Error\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel:    ASC=0x42 ASCQ=0x0\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel:\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: end_request: I/O error, dev sdb, sector 0\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: Buffer I/O error on device sdb, logical block 0\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: Buffer I/O error on device sdb, logical block 1\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: Buffer I/O error on device sdb, logical block 2\r\nApr 13 23:19:17 NMAD01BCT02-bcp5 kernel: Buffer I/O error on device sdb, logical block 3\r\nApr 13 23:19:23 NMAD01BCT02-bcp5 shutdown[8343]: shutting down for system reboot\n\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310228) ERROR: Heartbeat to packet engine has failed. Forcing Reload\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310339) INFO: Reload done...\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310372) INFO: REACTOR event loop has ended\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310395) INFO: < ------------ >\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310412) INFO: MEDIADB 10.1a started.\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310505) INFO: Reading configFile (/admin/mediadb.config) ...\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310533) INFO: ConfigFile: Using /admin/defaultSignals for mediadir\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310552) INFO: ConfigFile: Using 0 for debuglevel\r\nApr 13 23:57:18 NMAD01BCT02-bcp5 /opt/mcp/mediaportal/bin/mediadb[2578]: (23:57:18.310813) WARNING: Received an error from the packet engine constructor: 92, Protocol not available\n\r\nUnder these circumstances, since we observed the above hardware issues for this blade and SWD has a short time for completing this upgrade, we have skipped the post system setup screen in debug mode.  In this regard, customer will RMA for this blade.. \n\r\nI will keep the customer update via created follow-up case: 180414-126619','null'),(89,'Senem Gultekin','AS-OAM','2018-04-03','180403-124631','York University','Description:\n\r\nSWD performed A2 upgrade from 18.0.26.5 to 18.0.31.0 at York University (TOROONAUDS0).  \r\nAfter the upgrade, SWD faced OPI and OMI certificate failures while importing from CMT. This is one of the post upgrade steps\n\r\nError:\r\nkeytool -import -alias mcsopi -keystore /opt/jakarta/dist/tomcat/conf/pkclientTruststore -storepass sesmkey -noprompt -file /tmp/OPIcert.pem\n\r\nkeytool error: java.lang.Exception: Input not an X.509 certificate\n\r\nThis issue shouldn\'t be handled over a pager. However, since it was my office hours Ive worked with SWD to solve the issue.  \n\r\nSolution:\n\r\n	Checked the server and service IPs for the site, they were correct.\r\n	Followed document NN10440-450, the steps he followed were correct as well.\r\n	Checked the created OPIcert.pem and OMIcert.pem files, they were empty and size of 0. \r\n	These files are created in the previous steps. Reviewed the scripts and realized that there was a space between - and BEGIN. \r\nFollowing creates 0 byte file:\r\necho | openssl s_client -connect 10.14.1.69:8443 2>&1 | sed -ne \'/- BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p\' > /tmp/OPIcert.pem\n\r\nCorrect Syntax:\r\necho | openssl s_client -connect 10.14.1.69:8443 2>&1 | sed -ne \'/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p\' > /tmp/OPIcert.pem\n\r\n	After using the correct syntax, SWD was able to create proper certificate files.\r\n	With the correct certificates import was completed as well.\r\n	Issue solved.\n\r\nRCA:\r\nEven though the document has the correct syntax, these are long scripts and seems like a space was added while pasting it to the server.','null'),(90,'Cigdem Vural','AS-OAM','2018-03-30','180330-124333','Baylor University','Upgrade path: 19.0.1.1 to 19.0.20.2\r\nFailed screen: Prepare DB with message \"\"An error occured during stopping DB monitor of [].\"\n\r\nChecked the log files and the error was on cleanupReplication.pl script could not complete successfully.\r\nManually run the script and it passed. \r\nThen save&exit from the wizard and relaunch it.\r\nScreen passed and SWD continud with next steps.','null'),(91,'Emre OVA (NETAS External)','AS-OAM','2018-03-23','180323-123297','Genband US LLC','Current Customer Load: 17.0.12.X\n\r\nBrent Combs from ER paged me out to report that SM 0 is down.. \r\nI\'ve connected the site and started to investigate the issue. When I logged into the MCP GUI, SM 1 status was Online-Up-Active whereas the SM 0 status was Offline-Configured-Down... Apart from this, there were too many alarms on the active SM 1 unit (Out of sync alarms, write failures, etc.)\n\r\nIn order to make the unavailable SM 0 unit hotstandby, I\'ve connected the EMServer1 via SSH and tried to execute ./smStart command.. It was worked to start SM 0 but admin state of this unit was still in Configured state. In other words, SM 0 was working in Configured-Hotstandby status. In order to recover this unit from Configured state, I\'ve killed the process ID of SM 0 on the EMServer1 and restarted the NED but it was still stucked in Configured state. After these taken actions, I\'ve executed smUndeploy and smDeploy on the EMServer1 but it didn\'t work also. SM 0 was only able to start as Configured-Hotstandby...\n\r\nIn paralell, I was investigating the major alarms on the active SM unit (SM 1). I\'ve connected the EMServer2 and checked the disk space.. Accordingly, /var/mcp usage was %100. For this reason, there were Write failure alarms on the active SM unit. In this regard, I\'ve realized that /var/mcp/run/MCP17_0/SM_1/work directory occupies totally 30GB disk usage.. There were too many java.hprof files under this directory.. I\'ve deleted all of them to decrease the disk usage from critical levels to normal levels. Then, it\'s seen that /var/mcp usage has been decreased to %46 level..\n\r\nAfter the alarms are cleared on active SM unit (SM 1), I\'ve focused on the hotstandby unit which is stucked in Configured admin state...  After the active SM alarms have been cleared, this time I was able to undeploy-deploy and start the SM 0 unit successfully.. \n\r\nCurrently, both SM units are working as up and running without any problem...\n\r\nAfter the SM-related issues are recovered, I\'ve dropped from the bridge..','null'),(92,'Cigdem Vural','AS-OAM','2018-03-02','180302-111377','Verizon Communications','Verizon AS load: 14.1.10.3\r\nGVM load: GVMB 2.0\n\r\nER called me telling resync alarms from GVM is not working.\r\nFirst of all that is not a pager issue and a case work, but since it is day time I did work on the problem with GVM GPS.\n\r\nThe GUI access via VPN is not stable.\r\nI checked MCP GUI for GVM configuration and Log Northbound Server Feed Rules IEMS Server was missing. That is a step that should be followed from GVM documentation not an AS responsibility.\n\r\n1 Launch the MCP System Management Console. \r\n2 Navigate to  System Manager ==>Log \r\nProcessing ==>Log North Bound Server Feed Rules.\r\nAnd then at system Manager Log SNMP Push rules I added the rule\r\nThen log rules maintenance tab I enabled SNMP Push rule.\n\n\r\nThen we lost MCP GUI, and our EM VMs were up about 513 days.\r\nI rebooted both servers.\n\r\nGVM GUI on our side via VPN could not be launched so requested customer to run the resync alarm on their side and it worked.\n\r\nIt is a very old load that we do not provide RCA but here the issue is missing configuration for GVM and missing certificates for GVM. Kyle had renew the certificate at the beginning then we completed the configuration.\n\r\nThen I dropped','null'),(93,'Bill Picardi','AS-OAM','2018-02-25','180225-110390','BERMUDA TELEPHONE COMPANY LTD','During pre checks for an A2 MR, the team encounter problems on the EMServers with failures to write to disk, and out of disk space. The EMServer1 was out of disk space within /var/mcp, service alarms were seen, cpu load was extremely high on both EMS server due to DB processes.  The SM0 (active), PROV2 (running), and the primary database reside on that partition of this server.\r\n  Some files were identified and removed to clear disk space.  The SM and PROV service processes held deleted files open, which continued to consume large amounts of disk space.  These services were restarted to release the files and free disk space. Due to the filled partitions, the oracle database processes resulted in high CPU loads on the EMS servers.  The secondary and primary databases were restarted.  Monitor processes upon the server and the monitor within the MCP GUI were stopped and started.  The eth1 network interface on EMServer1 reported the link was down, but this cleared on it own.','null'),(94,'Cigdem Vural','AS-OAM','2018-02-27','TBD','Smart city','Upgrade path: 19.0.1.1 to 19.0.4.2\n\r\nUpgrade failed at Patching primary servers operating systems at DBServer1.\r\nFailure message:\n\r\n[*P-Info*]    Executing command: \"/bin/cp -f /var/mcp/os/install/workdirs/temp/resources/tar.bz2/19.0_fileset_8/desktopEnv.pl /o\r\npt/mcp/.support_pkgs/gnome/desktopEnv.pl 2>&1\"\r\n[*P-Error*]    [*P-Error*]    Retcode is     >>0x0100<<\r\n[*P-Error*]    Signal part is >>0x00<<\r\n[*P-Error*]    Main part is   >>0x01<<\r\n[*P-Error*]    System return strings:\r\n[*P-Error*]       >>/bin/cp: cannot create regular file `/opt/mcp/.support_pkgs/gnome/desktopEnv.pl\': No such file or directory<\r\n<\r\n[*P-Warning*] Failure force copying file \"/var/mcp/os/install/workdirs/temp/resources/tar.bz2/19.0_fileset_8/desktopEnv.pl\" to \"\r\n/opt/mcp/.support_pkgs/gnome/desktopEnv.pl\".\r\n[*P-Info*] --------------------------------------------------------------------------------\r\n[*P-Error*] \r\n[*P-Error*] Patch part #119 exited with an error (2).\r\n[*P-Error*]  \r\n[*P-Error*] **********************************************************************\r\nChecked for the /opt/mcp/.support_pkgs and there was no gnome directory here.\r\ngnome.tar.gz file was at /var/mcp/bulk directory. It should be transferred to that directory but seems not transferred and upgrade failed.\n\r\nMove the gnome.tar.gz from /var/mcp/bulk to the /opt/mcp/.support_pkgs manually and then untar at that directory. So we have the gnome directory.\n\r\nThen click on Retry button at Wizard and screen passed successfully.','null'),(95,'Emre OVA (NETAS External)','AS-OAM','2018-02-16','180216-570628','Claro (Puerto Rico)','Tony paged me out last time and informed me that the following error has been observed in last step of upgrade (Upgrade report) \n\r\n\"Unexpected error occurred\"\n\r\nUpgrade report was not displayed on the wizard GUI.\n\r\nI\'ve tried to re-launch the upgrade wizard but the result was same. Even though the fact that it\'s a post-upgrade step and I\'ve checked the local upgrade wizard logs and found the following exceptions:\n\r\n2018-02-16 07:11:10,875 DEBUG MainController - Starting screen from step : UPGRADE REPORT - null\r\n2018-02-16 07:11:10,884 ERROR UpgradeReportFileManagerImpl - Error occurred during reading upgrade report : \r\njava.lang.NullPointerException: null\r\n	at com.nortelnetworks.mcp.client.upgrade.report.impl.UpgradeReportFileManagerImpl$10.handle(UpgradeReportFileManagerImpl.java:216) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.report.impl.UpgradeReportFileManagerImpl.getReportText(UpgradeReportFileManagerImpl.java:238) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.control.UpgradeReportController.start(UpgradeReportController.java:57) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.share.ui.control.PanelController.start(PanelController.java:142) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.share.ui.control.MainController$4.run(MainController.java:587) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.base.ui.swing.SwingExecutor$Runner.run(SwingExecutor.java:56) [wizardws.jar:na]\r\n	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.8.0_144]\r\n	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.8.0_144]\r\n	at java.lang.Thread.run(Unknown Source) [na:1.8.0_144]\n\r\nSince it\'s a post-upgrade step and AS upgrade has been completed successfully, I\'ve suggested to work on this issue as a case working. So, we will investigate the detected exceptions with design support team and provide an update to customer at the beginning of the next week.','null'),(96,'Emre OVA (NETAS External)','AS-OAM','2018-02-16','180216-570625','Claro (Puerto Rico)','Tony Pittman from SWD paged me again and he stated that this time patching operating systems of the secondary servers screen has failed since the status of all guests seem as shut off. (Secondary EMServer, Secondary DBServer, PAServer, SESMServer2)\n\r\nI\'ve connected the site again and checked the current status of guests on the host server.\n\r\n- I have connected the HostServer2 via SSH. Host server was up and running properly.\r\n- Run virsh list --all\r\n- Confirmed that the status of all guests are shut off.\r\n- I have run the following command to activate the all guests again: \r\nvirsh start guestname\r\n- After the all servers came into running status, we were able to ping them and connect via SSH.\r\n- Then, all patching operations had been completed successfully. So, SWD could pass the screen.\n\r\nI suspect that the autorestart parameter might be closed (unclicked) for all guests and they might be remained in shut off status after rebooting the HostServer2.\n\r\nFollow-up investigation will be driven under this case: 180216-570625','null'),(97,'Emre OVA (NETAS External)','AS-OAM','2018-02-16','180216-570625','Claro (Puerto Rico)','UPGRADE PATH (FROM/TO): 18.0.24.X - 18.0.31.X\n\r\nTony paged me again and stated that the patching operating systems of the Secondary Host server screen is failed due to the following reason:\n\r\nUnable to connect daemon.\n\r\nI\'ve connected the site again and tried to reach HostServer2, but this server was not replying to our ping request. In other words, destination host was unreachable.\n\r\nAt this point, I\'ve checked the server type of this host server and confirmed that it\'s a Hypervisor-Sandybridge-ATCA. Since the hardware environment is Sandybridge-ATCA, I wanted to learn frame, shelf, slot and sub-slot info in order to reboot this server via NDM but customer was not know this. They started to check it from specbook but couldn\'t find for a long time. Then customer stated that they know the place of this blade but don\'t know the slot, sub slot number. In parallel, I\'ve checked the upgrade logs and observed that patching operation was completed successfully. \n\r\nCustomer was know the place of problematic blade since they labeled it before. In this way, they had took a picture from blade and send us via e-mail. We\'ve confirmed the frame shelf slot and sub-slot number of problematic blade and connected the NDM.\n\r\nThe following commands have been executed via NDM:\n\r\nha app-blade deactivate    \r\nha app-blade offline    \r\nha app-blade online    \r\nha app-blade activate    \n\r\nAfter performing the above commands, problematic blade had been rebooted successfully and this screen has passed.\n\r\nThe following case has been opened in order to provide RCA: 180216-570625','null'),(98,'Emre OVA (NETAS External)','AS-OAM','2018-02-16','180216-570597','Claro (Puerto Rico)','UPGRADE PATH (FROM/TO): 18.0.24.X - 18.0.31.X\n\r\nTony Pittman from SWD paged me out to report that Prepare DB screen is failed due to the following error:\n\r\n\"An error occured during stopping DB monitor of [].\"\n\r\nWhen I check the latest upgrade logs, I\'ve observed that the following errors:\n\n\r\nCleaning up primary DB replication\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20180215_231158_l2r8 | /opt/mcp/ned/bin/nedclient 10.2.69.78 4890\r\nCommand Output:\r\n> config ok\r\n> run ok 1\r\n> exited\n\r\nError occurred executing NE commands (see below):\n\r\nNE command exited with the value: 1\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20180215_231158_l2r8\r\nNot unlinking file\n\r\nIn this regards, SWD stated that there are network issues and it might be caused this problem. VPN connection was dropping during the upgrade frequently. (SWD was connected the site via FortiClient VPN.) \n\r\nAction taken\r\n---------------------\r\n1 - I have connected the site via GTS VM servers and started to investigate the logs. Accordingly, I have found that the cleanupReplication couldn\'t be executed successfully. Here you can find the detailed logs.. \n\r\nCleaning up primary DB replication\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20180215_231158_l2r8 | /opt/mcp/ned/bin/nedclient 10.2.69.78 4890\r\nCommand Output:\r\n> config ok\r\n> run ok 1\r\n> exited\n\r\nError occurred executing NE commands (see below):\n\r\nNE command exited with the value: 1\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20180215_231158_l2r8\r\nNot unlinking file\n\r\n> config ok\r\nNE command exited with the value: 1\r\n> exited\n\n\r\nFailed to cleanup replication on the server 10.2.69.78\n\r\n2 - I have connected the primary DB server via SSH.\r\n3 - I have tried to run cleanupReplication.pl manually and it\'s completed successfully.\r\n4 - After save and exit the wizard, we have relaunched again.\r\n5- So, Prepare DB screen has been completed successfully.\n\r\nFollow-up will be tracking under the following case: (180216-570597)','null'),(99,'Emre OVA (NETAS External)','AS-OAM','2018-02-14','180213-569648','Shaw Communications','UPGRADE PATH (FROM/TO): 18.0.26.1/19.0.3.0\n\r\nDonnell Williamson from SWD paged me out to report that Patching Operating Systems of the Primary Host Servers screen is failed due to the disk failure on HostServer5. All other host servers were patched successfully.\n\r\nI\'ve connected the site and run mcpSwRaid.pl -status to see that which disk partition is faulty.\n\n\r\n                ***** Software RAID-1 Devices *****       \n\r\n              |            |           |           |     Sync & Recovery      \r\n  MD Device   |            |  Member-0 |  Member-1 |      Speed Finish Done \r\nName  Size(MB)|   Usage    | Name  Flg | Name  Flg | Mode (MB/s) (min)  (%)  \r\n--------------+------------+-----------+-----------+--------------------------\r\nmd0      93.0 | /admin     | sda1  U   | sdb1  U   |  .    .      .     .    \r\nmd1     250.0 | /boot      | sda3  U   | sdb3  U   |  .    .      .     .    \r\nmd2    4095.0 | swap       | sda2  U   | sdb2  U   |  .    .      .     .    \r\nmd3    4095.0 | swap       | sda5  U   | sdb5  U   |  .    .      .     .    \r\nmd4  849943.9 | LVM        | sda6  (F) | sdb6  U   |  .    .      .     .    \n\n\r\nApart from this, disk checks of this host server was failed due to the same sda partition during the pre-upgrade steps yesterday. After reseating the sda disk, sda6 was came into UP status and working properly. So, all prep steps had been completed successfully. This time it seems that the same disk partition was failed during the Patching OS of the Primary Host Servers screen again.\n\r\nI\'ve checked the guests of this host server and result was as follows:\n\r\n[root@vpd6vhm11no ~]# virsh list --all\r\n Id    Name                           State\r\n----------------------------------------------------\r\n 4     MAS11Server                    running\r\n 5     MAS3Server                     running\r\n 6     MAS7Server                     running\n\r\nIn addition to this, I\'ve tried to patch platform manually by executing patchPlatform.pl script but it\'s failed due to the same disk failure..\n\r\n[root@vpd6vhm11no ~]# patchPlatform.pl \n\r\nScanning directory \"/root\"...\r\nScanning directory \"/var/mcp/os/install/images\"...\r\nScanning directory \"/var/mcp/media/patches\"...\n\r\nPatch Image Selection Menu\n\r\n(Action: Apply patches)\n\r\n   1) /var/mcp/media/patches/mcp_core_linux_ple4-19.0.7.patches.r-1.iso\r\n   2) Optical disc\r\n   3) Exit the program\n\r\nThis will install the following patch chain (all applicable\r\npatches).\n\r\n   18.0.7_u_19.0.1 -> 19.0.1.p-1      -> 19.0.2.p-1      -> 19.0.3.p-1      -> \r\n   19.0.4.p-1      -> 19.0.5.p-1      -> 19.0.6.p-1     \n\r\nThe following raid-1 devices are faulty:\r\n    \"md4\"\r\nFaulty software raid-1 device(s) must be correctly before patching this server.\n\r\nSince resync operation for LVM takes too long time (approximately 7-8 hours), SWD asked that can he pass this screen in debug mode.. At this point, I\'ve contacted to platform design team and confirmed that this operation can be done due to the fact that there are only 3 MAS Guests on this server. In this respect, SWD passed the problematic screen via debug mode. \n\r\nAs a result, after completing the upgrade steps, customer will order a new healthy disk and replace the problematic one with this healthy disk. Then, HostServer5 will be patched manually.','null'),(100,'Emre OVA (NETAS External)','AS-OAM','2018-02-13','180212-569608','SKY UK LIMITED','UPGRADE PATH (FROM/TO): 17.0.31.1/19.0.4.0 \n\r\nMeraz Aziz from SWD paged me to report that Prepare DB screen has failed due to the following error:\n\r\n\"An error occured during stopping DB monitor of [].\"\n\r\nI\'ve connected the site via ER/GTS VM servers and started to investigate the issue. When I check the latest upgrade logs, it was looking that screen has failed while trying to run cleanupReplication.pl script. \n\n\r\n#####################################################\r\nCleaning up primary DB replication \r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234834_m_90 | /opt/mcp/ned/bin/nedclient 10.234.207.27 4890 \r\nCommand Output: \r\n> config ok \r\n> run ok 1 \r\n> exited \n\r\nError occurred executing NE commands (see below): \n\r\nNE command exited with the value: 1 \r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234834_m_90 \r\nNot unlinking file \r\n#####################################################\n\n\r\nActions taken\r\n------------------------------\n\r\n1 - I tried to run cleanupReplication.pl script on primary EM/DB server but script was failed because of the following reason (deadlock):\n\r\nInvoking quiecseRepDB method\r\n quiecseRepDB completed\r\nCleaning up PRIMARY DB replication\r\nDROP REPSITE\r\nTRUNCATE REP QUEUE\r\nDROP REPGROUP MCSDBSCHEMAREPGROUP\r\n   DECLARE\r\n*\r\nERROR at line 1:\r\nORA-00060: deadlock detected while waiting for resource\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6470\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6016\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 7027\r\nORA-06512: at \"SYS.DBMS_REPCAT_MAS\", line 2695\r\nORA-06512: at \"SYS.DBMS_REPCAT\", line 635\r\nORA-06512: at line 70\n\n\r\nsqlplus  -S fails\r\nCleanup Replication failed on PRIMARY Database.  Please resolve the problem and then try again\n\r\n2- Since the both EM/DB servers are up for a long time (approximately 300 days), I performed a reboot operation and tried to relaunch upgrade wizard, but the result was same.\n\r\n3- I have opened mcpdb monitor on SM GUI and seen that neither mcpdb_0 nor mcpdb_1 were able to start monitoring. At the moment of clicking \"start monitor\" button, a pop-up appears and show \"unexpected error\" \n\r\n4- I have stopped/started database but upgrade wizard was still showing the same error.\n\r\n5- Restart NED with \"neinit restart\" command but it was still failing..\n\r\n6- As a last operation, I have connected the secondary EM/DB server and tried to run cleanupReplication.pl and this time script has been successfully executed and replication is dropped between two databases. \n\r\nAfter passing this screen, Meraz continued to perform rest of the upgrade steps and customer upgrade has been successfully completed.','null'),(101,'Yunus Ozturk','AS-OAM','2018-02-10','180206-568589','SINGTEL OPTUS','Problem Description:\r\n====================\n\r\nER paged out GPS and informed us that customer has requested an update for the ongoing provisioning outage on the customer site since last week.\n\r\nActions Taken:\r\n===============\n\r\nWe have updated the corresponding case as follows and explained the current situation;\n\r\nWe have worked on this issue during the entire week with ERS GPS Team. Based on the initial investigations, we have noticed high ping delays between the SM units and ERS units. Therefore, we have asked ERS GPS Team to investigate this issue first. As per ERS GPS Team\'s recommendation, customer has made some port/link changes on the ERS, but nothing changed.. The problem still exists. \n\r\nIn paralel, we have also investigated the A2 side. Due to the communications issues, we are seeing high amount of spool files created on all A2 network elements. Since they are not able to communicate with the SM units, the number of spool files on all A2 units always increasing and SM units cannot handle and process these spool files. This results with MCP GUI gray out issues and unable to manage the network elements problems. This issue may be recovered with the following workaround solution which was also applied on 1 week before;\n\r\n1-) Run \"neinit -autorestart=off\" on all servers \r\n2-)Stop all the NE\'s in the system at the same time with neStop.pl script for all instances running on all servers on the sytem (SM, SESM, BCP, IPCM, etc (one at a time))\r\n3-) Clean the /var/mcp/spool directories of all the servers with the following command\r\nfind /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f -exec rm -f {} \\; ; find /var/mcp/spool/tmom -type f -exec rm -f {} \\; ; echo DONE\r\n4-) Start the instances starting with SM, SESM, BCP, IPCM, etc (one at a time) with neStart.pl script\r\n5-) Run \"neinit -autorestart=on\" on all servers\n\r\nAs per our investigations on Friday, we were not able to make none of the SM units to Active State. It was always getting stuck at Activating status.. Therefore, we were not able to launch the MCP GUI. This situation is actually the results of the communication problems on the site.\n\r\nTo recover the system from this situation, we again need to apply the above workaround solution to be able to start up the SM units as we could not manage to make them Active on last week. This should be performed during MTCE window as the entire system will go down. \n\r\nAdditionally, during our investigations, we noticed that on SM0 unit, one of the eth interfaces (could not remember which one was it) has half duplex and 10Mbps configuration. It should actually be Full Duplex and 1000Mbps like the other interface. Customer needs to check that. \n\r\nAnd on SM1 unit, one of the interfaces is completely down. There was no detected link on it. It can be a cabling or port problem. So, SM1 unit is working with 1 interface. Customer may need to check that. \n\r\nSo, at this point, since we could not get the SM units up and running, it appears that we should apply the solution above again on the site. If that still fails, we need to get Design Teams involved to work on this problem. In paralel, we still need to know if the communication problems are resolved. Because having high ping delays between the ERS and SM units will cause this issue to re-happen again.','null'),(102,'Yunus Ozturk','AS-OAM','2018-02-08','TBD','Midcontinent Communications','Problem Description:\r\n====================\n\r\nSWD is doing an upgrade and failed at Step 29 at Host server OS patching as HOS1S1 is not reachable.\n\r\nUpgrade path: 17.0.31.5/19.0.4.2\r\nHardware Env: SandyBridge-Emerson-ATCA-7370\n\r\nActions Taken:\r\n===============\n\r\nLogged into the site and HOS1S1 was not reachable and ping was unsuccessful.\r\nConnected via NDM and asked for the blade slot number of the HOS1S1, but customer did not know. Asked for the specbook and found out the required information from the specbook. \n\r\nWe found out that HOS1S1 is blade 0 0 5 0.\n\r\nWe have performed the following steps through NDM to reboot the corresponding server;\n\r\nha app-blade deactivate 0 0 5 0\r\nha app-blade offline 0 0 5 0\r\nha app-blade online 0 0 5 0\r\nha app-blade activate 0 0 5 0\n\r\nAfter that server become up and reachable.\n\r\nSWD could go on with next steps.','null'),(103,'Yunus Ozturk','AS-OAM','2018-02-07','180206-568589','SINGTEL OPTUS','Problem Description:\r\n====================\n\r\nThis is an ongoing issue. SM1\'s applications are down. SM0 active. SM GUI is \'Grayed-out\'. Provisioning is down\n\r\nActions Taken:\r\n==============\n\r\nAccessed the site and noticed that MCP GUI was grayed out and SM1 unit was not reachable with SSH connection.\n\r\nAsked the customer to hard reboot the SM1 unit and it recovered the SSH connectivity.\n\r\nFor the MCP GUI gray out issue, we have recommended the customer to apply the following steps;\n\r\n1-) Run \"neinit -autorestart=off\" on all servers \r\n2-)Stop all the NE\'s in the system at the same time with neStop.pl script for all instances running on all servers on the sytem (SM, SESM, BCP, IPCM, etc (one at a time))\r\n3-) Clean the /var/mcp/spool directories of all the servers with the following command\r\nfind /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f -exec rm -f {} \\; ; find /var/mcp/spool/tmom -type f -exec rm -f {} \\; ; echo DONE\r\n4-) Start the instances starting with SM, SESM, BCP, IPCM, etc (one at a time) with neStart.pl script\r\n5-) Run \"neinit -autorestart=on\" on all servers\n\r\nCustomer wanted to apply the steps during MTC window as the actions above will cause a full outage on the site.. \n\r\nDuring business hours, we have worked with the ERS GPS Team and they came up with the following update;\n\r\nERS GPS compared the 8600 config in both sites and found no issues. They are thinking that issue is related to the capacity of the IO modules. Their suggestion is to move at least one of the links of the SM units to RS module in Slot 7 and check the results. They also observed the ping delay issues for the devices connected to Slot 7. We can suggest to the customer to replace one of the 8608GBE cards with an R or RS module if they have spares in hand. 8608 GBE card has 8 ports and all the ports are up and working.\n\r\nIssue still exists.. Working with ERS GPS Team..','null'),(104,'Cigdem Vural','AS-OAM','2018-02-04','180204-662891','Optus(Alphawest Services P/L (Carr) )','Tom paged and told customer has an issue with SM_1.\r\nSM_1 went down unavailable state and customer did stop it. That is the site we worked at this week related with spool issues.\n\r\nI did try to start SM_1 but it tried to be ACTIVE and stayed at ACTIVATING state so I did kill and stop SM_1.\n\r\nSM_0 and all other instances are working but SM_1 could not understand the other SM_0 is UP and ACTIVE. \n\r\nAt week time we also asked if customer did any change on network or on ERS and could not get any answer.\n\r\nThe earlier and today\'s issue seems related with network and NEs are losing the network connection between each other and causing system to be down.\n\r\nSalam, the customer was not reachable so I requested oss and work logs  for both SM instances and told ER that we will work with design team on the exceptions.\r\nAnd customer needs to tell about their network maintenance as well.\n\r\nAction plan:\n\r\n1- Customer to attach the oss and work logs for SM instances to the case\r\n2- Customer to tell about their network maintenance( ERS or any other network activity)\r\n3- GPS/Design check for the logs\r\n4- GPS can contact with customer at 1pm Istanbul/9pm Customer time with an action plan on SM_1 instance.\n\r\nThen dropped off.','null'),(105,'Cigdem Vural','AS-OAM','2018-02-02','180201-662611','Optus(Alphawest Services P/L (Carr) )','That was the planned maintenance from yesterday.\r\nAction plan was to shut down all BCPs and start SM after that.\n\r\nWe engaged with ER and SM1 was active and GUI could be launched. Customer only shut down the BCP blades and not the HT LAngley BCPs.\n\r\nSo left the blades stopped and worked on other instances.\r\nAll instances could be ACTIVE/HOT STANDBY on the MCP GUI.\r\nRestarted HT Langley BCPs as well since HAL was stayed at stopped yesterday night.\n\r\nThen pointed all blade BCPs to FPM from SM.\n\r\nCustomer started blades one by one and all come up.\n\r\nEverything is UP and running, no issue left.\n\r\nThe cause of the problem seems related with BCP blades.','null'),(106,'Cigdem Vural','AS-OAM','2018-02-02','180202-662708','Eastlink','SWD perfoming MAS upgrade for Eastlink and 2 of the MAS servers failed at platformpatch step.\n\r\nFailure was as below \n\r\nStopping. \r\nError backing up save_dynamic_cfg. \n\n\r\nError (2) executing patch application targets  \r\n[*PD-Error*]    Command: \"cd /var/mcp/os/install/mnt/18.0.3.p-1;./mcp_core_linux_ple2-18.0.3.p-1.pl -install img=mcp_core_linux_ple2-19.0.4.patches.r-1 2>&1\"  \r\n[*PD-Error*]    Error applying patch \"18.0.3.p-1\"  \r\n[*PD-Error*]    Retcode is     >>0x0200<<  \r\n[*PD-Error*]    Signal part is >>0x00<<  \r\n[*PD-Error*]    Main part is   >>0x02<<  \r\n[*PD-Error*]    System return strings:  \r\n[*PD-Error*]        Capture not attempted.   \n\n\r\nAnd the file logrotate was under cron.hourly and not under cron.daily.\r\nI moved the logrotate file to correct directory and re-run patchplatform, this time it passed','null'),(107,'Cigdem Vural','AS-OAM','2018-02-01','180201-662558','Eastlink','Upgrade patch: 17.0.31.5 to 19.0.4.1\r\nEastlink upgrade went on and failed at patching secondary FPM server.\r\nLogged in and saw that OS patching is going on with FPM and server itself but wizard could not detect.\n\r\nWait for FPM to be patched and reboot completed.\r\nAfter be sure it is patched to the expected platform, I clicked retry on wizard and it could go on.\n\r\nThen I waited for whole upgrade to be ended with SWD and it successfully completed.','null'),(108,'Cigdem Vural','AS-OAM','2018-02-01','180201-662611 & 180201-662655','Optus(Alphawest Services P/L (Carr) )','ER paged by telling SM instance stuck at ACTIVATING state and not coming up.\n\r\nLogged into the site and checked for the system.\r\nEMServer2 was down by customer manually. So I asked him to leave it as down until we could make SM_0 active. \n\r\nSM_0 stuck at Activating state with following exception on the logs:\n\r\njava.net.SocketException: Socket Closed\r\n                           at java.net.PlainSocketImpl.setOption(Unknown Source)\r\n                           at java.net.Socket.setSoLinger(Unknown Source)\r\n                           at com.sun.net.ssl.internal.ssl.BaseSSLSocketImpl.setSoLinger(Unknown Source)\r\n                           at com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession.closeSocket0(TCPRunnableSession.java:324)\r\n                           at com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession.closeSocket(TCPRunnableSession.java:312)\r\n                           at com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession$MessengerRunnable.run(TCPRunnableSession.java:595)\r\n                           at java.lang.Thread.run(Unknown Source)\r\n                   java.net.SocketException: Socket Closed\r\n                           at java.net.PlainSocketImpl.setOption(Unknown Source)\r\n                           at java.net.Socket.setSoLinger(Unknown Source)\n\n\r\nSM undeploy/deploy did not help either.\r\nChecked for the spool logs and saw it was so many. \n\r\nSo requested following action plan:\n\r\n1-) Run \"neinit -autorestart=off\"  on all servers \r\n2-)Stop all the NE\'s in the system at the same time with neStop.pl script for all instances running on all servers on the sytem (SM, SESM, BCP, IPCM, etc (one at a time))\r\n3-) Clean the /var/mcp/spool directories of all the servers with the following command\r\nfind /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f -exec rm -f {} \\; ; find /var/mcp/spool/tmom -type f -exec rm -f {} \\; ; echo DONE\r\n4-) Start the instances starting with SM, SESM, BCP, IPCM, etc (one at a time) with neStart.pl script\r\n5-) Run \"neinit -autorestart=on\"  on all servers\n\n\r\nAfter that SM again stuck at Activating state with same exception.\n\r\nSince all instances are down it was a planned E1 outage so asked customer to start SESM instances so we are out of E1 state.\n\r\nSESMs started successfully.\n\r\nOn the BCP servers since there is no MCP GUI HAL layer did not started but BCP is up and call signalling is ok.\n\r\nThat problem is seen on systems which has lots of BCP deployed.\r\nSo as next action plan we asked to powerdown all BCP servers and start on SM instance to make it RUNNING.\n\r\nCustomer requested to go on that work at 00:00 local time ( 4pm Istanbul time) and stop all BCP servers.\n\r\nWe dropped the call to go on at the mtc window.','null'),(109,'Cigdem Vural','AS-OAM','2018-02-01','180201-662558','Eastlink','Upgrade path: 17.0.31.5 to 19.0.4.1\n\r\nUpgrade failed at step  36 with Upgrading OS platform for Secondary Host servers.\n\r\nFailure reason was Secondary SM state. It expect to see Secondary SM as ONLINE-HOTSTANDBY.\r\nI checked the status of Secondary SM and saw its state is Online-Down-Unavailable.\r\nSuspected if neinit autorestart is off so after rebooting host server the instances could not be up. And the autorestart was off. Set the autorestart as on and manually kill and start the secondary SM via MCP GUI. \r\nafter it became Online-Hotstandby, click Retry on wizard and it passed the screen','null'),(110,'Cigdem Vural','AS-OAM','2018-02-01','180201-662558','Eastlink','SWD is doing an upgrade and failed at Step 29 at Host server OS patching as HOS1S1 is not reachable.\n\r\nUpgrade path: 17.0.31.5/19.0.4.1\r\nHardware Env:    SandyBridge-Emerson-ATCA-7370\n\r\nLogged into the site and HOS1S1 was not reachable and ping was unsuccessful.\r\nConnected via NDM and asked for the blade slot number of the HOS1S1, but customer did not know. Asked for the specbook and they could find a specbook after 15 minutes.\n\r\nAt the end we figured out that HOS1S1 is blade 0 0 5 0.\n\r\nI did deactivate/activate via NDM and it did not help.\r\nThen I did online/offline as well with below order on the blade:\n\r\nha app-blade deactivate 0 0 5 0\r\nha app-blade offline 0 0 5 0\r\nha app-blade online 0 0 5 0\r\nha app-blade activate 0 0 5 0\n\r\nAfter that server become up and reachable.\n\r\nSWD could go on with next steps.','null'),(111,'Cigdem Vural','AS-OAM','2018-01-31','180130-662321','TwinLakes','David paged and told he tried to replace the bad drive on EM1.\r\nReplaced the bad disk and started mirroring but during mirroring the server went down and after reboot DB0 has alarms and not writable. \n\r\nThen we asked customer to go site and pull out the new disk that he puts on the server. After he pulled out the new disk, server come up again and DB alarms cleared.\n\r\nSo that shows the new disk is faulty, it is provided by Ribbon so it is not a brand new disk I guess. We had seen bad hardware from Ribbon repair&return service before.\n\r\nAdvised to ask for another disk to do the disk replacement again.\r\nDropped the call','null'),(112,'Cigdem Vural','AS-OAM','2018-01-30','180130-662321','Twinlakes','Customer load: 14.1.8.2\n\r\nThey had disc problems so would replace the disk with the case 180125-661849.\r\nER called with the case 180125-661849 and I told that MJ case is not a pager call please create an emergency case if there is a real outage here.\n\r\nThen connected to the site and saw that none of the instances could communicate with secondary DB except the SM and PROV which are on same server with secondary server.\n\r\nPing was ok. Restarted DB but not helped.\r\nreplication was broken as well. Logs were not helping about any error.\n\r\nDid a reboot and server did not come UP after reboot, so that pointed there might be some hw problem on the server.\r\nCustomer went to the site and did a power cycle but server did  become UP after 45 minutes so it is so slow even for CC3310 server.\n\r\nAnd reboot did not help either.\n\r\nDB monitor for mcpdb_1 is showing it as UNKNOWN.\n\r\nAction plan:\n\r\nDavid will try to replace with new hardware with old disks.\r\nIf that also not help, I will check the below swerr with design team:\n\n\r\nSM_0 SWERR 799 ALERT JAN30 09:56:19:337 MCP_14.1.8.2\r\n  RequestContext: DBNameContext, AgentMonitor: mcpdb_1\r\n  java.lang.IllegalStateException: Invalid timer state: 1\r\n        at com.nortelnetworks.mcp.base.timer.TimerManager.startTimer(TimerManager.java:441)\r\n        at com.nortelnetworks.mcp.ne.sm.base.snmp.mgmt.fw.RequestContext.startPollTimer(RequestContext.java:143)\r\n        at com.nortelnetworks.mcp.ne.sm.base.snmp.mgmt.fw.RequestContext.processError(RequestContext.java:104)\r\n        at com.nortelnetworks.mcp.ne.sm.base.snmp.mgmt.fw.AgentMonitor.processError(AgentMonitor.java:701)\r\n        at com.nortelnetworks.mcp.ne.sm.snmp.db.fw.DBMonitor.processError(DBMonitor.java:366)\r\n        at com.nortelnetworks.mcp.ne.sm.base.snmp.mgmt.fw.AgentMonitorFSMTemplate$ErrorEvent_INITIAL.handle(AgentMonitorFSMTemplate.java:346)\r\n        at com.nortelnetworks.mcp.base.fsm.SimpleStateMachine.handle(SimpleStateMachine.java:63)\r\n        at com.nortelnetworks.mcp.base.task.DelegatorTask.handle(DelegatorTask.java:46)\r\n        at com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:709)\r\n        at com.nortelnetworks.mcp.base.task.Task.run(Task.java:543)\r\n        at com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:152)\r\n        at java.lang.Thread.run(Unknown Source)\n\n\r\nSM_0 OMIFSEC 610 INFO JAN30 09:56:50:945 MCP_14.1.8.2\n\r\nDavid also told when they did SM_1 active the server has high CPU alarm as well. So that also shows EMserver2 has hw failure.\n\r\nAnd as last action, customer/ER will do full hardware replacement with new disks on secondary EM server with the GPS provided MOP.\n\n\r\nThen I dropped the call.','null'),(113,'Cigdem Vural','AS-OAM','2018-01-30','180130-662273','Axtel','Axtel is running on 17.0.22.6. We had advised them to put new FPM pair to stabilize SM activity on August. Cause with more than 40 instances managed by SM, SM can lose the activity or get into stuck states.\n\r\nAnd Axtel has not added FPM servers yet so running on an unsupported config.\r\nTheir secondary SM stuck at Synchronizing state and they paged us.\r\nChecked for the logs and all others it is mentioning about Checkpoint and sync between both SMs so could not get the Hot Standby state.\n\r\nLogs on oss for SM1:\r\nSM_1 SWERR 799 ALERT JAN30 00:39:02:869 MCP_17.0.22.6\r\n  Task Runner exception:\r\n  priority=10\r\n  task=Task: com.nortelnetworks.mcp.ne.base.sync.CheckpointSender@1d30fe7\r\n  State: run\r\n  java.lang.NullPointerException\r\n        at com.nortelnetworks.mcp.ne.base.sync.CheckpointSender.handleConnectConfirmEvent(CheckpointSender.java:383)\r\n        at com.nortelnetworks.mcp.base.io.tcp.ConnectConfirmEvent.handle(ConnectConfirmEvent.java:34)\r\n        at com.nortelnetworks.mcp.base.task.SimpleTask.handle(SimpleTask.java:26)\r\n        at com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:709)\r\n        at com.nortelnetworks.mcp.base.task.Task.run(Task.java:543)\r\n        at com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:152)\r\n        at java.lang.Thread.run(Unknown Source)\n\n\r\nSM_1 SMGE 701 ALERT JAN30 00:39:02:869 MCP_17.0.22.6\r\n  DisconnectIndicationEvent is being generated. Details: DisconnectIndication Message dump follows\r\n  Cause: java.net.SocketException: Connection reset\n\r\nSM_1 SMGE 701 ALERT JAN30 00:39:02:871 MCP_17.0.22.6\r\n  Socket closure failed in TCPRunnableSession. Details:\r\n  msg: Socket Closed\r\n  cause:\n\r\nLogs on SM1 work:\n\r\nSMGE/SMGE 701 ALERT  JAN30 01:28:49\r\nDisconnectIndicationEvent is being generated. Details: DisconnectIndication Message dump follows\r\nCause: java.io.IOException: Failed to read 4 bytes.\r\nOnly received -1.\n\r\nSMGE/SMGE 701 ALERT  JAN30 01:28:49\r\nDisconnect indication generated from TCPRunnableSession. Details:\r\n<201.158.135.20:12112,201.158.135.22:12113,5041224654023295071,3,5041224654023294977,2>\n\r\nSMGE/SMGE 701 ALERT  JAN30 01:28:49\r\nSocket closure failed in TCPRunnableSession. Details:\r\nmsg: Socket Closed\r\ncause:\n\r\njava.net.SocketException: Socket Closed\r\n        at java.net.PlainSocketImpl.setOption(Unknown Source)\r\n        at java.net.Socket.setSoLinger(Unknown Source)\r\n        at com.sun.net.ssl.internal.ssl.BaseSSLSocketImpl.setSoLinger(Unknown Source)\r\n        at com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession.closeSocket0(TCPRunnableSession.java:324)\r\n        at com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession.closeSocket(TCPRunnableSession.java:312)\r\n        at com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession$MessengerRunnable.run(TCPRunnableSession.java:595)\r\n        at java.lang.Thread.run(Unknown Source)\r\nSMGE/SMGE 701 ALERT  JAN30 01:28:49\r\nDisconnect indication generated from TCPRunnableSession. Details:\r\nPerfectChannelServer12113\n\n\r\nStopped both SM instances. Started only SM1 and it took the Active state.\r\nThen started the SM0 and it became Hot standby. Then did a swact and no problem seen.\n\r\nIt is again highlighted to have FPM pairs on this site to have a supported and stabilize system. Also they are running on CC3310 servers which are very old as well.\n\r\nAfter customer approved all good dropped the call.','null'),(114,'Emre OVA (NETAS External)','AS-OAM','2018-01-19','180119-660944','Unitymedia','Customer Release: 11.2 (18.0.30.1)\n\r\nJohn Kishner from ER paged me out to report that SESM2_0 (hotstandby instance) couldn\'t be activated after restarting operation. \n\r\nI\'ve connected the site via GTS VM servers and observed the SESM2_0 instance status as Configured-Down-Unavailable. Also, when I try to connect the server via SSH, server was not reachable. Also ping tests were unsuccessful.\n\r\nSince we couldn\'t reach the server, I suggested to deact/act blade on the NDM and ER completed this operation with the customer approval.\n\r\nha app-blade deactivate 0 1 1 0\r\nha app-blade activate 0 1 1 0\n\r\nAfter deactivating and activating operation, server became reachable and I\'ve killed the working process ID of SESM2_0, then saw that instance came into the offline-down-unavailable status. After that, we did undeploy-deploy and start on SESM2_0 and seen that the instance came into the correct status. (Online-Up-Hotstandby)\n\r\nAfter the issue has been resolved, I requested the necessary logs to continue further investigation for RCA and ER will attach them into the case.','null'),(115,'Emre OVA (NETAS External)','AS-OAM','2018-01-18','180118-660650','TELUS Communications Company','Caleb Coleman from SWD stated that TELUS is having an NSP order failure issue in their site during the core upgrade. When he sends the XML commands it was hanging and he was not able to get a response. \n\r\nSince the issue is not within the scope of A2 OAM GPS team, I recommended to route the case to CS2K OAM GPS team.','null'),(116,'Emre OVA (NETAS External)','AS-OAM','2018-01-17','180117-660501','Tbaytel','UPGRADE PATH (FROM/TO: 18.0.26.5 / 18.0.31.0)\n\r\nDonnell Williamson from SWD paged me out to report that Step 24, Patching OS of the Primary Servers couldn\'t be completed since the BCP Server failed to reboot.\n\r\nHe stated that BCP Server couldn\'t be rebooted in spite of waiting more than 30 minutes. At this point, SWD (Donnell) paged me via trillian and asked how he can reach the management module. Since the patching operation has been completed, I recommended to reseat the blade. Then, an on-site engineer reseated the blade and BCP Server became up and running. So, this screen had been completed successfully. We\'ve collected the required server logs and SWD attached them into the created following follow-up case. (171218-657580) After the problem has been resolved and passing this screen, SWD continued to perform ugprade and rest of the upgrade steps have been completed successfully.','null'),(117,'Emre OVA (NETAS External)','AS-OAM','2018-01-17','180117-660464','Singtel Optus Pty Ltd','Customer Release: 11.2 (18.0.28.1)\n\r\nMark Zattiero from ER paged me to report that SESM2_1 is stuck in Synchronizing status.\n\r\nI\'ve connected the site via GTS VM Servers and started to investigate the problem. Before starting the investigation, ER stated that the customer tried to restart SESM2_1 instance since the following SWERRs had been observed:\n\r\nSESM2_1 SWERR 799 ALERT JAN17 10:25:26:252 MCP_18.0.28.1 \r\nError processing IMDB checkpoint \r\n Error processing IMDB checkpoint\r\n                     Subsystem: IMDB\r\n                     seq: 118133893\r\n                     Payload:\r\n                     00 00 00 12 00 00 00 01 00 07 00 02 00 05 c1 4e     ...............N\r\n                     00 00 00 02 00 00 00 80 3c 73 69 70 3a 36 31 32     ........\r\n                     39 38 36 33 32 37 39 35 40 31 30 2e 32 31 39 2e     98632795@10.219.\r\n                     31 39 31 2e 32 30 31 3a 35 30 36 30 3b 74 72 61     191.201:5060;tra\r\n                     6e 73 70 6f 72 74 3d 75 64 70 3b 6e 74 6c 6f 63     nsport=udp;ntloc\r\n                     73 3d 35 34 35 37 36 3b 6e 6f 72 74 65 6c 64 65     s=54576;nortelde\r\n                     76 69 63 65 3d 44 50 51 33 39 32 35 58 31 31 30     vice=DPQ3925X110\r\n                     35 31 36 3e 3b 69 64 3d 32 34 38 3b 65 78 70 69     516>;id=248;expi\r\n                     72 65 73 3d 31 37 38 30 20 72 6f 75 74 65 4c 69     res=1780 routeLi\r\n                     73 74 3a 20 6e 75 6c 6c 00 00 00 80 3c 73 69 70     st: null....\r\n                     3a 36 31 32 39 38 36 33 32 37 39 35 40 31 30 2e     :61298632795@10.\r\n                     32 31 39 2e 31 36 37 2e 31 32 32 3a 35 30 36 30     219.167.122:5060\r\n                     3b 74 72 61 6e 73 70 6f 72 74 3d 75 64 70 3b 6e     ;transport=udp;n\r\n                     74 6c 6f 63 73 3d 35 34 35 37 36 3b 6e 6f 72 74     tlocs=54576;nort\r\n                     65 6c 64 65 76 69 63 65 3d 44 50 51 33 39 32 35     eldevice=DPQ3925\r\n          com.nortelnetworks.ims.foundation.collection.ContainerFullException: the queue is full (102400 elements) \r\nat com.nortelnetworks.ims.foundation.collection.PriorityQueue.enqueue(PriorityQueue.java:484) \r\nat com.nortelnetworks.ims.foundation.collection.BlockingPriorityQueue.enqueue(BlockingPriorityQueue.java:315) \r\nat com.nortelnetworks.ims.foundation.task.JobTaskB.enqueueJobItem(JobTaskB.java:56) \r\nat com.nortelnetworks.ims.foundation.imdb.mgmt.TableManager.enqueueEvent(TableManager.java:286) \r\nat com.nortelnetworks.ims.foundation.imdb.mgmt.TableManager.enqueueDataSyncEvent(TableManager.java:270) \r\nat com.nortelnetworks.ims.foundation.imdb.mgmt.DataSyncManager.enqueueEvent(DataSyncManager.java:155) \r\nat com.nortelnetworks.ims.mw.imdb.checkpoint.IMDBCheckpointDispatcher.dispatch(IMDBCheckpointDispatcher.java:127) \r\nat com.nortelnetworks.mcp.ne.base.sync.CheckpointDispatcherTask.handle(CheckpointDispatcherTask.java:22) \r\nat com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:709) \r\nat com.nortelnetworks.mcp.base.task.Task.run(Task.java:543) \r\nat com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:152) \r\nat java.lang.Thread.run(Unknown Source) \r\nstarted at 07:54 this morning \n\n\r\nWhen customer tried to restart the SESM2_1 instance operational state was stucked in Synchronizing status. As a first action, I checked the SESM2_1 work logs but couln\'t find any clue that tell us the reason from OAM perspective. \n\r\nIn addition to this, I tried to restart the SESM2_1 but the result was same. I waited more than 20 minutes but SESM2_1 couldn\'t be activated and stucked in Synchronizing status again.\n\r\nI also performed the Kill- Undeploy- Deploy and Start operation but it didn\'t work. After these operation, we tried to swact SMs and restart SESM2_1 instance but it didn\'t resolve the problem.\n\r\nApart from this, I checked the current status of databases but both primary and secondary databases were working as up and running status without any problem. Also there was no any other alarm or abnormal behaviour related to database on the system.\n\r\nAs a last action, I tried to reboot SESM2_1 server but it didn\'t help to resolve the problem.\n\r\nDue to the fact that IMDB checkpoint SWERRs had been observed before restarting the SESM instance and synchronizing problem, we decided to page CallP GPS team.\n\r\nBerat from CallP GPS team involved in the pager call and started to investigate the problem.\n\r\nThen I dropped from the bridge.','null'),(118,'Yunus Ozturk','AS-OAM','2018-01-09','180108-659291','Hawaiian Telcom','Problem Description:\r\n====================\n\r\nCustomer Release : AS 8.0 BRC\r\nHW: CC3310 \n\r\nER paged out GPS and reported that customer was unable to make provisioning. \n\r\nIssue was initially thought to be the CMT, but later discovered that SM 1 was \r\noffline / unavailable along with both Prov instances\n\r\nActions Taken:\r\n===============\n\r\nAccessed the site and noticed 1 SM instance was at offline/unavailable and the other SM instance was at configured/up status.\n\r\nBoth Prov Instances were also at offline/unavailable status.\n\r\nSeveral undeploy/deploy/kill operations were done to restore the SM instances. Additionally, there was no space left on EMServer2 under /var/mcp directory. We have removed the older log files to increase the disc space.\n\r\nWe were then able to start Prov 0 instance which restored the customers ability to provision.\n\r\nTo restore Prov 1 instance, the server had to be rebooted as the server uptime was around 300 days.\n\r\nWe will not provice RCA for this issue as the customer is running on an EOL Release.','null'),(119,'Emre OVA (NETAS External)','AS-OAM','2018-01-06','180106-659017','U.S. Air Force Falcon AFB','Bradley Hetzel paged me out to report that the 31 jar files are missing for SESM3_1 instance. Customer was realized this circumstance after swacting SESM3 instances and making SESM3_1 active.\n\r\nI\'ve joined the conference bridge and shared the required logs that are necessary to be collected for proceeding with further investigation. I also confirmed that these jar files are missing just for SESM3_1 instance whereas all these are applied on other SESM instances successfully. At this point, I suspected that an undeploying operation might be done for this specific instance and it might cause to loose the custom jars under /var/mcp/run/MCP_18.0/SESM3_1/jars directory.\n\r\nSince there is no any service impact of this circumstance, missing JAR files have been re-applied on SESM3_1 and it\'s seen that all tests were passed successfully.\n\r\nI\'ve requested the required logs for root cause analysis and customer attached these into the following case. (180106-659017) Accordingly, I\'ve taken ownership of this case and I will keep the customer updated about our latest findings.\n\r\nAfter the problem has been resolved and agreed with the customer, I dropped from the bridge. The follow-up action and RCA will be driven by the case 180106-659017.','null'),(120,'Cigdem Vural','AS-OAM','2017-12-29','171228-658408','Axtel','Kevin paged me by telling customer has registration failures on the system so he paged me.\r\nI asked what is the failure reason cause registration might be related with AS Callp team.\r\nAnd he could not tell any details. Since I already woke up I connected to site to see the issue. But again there is no details, customer is talking in Spanish and ER cannot understand.\n\r\nWe got an email from Gustavo, telling customer has a HW failure but again there is no details about which HW have the failure.\n\r\nIt was mentioned about a NFS failure and again there is no detail about what NFS is. It is not a AS related component or tool.\n\r\nSo on MCP GUI there was no problem seen  and no detail came in about AS registration problem any failure message or failure reason, I dropped off.\n\r\nBefore any pager calls, clear problem description needs to be had by ER to give to GPS so GPS will not go around and understand what the problem is and what needs to be checked.','null'),(121,'Burak Biyik','AS-OAM','2017-12-20','171220-657807','KBS Production','In KBS Production system running 18.0.28.2 load, it has been reported that Accounting Manager instances keep bouncing.\n\r\nStopping both and starting one by one did not help. Undeploying and rebooting EM virtual machines did not improve the situation as well.\n\r\nBased on the oss logs, floating IP was owned by active AM instance for few seconds and the instance was going deactivating stage immediately. \n\r\nThe following error was seen in AM work logs:\n\r\njava.lang.OutOfMemoryError: Java heap space\r\n        at java.util.Arrays.copyOf(Unknown Source)\r\n        at java.lang.AbstractStringBuilder.expandCapacity(Unknown Source)\r\n        at java.lang.AbstractStringBuilder.append(Unknown Source)\r\n        at java.lang.StringBuffer.append(Unknown Source)\r\n...\r\n...\n\r\nSo, this was most likely to be a software issue rather than the network itself. We noticed a critical alarm on SESM1_0 regarding the high number of spool files above defined threshold.\n\r\nThere were more than 5000 spool files in \".closed\" state under \"/var/mcp/spool/acct/MCP_18.0/SESM1_0\". We moved them under /var/mcp temporarily and swact SESM instances. Once the alarm was cleared on SESM instance, AM instances were in ACTIVE-STANDBY state properly. We monitored the spool directory on SESM for a period and saw that new files are being generated and sent to AM without any issues. \n\r\nMoving few unprocessed files under this directory for test purpose and swacting SESM instances again did not trigger the transfer of these files to AM. GPS will try to find a way to process those 5000 spool files that belong to 19th December. \n\r\nThe follow-up action and RCA will be driven by the case 171220-657807.\n\r\nAgreed and dropped the call.','null'),(122,'Emre OVA (NETAS External)','AS-OAM','2017-12-15','171215-657262','Tbaytel','Donnell Williamson from SWD paged me out again and reported that Step 22 (Switching Service to Secondary SM Instance) screen failed. He told me that while switching the active SM to SM_1, operational state of the system manager stucked on Unavailable status. \n\r\nI connected the site again and investigated the SM work logs, I couldnt find any abnormal behavior or error then SM become Active itself without performing any extra operation but wizard got stuck and couldnt get the success response to go on with next steps. According to Donnell, he states that this process took approximately 15 mins. Also I checked the uptime of the EMServers and observed that these servers are also up for 400 days and we rebooted both EMServers as well.\n\r\nDue to the fact that there are critical alarms on SM and SWD have limited time to complete upgrade (approximately 1 hour.)  while the upgrade was at Step 22 which is an early step that none of the instances and servers have been upgraded to new load. So, SWD Engineer didnt want to take that risk with 1 hour maintenance time, he also said customer will do test at half upgrade pause point for about 30 minutes as well (so only have 30 mins to do the upgrade). He decided to abort the upgrade. So, upgrade has been aborted due to these reasons.','null'),(123,'Emre OVA (NETAS External)','AS-OAM','2017-12-15','171215-657250','Tbaytel','UPGRADE PATH (FROM/TO): 18.0.26.5 - 18.0.31.0\n\r\nDonnell Williamson (SWD) paged me out to report that upgrade failed on Step 19 \"Prepare DB screen\".\n\r\nDonnell provided the VPN credentials and I connected the site to investigate the issue. I observed that the problem was occured while running cleanupReplication.\n\r\nFirst of all I connected the primary EM Server to take a look at the latest upgrade logs and observed that the related screen had been failed due to the timer expiration errors.\n\r\nHere we can see the raised errors that we could find in the related cleanupReplication upgrade logs:\n\r\n quiecseRepDB was not successful\r\n It may have timed out due  to a large queue.\r\n Thus, will continue with cleanup Replication operation\n\r\nCleaning up primary DB replication\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20171215_013416_XLH8 | /opt/mcp/ned/bin/nedclient 172.28.11.9 4890\r\nCommand Output:\r\n> config ok\r\n> run err Error! Timer expired while executing: perl\r\n> exited\n\r\nError occurred executing NE commands (see below):\n\r\n> run err Error! Timer expired while executing: perl\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20171215_013416_XLH8\r\nNot unlinking file\n\r\n> config ok\r\n> run err Error! Timer expired while executing: perl\r\n> exited\n\n\r\nFailed to cleanup replication on the server 172.28.11.9\n\r\nTerminated at  =>  Fri Dec 15 01:54:18 2017\n\r\nAt this point, I suspected that DBServers might be up for a long time and checked the uptime of the servers. Since DB servers are up for 388 days, I suggested to reboot these servers as a first action and it solved the issue. Prepare DB screen has been successfully completed.\n\r\nAfter the problem has been resolved, I dropped from the bridge.','null'),(124,'Oktay ESGUL','AS-OAM','2017-12-07','171207-656243','TELENET','David Bartlet from SWD paged me out to report secondary Oracle patch failure during the Major upgrade of Telenet.\n\r\nUpgrade Path:\n\r\nFrom:17.0.31.3\r\nTo:19.0.1.1\n\r\nI have connected to the site directly via vpn and investigated the issue.\n\r\nHere is the error:\n\r\nPrerequisite check \"CheckSystemSpace\" failed. \r\nThe details are: \r\nRequired amount of space(226.342MB) is not available. \r\nUtilSession failed: \r\nPrerequisite check \"CheckSystemSpace\" failed. \r\nLog file location: /opt/mcp/db/product/11.2.0/cfgtoollogs/opatch/opatch2017-12-07_03-12-14AM_1.log  \n\r\n#############\n\r\n[Dec 7, 2017 3:12:18 AM]     Running prerequisite checks... \r\n[Dec 7, 2017 3:12:18 AM]     Space Needed : 226.342MB \r\n[Dec 7, 2017 3:12:18 AM]     Space Usable : 148.0MB \r\n[Dec 7, 2017 3:12:18 AM]     Required amount of space(226.342MB) is not available. \n\r\n############\n\r\n==> I have checked the disk space at secondary DB server and saw that opt partition is %98 .\n\r\n[root@GE1a2db12 ntsysadm]# df -k\n\r\n                        516040    17140    472688   4% /home\r\n/dev/mapper/vg00-opt   6192704  5722512    155620  98% /opt\r\n/dev/mapper/vg00-tmp   2064208    68932   1890420   4% /tmp\r\n/dev/mapper/vg00-var   2064208   755516   1203836  39% /var\r\n/dev/mapper/vg00-var_log\r\n                       3096336    73560   2865492   3% /var/log\n\r\n==>Then, I have moved listener and alert logs to a backup directory and free enough opt partition to complete the step.\n\r\n==>David retried the screen and wizard succesfully patched the secondary oracle without any issue,then I dropped.\n\r\nThanks','null'),(125,'Oktay ESGUL','AS-OAM','2017-12-05','171205-655997','Tobago Limited (TSTT)','Robert paged me out to report secondary SM was down .\n\r\nCustomer Load:\n\r\n17.0.31.2 (A2 10.4)\n\r\nCala GTS reported they have tried to recover the secondary SM instance by deploy/start operation,yet the instance state was stuck at Configured status.\n\r\nIn order to recover instance, I have stop /start the ned daemon at secondary EM server and reattempt to start instance ,this time it activated succesfully.\n\r\nLogs for RCA collected and we dropped the call.\n\r\nThanks','null'),(126,'Oktay ESGUL','AS-OAM','2017-12-04','171204-655841','SASKTEL','Tom paged me to report provisioning outage at Sasktel .When I connected to the site secondary site of the system was down and primary site has block threah alarms.\n\n\r\nLoad :\n\r\n18.0.31.0\n\n\r\nTom reported, site had been ugpraded 5 days ago.\n\r\nUpgrade Path:\n\r\nFrom:18.0.26.5\r\nTo: 18.0.31.0\n\r\n==>I have connected to active SM ip and take a look at oss logs.\n\r\n==>SM_0 permgen values were increasing consecutively. (Permgen value was %85 which was over threshold)\n\r\nSM_0_2017-12-03-23-01-43-233_all_MCP.closed:  [PermGen [54379K/65536K (82%)]]\r\nSM_0_2017-12-03-23-01-43-233_all_MCP.closed:  [PermGen [54379K/65536K (82%)]]\r\nSM_0_2017-12-03-23-01-43-233_all_MCP.closed:  [PermGen [54379K/65536K (82%)]]\r\nSM_0_2017-12-03-23-01-43-233_all_MCP.closed:  [PermGen [54399K/65536K (83%)]]\r\nSM_0_2017-12-04-00-01-43-484_all_MCP.closed:  [PermGen [54404K/65536K (83%)]]\r\nSM_0_2017-12-04-00-01-43-484_all_MCP.closed:  [PermGen [54404K/65536K (83%)]]\n\n\r\n==>PROV2_0 had critical alarm (Block thread detected)\n\r\nAlarmName: Blocked Threads Detected\r\n                   TimeStamp: Sun Dec 03 06:08:31 EST 2017\r\n                   FaultNumber: 701\r\n                   ShortFamilyName: JMXM\r\n                   LongFamilyName: JMXM\r\n                   Severity: MAJOR\r\n                   ProbableCause: software error\r\n                   Description: Blocked threads detected.\r\n                   \"DefaultTaskRunnerGroup Priority 3 TaskRunner 3\" Id=27 BLOCKED on\r\n                   java.lang.Object@821b22 owned by \"Daemon proxy serving loads_0 at\r\n                   172.20.209.97\" Id=27392\r\n                      LOCK OWNER THREAD:\r\n                        at java.net.SocketInputStream.socketRead0(Native Method)\r\n                        at java.net.SocketInputStream.read(Unknown Source)\r\n                        at com.sun.net.ssl.internal.ssl.InputRecord.readFully(Unknown Source)\r\n                        at com.sun.net.ssl.internal.ssl.InputRecord.read(Unknown Source)\r\n                        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(Unknown Source)\r\n                        at\n\r\n==>ERROR in OSS logs for the alarm present at PROV:\n\n\r\nPROV2_0 SWERR 799 ALERT DEC03 11:02:18:858 MCP_18.0.31.0 \r\n  pool failed to connect due to sql exception \r\n  java.sql.SQLRecoverableException: IO Error: Connection timed out (Read failed) \r\n        at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:752) \r\n        at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:662) \r\n        at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:32) \r\n        at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:560) \r\n        at oracle.jdbc.pool.OracleDataSource.getPhysicalConnection(OracleDataSource.java:311) \r\n        at oracle.jdbc.pool.OracleDataSource.getConnection(OracleDataSource.java:235) \r\n        at oracle.jdbc.pool.OracleDataSource.getConnection(OracleDataSource.java:178) \n\n\r\n==>Even though, above symptoms proves a kind of memory leak, I observed that there was a network communication issue between primary and secondary EM.\n\r\n==>Secondary EM was pingable ,yet no response neither ssh nor ned connections attempts.Requested terminal access,yet it did not work too.The only option to recover\r\nthe secondary EM server was physically power cycle the server.Customer rebooted the server and it recovered.\n\r\n==>Once the secondary EM recovered, I have restarted the primary SM and PROV instances ,then customer performed provisioning tests successffully.\n\r\n==>Furthermore, most probably due to communication issue at secondary DB server, there was a oracle broken job alarms at primary db, in order to clean this alarms,\r\nresynched both dbs from primary to secondary.\n\r\n==>I have shared required logs information for RCA and than dropped the call.','null'),(127,'Yunus Ozturk','AS-OAM','2017-12-01','171201-655599','Genband','Problem Description:\r\n====================\n\r\nSWD Team paged out GPS and reported that no MCP_17.0_loadlineup.xml file with the new MNCL 17.0.32.0 was available in the Software Center\n\r\nActions Taken:\r\n===============\n\r\nEven this is not a pager issue, we have found out and provided the new version of the loadlineup file to the SWD Team as they have already scheduled a upgrade with the customer and they did not want to cancel it. \n\r\nNew version is now uploaded to GSC. It appears that this was missed for some reason in the past','null'),(128,'Yunus Ozturk','AS-OAM','2017-11-30','171130-655480','Bragg Communications (Eastlink)','Problem Description:\r\n=====================\n\r\nSWD paged out GPS for the following issue while upgrading the system;\n\r\nUpgrade Path : From 18.0.30.1 To 19.0.4.1\n\r\nThe upgrade wizard failed on step 36 \"Patching OS of secondary host servers\" \n\r\nHost1server2 fails as soon as the first patch is attempted\n\r\nActions Taken:\r\n===============\n\r\nAs per the OS Patch upgrade logs, there was an extra RPM installed on the previous OS version which was \"ConsoleKit-x11-0.4.1-3.el6.x86_64\" and this was causing a dependency issue. \n\r\nThis RPM does not seem to exist on the other server that was successfully upgraded. This made us think that this RPM package was manually installed on the previous OS version on this server.\n\r\nOn the problematic server, we have seen this RPM package as follows;\n\r\n[root@TORO3A2H12 patches]# rpm -qa | grep ConsoleKit\r\nConsoleKit-libs-0.4.1-3.el6.x86_64\r\nConsoleKit-x11-0.4.1-3.el6.x86_64  <----------\r\nConsoleKit-0.4.1-3.el6.x86_64\n\r\nWe have removed this RPM package and re-executed the patchPlatform.pl script manually and the issue is resolved.\n\r\nAt the end, RPM packages were successfully upgraded as follows;\n\r\n[root@TORO3A2H12 patches]# rpm -qa | grep ConsoleKit\r\nConsoleKit-libs-0.4.1-6.el6.x86_64\r\nConsoleKit-0.4.1-6.el6.x86_64\n\r\nSWD Team continued with the upgrade without any further issue.','null'),(129,'Yunus Ozturk','AS-OAM','2017-11-29','171129-655264','Vodafone Fiji Limited (Fiji Islands)','Problem Description:\r\n====================\n\r\nGTS paged out for an existing alarm on customer system.\n\r\nThey are seeing a critical alarm below on SM_1 unit;\n\r\n AlarmName: LKEY_HARDWARE_MISMATCH_ERROR_CRITICAL_754\n\r\nActions Taken:\r\n==============\n\r\nSince this was not a pager issue, we have asked the GTS Engineer to follow up the issue with a customer support case. \n\r\nThe issue was resolved after restarting the SM_1 unit.','null'),(130,'Burak Biyik','AS-OAM','2017-11-24','171121-654505','Midcontinent Communications','As part of the procedure provided within 171121-654505, customer re-synchronized two databases successfully but this did not help clear replication link errors.\n\r\nI told ER that this does not cause loss of provisioning and loss of redundancy. The customer could continue their provisioning tests.\n\r\nAll parties were agreed to investigate the replication link errors with the existing case.','null'),(131,'Burak Biyik','AS-OAM','2017-11-24','171121-654505','Midcontinent Communications','As part of the procedure provided within 171121-654505, the customer was instructed to run \"resync.pl\" from primary DB to secondary DB in order to clear \"Oracle Replication Link Errors\" alarm.\n\r\nEven if the provided steps were clear enough, the customer could not find the db instance directory.\n\r\nActions; \r\n1- Log on to the primary Database server as a DBA (ntdbadm). \r\n2- Change directory: \r\ncd /var/mcp/run/MCP_17.0/mcpdb_0/bin/util \r\n3 Run the resync script; \r\n./resync.pl \n\r\nI told ER to make customer ensure that they connected to the DB server, not any other. \n\r\nCustomer then connected to the right server and run the procedure.\n\r\nAgreed and dropped the call.','null'),(132,'Emre OVA (NETAS External)','AS-OAM','2017-11-15','171115-653783','Smart City Telecommunications LLC','SWD (James Wilkerson) paged out GPS via trillian for the following issue:\n\r\nUpgrade Path: 19.0.1.1 / 19.0.4.1\n\r\nDuring the DB Mock Upgrade, checking db server 10.202.0.47) space was failing. \n\r\nFirst of all, I requested to create a new case due to the related screen belongs to the pre-upgrade steps.\n\r\nIn parallel, I continued to investigate the issue.\n\r\nWhen we check the db diskspace, it was looking that there is enough space to continue the upgrade.\n\r\n[root@lkbn3a2db12 ~]# df -k\r\nFilesystem           1K-blocks     Used Available Use% Mounted on\r\n/dev/mapper/vg00-root\r\n                       2064208  1140384    818968  59% /\r\ntmpfs                  8165788  1244572   6921216  16% /dev/shm\r\n/dev/vda1                92219     5768     81690   7% /admin\r\n/dev/vda2                99150    34641     59389  37% /boot\r\n/dev/mapper/vg00-home\r\n                        516040    17100    472728   4% /home\r\n/dev/mapper/vg00-opt   6192704  4060824   1817308  70% /opt\r\n/dev/mapper/vg00-tmp   2064208    68944   1890408   4% /tmp\r\n/dev/mapper/vg00-var   2064208   121684   1837668   7% /var\r\n/dev/mapper/vg00-var_log\r\n                       3096336   141992   2797060   5% /var/log\r\n/dev/mapper/vg00-var_mcp\r\n                      99372552 37087716  57236928  40% /var/mcp\n\r\nAt this point, I suspected that the usage of /opt directory might be causing the problem and deleted old alert and listener logs to eliminate this possibility.\n\r\nAfter the usage of /opt directory has decreased to %67-68 levels, I retried the wizard but the result was same.\n\r\nIn addition to these, I thought that there might be a ned communication problem and restarted the ned. Then, I restarted the db mock screen but problem was still observing.\n\r\nApart from this, I observed the following logs from the secondary db work logs:\n\n\r\n OracleMethod_checkDbForDryRunUpgrade   Started at  =>  Wed Nov 15 05:59:30 2017 by ntdbsw\r\nInput Parameters received:\r\n   method = checkDbForDryRunUpgrade\r\n End of parameters.\n\r\nDrop db user DRYRUNUSER...\n\n\r\nARRAY(0x16e3ce8) at /usr/share/perl5/DB/OraSQLWrapper.pm line 663\r\n        DB::OraSQLWrapper::sqlCmd(\'AppSqlWrapper=HASH(0x1377f30)\', \'backtic\', \'-S\', undef, \'SYS\', \'select count(*) from dba_users where username = \\\'DRYRUNUSER\\\';\') called at /usr/share/perl5/DB/OraSQLWrapper.pm line 604\r\n        DB::OraSQLWrapper::sqlBackticCmd(\'AppSqlWrapper=HASH(0x1377f30)\', \'-S\', undef, \'SYS\', \'select count(*) from dba_users where username = \\\'DRYRUNUSER\\\';\') called at /usr/share/perl5/DB/Oracle.pm line 393\r\n        DB::Oracle::doesDbuserExist(\'DBCommands=HASH(0x1e16708)\', \'DRYRUNUSER\', undef) called at /usr/share/perl5/DB/Oracle.pm line 1262\n\r\nARRAY(0x16e3ce8)\n\r\n ERROR: OracleMethod_checkDbForDryRunUpgrade Terminated at  =>  Wed Nov 15 05:59:30 2017\n\r\nAt this point, I thought that the above errors might be the root cause of the problem instead of the disk space and searched the similar cases related to Drop DB user DRYRUNUSER issue.. In this regards, I found a similar case. (151029-553934).\n\r\nAccording to this case, the problem was occuring due to the wrong hostname for the secondary database server.\n\r\nAfter re-configuring the hostname of the secondary database with the correct values, problem has been resolved.\n\r\nChecking db server 10.202.0.47 disk space to perform dry run upgrade ...\r\nDB server 10.202.0.47 total tablespace size is 9583616 KB\r\nDeploying DB run time scripts necessary for the Dry-run DB upgrade ...\r\nPerforming DB Upgrade Dry-run, please be patient.\r\nGetting DB Upgrade Dry-run result ...\r\nCompleted successfully\n\r\nSWD will continue to rest of the upgrade steps.','null'),(133,'Emre OVA (NETAS External)','AS-OAM','2017-11-15','171114-653672','Smart City Telecommunications LLC','UPGRADE FROM/TO LOAD: 19.0.1.1 / 19.0.4.1\n\r\nJames Wilkerson from SWD paged me to report that Run Audit screen fails due to the following failures: \n\r\nDB1Server2 \r\nFile System Validation : ............................................ [FAILED] \r\n- The below files and/or directories do not exist in the system. \r\n/opt/mcp/.support_pkgs/build/cpp-4.4.7-16.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/gcc-4.4.7-16.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/mpfr-2.4.1-6.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/ppl-0.10.2-11.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/cloog-ppl-0.15.7-1.2.el6.x86_64.rpm \r\n/opt/mcp/install/jre-7u121-windows-i586.exe \r\n/opt/mcp/install/jre-7u121-linux-i586.tar.gz \r\n- Permissions and/or owners of below files and/or directories have been corrected: \r\n/usr/local/bin/diagnosticCommonUtils.pm \n\r\n- Recommended Action: \r\nIf the difference is only because permissions of some files could not be corrected, \r\ntry to change the permissions manually as in \"Required privileges\". \r\nOtherwise, contact next level of support. \n\n\n\n\n\r\nDB1Server1 \r\nFile System Validation : ............................................ [FAILED] \r\n- The below files and/or directories do not exist in the system. \r\n/opt/mcp/.support_pkgs/build \r\n/opt/mcp/.support_pkgs/misc \r\n/opt/mcp/.support_pkgs/build/buildEnv.pl \r\n/opt/mcp/.support_pkgs/misc/toolsEnv.pl \r\n/opt/mcp/.support_pkgs/build/cpp-4.4.7-16.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/gcc-4.4.7-16.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/mpfr-2.4.1-6.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/ppl-0.10.2-11.el6.x86_64.rpm \r\n/opt/mcp/.support_pkgs/build/cloog-ppl-0.15.7-1.2.el6.x86_64.rpm \r\n- Permissions and/or owners of below files and/or directories have been corrected: \r\n/usr/local/bin/diagnosticCommonUtils.pm \n\r\nSWD provided the site access via GTS VM and I took a look at the current status of the screen. In this way, the missing support packages failure was only observing for DBServer1 and DBServer2. All other servers were successfully passed the audit.\n\r\nSince it\'s a pre-upgrade step and there is an already opened BC case (171114-653672), I recommended to work on this issue as a case working and agreed with SWD. Accordingly, I\'ve taken ownership of this case. In this respect, I will connect the site within today and provide an update.\n\r\nThen I dropped from the bridge.','null'),(134,'Oktay ESGUL','AS-OAM','2017-11-12','171112-653399','CIBC','Donnell paged me out again to report that IMMMAS4 and IMM4 can not be activated in order to perform upgrade.\n\r\nUpgrade Path:\n\r\nFrom:\n\r\nMas Application:16.0.0.846\r\nServer Platform: 17.0.19\n\r\nTo:\n\r\nMas Application : 16.0.0.863\r\nServer Platform : 18.0.11\n\n\r\nNormally, CIBC has 3 IMM pairs in their cluster,I have asked if this IMM4 was working before upgrade as Donnell highlighted it was down prior he started to upgrade as well.\n\r\nHe discussed with customer and informed that this was not actively used IMM pair. Based on this info, I recommended Donnell to open a case and track this item seperately ,then dropped the call.\n\r\nThanks','null'),(135,'Oktay ESGUL','AS-OAM','2017-11-11','171111-653390','TWT Spa','Brad paged me out to report provisioning outage at Twt .\n\r\nMCP Release: 18.0.28.1\n\r\nI have connected to the site for initial investigation. All applications running on secondary host server were down which indicates a host hardware problem.\n\r\nIn the meantime, I have tried to make basic provisionings which all passed as the primary db was up and active.\n\r\nIn order to investigate the server , I requested terminal access yet they do not have. That s why ,customer sent a technician to the site to power cycle the server since there is no terminal and ssh was not working.\n\r\nPrior power cycle the server, they sent a phone of the server leds and fan was not working somehow. \n\r\nApparently, due to fan was not functional server went down and we lost the secondary side of the applications.\n\r\nOnce the server powered up, all application recovered automatically and the E2 over.\n\r\nCustomer will raise a follow up case for the hardware issue.\n\r\nThanks','null'),(136,'Yunus Ozturk','AS-OAM','2017-11-01','171031-652040','Bahamas Telecommunication Co','Problem Description:\r\n=====================\n\r\nER paged out GPS and reported the following problem on customer site. At the time of we were paged out, Callp GPS was already working on this problem and they found out the following issue..\n\r\nRegistrations were failing due to authentication failure and SESM was sending \"403 Forbidden\" message to client. Appears that there may have been issue with A2 subscribers with the \"res.siplines.btcbs domian\"\n\r\nThere was a SIP line provisioning issue. \"Details: Invalid Domain. Domain res.siplines.btcbs not found.\" and \"res.siplines.btcbs domain\" does not exist in the A2 provisioning.\n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and verified that the the corresponding sub-domain (res.siplines.btcbs) does not exist on A2 DB/Prov.. Most of the customer subscribers should have been configured under this sub-domain but somehow they were lost on A2 side even they exist on C20/Core side. \n\r\n- We have collected some logs and noticed that the corresponding sub-domain was manually removed by someone intentionally or by mistake based on the following logs;\n\r\nPROV1_0 PVIFSEC 620 INFO OCT30 12:43:10:906 MCP_17.0.4.3\r\n  Operation: Delete\r\n  Admin ID: admin@10.0.200.151 succeeded\r\n  Resource Type: DomainNaturalKeyDO\r\n  Resource(s): res.siplines.btcbs\n\r\n- We have decided to restore a recent A2 backup to recover this domain and its subscribers but the latest A2 backup was taken at 2013. So that, customer did not want to restore this backup file. They informed us that they have latest backups for the C20 side but they never took A2 backups since the last A2 upgrade. \n\r\n- At this point, we asked them to provision their subscribers again manually with a script. We have re-created the corresponding sub-domain and they started to prepare a script to configure all the subscribers again. We also asked them to remove the subscribers on C20/Core side as well and re-add both C20/Core and A2 side subscribers again not to cause any data mismatch between 2 sides. \n\r\n- Customer added a few subscribers for testing purpose and the calls succeeded. They will contiune to add the subscribers during the day and let us know latest status. Once it is completed, they will ask us to take fresh A2 backups.','null'),(137,'Yunus Ozturk','AS-OAM','2017-10-31','171031-651979','Bahamas Telecommunication Co','Problem Description:\r\n=====================\n\r\nER paged out GPS and reported a communication problem on SESM1_0 unit. The corresponding SESM1_0 unit was not reachable and SESM1_1 unit was also having some problems while handling the calls. Callp GPS Team asked to recover the SESM1_0 unit first as they wanted to restart the SESM1_1 unit as a possible solution for the ongoing problem on SESM1_1 unit.\n\r\nActions Taken:\r\n=================\n\r\n- Accessed the site and verified that SESM1_0 unit was not pingable. \r\n- Since the corresponding SESM1_0 unit was running on ATCA i7 hardware, we asked the customer to provide us the exact Slot/Frame number of this blade to power-cycle it through NDM. \r\n- It took some time for customer to provide this information to us as their specbook was also not correct.\r\n- Once customer provided the required information, we have power-cycled the SESM1_0 unit through NDM with the following commands;\n\r\nha app-blade deactivate 0 0 6 0\r\nha app-blade offline 0 0 6 0\r\nha app-blade online 0 0 6 0\r\nha app-blade activate 0 0 6 0\n\r\n- The connection problem on SESM1_0 unit was recovered after applying the steps above.\n\r\n- Once the connection to SESM1_0 is recovered, we dropped the call and Callp GPS continued to work on the problem for SESM1_1 unit.','null'),(138,'Cigdem Vural','AS-OAM','2017-11-01','171101-652166','Verizon Communications','Verizon is performing upgrade from 14.1.0.3 to 14.1.10.3\n\r\nIt had failed at Screen 33, and says SESMServer2 failed at Connection Loss.\r\nWe rebooted the SESM server, it was up for ~400 days and so slow even responding easy commands.\n\r\nRebooted the SESM server, and retried the failed step and pass the failure.\n\r\nThen dropped off','null'),(139,'Cigdem Vural','AS-OAM','2017-11-01','171101-652153','Verizon Communications','Verizon is performing an upgrade to be prepared for their HW migration.\n\r\nUpgrade path: 14.1.0.3 to 14.1.10.3\r\nUpgrade failed on Step 25: Upgrading Primary DB&SM screen with a timeout on wizard.\r\nI checked on the logs files and confirmed SM and DB upgrades completed successfully.\r\nI checked from database and saw that all related scripts for DB upgrade also completed and SM-DB is loaded with the new load.\n\r\nWhen do a retry on wizard it says there is already a running script, so i found the stuck process and kill it. The re-launched the wizard on debug mode and pass the SM&DB screen.\n\r\nAfter primary network elements screen also passed successfully, closed the debug mode upgrade wizard and re-launch it for customer to be able to use on his own.\n\r\nWith agreement with Stuart-customer, I dropped the call.\r\nThere will not be any RCA since that is a retired load.','null'),(140,'Yunus Ozturk','AS-OAM','2017-10-30','171030-651865','Singtel Optus Pty Ltd','Problem Description:\r\n====================\n\r\nSWD paged out GPS for the following issue;\n\r\nUpgrade Path : From 14.1.15.3 to 18.0.26.5\n\r\nPrepareDB failed: \n\r\n[root@O2NLSSLM01 prepareDB]# cat ut_prepareDB.EMS1_2ce06a56-2b8a-1b21-95de-00e0ed5725fe_ut_prepareDB.pl_PREPARE_DB_0.20171030_212304.log \n\r\nStarted at => Mon Oct 30 21:23:04 2017 \r\nScript invoked with: -bk preUpgradeMCP_18.0.26.5_2016-11-15-1359 -cr \r\n21:23:04 Starting to edit the state file \r\nRead initial properties from: /var/mcp/install/installprops.txt \r\nFound ne.load \r\nFound db.host \r\nFound db.secHost \r\nFound db.neName \r\nFound db.type \r\nFound db.backup \r\nGID = 92 and stats[5] = 92 \r\nDirectory permissions are = rwx, rwx, --- \r\nGID = 92 and stats[5] = 92 \r\nDirectory permissions are = rwx, rwx, --- \r\nclearRep command: /var/mcp/upgrade_tools/bin/ut_cleanupReplication.pl -nc -m EMS1_2ce06a56-2b8a-1b21-95de-00e0ed5725fe_ut_prepareDB.pl_PREPARE_DB_0 -s 211.29.121.121 \r\n*** Cleanup Replication FAILED *** \r\nSee Cleanup Replication log for details \n\r\nPrepare Database FAILED \n\r\n21:48:19 Cleaning ned session files \r\n21:48:19 /opt/mcp/ned/bin/nedclient 211.29.121.121 4890 2>&1 << EOF \r\nconfig 3 dummyRel.prepareDB_delete ROGUE_NE 0 root:root \r\nrun /bin/rm true root:root 400 -rf /var/mcp/run/dummyRel.prepareDB /var/mcp/run/dummyRel.prepareDB_state /var/mcp/run/dummyRel.prepareDB_delete \r\nexit \r\nEOF \r\n21:48:21 Running ned cmd: /bin/rm -rf /var/mcp/run/dummyRel.prepareDB \n\r\nTerminated at => Mon Oct 30 21:48:22 2017 \n\r\nAttempted to reboot SM_0, but now cannot login to MCP GUI nor Upgrade wizard. \n\r\nActions Taken:\r\n==============\n\r\nAccessed the site and checked Cleanup Replication logs. The logs were indicating that the script failed due to time out issues.\n\r\nWe have seen that the uptime of the DB Servers were around 250 days.\n\r\nWe have rebooted both DB Servers and the issue is resolved.','null'),(141,'Burak Biyik','AS-OAM','2017-10-25','171024-651271','TELUS','Following R19 (19.0.3.1) Upgrade, TELUS reported that they can\'t provision SIP PBX from NSP while all other types of provisioning work well.\n\r\nThe reason of the failure is logged into CMTg. In response to \"addPbxSessMgrAssocResponse\" request, AS was sending back the following error:\n\nUnable to validate data. Did notrecieve a response from the validating Network Element SESM2\n\r\nThere was no issue observed with SESM2 state.\n\r\nAs a first action, I added a dummy SIP PBX and associate it to SESM2 from MCP GUI and it worked well. We suspected if there was a syntax error in provisioning command, but customer stated that they used to follow the same syntax before the upgrade.\n\r\nIn our LAB, we stopped both SESM instances and run the same OPI request, and it worked to associate SIP PBX with that SESM pair. So, OMI request had nothing to do with SESM state. The error was misleading.\n\r\nThe solution steps:\n\r\n-> We performed double swact on SM since it is responsible for handling OMI requests.\r\n-> We added a dummy SIP PBX on customer MCP GUI, but did not associate it with SESM2 on purpose.\r\n-> We switched to xml mode in OSSGate and sent related (failing) request to AS\r\n-> We had a successful response and SIP PBX was displayed associated in MCP GUI.\n\r\nThe issue seems to be resolved with SM double swact, yet GPS emailed the customer to perform the same commands from NSP when they get back to office.\n\r\nGPS will be engaged if customer reports the same issue again.','null'),(142,'Oktay ESGUL','AS-OAM','2017-10-22','171019-650698','Telus','Caled paged me out to ask Critical alarms impact to during upgrade to 19.0.3.1.\n\r\nHe reported that 17.0.22.20 upgrade to 17.0.31.2 is completed. Now he is upgrading to 19.0.3.1 and there is a critical alarms for PA7.\n\r\nI have recommended him to restart the instance which fixed the issue.\n\r\nThen, I let him continue to upgrade.\n\r\nThanks','null'),(143,'Oktay ESGUL','AS-OAM','2017-10-21','171021-650979','Telus','Caleb from SWD team paged me out to request assistance for rollback of the system which they upgraded to 17.0.31 load.\n\r\nUpgrade Path:\n\r\nELEMENT: A2 \r\nFROM/TO LOAD: 17.0.22/17.0.31.2 \n\r\nHe also reported that once this upgrade completed to 17.0.31.2 pathc, tonight they will have another MW to upgrade the system to 19.0.3.1 .\n\r\nI have asked him the reason of rollback and he reported there are several CallP issues at half pause point sanity tests.\n\r\nWe discussed with CallP GPS and they informed that the corrective jar is allready ready for the next target load 19.0.3.1 patch.\n\r\nSince there is no issue with upgrade perspective of view,I have informed Management team and they discussed with customer to convince them to proceed with upgrade instead of roll back.\n\r\nPer discussion, customer aggreed to keep the system at pause point and fail over the sesm instances to old load at least till tonight, then continue with the rest of the upgrade plan.\n\r\nWe failed over the sesm instances and then dropped.\n\r\nThanks','null'),(144,'Oktay ESGUL','AS-OAM','2017-10-20','171020-650858','Verizon','Brent Combs from ER paged me out to report prep-upgrade failures at Verizon.\n\n\r\nUpgrade Path:\n\r\n14.1.0.13 to 14.1.10.3\n\r\nFirst of all, I informed ER that Pre-Upgrade steps are not in pager support scope and any issue during prep steps should be investigated via a case.\n\r\nJust to ensure the issue is really at Prep-step, I have take a look at the case and explain them what logs should be collected.\n\r\nCase is dispatched to NA queue,investigation will be continued.\n\r\nThanks','null'),(145,'Oktay ESGUL','AS-OAM','2017-10-19','171019-650698','Boston College','Donnell paged me out to report Critical License alarm for secondary SM instance after performing a patch upgrade which is not normal.\n\r\nUpgrade Path:\n\r\nFrom :17.0.31.3\r\nTo: 17.0.31.5\n\r\nExcept majora upgradeS, There should not be license issue during a patch or MR upgrade .In order to ensure all system is stable and this alarm is real.I have followed up below steps:\n\r\n1.Ssh to secondary EM.\r\n2.Run getMac.sh to ensure ,mac adresses can be listed, this worked.\r\n3.Run getHostMac.pl to ensure  virtual secondary EM can get its host mac adresses,which worked either.\r\n4.Tried to restart the instance to see if it helps,yet at work logs license mismatch alarms persisted.\r\n5.Connected customer DB.\r\n6.Copy the encrypted license from DB and decrypt it to see the defined MAC addresses in license.\r\n7. Proved that MAC list in license do not cover secondary EM MACs.\n\r\nAt this point, since it is quite clear there is a real mac mismatch error, I recommended Donnnel to ask KRS to provide a new license which covers all MAC,then dropped .\n\r\nThanks','null'),(146,'Oktay ESGUL','AS-OAM','2017-10-16','171012-649965','Telesur Suriname','Haydee Hernandez from CALA GTS team paged me out to report customer can not access to PROV and SM guis.\n\r\nEven I highlighted that this is known issue at this customer site  as they disabled http/https ports for vpn adapter.\n\r\nIn order to prove them that the system is stable. I have connected to EMServesr and verified those all applications are active .\n\r\nThen , somehow they accepted everything  is OKEY and let me drop from the call.','null'),(147,'Oktay ESGUL','AS-OAM','2017-10-16','171016-650241','Vodafone (New Zealand)','Ozgur from CallP GPS team requested assistance to recover Vodafone Fiji db issues which was impacting CallP side.\n\r\nCustomer Systm Load\n\r\nMCP_14.1.10\n\r\nWe have connected site and figured out that the secondary EM server which is hosting secondary SM/PROV/DB application was down. There was not console access to server that s why customer sent and technician to the site for power cycle.\n\r\nWhen the engineer arrived to the site, he powered cycle the server and secondary EM recovered.\n\r\nOnce the server boot up, there were replication broken alarms at MCP gui between primary/secondary DBs. In order to sync both DBs,I have taken backup of primary DB, copied it to secondary and restore this backup.\r\nThen, run resycn from secondary to primary which completed successfully and cleared out all DBComm alarms.\n\r\nThen,as a final recovery action we restarted the hot standby sesm instances to clean up imdb resync alarms.\n\r\nCustomer performed call tests and reported system is  stable again ,then we dropped the call.\n\r\nThanks.','null'),(148,'Emre OVA (NETAS External)','AS-OAM','2017-10-13','171013-650030','GENBAND Nuvia','John Kisner (ER) paged me to report that there is an outage during attempt to sync database. Before this outage, he also stated that there was a call failing and restored by swacting SESMs. They stated that there were DB Overload alarms on the SESM instance and it has gone after swacting operation. In order to check the current status of the databases, I\'ve connected the site via Bomgar session. First of all, all network elements were fully functional and running properly. In other words, there was no any alarm on the NEs. When we take a look at the status of the DBs, there was just a major alarm on the secondary DB: DBMON727 - Conflicts or incorrectly formed transactions have caused 1 errors on link [ ssdvdb_1 ] \n\r\nCustomer stated that they tried to resync the databases as from primary to secondary but fails due to the following reason (ORA00060-deadlock detected):\n\r\nRepadmin user created.\r\nDROP REPSITE\r\nTRUNCATE REP QUEUE\r\nDROP REPGROUP MCSDBSCHEMAREPGROUP\r\n   DECLARE\r\n*\r\nERROR at line 1:\r\nORA-00060: deadlock detected while waiting for resource\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6470\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6016\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 7027\r\nORA-06512: at \"SYS.DBMS_REPCAT_MAS\", line 2695\r\nORA-06512: at \"SYS.DBMS_REPCAT\", line 635\r\nORA-06512: at line 70 \n\r\nI\'ve connected the DBServer and executed ./viewRepConflict.pl, there were a few conflicts. In order to clear this conflicts, I\'ve also run ./deleteRepConflict.pl but if failed and alarm was still appearing.\n\r\nAlso I\'ve checked the uptime and load average of the secondary DB server. Since the uptime and load average is high, I recommended to reboot the secondary DB server. So, secondary db is rebooted. After that I\'ve connected the primary DB and run ./cleanupReplication.pl. When the replication is cleaned between two databases, I run setupDBReplication so that establishing the replication again.\n\r\nAfter this operation has completed, alarm was cleaned and both of the DBs were running properly. \n\r\nI\'ve collected all required logs and follow-up case is created. (171013-650034). Accordingly, investigation will be ongoing so that analyzing the root cause deeply by OAM GPS.\n\r\nSince the problem is resolved and follow-up case is created, I dropped from the bridge.','null'),(149,'Emre OVA (NETAS External)','AS-OAM','2017-10-09','171009-649485','Singtel Optus Pty Ltd','UPGRADE PATH (FROM/TO LOAD): 14.1.15.3/18.0.26.5\n\r\nPeter Maloney (SWD) paged me to report that upgrade is failed in Prepare DB screen. In order to investigate the issue, I\'ve connected the site via TeamViewer. When we launch the upgrade wizard, it was seen that Script hasn\'t completed in predefined time internal (CONNECTION LOSS). \n\r\nPossible reasons are: \r\n- Network speed is slower than expeted.\r\n- There is not enough available network bandwidth for the system.\r\n- Script is stuck due to an unexpected scenario.\n\r\nUse retry button to resolve the issue. \n\r\nAs a first action, we\'ve retried the upgrade wizard but it didn\'t resolve the issue. In this regards, I\'ve started to investigate the upgrade tools work logs. So, it\'s seen that the screen has failed while taking the db Backup.\r\nIt was written that ORA-01013 failure is occured. At this point, I suspected that there might be a stuck or corrupted oracle process. For this reason, I\'ve checked the uptime of EM/DBServer. DBServer was up for 264 days. Since the server uptime is so high, I wanted to perform DBServer reboot operation. In this regards, I rebooted the EM/DBServer and retried the wizard. This time db backup has been successfully taken and screen has completed successfully.\n\r\nIn order to provide RCA, I\'ve collected all required logs. When the SWD create a follow-up case, he will attach these logs and investigation will be ongoing.\n\r\nAfter the issue has been resolved, I dropped from the bridge.','null'),(150,'Oktay ESGUL','AS-OAM','2017-10-07','171007-649420','BT TELECOMUNICACIONES S A','BT  had reported Music on hold failures after the system upgrade completed.\n\r\nUpgrade Path:\n\r\nFROM/TO LOAD: 17.0.31.5->19.0.4.0 \n\r\nSabri from CallP team paged me out to assist to check A2 DB Media Portal Location MAP table since they were suspecting a sync issue between IMDB and DB.\n\r\nI have connected and started to investigate the issue.In the meantime, Sabri reported that the missed location map data is actually exist at IMDB .That s why ,they let me drop as there is no need further db operation assistance.\n\r\nCallP team will continue the investigation.\n\r\nThanks','null'),(151,'Oktay ESGUL','AS-OAM','2017-10-06','171004-649154','BT PLC','I have been paged again to support the BT PLC primary recovery.\n\r\nThis time customer aggreed to swapped the disks from the recovered one on tuesday and field engineer inserted the hard drivers to new server.\n\r\nServer booted up succesfully .\n\r\nI have deployed the SM instances and both SM became active/hotstandby.\n\r\nThen applied the new license since the EM server hardware is replaced.\n\r\nIn order to clean up the db replication alarms, run resync from primary db to secondary.\n\r\n2 of the BCP instances are down,yet since there are 20 blades on this system which is quite enough to handle traffic, we decided to follow up these blades issues via a seperate case .\n\r\nEventually, we completed this long recovery period and BT PLC system became stable.Customer aggreed to close E2 case and we dropped the conf.\n\r\nThanks','null'),(152,'Oktay ESGUL','AS-OAM','2017-10-06','171004-649024      ','BT PLC','Gary paged me to report the new server arrived to site.\n\r\nEven though , we recommend customer to swapped the hard drivers from installed server to new delivered one, they did not accept this recommendation.\n\r\nThey reinstalled the server platform with genband assistance.\n\r\nWhen the console access shared after reinstallation ,the first thing I did is to check hardware prior doing any recovery action.\n\r\nYet, mcpRelease surprized me and the output shows that the new third RMAed server type is not HP3310. \n\r\nThe hardware type is BellMicro-SamXTS .\n\r\nI have double checked the cpu and memory if they are exist or not and ensured that all component of hardware are present.\n\r\nBased on internal discussion ,we decided to proceed with this BellMicro since it has same specs with 3310.\n\r\nIn the meantime,since the customer unplugged the network cable of the server as they have concerns about the operation may impact the sysmtem even they aware that the half of the system is down , they did not allow us to proceed.\n\r\nThey arranged a MW for tonight to see the  sunrise again with GPS assistance.\n\n\r\nThanks','null'),(153,'Oktay ESGUL','AS-OAM','2017-10-06','171004-649024','BT PLC','Brad hetzel paged me out to report that the new server which is third one for this week has arrived to the site and customer needs assistance for recovery.\n\r\nAs recovery plan ,we had discussed to install the new server physically and then swapped the disk from second faulty server to this third new one to restore the service.\n\r\nYet, what a coincidence,the third spare unit which delivered today is also faulty. We could not neither boot it up nor swapped the disks.\n\r\nCustomer decided to order a new server which will be fouth order for this week,seems this loop will be continued for a while.\n\r\nIn the meantime, they reported that one of their sesm instances in restart loop and asked GPS to investigate.\n\r\nBased on initial investigation, seems as the SESM instances trying to handle registration at their IMDB since the customer unplugged the network cable of the faulty PRIMARY EM which Primary DB is also running on, SESM instance goes to overload condition, performs auto-swact automatically.\n\r\nPer oss logs,while the restarted instances tries to sync from active sesm unit it failed via below error:\n\r\nSESM2_1 SYSTEM 813 INFO DEC13 20:27:03:232 MCP_14.1.0.12 \r\nSynchronization send status: Aborted. \n\r\nWhile investigating the issue, sesm went to auto-swact again and this time both instances activated as expected active/hotstandby.\n\r\nCurrently, Primary SM/DB is down due to server fault.As a next phase of this long recovery period, we ll wait for the 4th (fourth) spare unit to be shipped to the customer.\n\r\nThanks','null'),(154,'Oktay ESGUL','AS-OAM','2017-10-05','171004-649154','Singtel OPTUS','Kyle Mawst paged me out to report Optus can not boot up their new spare server  with old drivers which taken from faulty SESM3_1 server.\n\r\nCustomer had replaced the server since the SESM3_1 hardware down and would like to insert this faulty server hard drivers to new spare unit .\n\r\nYet, all boot up attempts failed with different errors like below ones:\n\r\n###########\r\nroot (hd1,1)\n\r\n                   Error 5: Partition table invalid or corrupt\n\r\n                   Press any key to continue...\r\n##########\r\nroot (hd1,1)\r\n                    Filesystem type unknown, partition type 0xfd\r\n                   kernel /vmlinuz-2.6.18-194.el5 ro root=/dev/md4 selinux=0 audit=0\n\r\n                   Error 17: Cannot mount selected partition\r\n################\n\r\nAs a last chance, I recommend them to boot up new spare with it` own harddrives to be able to install the server platform from scratch.Yet,this attempt also failed as the server can not detect the hard drives.\n\r\nBased on above steps, we decided to replaced the new spare server with a newer one. Customer will RMA and get a new server for recovery.\n\r\nThanks','null'),(155,'Oktay ESGUL','AS-OAM','2017-10-04','171004-649024','BT PLC','Jim Shelby paged me out to assist BT PLC to recover their system again.They unplugged primary EM server`s network cables due to applications running on EM0 causing outages.\n\r\n==>The customer requested to investigate why the recently replaced primary EM servers  causing outage.\n\r\n==>I have connected site and see the cpu alarms again on EM1.\n\r\n==>When I run \"top\" to see cpu usage see below weird situation.\r\nThere were 2 CPU present which is not normal as this old servers must have at least 4 CPUs.\n\r\n==>I also double checked the cpu number by running cat /proc/cpuinfo  and confirmed that there are only 2 CPUs working on the server .\n\n\r\n#########################\r\n 22:58:57  up  9:57,  2 users,  load average: 11.14, 10.52, 8.69\r\n891 processes: 887 sleeping, 3 running, 1 zombie, 0 stopped\r\nCPU states:  cpu    user    nice  system    irq  softirq  iowait    idle\r\n           total   54.2%    0.0%   25.4%   0.0%     1.6%   18.6%    0.0%\r\n           cpu00   56.6%    0.0%   16.6%   0.0%     3.3%   23.3%    0.0%\r\n           cpu01   51.7%    0.0%   34.4%   0.0%     0.0%   13.7%    0.0%\r\nMem:   510312k av,  470968k used,   39344k free,       0k shrd,    1780k buff\r\n                    276288k actv,   46252k in_d,    4684k in_c\r\nSwap: 4192752k av, 1974676k used, 2218076k free                   51232k cached\n\r\n##########################\n\r\nProblematic Server:\r\n=================\r\n[ntappadm@vpsslem0 bin]$ cat /proc/cpuinfo |grep processor\r\nprocessor       : 0\r\nprocessor       : 1\n\r\nHealty Server:\r\n================\r\n[ntappadm@VPSSLEM1 bin]$ cat /proc/cpuinfo |grep processor\r\nprocessor       : 0\r\nprocessor       : 1\r\nprocessor       : 2\r\nprocessor       : 3\n\n\r\n##########################\r\n==>Even though we replaced the server ,seems the new spare server has also hardware issues.\n\r\n==>I have activated the secondary SM  to reduce the traffic rate to EM1 . Primary DB is also running on EM1.\n\r\n==>Customer will need to replaced this new server as well which means we need to repeat all work we did during last two days.\n\r\n==>As this 3310 server type is already EOL,the only thing we can hope at this point is the new server which will be delivered to the customer tomorrow morning has all mandatory hardware components in it.\n\n\r\nThanks','null'),(156,'Emre OVA (NETAS External)','AS-OAM','2017-10-04','171004-649024','BT PLC (Manchester)','Thomas Godwin (ER) paged me to report that EMServer1 is down. They stated that EMServer1 went down due to memory and CPU issues on the server this morning. When we ping the EMServer1 from EMServer2, it was seen that destination host is unreachable. In this respect, we wanted to power cycle the EMServer1 and on-site engineer performed this operation. Also customer stated that the server is disconnected from network so that cooling down the server. For this reason, we could login to server via console. When we check the cpu usage by running \"top\" command, cpu usage was at 10% level. Customer also stated that they\'re planning to re-connect the network connections and see the cpu usage approximately 6 hours later. Since we don\'t have any action to take, I dropped from the bridge. If customer observe any abnormal behavior after network connection is provided again, they will contact ER.','null'),(157,'Oktay ESGUL','AS-OAM','2017-10-03','171002-648724','BT PLC','Thomas Godwin paged me out to report they need help for platform instalaltion of the replaced server.\n\r\nI have assisted them during platform installation and on site engineer performed the installation since there was no console access.\n\r\nOnce the server OS installation completed, we started application recovery.\n\r\nFirst, I have extracted oracle installer from DVD ,then installed the oracle at primary EM server.\n\r\nThen, run dbInstall  for db instalaltion.\n\r\nIn paralell, deployed primary SM and PROV instances.\n\r\nI have tried to resynch the  DBs from secondary to primary,yet this failed due to logon denied errors.\n\r\nIn order to skip this error,I have tried to restore the primary db via backup which taken from secondary db via scheduled dbbackup script,yet this also failed.\n\r\nThen, since the user/pass seems an issue, I have copied .dbuserdata from secondary EM server to primary`s below directories.\n\r\n.dbuserdata\r\n/var/mcp/install \r\n/var/mcp/run/MCP_17.0/mcpdb_0/data\r\n/var/mcp/run/MCP_17.0/SM_0/data\n\r\nOnce the mcpdbapp and mcsdbschema users became same name/passwd credentials,I have run restoreEmptyDB.pl at primary db first, then run resync from secondary to primary. This failed neither.\n\r\nThen,I have copied the daily backup of the customer to primary db and restored it.Yet,somehow primary db was not reachable and all instances had DBCOMM alarms.\n\r\nTried to login to sqlplus ,yet it also failed.\n\r\nThen, I have uninstall the db and oracle and install them from scratch with the same username/passwd with secondary DB. \n\r\nSync attempt from secondary to primary failed again, so then restored the primary db seperately.\n\r\nAbove actions solved the primary db access issue and provisioning outage ended.\n\r\nThough, there were DBComm alarms at instances,so that I have setup the DB replication from primary to secondary.\n\r\nAlso, we have requested a new license since the primary EM is replaced .KRS team provided new license and we applied it.\n\r\nCurrently ,replacement is completed and system is stable with exceptions. Rest of the problem will be investigated via new cases.\n\r\nThank you','null'),(158,'Emre OVA (NETAS External)','AS-OAM','2017-10-03','171003-648844','Liberty Global Europe B.V.','David Hedge (ER) paged me via trillian and reported that SESM5_1 down and unreachable. He also informed me that there were disk failures in the system. Accordingly, I\'ve connected the site and observed the disk failures.\n\r\nSES5S2 HWER 708  OCT03 02:15:08:642 MCP_17.0.22.20 \r\nListed partition(s) is/are faulty. \r\nmd5 (Failed: [sda12]) \r\nmd7 (Failed: [sda5]) \r\nmd6 (Failed: [sda11]) \r\nmd8 (Failed: [sda9]) \r\nmd9 (Failed: [sda8]) \r\nmd3 (Failed: [sda7]) \r\nmd4 (Failed: [sda10]) \r\nmd10 (Failed: [sda6]) \r\nmd1 (Failed: [sda2]) \r\nmd2 (Failed: [sda3]) \r\nmd0 (Failed: [sda1]) \n\r\nWhen we try to ping the problematic blade, it was not reachable. \n\r\n[ntsysadm@HELM0A2SM10 ~]$ ping 212.142.44.44 \r\nPING 212.142.44.44 (212.142.44.44) 64(92) bytes of data. \r\nFrom 212.142.44.179 icmp_seq=2 Destination Host Unreachable \n\r\nAlso, David noticed that card reseating operation has done but didn\'t solve the issue. As a first action, we suggested that deact/act problematic blade via NDM but it didn\'t work. \n\r\ncli>ha app-blade deact 1 0 13 0\r\ncli>ha app-blade act 1 0 13 0 \n\r\nThe same issue was observed approximately 2 months ago and deact/act operation was solved the issue (170524-632713). At that time, we were also highly recommended to replace rear card but as far as we learned, it was not replaced. Since this time deact/act operation via NDM did not work and there were hard drive failures in the system, we suggested rear card replacement. \n\r\nIn this regards, customer will check that if they have spare rear card or not as the next action. When they have new rear-card, reinstallation of atca blade will be performed.','null'),(159,'Oktay ESGUL','AS-OAM','2017-10-02','171002-648724','BT PLC','Tom Draper paged me out to request assistance for disk replacement of the faulty 3310 server .\n\r\nThere was not terminal access and all operation performed by on site engineer.\n\r\nWe have tried to insert one disk and boot up ,this failed.\n\r\nThen, replaced the hard drives with new one, tried to boot up ,failed.\n\r\nThen ,replaced both drivers ,this also failed.\n\r\nThen ,all parties of the conf bridge aggreed to replaced the server ,most probably server hard drive slots are faulty.\n\r\nCustomer ordered a new spare server, once the new server is replaced with existing one they will paged out for assistance.\n\r\nAfter this aggrement, all parties dropped from the bridge.\n\r\nThanks','null'),(160,'Oktay ESGUL','AS-OAM','2017-10-02','171002-648724','BT PLC','Thomas Godwin from ER paged me out to report that SM/DB is down at BT PLC.\n\r\nSystem Level:\n\r\nMCP_17.0.7.14.\n\r\nThe customer reported their primary EM Server(HP CC3310) had disk issues  on friday and  in order to recover they rebooted the server .Yet, when the server power cycled it could not boot up.\n\r\nThey tried several times,yet it did not help.\n\r\nCustomer has ordered new disks to replace the faulty ones,when they got new ones we ll continue  recovery of the system.\n\r\nThanks','null'),(161,'Yunus Ozturk','AS-OAM','2017-09-29','170929-648547','Bragg Communications (Eastlink)','Problem Description:\r\n====================\n\r\nFROM/TO LOAD: ATCA AS F/W upgrade\n\r\nFollowing the f/w upgrade, the FPM Server would not recover. Following actions were attempted: \n\r\n- kill the FPM instance. \r\n- restart/start/stop all did not work. \r\n- re-deploy \r\n- reboot the Host server. \n\r\nActions Taken:\r\n===============\n\r\nOnce the FW upgrade procedure was applied on the Host Server, the FPM Guest Server did not boot up properly after rebooting the Host Server. The network connection of this FPM Servers was lost.\n\r\n- We have connected the FPM Guest Server through MobaXterm on the Host Server and checked the console screen. It was noticed that the server stayed at the Linux Installation Menu screen and could not boot up for this reason which means that there is a virtual CD-ROM attached to the FPM Guest Server\n\r\n- We have removed the attached virtual CD-ROM from the FPM Guest Server and the issue is resolved','null'),(162,'Yunus Ozturk','AS-OAM','2017-09-25','170925-647931','Liberty Global Europe B.V.','Problem Description:\r\n====================\n\r\nER paged out us and reported the following problem;\n\r\nCustomer has provisioning requests failing with the following error through OSSGate: \n\r\nCFB P CFD P CWI CFU N 3WC ACRJ INACT WUCR CNDB LNR $^M \r\n^M \r\n^MSystem:LineProv; EndPoint can not be added to GateWay.^M \r\n^M System:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP vmg25.sesm9:SS/044 \r\n/9/0895 ^M \r\n^MDetails: Cannot perform operation. A Data access error occurred, please try later or contact your next level of support. Code is 4031^M \r\n^M> \r\n>> 161532.549:Connection handler 84.116.0.183-49322:LOW :com.metasolv.cartridge.oss.nt_cs2k_sip_isn09.CS2KSIPLineProv: \r\nresponse message = [ CFB P CFD P CWI CFU N 3WC ACRJ INACT WUCR CNDB LNR $^M \r\n^M \r\n^MSystem:LineProv; EndPoint can not be added to GateWay.^M \r\n^M System:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP vmg25.sesm9:SS/044 \n\r\nCustomer was not able make proviosioning through OSSGate due to an issue on A2 side..\n\r\nActions Taken:\r\n==============\n\r\nWhile trying to launch the MCP GUI, we noticed the following Oracle error and that prevented us to launch the GUI\n\r\n\"ORA-04031: unable to allocate 3896 bytes of shared memory\"\n\r\nTo fix this issue, we wanted to stop/start the Oracle but then noticed that the usage of /opt directory of the EM Servers was %100.\n\r\nWe removed the following files / logs to save space under /opt directory\n\r\ntrace logs:\n\r\n/opt/mcp/db/diag/rdbms/mcpdb/mcpdb/trace/mcpdb*trm\r\n/opt/mcp/db/diag/rdbms/mcpdb/mcpdb/trace/mcpdb*trc\n\r\ngiven that the DB name is configurable, this should be:\n\r\n/opt/mcp/db/diag/rdbms/*/*/trace/*trc\r\n/opt/mcp/db/diag/rdbms/*/*/trace/*trm\n\r\nlistener logs:\n\r\n/opt/mcp/db/diag/tnslsnr/*/listener/alert/log_*.xml\r\n/opt/mcp/db/diag/rdbms/mcpdb/mcpdb/alert/log.xml\r\n/opt/mcp/db/diag/tnslsnr/*/listener/trace/listener.log\r\n/opt/mcp/db/logs/lsnradm.log.*\n\r\nrdbms audits:\n\r\n/opt/mcp/db/product/11.2.0/rdbms/audit\n\r\narchivelog:\n\r\n/opt/mcp/db/logs/archivelogctl.log*\n\r\nstart and stop logs:\n\r\n/opt/mcp/db/logs/startDatabase.log*\n\r\nbackupLogs:\n\r\n/opt/mcp/db/logs/startDBBackup.log.*\r\n/opt/mcp/db/logs/stopDatabase.log.*\r\n/opt/mcp/db/logs/backupIndividualUser.log\n\r\nLogs about rotating logs and miscellaneous:\n\r\n/opt/mcp/db/slogs/manageFilesInAuditDir.log.*\r\n/opt/mcp/db/slogs/reconfigDbServer.log.*\r\n/opt/mcp/db/logs/dbRelease.log.*\r\n/opt/mcp/db/logs/dbmonitor.log.*\r\n/opt/mcp/db/product/11.2.0/log/dtfra1-a2db02/client/clsc2.log\r\n/opt/mcp/db/product/11.2.0/log/dtfra1-a2db02/client/clsc1.log\n\r\nAfter saving some space under /opt directory, we performed stop/start the oracle as follows;\n\r\ncd /etc/init.d/\r\n./dbora stop\r\n./dbora start\n\r\nThese actions recovered the issue that we had while launching the MCP GUI\n\r\nCustomer has informed us that they still had provisioning problems. At that point, we decided to restart the Prov Instances on MCP GUI and it fixed the provisioning issue.\n\r\nThen, we noticed that there were several Oracle Replication Link errors on Secondary DB. \n\r\nWe recommended the customer to perform resync.pl script from Primary DB to Secondary DB during MTC window. The instructions have been provided to the customer.','null'),(163,'Yunus Ozturk','AS-OAM','2017-09-25','170925-647945','Singtel Optus Pty Ltd','Problem Description:\r\n=====================\n\r\nSWD paged out us and reported the following problem;\n\r\nFROM/TO LOAD: 14.1.15.3/18.0.26.5 \n\r\nAfter upgrading the primary DB/SM, SM_0 did not take activity. \n\r\nStarting SM_0 \n\r\nStopping MCP_14.1/SM_1 instance \n\r\nTrying to ping and access primary SM (SM_0). \r\nPing to primary SM (SM_0) is successful, primary SM is accessible. \r\nStarting MCP_14.1/SM_1 instance \r\nDB/SM Upgrade Completed Successfully \n\r\nSM_1 remains active. \n\r\nCritical alarm exists on SM_0: \n\r\nAlarmName: LKEY_SOFTWARE_MISMATCH_ERROR_CRITICAL_755 \r\nTimeStamp: Tue Sep 26 02:28:34 EST 2017 \r\nFaultNumber: 755 \r\nShortFamilyName: LKEY \r\nLongFamilyName: LKEYSYS \r\nSeverity: CRITICAL \r\nProbableCause: file error \r\nDescription: License key problem detected: License key not compatiable with installed software version. \r\nCorrective Action: Install a valid license key. \n\r\nActions Taken:\r\n==============\n\r\nWe have checked the work logs for the problematic SM unit and noticed the following errors;\n\r\nSEVERE: Error initializing endpoint\r\njava.net.BindException: Address already in use :12120\r\nat org.apache.tomcat.util.net.JIoEndpoint.init(JIoEndpoint.java:549)\r\nat org.apache.coyote.http11.Http11Protocol.init(Http11Protocol.java:176)\r\nat com.nortelnetworks.mcp.ne.share.tomcat.MCPHttp11Protocol.init(MCPHttp11Protocol.java:57)\r\nat org.apache.catalina.connector.Connector.initialize(Connector.java:1014)\r\nat org.apache.catalina.startup.Embedded.start(Embedded.java:830)\r\nat com.nortelnetworks.mcp.ne.share.tomcat.BaseTomcatSubsystem.startTomcat(BaseTomcatSubsystem.java:159)\r\nat com.nortelnetworks.mcp.ne.share.tomcat.BaseTomcatSubsystem.activateCold(BaseTomcatSubsystem.java:87)\r\nat com.nortelnetworks.mcp.ne.base.subsystem.ActivateWarmRequest.handle(ActivateWarmRequest.java:54)\r\nat com.nortelnetworks.mcp.ne.base.subsystem.SubsystemManager.handle(SubsystemManager.java:58)\n\r\nThis was a known issue with 18.0.26 MR and it is already fixed with 18.0.28 and upper releases\n\r\nThis issue is seen intermittently and occurs while tomcat is being started. \"Address already been used\" error is thrown out. Further investigations showed that while tomcat is being restarted, addresses of embedded object on BaseTomcatSubsystem class are not filled and default IP is used instead of it. This leads sometimes (not properly shoutdown of tomcat or may be race condition) starting same ip-port binding while starting of tomcat.\n\r\nTo fix this issue, we have provided a JAR file to SWD and asked to apply it on both SM units. \n\r\nAfter applying the JAR file, the problem is resolved. The official fix exists on 18.0.28 and upper releases.','null'),(164,'Burak Biyik','AS-OAM','2017-09-22','170922-647799','Midcontinent Communications','During the investigation of the registration failures reported with E1 case (170922-647799), CallP GPS recommended swact operation for System Manager.\n\r\nFollowing this operation, it was reported that SESM instance state is not being displayed on MCP GUI, instead related window is blank.\n\r\nI noticed that this is not specific to SESM instance state, yet MCP GUI was throwing an error whatever it is clicked on. The error was complaining about user rights.\n\r\nI asked ER to re-launch MCP GUI with another user, which allowed all operations.\n\r\nIt was somehow related with the user being used for logging.\n\r\nI dropped the call as soon as everything was back on MCP GUI to let CallP GPS continue investigation.','null'),(165,'Emre OVA (NETAS External)','AS-OAM','2017-09-22','170922-647691','Suddenlink (Cebridge Connections Inc)','Kyle Mawst (ER) paged me to report that SESM1 unit 0 down after restarting the EMServers. Also there were DBCOM alarm for secondary database.\n\r\nFirst of all, I\'ve started to investigate the DB communication issue. In this regards, I\'ve connected the DBServer2 and tried to execute dbora stop/start so that restarting the DB.\n\r\nWhen I run this command, I saw that starting or stopping database was unsuccessful.\n\r\n[root@stal-sslem2 tmp]# /etc/init.d/dbora start\r\nStarting Oracle listener ...\r\nFail to start oracle listener...\r\nStarting Oracle database ...\r\nFail to start oracle database...\n\r\nIn order to investigate the reason for why these commands not working, I\'ve checked the related db logs under /opt/mcp/db/ and mcpdb work logs, but there were no any logs that is created in today\'s date.\n\r\nWe suspect that the reason for why any logs didn\'t produced might be related to the overloading of the /opt directory. In this regards, when we run \"df -k\" command on the secondary DB server, it\'s seen that /opt directory is reached at 100% level.\n\r\nWhen we look at the usage of the directories that are exist under /opt/mcp/db, it\'s seen that, old and unremoved alert logs caused the reaching and exceeding the critical levels for /opt directory.\n\r\n[root@stal-sslem2 listener]# du -h --max-depth=1 /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/lck\r\n276K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/metadata\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/incpkg\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/incident\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/metadata_pv\r\n661M    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/trace\r\n1.2G    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/alert\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/stage\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/metadata_dgif\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/sweep\r\n4.0K    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/cdump\r\n1.8G    /opt/mcp/db/diag/tnslsnr/stal-sslem2/listener/\n\r\nAfter deleting the old & unremoved logs which belong to the year 2010, 2011, usage of the /opt directory has decreased to 80% level.\n\r\n[root@stal-sslem2 alert]# df -k\r\nFilesystem           1K-blocks      Used Available Use% Mounted on\r\n/dev/md4               2030672   1087512    838344  57% /\r\n/dev/md1                101018     11852     83950  13% /boot\r\ntmpfs                  4077552         0   4077552   0% /dev/shm\r\n/dev/md5                497765     11229    460837   3% /home\r\n/dev/md7               6092288   4594208   1183616  80% /opt\r\n/dev/md6                497765    497765         0  10% /tmp\r\n/dev/md8               2030672     54524   1871332   3% /var\r\n/dev/md9               3049960     73360   2819172   3% /var/log\r\n/dev/md10            115041144  33396964  75706164  31% /var/mcp\r\n/dev/md0                 93207      5751     82644   7% /admin\n\r\nThen, when we retry to execute dbora stop/start, this time the same commands was successfully running.\n\r\n[root@stal-sslem2 tmp]# /etc/init.d/dbora start\r\nStarting Oracle listener ...\r\nOracle listener started.\r\nStarting Oracle database ...\r\nOracle database started.\n\r\nAfter the first part of pager support is completed, we moved on to the another issue. It was about that the SESM server is unreachable due to the disk issues on the server.\r\nIn order to understand that which disks are faulty, we\'ve investigated the server logs and the following logs was showing the reason.\n\r\n Info fld=0x96cc0a\r\n                   end_request: I/O error, dev sda, sector 9882634\r\n                   raid1:md4: read error corrected (8 sectors at 66856 on sda6)\r\n                   raid1: sda6: redirecting sector 66856 to another mirror\r\n                   sd 0:0:0:0: SCSI error: return code = 0x08000002\r\n                   sda: Current: sense key: Hardware Error\r\n                       Add. Sense: No defect spare location available\n\r\n                   Info fld=0x96cc13\r\n                   end_request: I/O error, dev sda, sector 9882643\r\n                   raid1:md4: read error corrected (8 sectors at 66864 on sda6)\r\n                   sd 0:0:0:0: SCSI error: return code = 0x08000002\r\n                   sda: Current: sense key: Hardware Error\r\n                       Add. Sense: No defect spare location available\n\r\n                   Info fld=0x96cc1a\r\n                   end_request: I/O error, dev sda, sector 9882650\r\n                   raid1:md4: read error corrected (8 sectors at 66872 on sda6)\r\n                   printk: 1 messages suppressed.\r\n                   raid1: sda6: redirecting sector 66872 to another mirror\r\n                   sd 0:0:0:0: SCSI error: return code = 0x08000002\r\n                   sda: Current: sense key: Hardware Error\r\n                       Add. Sense: No defect spare location available\n\r\nUnder these circumstances, we provided some actions plan to overcome disk failures on the server.\n\r\nFirst of all, disk reseating was tried but it didn\'t work.\r\nSecondly, disk swapping is suggested. It was performed by customer but it was also failed. This time sdb was seems as faulty.\n\r\n         Info fld=0x95c625\r\n                   end_request: I/O error, dev sdb, sector 9815589\r\n                   Mounting root filesystem.\r\n                   sd 0:0:1:0: SCSI error: return code = 0x08000002\r\n                   sdb: Current: sense key: Hardware Error\r\n                       Add. Sense: No defect spare location available\n\r\n                   Info fld=0x95c625\r\n                   end_request: I/O error, dev sdb, sector 9815589\r\n                   printk: 1 messages suppressed.\r\n                   Buffer I/O error on device sdb5, logical block 1027971\r\n                   sd 0:0:1:0: SCSI error: return code = 0x08000002\r\n                   sdb: Current: sense key: Hardware Error\r\n                       Add. Sense: No defect spare location available\n\r\nSince both of the disks are faulty, we suggested hardware replacement. Due to the fact that customer has already healthy disks, they wanted to perform this operation within today. In this respect, OS platform installation was needed because of the fact that both of the disks are replaced. \n\r\nWe shared the related IM with ER so that installing the platform. Finding the correct platform installer CD took a long time by customer. \n\r\nAfter the correct platform installer CD is inserted into the server, we\'ve started to install the OS Platform. After the platform installation has completed, it\'s seen that the replaced disks are healthy and up&running properly. So, problematic SESM instance was recovered.\n\r\nAfter the follow-up cases are created and required logs have been collected, I dropped from the bridge. Investigation will be ongoing for the further analysis.','null'),(166,'Burak Biyik','AS-OAM','2017-09-22','170922-647678','BSkyB','I have been paged by SWD to report a failure in \"PrepareDB\" screen while performing cleanup operation for the upgrade path 17.0.22.16 -> 17.0.31.3.\n\r\nThe same screen had failed for the first attempt of the upgrade and RCA analysis was provided within the case: 170802-642137.\n\r\nThe wizard was failing with the following error;\n\r\n\"An error occured during stopping DB monitor of [].\"\n\r\nThe cleanUpReplication logs did not show the reason of the failure clearly. Even if db was in quiesced (read-only) mode in the previous incident, this time it was accessible.\n\r\n#####################################################\r\nCleaning up primary DB replication \r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234834_m_90 | /opt/mcp/ned/bin/nedclient 10.234.207.27 4890 \r\nCommand Output: \r\n> config ok \r\n> run ok 1 \r\n> exited \n\r\nError occurred executing NE commands (see below): \n\r\nNE command exited with the value: 1 \r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234834_m_90 \r\nNot unlinking file \r\n#####################################################\n\r\nI followed below steps for the resolution of the issue;\n\r\n1. Try start/stop db monitor on MCP GUI --> failed with \"Unexpected error! Command not processed\"\r\n1. Restart NED with \"neinit restart\" --> failed after re-try\r\n2. Ensure that oracle is not quiesced mode and run \"activateRepDB.pl\" anyways --> failed after re-try\r\n3. Restart oracle database --> failed after re-try\r\n4. Run \"cleanupReplication.pl\" manually to drop replication between databases.  --> passed screen\n\r\nI requested db and upgrade logs for RCA analysis.\n\r\nAgreed and dropped the call.','null'),(167,'Burak Biyik','AS-OAM','2017-09-20','170920-647531','TELUS','With regards to JAR application MOP provided by CallP GPS within the case 170428-629247, customer claimed that Hot Standby SESM instance admin state was changed to CONFIGURED state while following the MOP.\n\r\nI explained that there is no way that instructions within the JAR application MOP may cause this change as CONFIGURED state is a part of UNDEPLOY operation. There is no such operation required for JAP application.\n\r\nThe solution was;\n\r\n1. Stopping SESM2_0 from cli with neStop.pl\n\r\n2. Deploying SESM2_0 from MCP GUI to change its state to ONLINE (re-deploy)\n\r\n3. Start SESM2_0 from cli with neStart.pl\n\r\nCustomer wanted to track the RCA of this problem with a case.\n\r\nAgreed and dropped the call.','null'),(168,'Oktay ESGUL','AS-OAM','2017-09-15','170914-646834','Frontier Communications','Martha from GVVP GPS reported Frontier can not make provisioning from GVPP due to below error:\n\r\n\" Cannot add users to this domain as user limit has not been set \"\n\r\nSystem Level :\n\r\n##########\r\nMCP_14_1.0.13\r\n##########\n\n\r\nI have connected site and take a look resource setting of the domain  which was set to zero. I have assigned resources and tried to add user from PROV manager which completed succesfully.\n\r\nIn paralell ,updated the customer to make several tests,yet they reported their reqular scripts are failing.\n\r\nWhen we investigated their GVPP  script, realized that they were trying to add the users to subdomain which does not exist.\n\r\nAlso, the customer reported there were ~14000 users at A2 side,yet when I checked  A2 DB, just saw 30 users.\n\r\nThese all pointed us a kind of undesired domain delete operation at PROV Gui .\n\r\nIn order to ensure what happened, investigated the OSS logs and found out that the domain contains 13900 users were removed manually by a mistake ..\n\r\n###################\r\nPROV2_0 PVIFSEC 620 INFO SEP13 14:36:49:295 MCP_14.1.0.13\r\n  Operation: Delete\r\n  Admin ID: admin@127.0.0.1 succeeded\r\n  Resource Type: DomainNaturalKeyDO\r\n  Resource(s): sm.ssl1.irngtxxbpsa.fttpvoice.com\r\n##################\r\nPROV2_0 PVIFSEC 620 INFO SEP13 15:11:03:750 MCP_14.1.0.13\r\n  Operation: Add\r\n  Admin ID: admin@127.0.0.1 succeeded\r\n  Resource Type: DomainNaturalKeyDO\r\n  Resource(s): sm.ssl1.irngtxxbpsa.fttpvoice.com\r\n###################\n\n\n\r\nFortunately , the customer has scheduled db backup and they have daily backup of the system.\n\n\r\nIn order to recover , I have tried to restore the db by  latest backup,yet db restore operation failed via below error.\n\r\n\"ORA-01403: no data found exception occurred\"\"\n\r\nIn order to move forward,applied below two steps;\n\r\n1. I have jumped to /var/mcp/install directory and run \"dbInstall -fo\" for primary server ,\r\n2. Jumped to /opt/mcp/db/bin directory and run \"restoreEmptyDB\" . \n\r\nThen, re-tried to restore the customer db backup ,which completed succesfully.\n\r\nOnce the db operation completed, we observed db communications alarms at secondary db .To clean up this alarm, I have re-setup the replication.\n\r\n==>As a last step,I have restarted all applications to reflect new data .\n\r\n==> Customer performed several call and provisioning tests which are all successfull.\n\r\nThen ,dropped the call.','null'),(169,'Emre OVA (NETAS External)','AS-OAM','2017-09-12','170912-646479','NetFortris','UPGRADE PATH (FROM/TO LOAD): 18.0.28.1 - 19.0.4.0\n\r\nDonnell Williamson (SWD) paged me to report that Prepare DB step failed. I\'ve connected the site via TeamViewer connection and started to work on the issue. When we launch the wizard, the following alarm was seen: \"An error occurred during stopping DB monitor of {}\". In order to understand the reason for why this screen has failed, we\'ve looked for the DB work logs. In this regards, it\'s seen that deadlock detected failures have been occured. We\'ve taken several actions (oracle stop/start, server reboot, ned restart) but the issue was still observing due to the same reason. (ORA-00060: deadlock detected while waiting for resource, ORA-06512: at \"SYS.DBMS_REPCAT_UTL\"). Apart from these, we have connected the db as sysdba and kill the oracle process id that we suspect the root cause of the deadlock issue but it didn\'t work.\n\r\nWe\'ve collected the required all logs (mcpdb oss logs, mcpdb trc/trm logs, AWR Report) and created a JIRA. This issue will be investigating under the following JIRA: AAK-51708\n\r\nIn parallel, we have created a ticket to Oracle (3-15727533581). \n\r\nIn the light of the opened tickets that I indicate the ticket IDs above, investigation will be ongoing.\n\r\nAfter the above actions has taken, I dropped from the bridge.','null'),(170,'Burak Biyik','AS-OAM','2017-08-25','TBD','NetFortris','I have been paged by SWD to report a failure in \"Patching Operating Systems of the Primary Servers\" screen for the upgrade path 18.0.20.6 -> 18.0.28.1.\n\r\nThe error was \"Error occured checking status of primary SM Server. Please be sure that its state is ONLINE-STANDBY.\n\r\nThe state of the SM_0 was ONLINE-DOWN-UNAVAILABLE. It seemed to fail starting properly. Because we do instance state validation at the end of this screen, it failed.\n\r\nI killed that instance and start it from MCP GUI to make its state \"ONLINE-UP-HOT STANDBY\".\n\r\nAfter \"re-try\", we could successfully passed the screen.\n\r\nAgreed and dropped the call.','null'),(171,'Burak Biyik','AS-OAM','2017-08-25','170825-644773','NetFortris','I have been paged by SWD to report a failure in \"PrepareDB\" screen while performing cleanup operation for the upgrade path 18.0.20.6 -> 18.0.28.1.\n\r\nThe cleanUpReplication logs did not show the reason of the failure clearly. We suspected current db status and tried to provision a test user from MCP GUI. This test failed since db was in quiesced mode.\n\r\nWe run \"activateRepDB.pl\" to turn db back to writeable mode, retried the screen and got the same failure. The database was again in quiesced mode.\n\r\nWe restart oracle database after then.\n\r\nWe noticed that the DB Servers have been running for around 700 days, which would definitely impact server load and applications running on it.\n\r\nWe rebooted the each DB VMs to ensure that high uptime would not cause any issues for the next screens.\n\r\nWe retried the screen and successfully passed it.\n\r\nAgreed and dropped the call.','null'),(172,'Cigdem Vural','AS-OAM','2017-08-20',' 170820-644190      ','Razorline','Customer is on the load 17.0.22.19\n\nMartha paged and reported that provisioning is not working for the customer. I asked if there is any prior action done and resulted with that and no action reported.\n\nWe logged into the site and see that SM/PROV/SESM instances were on configured step and SESM1_1 has the following alarm:\n\nAlarmName: Provisioning Manager or Personal Agent Manager unreachable \nTimeStamp: Thu Jun 01 15:04:49 EDT 2017 \nFaultNumber: 191 \nShortFamilyName: UAS \nLongFamilyName: UAS \nSeverity: CRITICAL \nProbableCause: configuration or customization error \nDescription: There is not any reachable Provisioning Manager or Personal \nAgent Manager \nCorrective Action: Configure a Provisioning Manager or Personal Agent Manager on \nthe system an activate NE \nlarmName: Provisioning Manager or Personal Agent Manager unreachable \nTimeStamp: Thu Jun 01 15:04:49 EDT 2017 \nFaultNumber: 191 \nShortFamilyName: UAS \nLongFamilyName: UAS \nSeverity: CRITICAL \nProbableCause: configuration or customization error \nDescription: There is not any reachable Provisioning Manager or Personal \nAgent Manager	\nCorrective Action: Configure a Provisioning Manager or Personal Agent Manager on \nthe system an activate NE \n\n\nI applied action plan as below:\n\n1- Kill/undeploy/deploy/start PROV instances first. Checked there is no JAR applied on PROV instances since they will be lost after undeploy/deploy action and there was no jar applied.\n\n2- Kill/undeploy/deploy/start SM instances . Checked there is no JAR applied on SM instances since they will be lost after undeploy/deploy action and there was no jar applied\n\n3- SESM1_1 came up as online/UP/Active after SM instances redeploy.  Checked and found there are 2 JAR applied on SESM instances. Before any action on SESM1_0, I backed up jar files to apply after redeploy.Kill/undeploy/deploy SESM1_0 instance. And after redeploy I applied the jar files to SESM1_0 and started it.\n\n4- After SESM1_0 is Online /UP/HotStandby I swacted the SESM instances and UAS 191 alarm got cleared.\n\nAfter all these actions, requested customer to test provisioning. they tested it and told everything is OK.\n\nThere was one alarm left on the system for secondary DB as below:\n\nlarmName: Oracle Replication Link Errors \nTimeStamp: Sun Aug 20 11:18:52 EDT 2017 \nFaultNumber: 727 \nShortFamilyName: DBMN \nLongFamilyName: DBMON \nSeverity: MAJOR \nProbableCause: unspecified reason \nDescription: Conflicts or incorrectly formed transactions have caused 1 \nerrors on link [ mcpdb_1 ]	\nCorrective Action: Contact next level of support if the problem persists. \n\n\nTold ER, that is not an urgent pager issue and can be work as a case. so requested them to open a case and send to our queue for investigation adn resolution.\n\nThen dropped from the call.','null'),(173,'Cigdem Vural','AS-OAM','2017-08-18','170818-644055','VTR GLOBAL COMM. SA','VTR Global was performing an upgrade from 14.0.9.12 to 14.0.16.5 load and it failed at DB MOCK screen.\n\r\nFailure was about a timeout on the running script:\r\n\"Script hasn\'t completed in predefined time arrival\"\n\r\nLogged in and checked for the logs, there was no failure written for the logs.\r\nThe only log was telling there might be a connection timeout or ned is unresponsive to complete.\n\r\nI restarted ned(neinit restart) on both EM servers where DB instances are located and then RETRY the failed DB mock screen.\n\r\nThis time it is passed and SWD will go on with the upgrade.','null'),(174,'Cigdem Vural','AS-OAM','2017-08-15','170815-643564','Claro (Puerto Rico)','Site Load:18.0.24.0\n\r\nTom paged and told customer has DB communication errors. MCP GUI was not launched because of a java issue on the ER VM PC. When I checked the logs ad saw that primary DB is down. I tried to login to HOST server where primary DB is guested and it was not reachable as well.\n\r\nSince HOST server is not reachable in any way, I requested a powercycle on the HOST server.\r\nAfter powercycle HOST server came up and all instances are recovered on it.\r\nPrimary DB is also come up. Customer told everything is OK and I requested /var/log/messages* and /var/log/dmesg* logs to check the failure reason of HOST server.\n\r\nThen dropped the call.','null'),(175,'Emre OVA (NETAS External)','AS-OAM','2017-08-11','170811-643206','Liberty Global Europe B.V.','Rodney Neese (ER) paged me to report that SESM7_1 is out of service. ER provided the site access and I connected the site. When I check the state of SESM7_1 instance, it was in \"online - unavailable\" status. First of all, I killed the instance and tried to start the instance again. It was killed successfully but wasn\'t starting properly.\n\r\nAlso I was able to ping the related sesm server but connecting the server via ssh were failing. In this respect, I suspected that there might be a restriction while connecting the server directly and tried to connect another sesm server first. I connected another sesm server and tried to jump to problematic sesm server but it\'s failed.\n\r\nDue to the fact that we can\'t do anything remotely, we asked that performing a physical reboot if someone is available in the site. Then, there were no available person to perform this operation and customer said that they can\'t do this operation until Sunday night.\n\r\nThen I dropped from the bridge since next action plan is provided. Customer will update the case once they go to site and reboot the SESMServer14 (SESM7_1)','null'),(176,'Emre OVA (NETAS External)','AS-OAM','2017-08-08','170808-642703','Canadian Imperial Bank of Commerce (CIBC)','Tom Draper (ER) paged me to report that IMMServer1 is not reachable. I connected the site and started to work on the issue.\n\r\nWhen I check the issue, I found a related case about the disk failure on IMMHostServer (170807-642607). In parallel, I was trying to connect IMMServer1.\n\r\nBy the way, IMM GPS (Nevriye) stated that they couldn\'t shutdown & reboot the IMM1Server from host server via the following commands:\n\r\nvirsh shutdown IMMServer1 / virsh reboot IMMServer1\n\r\nIn addition to this, pinging IMMServer1 from IMMServer2 was unsuccessful. IMMServer1 was unreachable. In order to be sure that the guest servers are up and running properly, I connected to the IMMHostServer1 and run \"virsh list --all\" command. \n\r\nIn spite of the fact that IMMServer1 and MASServer1 was looking as running, we were not able to connect IMMServer1 whereas MAS1Server is reachable. Also we were not able to ping IMMServer1 (10.28.0.155) from IMMServer2 (10.28.0.156)\n\r\nApart from these, I checked the uptime of IMMHostServer and it was up for 198 days.\n\r\nThe only way was performing reboot operation on HOSTServer1 since we can\'t do anything on IMMServer1 remotely. \n\r\nAfter rebooting the HostServer1, IMMServer1 became reachable and pingable.\n\r\nThen, I dropped from the bridge.\n\r\nCase is transferred to PS A2 IMM queue for root cause analysis.','null'),(177,'Yunus Ozturk','AS-OAM','2017-08-03','170802-642137','BSkyB','Problem Description:\r\n====================\n\r\nSWD paged out OAM GPS for the following issue;\n\r\nUpgrade Path : 17.0.22.16 to 17.0.31.3\n\r\nA2 DB cleanup replication failed. But showed 100% complete. We then save & exit but while started Step 19 Prepare DB failed again. It appeared an error occurred during stopping DB monitor.\n\r\nStarted at => Wed Aug 2 23:46:32 2017 \r\nNo paramaters passed to script \n\r\nValidating the required scripts and files for cleanupReplication \r\nRead initial properties from: /var/mcp/install/installprops.txt \r\nFound ne.load \r\nFound db.host \r\nFound db.secHost \r\nFound db.neName \r\nFound db.type \r\nFound db.backup \r\nGID = 92 and stats[5] = 92 \r\nDirectory permissions are = rwx, rwx, --- \r\nGID = 92 and stats[5] = 92 \r\nDirectory permissions are = rwx, rwx, --- \n\r\nLoading available information for MCP_17.0.22.16_2016-02-12-1214 \r\nloadinfo keys found: \r\nMinorRelease \r\nPatchRelease \r\nMajorRelease \r\nMaintenanceRelease \r\nBuildTime \r\nLoadVersion \r\nLoadName \r\nProduct \r\nDescription \n\r\nrelease = MCP_17.0 \r\nChanged value to Single for tag db\\.type \r\nMade 1 changes to the /var/mcp/install/installprops.txt file \n\r\nDisabling write access to the Regdest table \r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234635_6tWT | /opt/mcp/ned/bin/nedclient 10.234.207.27 4890 \r\nCommand Output: \r\n> config ok \r\n> run ok 0 \r\n> exited \n\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234635_6tWT \r\nNot unlinking file \n\r\nQuiescing replication groups \r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234701_qAVA | /opt/mcp/ned/bin/nedclient 10.234.207.27 4890 \r\nCommand Output: \r\n> config ok \r\n> run ok 0 \r\n> exited \n\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234701_qAVA \r\nNot unlinking file \n\r\nCleaning up primary DB replication \r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234834_m_90 | /opt/mcp/ned/bin/nedclient 10.234.207.27 4890 \r\nCommand Output: \r\n> config ok \r\n> run ok 1 \r\n> exited \n\r\nError occurred executing NE commands (see below): \n\r\nNE command exited with the value: 1 \r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170802_234834_m_90 \r\nNot unlinking file \n\r\n> config ok \r\nNE command exited with the value: 1 \r\n> exited \n\r\nFailed to cleanup replication on the server 10.234.207.27 \n\r\nTerminated at => Wed Aug 2 23:55:12 2017 \n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and checked the logs..\r\n- Noticed the following errors on the cleanupReplication logs;\n\r\n cleanupReplication.pl   Started at  =>  Thu Aug  3 00:51:05 2017 by ntdbadm\n\r\n  You are about to drop replication between the 2 Databases. This will put both\r\n  Databases into a \"single mode\", and changes will not be replicated between them.\r\n  Please Confirm(Y/N): [N]y\n\r\n Invoking quiecseRepDB method\r\n quiecseRepDB completed\r\nCleaning up PRIMARY DB replication\r\nERROR:\r\nORA-03114: not connected to ORACLE\n\r\nsqlplus  -S fails\r\nCleanup Replication failed on PRIMARY Database.  Please resolve the problem and then try again\n\r\nERROR:\r\nORA-03113: end-of-file on communication channel\r\nProcess ID: 0\r\nSession ID: 0 Serial number: 0\n\r\nERROR:\r\nORA-01017: invalid username/password; logon denied\n\n\r\nSP2-0306: Invalid option.\r\nUsage: CONN[ECT] [{logon|/|proxy} [AS {SYSDBA|SYSOPER|SYSASM}] [edition=value]]\r\nwhere  ::= [/][@]\n ::= [][/][@]\r\nSP2-0157: unable to CONNECT to ORACLE after 3 attempts, exiting SQL*Plus\r\nDECLARE\r\n  v_synonym all_synonyms.synonym_name%type;\r\n  v_cnt     number;\r\nBEGIN\r\n   select synonym_name INTO v_synonym from all_synonyms \r\n     where table_owner=\'MCSDBSCHEMA\' and table_name=\'REGDEST\';\r\n   IF 1 = 0 THEN\r\n      -- Revoke the privs - if they exist\r\n      select count(*) INTO v_cnt from dba_tab_privs \r\n        where grantee=\'MCSDBAPP_ROLE\' and table_name=\'REGDEST\'\r\n          and PRIVILEGE IN (\'INSERT\', \'UPDATE\', \'DELETE\');\r\n      IF v_cnt > 0 THEN\r\n         execute immediate \'revoke insert, update, delete on  \'||v_synonym||\' from MCSDBAPP_ROLE\';\r\n      END IF;\r\n   ELSE\r\n      -- Grant the privs\r\n      execute immediate \'grant insert, update, delete on  \'||v_synonym||\' to MCSDBAPP_ROLE\';\r\n   END IF;\r\nEND;\r\n/\n\r\nsqlplus  -S fails at /usr/share/perl5/mcsBase/SysUtl.pm line 400\r\n	mcsBase::SysUtl::pipedCmd(\'mcsBase::SysUtl=HASH(0x260df30)\', \'sqlplus  -S\', \'ARRAY(0x2b46e28)\', \'sys\') called at /usr/share/perl5/mcsBase/SysUtl.pm line 445\r\n	mcsBase::SysUtl::pipedSysCmd(\'mcsBase::SysUtl=HASH(0x260df30)\', \'sqlplus  -S\', \'ARRAY(0x2b46e28)\') called at /usr/share/perl5/DB/OraSQLWrapper.pm line 613\r\n	DB::OraSQLWrapper::sqlCmd(\'AppSqlWrapper=HASH(0x2a89af0)\', \'sys\', \'-S\', \'PRIMARY\', \'DECLARE\\x{a}  v_synonym all_synonyms.synonym_name%type;\\x{a}  v_cnt  ...\') called at /usr/share/perl5/DB/OraSQLWrapper.pm line 551\r\n	DB::OraSQLWrapper::sqlSysCmd(\'AppSqlWrapper=HASH(0x2a89af0)\', \'-S\', \'PRIMARY\', \'DECLARE\\x{a}  v_synonym all_synonyms.synonym_name%type;\\x{a}  v_cnt  ...\') called at /var/mcp/run/MCP_17.0/mcpdb_0/bin/util/../base/DBApp.pm line 1635\r\n	DBApp::grantOrRevokeWriteRegdest(\'CleanupReplication=HASH(0x2b351c8)\', 1, \'PRIMARY\') called at ./cleanupReplication.pl line 175\r\n	CleanupReplication::dropReplication(\'CleanupReplication=HASH(0x2b351c8)\') called at ./cleanupReplication.pl line 89\r\n	CleanupReplication::start(\'CleanupReplication=HASH(0x2b351c8)\') called at /usr/share/perl5/mcsBase/BaseOperation.pm line 561\r\n	mcsBase::BaseOperation::main(\'CleanupReplication=HASH(0x2b351c8)\') called at ./cleanupReplication.pl line 55\n\r\nsqlplus  -S fails\r\nERROR:\r\nORA-03113: end-of-file on communication channel\r\nProcess ID: 0\r\nSession ID: 0 Serial number: 0\n\r\nERROR:\r\nORA-01017: invalid username/password; logon denied\n\r\nSP2-0306: Invalid option.\r\nUsage: CONN[ECT] [{logon|/|proxy} [AS {SYSDBA|SYSOPER|SYSASM}] [edition=value]]\r\nwhere  ::= [/][@]\n ::= [][/][@]\r\nSP2-0157: unable to CONNECT to ORACLE after 3 attempts, exiting SQL*Plus\r\nsqlplus  -S fails at /usr/share/perl5/mcsBase/SysUtl.pm line 400.\n\r\n ERROR: cleanupReplication.pl Terminated at  =>  Thu Aug  3 00:52:39 2017\n\r\n     Reference the following log file for additional details:\r\n      /var/mcp/run/MCP_17.0/mcpdb_0/work/cleanupReplication.log\n\r\n- Tried to restart the NED process on the EM/DB Servers. It did not fix the issue.\r\n- Manually tried to run the cleanupReplication.pl script. Failed with the same reason. \r\n- Stopped / Started the Oracle. It did not fix the issue.\r\n- Tried to add a test user account through MCP GUI and noticed that the DB was at quisced mode and that was preventing write operations\r\n- Ran the script activateRepDB.pl to take the DB from quisced mode\r\n- Saved&Exited from the wizard and re-launched it again. This time the screen passed successfully\r\n- Even the screen passed successfully, customer did not want to proceed with the rest of the upgrade as they had limited MTC window time and they did not want to risk the upgrade in case of any further failures on the next steps of the wizard. \n\r\nBased on the logs, we thought that before dropping the replication, DB was taken into quisced mode and at that time a problem occurred on oracle connection as we have seen the following error;\n\r\nERROR:\r\nORA-03114: not connected to ORACLE\n\r\nsqlplus  -S fails\r\nCleanup Replication failed on PRIMARY Database.  Please resolve the problem and then try again\n\r\nDue to this oracle connection issue, cleanup replication step did not work successfully and DB stayed stuck at quisced mode. When we have stopped/started the oracle, the oracle connection issue was resolved but since DB was still at quisced mode, cleanup replication still failed as it could not access/write the DB. \n\r\nWhen we have manually took the DB from quisced mode, cleanup replication was able to access/write the DB successfully this time and the screen passed. \n\r\nWe will investigate the issue further to see the reason of the oracle connection issue within the case. \n\r\nCustomer will re-arrange another MTC window and start the upgrade from the beginning.','null'),(178,'Oktay ESGUL','AS-OAM','2017-07-28','170709-638757','BT MSL','John Kishner from ER paged me out for reinstallation of SESM3_0 Nehalem blade which is replaced due to hard drive issues.\n\r\n##############\r\nMCP SW Release\n\r\n17.0.22.8\r\n##############\n\r\n==>John connected site and Josh from Genband was on site for physical interactions.\n\r\n==>Firstly, I have offline/deactivate  the 0 0 13 0 blade by running below command.\n\r\n-ha app-blade deactivate 0 0 13 0\n\r\n-ha app-blade offline 0 0 13 0\n\r\n==>Josh replaced the rear card first ,since the hard drives located at rear.\n\r\n==>Then, I have applied below procedure at NDM to start pxe installation from NDM:\n\r\n- Go to /opt/corp/a2 file system on the NDM server. \r\n- Removed the content in the /opt/corp/a2/mcpcd directory by typing the following command: \r\nrm -Rf /opt/corp/a2/mcpcd/* \r\n-Mounted the ISO to `mnt` directory as follows; \r\n- mount -o loop mcp_core_linux_ple2-17.0.18.iso /opt/corp/a2/mnt/ \r\n- Copied the configNDMForA2E.pl and cliCommands.exp scripts from /opt/corp/a2/mnt/extra/pxe/ directory to /opt/corp/a2/ directory \r\n- cd /opt/corp/a2/mnt/extra/pxe/\r\n- cp configNDMForA2E.pl /opt/corp/a2/ \r\n- cp cliCommands.exp /opt/corp/a2/ \r\n- cd /opt/corp/a2/ \r\n- chmod 775 configNDMForA2E.pl \r\n- chmod 775 cliCommands.exp \r\n- Run ./configNDMforA2E.pl -blade 0 0 13 0 under /opt/corp/a2/ directory .\n\r\n==>Eventhough, configNDM script run succesfully, server could not be booted from PXE somehow .\n\r\n==>I have tried several attempts by deactivate/activate the blade ,yet it did not help.\n\r\n==>In the meantime, NDM blocked to run maintenance commands due to below error:\n\r\n#####\r\n%Warning: A routine exercise (REx) is currently underway. This command may interact destructively with the REx tests. Do you want to continue?\r\n#####\n\r\n==>In order to recover this failure, I have run below commands per salesforce research:\n\r\n###\r\n-rex test abort\r\n#####\n\r\n==>Once the NDM recovered, we decided to replace the front card as well.\n\r\n==>I have deactivate the blade and Josh replaced the front card.\n\r\n==>Activated the server and check the BIOS in order to disable O/S Watchdog to avoid unexpected reboot in the future.\n\r\n==> Then, re-run the above procedure and  configNDMforA2E.pl , this time server booted from PXE succesfully and installation started.\n\r\n==>Server platform installation completed successfully .Yet, while  dropping the PXE connection between NDM and Blade , NDM blocked the actions via below error.\n\r\n############\r\n%Error[0x18]: Command is not allowed during data restore process. \n\r\nData restore process issue got cleared after aborting a stuck backup process\r\nCustomer was able to issue maintenance commands successfully\r\n############\n\r\n==>To solve this issue, run below command at NDM.\n\r\n############\r\n-cli>software backup archive abort---------------aborted the backup process \r\n%Warning: If a backup is in progress, this command will abort it. Do you want to continue? \r\nconfirm (y/n)>y\r\n###############\n\n\r\n==>Ultimately, server replacement and reinstallation completed successfully.\n\r\n==>In order to revert the temporary system conf changes back, I have undeployed SESM3_0 instance and redeployed it to newly installed server.\n\r\n==>Also, below jars are applied to new SESM instances.\n\r\n17.0.22.8_AAK-40694_24July.jar\r\n17.0.22.8_AAK-41795_Oct_15th.jar\r\n17.0.22.8_AAK-46776_Sept22.jar\n\r\n==>We failed over the traffic to newly installed SESM3_0  and customer verified system functionality,then I dropped.\n\r\nThanks','null'),(179,'Oktay ESGUL','AS-OAM','2017-07-25','170725-640967','VTR GlobalCom S.A','Tom draper from ER paged me to report that VTR Global Com SA techician has removed the entire domain while he was working to remove a user. The worst thing this removed domain had been the only domain in the system which means all system provisioning is lost.\n\r\nI have connected site and verified the domain is removed.\n\r\n==>However,we faced with another tragedy  while planning to restore a db backup since the  latest backup of the system was from 2012 (They have never taken a backup of the system for last 5 years )\n\r\n==>Since there is nothing to do with gps perspective of view, I just recommended them to restore the post upgrade backup of current MCP_14.0.9.11 release.\n\r\n==>I have take a backup prior performing any action.This took almost an hour.\n\r\n==>In paralell, server spool files are full and there were critical alarms.I have cleaned up the server spool directories by running below instruction.\n\n\r\n######\r\nfind /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f -exec rm -f {} \\; ; find /var/mcp/spool/tmom -type f -exec rm -f {} \\; ; echo DONE \r\n#########\n\r\n==>Then , restored the post upgrade backup from 2012 with customer approval.\n\r\n==>Restore operation failed with below error:\n\r\n#########################\r\nProcessing object type SCHEMA_EXPORT/USER\r\nORA-39126: Worker unexpected fatal error in KUPW$WORKER.PUT_DDLS [USER:\"MCPUSER\"]\r\nORA-01403: no data found\n\r\nORA-06512: at \"SYS.DBMS_SYS_ERROR\", line 95\r\nORA-06512: at \"SYS.KUPW$WORKER\", line 6345\n\r\n----- PL/SQL Call Stack -----\r\n  object      line  object\r\n  handle    number  name\r\n0x707d5860     15032  package body SYS.KUPW$WORKER\r\n0x707d5860      6372  package body SYS.KUPW$WORKER\r\n0x707d5860     12391  package body SYS.KUPW$WORKER\r\n0x707d5860      3346  package body SYS.KUPW$WORKER\r\n0x707d5860      6972  package body SYS.KUPW$WORKER\r\n0x707d5860      1314  package body SYS.KUPW$WORKER\r\n0x707b7bc8         2  anonymous block\n\r\nJob \"SYSTEM\".\"SYS_IMPORT_SCHEMA_01\" stopped due to fatal error at 16:33:35\n\n\n\r\nDatabase impdp fatal error at /usr/lib/perl5/site_perl/DB/OraDataPump.pm line 826\r\n        DB::OraDataPump::checkFatalError(\'DbRestore=HASH(0x88c02a4)\') called at /usr/lib/perl5/site_perl/DB/OraData Pump.pm line 96\r\n        DB::OraDataPump::impExpData(\'DbRestore=HASH(0x88c02a4)\') called at /usr/lib/perl5/site_perl/DB/OraDataPump. pm line 215\n\n\r\n###################\n\r\n==>I  did ora stop/start .\n\r\n==>Then,run  /opt/mcp/db/bin/restoreEmptyDb.pl after dropping the replication between primary and secondary db.\n\r\n==>This helped me to have  a fresh db to restore.\n\r\n==>Once the previous action completed,I have retried the dbrestore via old backup file.\n\r\n==>Restore operation completed succesfully.\n\r\n==>Restarted the prov and sm instances.\n\n\r\n==>## 8247 ### user`s datafill is restored per userinfo table at db. However, customer informed us that they lost many customers at recent five years ,that s why there was around ##1000### users before removing the entire domain.\n\r\n==>In order to make the both DBs sync, I have setup the db replication and all db related alarms are cleared.\n\r\n==>Long story to short, db restored via 5 years old backup succesfully and  customer will need to work further on their own side to understand the impact of this tragic mistake they did.','null'),(180,'Oktay ESGUL','AS-OAM','2017-07-25','170725-640978','OPTUS','Peter pinged us from trillian to report secondary oracle patch failure.\n\r\nUpgrade Path:\n\n\r\nFrom : 14.1.15.3\r\nTo: 18.0.26.5\n\r\nPer secondar oracle patch logs, we observed below error.\n\n\n\r\n##############\r\n\"Prerequisite check \"CheckSystemSpace\" failed. \r\nThe details are: \r\nRequired amount of space(226.342MB) is not available.\" \n\r\n################\n\r\nIn order to not to loose time to detect which file causing high disk threshold, we have manually reinstall the secondary oracle which let the wizard complete corresponding successfully.\n\r\nOnce we complete the actions, Peter continue with the rest of the upgrade.\n\r\nThank you','null'),(181,'Oktay ESGUL','AS-OAM','2017-07-25','170725-640923','OPTUS','Peter Maloney paged me out to report Prepare DB failure at OPTUS Major upgrade.\n\r\nFrom : 14.1.15.3\r\nTo: 18.0.26.5\n\r\nPrepare DB screen was failing while taking the backup of the DB due to below error.\n\r\n#############\r\nORA-01013 user requested cancel of current operation\n\r\nCause: The user interrupted an Oracle operation by entering CTRL-C, Control-C, or another canceling operation. This forces the current operation to end. This is an informational message only.\n\r\nAction: Continue with the next operation.\r\n#############\n\r\nAs a recovery action, I have tried to kill stuck dbBackup processes, yet this did not help .\n\r\nThen , per web search  I have rebooted the server to clean up all stuck process and for fresh oracle start since the potential trigger is corrupted processes.\n\r\nOnce the reboot performed, server boot up succesfully.\n\r\nWe failed over the SMs,then re-launch the wizard.\n\r\nRetry is clicked and the operation completed succesfully by wizard.\n\r\nPeter will continue the upgrade.\n\r\nThank you','null'),(182,'Yunus Ozturk','AS-OAM','2017-07-13','170711-638948','BT MSL','Problem Description:\r\n====================\n\r\nER paged out GPS to recover the SESM3_0 unit on customer site. Since the previous replaced rear card has also problem, customer has decided to deploy the SESM3_0 unit into SESM4Server1 blade/slot where the SESM4_0 instance is deployed into in order to take the SESM3 from E2 situation.. \n\r\nActions Taken:\r\n===============\n\r\nSESM3_0 instance is deployed into SESM4_0 blade/slot (SESM4Server1).  Now SESM3_1 is at Active / SESM3_0 is at Hot Standby status. SESM4_0 instance is removed and SESM4_1 is working as standalone at Active status.\n\r\nWe have applied the SESM Jar files into new SESM3_0 unit.\n\r\nCustomer made test calls and they all passed\n\r\nOnce customer has the new spare rear card, the OS installation will be performed on SESM3Server1 blade/slot (0 0 13 0) and SESM3_0 instance will be deployed back to this blade.. Additionally, SESM4_0 instance will be deployed back to its original blade (SESM4Server1)','null'),(183,'Yunus Ozturk','AS-OAM','2017-07-12','170711-638948','BT MSL','Problem Description:\r\n====================\n\r\nER paged out GPS for new ATCA blade installation activity.. Customer was advised to replace the problematic RTM card on a previous pager call. After replacing the card, we have been asked for assistance to install the blade from scratch. \n\r\nCustomer\'s A2 Release : 17.0.22.8\r\nPlatform Level : 17.0.18 ple2\n\r\nActions Taken:\r\n===============\n\r\n- Applied the following official procedure for this re-installation activity. The procedure that we have used on the previous MTC window did not work for some reason. \n\r\n- Go to /opt/corp/a2 file system on the NDM server.\r\n- Remove the content in the /opt/corp/a2/mcpcd directory by typing the following command:\r\nrm -Rf /opt/corp/a2/mcpcd/*\r\n- Mount your ISO to `mnt` directory as follows;\r\n- mount -o loop mcp_core_linux_ple2-17.0.18.iso /opt/corp/a2/mnt/\r\n- Copy the configNDMForA2E.pl and cliCommands.exp scripts from /tftpboot/cnp/ssl/mnt/extra/pxe/ directory to /opt/corp/a2/ directory \r\n- cd /tftpboot/cnp/ssl/mnt/extra/pxe/\r\n- cp configNDMForA2E.pl /opt/corp/a2/\r\n- cp cliCommands.exp /opt/corp/a2/\r\n- cd /opt/corp/a2/\r\n- chmod 775 configNDMForA2E.pl\r\n- chmod 775 cliCommands.exp\r\n- Run ./configNDMforA2E.pl -blade 0 0 13 0 under /opt/corp/a2/ directory\n\r\n- Once the OS installation is completed with the procedure above, the blade was rebooted but it did not boot up properly from the disc even if the iso file was unmounted. \n\r\n- We have re-tried the same installation process for a few times and we have seen the same issue all the time. \n\r\n- We have tried to deactive/offline and online/activate operations several times but we were not able to get the blade to boot up from the disc.\n\r\n- We have tried to re-seat the front/rear cards one by one but it did not help. We have replaced the front card with a spare one and used the same rear card but it did not help as well. \n\r\n- When we have checked the BIOS settings, we noticed the corresponding disc was not listed on the Boot Order list. Even we were able to complete the OS installation, the blade did not boot up from the disc as the disc was not listed on the Boot Order list. \n\r\n- The Boot Order list was showing the 2 Boot Options (IBA GE Slot 0600 v1327 and Built-in EFI Shell). We expect to see 3 Boot Options here including the disc (0900 ID00 LUN0 SEAGATE ST9146). This is what we see on our labs. \n\r\n- We did not understand how we were able complete the OS installation even it is not listed on the BIOS Boot Order list. May be the installation was completed on flash or cache of the server and that made us think that we completed the investigation.  \n\r\n- Since we did not see the disc on the BIOS list, we are thinking that we still have a hardware problem. Without having the disc on the Boot Order, the blade will not boot up from the disc. Instead, it will continue to boot up from the network.\n\r\n- Even customer got this rear card recently from Genband, it seems that it still has a problem. \n\r\n- We are thinking that customer needs to order new front/rear cards and we can retry the re-installation process.','null'),(184,'Yunus Ozturk','AS-OAM','2017-07-11','170709-638757','BT MSL','Problem Description:\r\n====================\n\r\nER paged out GPS for new ATCA blade installation activity.. Customer was advised to replace the problematic RTM card on a previous pager call. After replacing the card, we have been asked for assistance to install the blade from scratch. \n\r\nCustomer\'s A2 Release : 17.0.22.8\r\nPlatform Level : 17.0.18 ple2\n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and tried to apply the following procedure to install the blade\n\r\n- Go to /opt/corp/a2 file system on the NDM server.\r\n- Mount your ISO to `mnt` directory  as follows;\r\n- mount -o loop mcp_core_linux_ple2-17.0.18.iso /tftpboot/cnp/ssl/mnt/\r\n- cd /tftpboot/cnp/ssl/mnt\r\n- Copy the configNDMForA2E.pl and cliCommands.exp scripts from /tftpboot/cnp/ssl/mnt/extra/pxe/ directory to /tftpboot/cnp/ssl/ directory  \r\n- cd /tftpboot/cnp/ssl/mnt/extra/pxe/\r\n- cp  configNDMForA2E.pl / tftpboot/cnp/ssl/\r\n- cp cliCommands.exp / tftpboot/cnp/ssl/\r\n- cd /tftpboot/cnp/ssl/\r\n- chmod 775 configNDMForA2E.pl\r\n- chmod 775 cliCommands.exp\r\n- Run ./configNDMforA2E.pl -blade 0 0 13 0 script and  watch installation steps through the console connection screen\n\r\nWhen we tried to run the ./configNDMforA2E.pl -blade 0 0 13 0 script, we received the following error;\n\r\nroot@typhoon-base-unit0:/tftpboot/cnp/ssl> ./configNDMForA2E.pl -blade 0 0 13 0 \n\r\nWARNING!!! Platform iso should be mounted under /opt/corp/a2 \n\r\nWe thought that we have a problem with the procedure or the iso file is corrupted..\n\r\nWe have asked the customer to re-schedule another MTC window after we verify the correct procedure.. \n\r\nSeems like the correct procedure should be as follows;\n\r\n- Go to /opt/corp/a2 file system on the NDM server.\r\n- Remove the content in the /opt/corp/a2/mcpcd directory by typing the following command:\r\n  rm -Rf /opt/corp/a2/mcpcd/*\r\n- Mount your ISO to `mnt` directory  as follows;\r\n- mount -o loop mcp_core_linux_ple2-17.0.18.iso /opt/corp/a2/mnt/\r\n- Copy the configNDMForA2E.pl and cliCommands.exp scripts from /tftpboot/cnp/ssl/mnt/extra/pxe/ directory to /opt/corp/a2/ directory  \r\n- cd /tftpboot/cnp/ssl/mnt/extra/pxe/\r\n- cp configNDMForA2E.pl /opt/corp/a2/\r\n- cp cliCommands.exp /opt/corp/a2/\r\n- cd /opt/corp/a2/\r\n- chmod 775 configNDMForA2E.pl\r\n- chmod 775 cliCommands.exp\r\n- Run ./configNDMforA2E.pl -blade 0 0 13 0 under /opt/corp/a2/ directory\n\r\nWe will be applying the procedure on the next MTC window. It seems that customer has 3 SESM Jar files that need to be applied after completing the re-installation activity.','null'),(185,'Emre OVA (NETAS External)','AS-OAM','2017-07-10','170709-638757','BT MSL','Thomas Godwin (ER) paged me to report that SESM3_0 is down and unreachable. I connected the site via GTS VM and started to work on the issue.\n\r\nAs a first action, I tried to ping the problematic sesm server and couldn\'t receive any response. Due to the fact that sesm server is unreachable, we performed deact / act operation via the active NDM. \n\r\ncli> ha app-blade deact 0 0 13 0\r\ncli> ha app-blade act 0 0 13 0 \n\r\nAfter deactivating & activating the blade, we saw that it hasn\'t recovered and problem still remains.\n\r\nThen, we suggested reseating the blade. Customer re-seated the REAR blade NTRY53DA but that did not help. After that, we observed that there were still seeing I/O error on disk drive.\n\r\nAs a result, we decided that the REAR card have to be replaced since we can\'t perform any operation at this point. So, customer will RMA the card.\n\r\nIn order to continue the further investigation, a follow-up case has opened and dispatched to OAM GPS queue. (170710-638759)\n\r\nThen I dropped from the bridge.','null'),(186,'Cigdem Vural','AS-OAM','2017-07-08','170708-638736','Sejong Telecom','Tom paged as he was starting a SSL migration from AS 9.1 to AS 19.\r\nAnd his A2C tool is not working,he was trying to find AS db user/pwd.\n\r\nWe told him that is not a pager call and tool should be checked before the maintenance window start. \n\r\nProvided him the AS db user/pwd which was written at installprops.txt and the user/pwd was correct since we could login to DB with these user/pwd.\n\r\nTom insisted there is a problem with the AS DB but no issue.\r\nHe asked for DBSID and provided him as \"mcpdb\".\n\r\nAfter that let him work with A2C tool designer since all is OK on AS side and his tool could not connect.\n\r\nBy that time I figured that he is using port as 5121 to connect to AS listener with his tool. Corrected him to use 1521 and then tool team also provided a correct Java version and tool started to work.\n\r\nThat should not be a pager call and tool should be tested and verified before with that very old and retired load to be prepared for maintenance.','null'),(187,'Emre OVA (NETAS External)','AS-OAM','2017-07-07','170706-638590','Unitymedia','UPGRADE PATH (FROM / TO): 18.0.28.0 -> 18.0.28.2\n\r\nER (Brent Combs) paged me to report that critical threshold alarms appeared on SESM1, SESM2 & SESM3 active instances. (Totally there are 5 Session Managers on their system.\n\r\nAs the first action, we stopped the problematic instances then cleared the spool directories. After restarting the NEs that we cleared the spool directories, we saw that the spool logs continue to growing up and exceeding the threshold value of 200.\n\r\nIn addition to this, we performed an SM swact operation but it didn\'t work.\n\r\nAlso all SESM instances were managed by FPM, and there were a major memory alarm on FPM. In this respect, we rebooted the FPM server and changed the FPM value of one session manager from FPM to SM. The major alarm were cleared on FPM after reboot operation.\n\r\nThen, when we check the SESM oss logs, we faced that there were many logs related to SWER 799 and java.nullPointerExceptions. At this point, we decided that working with CallP GPS on the issue and Burak Ataoglu (CallP GPS) joined the bridge.\n\r\nAfter the log investigation has been completed, it\'s seen that Users who have  User-Agent:  AVM FRITZ!Box 6490 Cable (lgi) 141.06.50 TAL (Feb 19 2016) send register without contact header. Due to that, SESM produce so many logs that has SWERR and Exceptions which caused to memory issue.\n\r\nIn order to prevent this, CallP GPS team prepared a jar file and delivered to customer. After applying this jar, we cleared the spool directories and restarted the each SESM instances one by one. As a result, the issue has been resolved.\n\r\nIn spite of the fact that 18.0.28.2 is an obsoleted patch, upgrading to this load caused this issue.\n\r\nAfter the issue has been fixed by applying a jar file, we dropped from the bridge.','null'),(188,'Emre OVA (NETAS External)','AS-OAM','2017-07-06','170706-638438','Cable Onda','UPGRADE PATH (FROM/TO LOAD): 18.0.28 -> 19.0.3.1\n\r\nSWD (Caleb Coleman) paged me to report that customer can\'t launch the MCP GUI after the A2 upgrade. I connected site via GTS VM and could launch the MCP GUI successfully.\n\r\nCaleb stated that customer recives a warning while trying to launch the MCP GUI. (Received warning: \"Error at System Manager. handshake_failure\")\n\r\nIn this respect, we decided that it\'s a specific issue for customer PC. In order to resolve this issue, we suggested that checking the Java settings from the Java Config Panel with correct parameters.\n\r\nIn addition to this, we suggested that clearing the Java cache. \n\r\nAs a result, customer installed the Java 7 and the issue has been resolved. (They were using Java 6 before the issue)\n\r\nThen I dropped from the bridge.','null'),(189,'Emre OVA (NETAS External)','AS-OAM','2017-07-04','170704-638115','Boston College, Black Box Network Services','Current MCP Load: 17.0.31.3\n\r\nGTS (Kevin Gaines) paged me to report that EM Server 2 lost communication. I connected site via GTS VM and saw that SM1 & PROV 2 was down since the Host2 is unreachable. We suggested that performing a power-cycle on Host2 since we cannot do anything with host2 remotely. \n\r\nKevin informed the customer (Paul) about power-cycling the host server 2 but Paul was not able to get out to the site. He said that he would try to be at the site on June, 5 morning (at 7:15AM EST).\n\r\nAfter they power cycle the host server, they will let us know about the status of the problematic NEs again.\n\r\nAlso we requested that collecting /var/log/messages logs and /var/log/dmesg logs after the server is reachable for RCA.\n\r\nThen, I dropped from the bridge.','null'),(190,'Oktay ESGUL','AS-OAM','2017-06-22','170622-636610 ','Swisscomm','Meraz Aziz from SWD paged me to report SM/DB upgrade failure at Swisscomm.\n\r\nUpgrade Path:\n\r\nFrom : MCP 19.0.1.0 \r\nTo:  MCP 19.0.3.1 \n\r\nInitial failure was below timeout issue.\n\r\n################\r\nUpdating schemas in the Primary DB \n\r\nConnection Loss occurred. \r\nScript hasn\'t given a response till a predefined time interval. \n\n\r\nConnection Loss finished. \n\r\nScript hasn\'t completed in predefined time interval. \n\r\nPossible reasons are : \r\n     - Network speed is slower than expected. \r\n     - There is not enough available network bandwidth for the system. \r\n     - Script is stuck due to an unexpected scenario. \n\r\nUse retry button to resolve the problem\n\r\n###################\n\r\nPer recommendation in the error, Meraz had attempted  retries yet this caused multiple script running errors.\n\r\n###################\r\nError: Running multiple instances of this script (at the same time) is NOT allowed \r\n###################\n\r\n==>In order to skip this erros, I have killed running sm/db scripts at emserver1, this did not help.\n\r\n==>Also, restarted ned at both db and em servers,this did not help.\n\r\n==>Performed Save & Exit the wizards, it failed due to wrong SM instance active error,I have double swact SMs to put the instances\r\nstate how wizards request,this did not help.\n\r\n==> Wizards kept continued to fail due to multiple scripts error, even there is not any running scripts at servers.\n\r\n==>Noticed below errors while re-launching wizard which solved by dbInstall -fo\n\r\n##################\r\nUnlinked /var/mcp/run/MCP_19.0/SM_0/work/SMQueryData_IOnT\r\n        Unable to query the SM data.\r\n        com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionPoolException: No boot db instance available\r\n        at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.createBootDBInstanc\r\n#################\n\r\n==> In order to continue the upgrade, I have issued below script manually which wizards run at this screen normally.\n\r\n/var/mcp/upgrade_tools/bin/ut_mcpUpgrade.pl -primarySM -l MCP_19.0.3.1_2017-05-18-0914 -rboff -nc\" MonitorID=\"a2em01_92977484-2b71-1b21-820e-00e0ed349a22_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0\"\n\n\r\n==> Script completed successfully and SM/DB upgrade screen successfully skipped with upgrade perspective of view.\n\r\n==> However, somehow wizards kept continued to give unexpected errors .\n\r\n==> To continue the upgrade, I have run wizard in debug mode to skip next screen which helped.\n\r\n==> While wizards upgrading primary instances, it closed unexpectedly.\n\r\n==>I have relaunched it and it started at Wellcome Screen due to unknown reason .\n\r\n==> In paralell customer keep continued to force us for rollback since they think there is enough time even there is.\n\r\n==> I have connected DB to update wizard_state table to be able launch wizard where we should be normally \"UPGRADE_PRI_NEIS\" by running below instructions at db.\n\r\n#####################\r\nUPDATE WIZARD_STATE SET SCREEN=\'UPGRADE_PRI_NEIS\';\r\ncommit;\r\n#####################\n\r\n==> Previous action updated the wizard screen, yet wizard could not be launched due to below exception:\n\r\n#####################\r\n2017-06-22 00:43:38,670 ERROR UpgradeWizard - Error occurred while starting the wizard.\r\njava.lang.IllegalArgumentException: No enum constant com.nortelnetworks.mcp.client.upgrade.sequence.api.UpgradeScreen.UPGRADE_PRI_NEIS\r\n	at java.lang.Enum.valueOf(Unknown Source) ~[na:1.8.0_121]\r\n	at com.nortelnetworks.mcp.client.upgrade.sequence.api.UpgradeScreen.valueOf(UpgradeScreen.java:14) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.state.impl.WizardStateManagerImpl.restoreWizardState(WizardStateManagerImpl.java:509) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.state.impl.WizardStateManagerImpl.getWizardState(WizardStateManagerImpl.java:179) ~[w\n\r\n##################\n\r\n==>I have researched in the code, but could not find any state for this screen.\n\r\n==>Paged OAM DS and per discussion we decided to restore the pre upgrade backup for a stable upgrade and start from there since wizard state will be restored as well.\n\r\n==> As a next action, I have take current system backup, and restored the preupgrade backup.\n\r\n==>This action took much more than expected times, it took almost 1 hours.\n\r\n==> Management team of the customer did not want to continue the upgrade after this point ,we could not push back further since the MW time is not enough.\n\r\n==> I have started the rollback of the system.\n\r\n==> Rollbacked servers platform and primary db/sm.\n\r\n==> Resynced both db from secondary to primary.\n\r\n==>Restarted the primary instances,there was no any deploy operation so existing jars not reapplied as they are kept how it was before upgrade.\n\r\n==>Failed over all traffic to primary side and let the customer to test the system stability. SESM failovers took almost 60 minutes \n\r\n==>Meraz raised cases for each problem to follow up and rca investigation.\n\r\nThen,I dropped.','null'),(191,'Oktay ESGUL','AS-OAM','2017-06-21','170621-636519','Shaw CableSystems','David Berry from ER paged me to report that there is data lost at this site after the upgrade which is performed last night.\n\n\r\nUpgrade path performed on this site:night.\n\r\nFrom : 18.0.1.0\r\nTo: 18.0.26.1\n\r\nWith upgrade perspective of view ,there was just one reported issue which solved by gps contribution.\n\r\nOnce the problematic step completed, all upgrade steps are performed by wizards and no any new issue observed.\n\r\nHowever, customer informed that they got call failures reports from their end users.\n\r\nPer their initial investigation, they figured out there are several lost PBX configuration which seems the trigger of call failures.\n\r\nI have connected site and performed a health check to validate system stability. MCP gui was alarm free and all applications were running properly.\n\r\nIn the meantime ,customer shared the potential affected PBXes list. When I search the these problematic pbxes in service node list, could not see any of them at MCP and PROV gui.\n\r\nLater on , I requested  pre and post upgrade db backups which taken by wizards automatically.Then, restored these backups in our lab to compare if there is any missed data.\n\r\nWhen I queried namedelement table  where we stored node info in db ,I observed that there are 24 missed PBX delta between pre and post backups.\n\r\nPreupgrade backups contains 706 nodes, while the post has 684.\n\r\nWe discussed with customer in regards how to move forward and they offered to re-provision the missing C20 PBXes on MCP gui. (Missing PBXes data can be listed at C20 properly, yet  somehow there were deleted on A2 side)\n\r\nAt first attempt of reprovisioning,customer failed due to missing VMGes (Normally, C20 SIP PBX provisioning is being performed from OSSgate , but in our case this is not doable.)\n\r\nOzgur joint to call, assist customer to apply a workaround to complete PBX provisioning manually.\n\r\nEven we added,VMG manually it did not help first,then customer restarted GWCs which helped.\n\r\nOnce we completed these actions, customer reported call failures are over for re-added PBX.\n\r\nThey accepted to continue reprovisioning of rest missing PBXes .\n\r\nWe will investigate all upgrade related logs and db backups to find out root cause of thie data lost.\n\r\nThank you','null'),(192,'Emre OVA (NETAS External)','AS-OAM','2017-06-21','170621-636481','Shaw CableSystems','UPGRADE PATH (FROM / TO LOAD): 18.0.1.0 / 18.0.26.1\n\r\nSWD (Chris Henwood) paged me to report that an error has occured during the Patching / Migrating Oracle Database in the primary database server screen.\n\r\nWhile installing the Oracle, wizard was failed. After I\'ve investigated the logs, I\'ve searched for the similar issues and found that there might be duplicated cpp and gcc rpm files under /opt/mcp/.support_pkgs/build directory.\n\r\nIn this respect, I\'ve connected to primary db server and checked the /opt/mcp/.support_pkgs/build directory. There were duplicated cpp and gcc rpm files. After I\'ve removed one of the duplicated packages (which is the old one) I clicked the retry button and problematic screen has been successfully passed.\n\r\n[root@vpd6db00no build]# ll\r\ntotal 16168\r\n-r-x------ 1 root root     7260 May 16 23:59 buildEnv.pl\r\n-rw------- 1 root root    95452 Mar 12  2013 cloog-ppl-0.15.7-1.2.el6.x86_64.rpm\r\n-rw------- 1 root root  3920240 May 16 23:59 cpp-4.4.7-11.el6.x86_64.rpm\r\n-rw------- 1 root root 10599936 May 16 23:59 gcc-4.4.7-11.el6.x86_64.rpm\r\n-rw------- 1 root root   398076 Mar 11  2013 make-3.81-19.el6.x86_64.rpm\r\n-rw------- 1 root root   159928 Mar 12  2013 mpfr-2.4.1-6.el6.x86_64.rpm\r\n-rw------- 1 root root  1322304 Mar 12  2013 ppl-0.10.2-11.el6.x86_64.rpm\n\r\nAlso I\'ve checked the secondary db server to prevent a potential issue like this and saw that there is no any duplicated file in that server.\n\r\nAfter the issue has been resolved, I dropped from the bridge.\n\r\nA follow up case has been created for further investigation. (170621-636483)','null'),(193,'Oktay ESGUL','AS-OAM','2017-06-20','170217-619613','UnityMedia','ER paged me out to report that customer can not jump into to BIOS of the ATCA blade in order to take a look at O/S Watchdog timeout parameter .\n\r\nSystem Load : 17.0.31.2\n\r\nWhile the customer upgrade their system at last February, two host server`s OS patch had failed . For RCA investigation they are trying to check BIOS settings , yet somehow they could not jump into the BIOS.\n\r\nSince this is not an outage and also as the customer aggreed to work within business hours with  the case owner of the RCA case, I have just sent additional procedure of how to jump into BIOS and then dropped as per aggreement.','null'),(194,'Emre OVA (NETAS External)','AS-OAM','2017-06-13','170613-635257','Singtel Optus Pty Ltd','ER paged me to report that some blades were down due to the fact that overheating issue. There was no spare fan in customer. We\'ve waited for a while to let the servers cool down. Then, they move one the MAS MCP servers fans to BCP blades and reseated the servers. We\'ve powered up the blades and they booted up successfully. Then, we have connected to the MCP GUI and double confirmed that all aplication are up and running.\n\r\nThen I dropped from the bridge.','null'),(195,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-06-10','170610-634977','BT TELECOMUNICACIONES S A','Problem Description:\n\r\nDuring performing OAM migration to GenView in BT Barcelona with following IM 74-3282 Migrating the NTP server in Procedure 18, I ran step 26 which redirects to Appendix S. Once that procedure is done ntp seems to be configured.\n\r\nThe problem was that they didnt know if ntp configuration was working or not after the configuration. They are used \"ntpq -p\" command to verify and it is timed out to response.\n\r\nAlso,neither the DB server nor the HOST servers are migrated following the procedure as the A2/SSL is Virtualized but not over Genware.They had asked if they need to perform the procedure for the DB and Host servers or not.\n\r\nActions Taken:\n\r\nThere was no blocking issues to continue with the migration IM but the customer wanted to be sure that NTP configuration is working before switch off the CBM in the next mtce window.\n\r\nSince this is EM/DB seperated virtualized site,I have responded that they need to perform the procedure for db and host servers also. The procedure was not covering this type of AS system.\n\r\nAlso verified that \"ntpq -p\" command is timed out in our lab environment but ntp service&ntpdate command was running and the configurations are ok.\n\r\nFurther investigation with Juan, we founded that \"restrict 127.0.0.1\" line was missing in the ntp.conf file and that is the main reason why this command returns timed out.After adding the line to our lab environment, command returned response.\n\r\nJuan has explained the procedure to the customer side and then I dropped from call.','null'),(196,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-06-10','170610-634977','BT TELECOMUNICACIONES S A','Problem Description:\n\r\nDuring performing OAM migration to GenView in BT Barcelona with following IM 74-3282 Migrating the NTP server in Procedure 18, I ran step 26 which redirects to Appendix S. Once that procedure is done ntp seems to be configured.\n\r\nThe problem was that they didnt know if ntp configuration was working or not after the configuration. They are used \"ntpq -p\" command to verify and it is timed out to response.\n\r\nAlso,neither the DB server nor the HOST servers are migrated following the procedure as the A2/SSL is Virtualized but not over Genware.They had asked if they need to perform the procedure for the DB and Host servers or not.\n\r\nActions Taken:\n\r\nThere was no blocking issues to continue with the migration IM but the customer wanted to be sure that NTP configuration is working before switch off the CBM in the next mtce window.\n\r\nSince this is EM/DB seperated virtualized site,I have responded that they need to perform the procedure for db and host servers also. The procedure was not covering this type of AS system.\n\r\nAlso verified that \"ntpq -p\" command is timed out in our lab environment but ntp service&ntpdate command was running and the configurations are ok.\n\r\nFurther investigation with Juan, we founded that \"restrict 127.0.0.1\" line was missing in the ntp.conf file and that is the main reason why this command returns timed out.After adding the line to our lab environment, command returned response.\n\r\nJuan has explained the procedure to the customer side and then I dropped from call.','null'),(197,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-06-10','170610-634975','Falcon AFB','Problem Description:\r\nER reporting problems with all of the SESMs going into overload after cutover(user migration) from AS5300 to Virtualized AS.Customer has made decision to roll back.\r\nAfter rollback is done,I have taken a pager call from ER about SESM2_0 and SESM4_1 are Configured/Down/Unavaliable state and they can not make any operation from MCP GUI on this instances.\n\r\nActions Taken:\r\nSince we do not have site access, I have passed  my action plan which constains the basic maintenance commands to recover that instances after joined the bridge.\n\r\n1)execute \"neinit restart\"\r\n2) neinit -p \r\n3)neinit -autorestart=off\r\n4)neinit -p one more time to check if it is still listing the SESM instance\r\n5)execute \"kill - 9 \" if it is listing the instance\r\n6) deploy the problematic instance from MCP GUI\r\n7) start the problematic instance when deploy operation is completed on the MCP GUI\r\n8) neinit -autorestart=on\n\n\r\nAfter the upper action plan is done, SESM instances are recovered and then I have left from the bridge.','null'),(198,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-06-09','170608-634720','6 Degrees Group','Problem Description:\r\nSWD paged me for the MR upgrade from 17.0.5.0 to 17.0.31.3 on this ATCA based A2 system.\r\nAfter patching the OS on the primary host server the blade did not recover from reboot. \r\nNo output/response was observed on the console. \n\r\nActions Taken:\r\nIn order to recover the server,suggested to SWD to execute hardware app-blade deactivate/activate command from NDM for the failed host.Then try retry on UW.\n\r\nNote: this issue can be observed if the NMI watchdog parameter is enabled \r\non the host server BIOS setting. Normally, before platform installation, this configuration should be done according to IM 24-3469','null'),(199,'Emre OVA (NETAS External)','AS-OAM','2017-06-07','170607-634366','Shaw CableSystems','FROM/TO LOAD: 18.0.1.0 / 18.0.26.1\n\r\nSWD paged me to report that primary servers failed to upgrade. Chris Henwood contacted me via trillian. I\'ve connected to site by creating a bomgar session and started to working on the issue. \n\r\nThe following alarm was appeared on the problematic screen:\n\r\n\"Upgrade operation failed on Network Element Instance: SESM4. \r\nValidating Network Element Instance : Network Element Instance Administrative state should be Online. Network Element Instance Operational state should be Hot Standby.  \r\nPlease wait until validation is finished. If operation has failed, please contact with next level of support.\"\n\r\nWhen I connected to the site, upgrade wizard was closed. (Chris was thought that closing and re-opening the wizard might work to solve the issue). In this respect, when he attempted to re-open the wizard, he was receiving the \"SM 0 should be active\" warning.\n\r\nI\'ve connected the EMServers and stopped the active SM_1. So, SM 0 came into the online - active status.\n\r\nIn addition to these, after correcting the operational state of the problematic NE\'s, problematic screen is passed successfully.\n\r\nAfter the issue has been resolved, I dropped from the call.','null'),(200,'Oktay ESGUL','AS-OAM','2017-06-07','170607-634366','Shaw','Chris pinged me from trillian to report HOST2S2 has not boot up after OS patch screen of the wizards.\n\r\nUpgrade Path:\n\r\nFrom:18.0.1.0 \r\nTo : 18.0.26.1\n\r\nServer was not reachable , I requested Chris to share terminal access to take a look.\n\r\nWhen we connected via terminal, observed power issues at console and server could not boot up properly due to this power related errors.\n\r\nIn order to recover the server,connected to NDM and run below command which recovered the server.\n\r\nhardware app-blade deactivate 0 0 11 0\r\nhardware app-blade activate 0 0 11 0\n\r\nOnce ther server boot completed succesfully, Chris continued the upgrade.','null'),(201,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-06-05','170605-634047','University of Texas - Austin','Problem Description:\n\r\nOver the weekend the site performed several restarts on the prov servers. PROV1 is up but is out of sync. It goes out of sync every morning @~0130 due to the GVPP running a program called \'A2 user load\'. The GVPP loads all SIP users to the A2, and there are ~20K SIP users. One thing that Penn noticed was an alarm complaining about the number of files in the spool directory. During our conversation was asked him to delete the files, and he stated that someone must have just deleted them. He checked and call forwarding is still failing. Penn also stated that PROV-2 is down and has been down for several months.\n\r\nFurther clarificatin on this issue: \r\nUsers are unable to unforward their phones using the Vertical Service Codes. (73). \r\nI am unsure if it is actually related to the Prov going out of sync.(170331-625486)\n\r\nAS is running with 10.4 release.\n\r\nActions Taken:\n\r\nBefore OAM GPS involved the issue,CallP GPS has requested SESM trace from GTS but it was not collected to understand the problem scenario correctly.\n\r\nThen I have taken pager call and connected to the site and made a health check on the A2 system.\r\nSeen that there was no alarm on the MCP.Then seen PROV2\'s monitoring status is closed. Checked all the NE\'s and see PROV2 is hang at Online/Up/Shutdown status.\n\r\nStop command from the MCP not worked. I have used CLI commands to stop the instance process then started from MCP.\n\r\nCallP GPS has collected SESM trace and analysed the issue again and log was clear.\r\nAfter that Customer tested his call forwarding scenario with VSC and stated that issue is resolved.\n\r\nThere is no relation with PROV2\'s status about this call forwarding issue.\r\nThere is no redundancy on PROV instances and customer uses PROV1 for provisioning purposes.\n\r\nSince we have not had the failing SESM trace in our hand, we could not address the issue and dropped from bridge.','null'),(202,'Emre OVA (NETAS External)','AS-OAM','2017-05-24','170522-632282','Liberty Global Europe B.V.','Problem Description:\n\r\nA2 SESM 5_1 is Down as of now , After Blade Replacement done for genband case 170512-631073 SESM was fine and we were able to login , but now again we are not able to login in SESM 5_1 .please suggest further .\n\n\r\nActions Taken:\n\r\nInvestigated the case comments 170512-631073 and procedure given to the customer.\n\r\nCustomer was attempting to reach SESM5_1 and they stated that they can\'t reach the server in spite of the fact that they can ping from SESM5_0. The ER logged in and was able to ping SESM5 -1 via SESM5-0 , but also unable to login. A2 OAM GPS was contacted. SESM5-1 was restore after performing a power cycle ( deact/act) via the NDM by GPS. ER will collect data GPS has requested and open route a follow-up case\n\r\nThen I dropped from call.','null'),(203,'Emre OVA (NETAS External)','AS-OAM','2017-05-23','170523-632366','Paltel','Due to the fact that SM manages more than 40 instances, some NE elements dropped into the unmanaged status. When Scott paged me, Ive connected to site and saw that some SESM instances were dropped. As first action, Ive restarted them and cleaned the spool directory for each of server.\n\r\nWhen I looked for the root cause, I saw the following alarm on SM:\n\r\nDescription: Number of Managed Elements(MEs) has exceeded the capacity of SystemManager. The Network Element Instance manages the following MEs : \r\nMAS4S\r\nPA2_0\r\n..\r\n..\r\nMAS1S\n\r\nThen, Ive checked the Servers from Server tab on MCP GUI. I saw that all servers are managed by SM. To prevent this type of issue, an FPM should be deployed and customer should change the FPM Value of some servers from SM to FPM. (EMServers should be managed by SM.) I also shared the related bullettin in case attachment. (170523-632366) Its clearly written there.\n\r\nAs corrective action, Ive cleared spool directories of servers and restarted problematic instances. Also I performed an SM swact operation. Now, all instances are in Online  Up status. So, there is no any outage.','null'),(204,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-05-12','170512-631060','Cincinnati Bell','Problem Description:\n\r\nEr paged me to report that EMServer2 was raising and clearing a Critical High CPU alarm. \r\nWhile ER was logging into site, SM1 dropped and went UNAVAILABLE.\r\nER performed a STOP / START on SM1 from the MCP and SM1 recovered but alarms remained on EMServer2.\r\nER swacted the SMs but the problem remained.\r\nER ran into issues with starting SM0 from the MCP so ER rebooted SM0.\r\nSM0 recovered and alarms on EMServer1 cleared but alarms on EMServer2 remained. \n\r\nCustomer was running on 8.0 BRC A2 load.\n\r\nActions Taken:\n\r\nRequested to reboot EMServer2 and start the PROV from MCP GUI after that action.\r\nER reported that alarms are cleared but db1 instance has raised an Database Communication Error alarm.\r\nRequested to start db instance as manual and then all alarms got cleared and system became stable.\n\r\nThen dropped from call.','null'),(205,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-05-12','170512-631073','Liberty Global Europe B.V.','Problem Description:\n\r\nCustomer was attempting to replace SESM 5_1 (blade 1 0 13 0) regarding to case 170104-612524 and they stated that after they performed step 19 of the procedure \"Installing an operating system on a core ATCA server\" in 630-01217-02 07.07 (starts on page 141), \r\nthe A2 blade failed to boot after step 19 completed without any errors. Customer noted that they tried deactivating and activating the A2 blade 1 0 13 0 but the blade still failed to boot. ER had site reseat the blade but the blade still failed to boot. The console port for 1 0 13 0 was showing no output after a deact/act was performed on the blade. Customer confirmed that the console port for 1 0 13 0 was working fine prior to the blade replacement.\r\nER then engaged A2 OAM GPS.\n\r\nActions Taken:\n\r\nInvestigated the case comments[170104-612524] and procedure we have given to the customer.\r\nSeen that customer is having disk issues for this blade and the java application(SESM) which is deployed on this blade is dropping from Up to Down constantly.\n\r\nExecuted mcpRelease.pl to learn installed ple type on the other SESM instance.\n\r\nSystem Type:     mcp_core_linux_ple2\r\nRelease Level:   17.0.19 (via patching)\r\nHardware Env:    ATCA-i7\n\n\r\nConnected to NDM to see GENiUS buildversion:\r\nBuildversion: 7.0.0.124006.0\n\r\nThe procedure on the document which is following by customer is not supported for the GENiUS 6.0 or above.\r\nWe have updated supported procedure in the 10.3 Administration document.\r\nNN48111-611_09.08_10.3_as-administration.pdf\n\r\nI have provided to the customer supported document and customer is requested to apply this document by GPS.\r\nI have started to apply the procedure then suddenly NDM is frozen and could not go ahead.Requested from ER to page GENiUS GPS.\n\r\nGENiUS GPS is involved and started working on the issue.\r\nThen I have waited an answer from customer side if we continue to the activity after NDM problem is resolved.\r\nCustomer responded that they will arrange this mtce action for next week.\n\r\nI have attacted the supported replacement procedure for rear-card replacement to the case and then dropped from call.','null'),(206,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-05-10','170510-630712','','Problem Description:\n\r\nFailure during patching of primary DB, A2 OAM GPS (Kemel Aydemir) is investigating. \n\r\nAppears to be a disk space issue: \n\r\n\"Prerequisite check \"CheckSystemSpace\" failed. \r\nThe details are: \r\nRequired amount of space(226.342MB) is not available.\" \n\r\n[May 10, 2017 5:00:15 PM] Space Needed : 226.342MB \r\n[May 10, 2017 5:00:15 PM] Space Usable : 120.0MB \n\r\nNeed to clean up \"/opt\", only has 124M available: \n\r\n/dev/mapper/vg00-opt 6.0G 5.5G 124M 98% /opt\n\r\nFROM/TO LOAD: 17.0.31.3 / 19.0.1.0 \n\r\nActions taken:\n\r\n1)Connected to the DB server and analysed the directories in the /opt partition which are filling up there.\n\r\n2)Removed the non-rotated logs in the below directories.\r\n/opt/mcp/db/diag/rdbms/mcpdb/mcpdb/trace/mcpdb*trm\r\n/opt/mcp/db/diag/rdbms/mcpdb/mcpdb/trace/mcpdb*trm\n\r\n3) 995 MB is free now.\r\n/dev/mapper/vg00-opt  6.0G  4.7G  995M  83% /opt\n\r\n4)Requested retry on UW and then passed the screen.\n\r\nRoot cause: AAK-47858 will address this issue in order to manage /opt partition.\r\nCurrently in development process.','null'),(207,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-05-09','170509-630578','Optus','Problem Description:\n\r\nAfter primary servers were upgraded and made active, SESM1_1/SESM2_1/PA2 were stuck \"initializing\" on Wwitching service to primary network element instances screen.\n\r\nRecover Steps: \r\n1) restarted units from MCP GUI \r\n2) shutdown / start VM \r\n3) rebooted host server \n\r\nNone of these actions helped, paged GPS and Kemal Aydemir is now investigating.\n\r\nFROM/TO LOAD: 17.0.22.7 / 17.0.31.3 \n\r\nActions Taken:\n\r\n1)Investigated the work log.The following is found.\n\r\nSubsystemManager Failed: SIPProfileSubsystem\r\njava.lang.ArrayIndexOutOfBoundsException: 81\r\n        at com.nortelnetworks.mcp.share.sipprofile.util.SIPProfileDecoder.decodeSignalingBitmap(SIPProfileDecoder.java:66)\r\n        at com.nortelnetworks.mcp.ne.share.sipprofile.roc.SIPProfile.signalParsing(SIPProfile.java:573)\r\n        at com.nortelnetworks.mcp.ne.share.sipprofile.roc.SIPProfile.(SIPProfile.java:143)\r\n        at com.nortelnetworks.mcp.ne.share.sipprofile.roc.ObservableSIPProfileList.addSipProfile(ObservableSIPProfileList.java:58)\r\n        at com.nortelnetworks.mcp.ne.share.sipprofile.roc.SIPProfileROC.applyAdd(SIPProfileROC.java:136)\r\n        at com.nortelnetworks.mcp.ne.base.ro.RuntimeObjectCollection.loadObject(RuntimeObjectCollection.java:187)\r\n        at com.nortelnetworks.mcp.ne.base.ro.RuntimeObject$1.loadObject(RuntimeObject.java:80)\r\n        at com.nortelnetworks.mcp.ne.base.ro.BrokerDataLoader.load(BrokerDataLoader.java:106)\r\n        at com.nortelnetworks.mcp.ne.base.ro.RuntimeObject.loadFromDB(RuntimeObject.java:107)\r\n        at com.nortelnetworks.mcp.ne.share.sipprofile.roc.SIPProfileROC.initialize(SIPProfileROC.java:72)\r\n        at com.nortelnetworks.mcp.ne.share.sipprofile.roc.SIPProfileSubsystem.initialize(SIPProfileSubsystem.java:44)\r\n        at com.nortelnetworks.mcp.ne.base.subsystem.InitializeRequest.handle(InitializeRequest.java:50)\r\n        at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemManager.handle(SubsystemManager.java:58)\r\n        at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemTask.dispatchSubsystemRequest(SubsystemTask.java:100)\r\n        at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemRequestEvent.dispatch(SubsystemRequestEvent.java:47)\r\n        at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemTask.dispatch(SubsystemTask.java:366)\r\n        at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemTask.handle(SubsystemTask.java:80)\r\n        at com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:709)\r\n        at com.nortelnetworks.mcp.base.task.Task.run(Task.java:543)\r\n        at com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:152)\r\n        at java.lang.Thread.run(Unknown Source)\n\r\n2)ArrayIndexOutOfBoundsException : Thrown to indicate that an array has been accessed with an illegal index. The index is either negative or greater than or equal to the size of the array.\n\r\n3) I have accessed to the DB and deleted the signaling record where the index 81 locates in the SIP_PROFILES table.\n\r\n4) Double swact\'ed the SM\'s to load new data from DB.\n\r\n5)Start the problematic instances from MCP GUI and recovered.Opened wizard and SWD continued to the upgrade.\n\n\r\nRoot cause : After primary side of the MCP system upgraded, customer made a test with PBX\'s after updating \"Unidentified\" SIP Profile to the MCP system.\r\nThis illegal index is updated during that time. I have requested the SIP Profile templates(before and after)and pre upgrade db backup from customer to reproduce this issue in our system and then merge the corrective content with a JIRA.','null'),(208,'Bill Picardi','AS-OAM','2017-05-02','170502-629629','TELENET N.V.','Call to assist with upgrade of application server from 17.0.18.5 to 17.0.31.2.  At step 26 of the upgrade, the DB completes, but the upgrade of the System Manager fails.  SM1 was active and SM0 failed the software update.  The SM0 was in a non-functional state and could not be started from MCP.  We performed a Undeploy and Deploy of the SM from the cli, and then started the service from the cli.\r\n    The SM0 load had the new load, and it must be changed back to the old load for the upgrade to continue.  We stopped the SM0 from MCO GUI, and ran smUndeploy from cli, then changed the load for the SM0 instance in the MCP GUI.\r\nNext, we deployed the SM0 running the old load and started it.  Following this we continued the upgrade wizard, and it succeeded.\n\r\n    Checking the release notes of 17.0.31.2 to 17.0.31.3, there is a bug that my fix this issue.  This may happen for all upgrades to 17.0.31.2, and customer comments that a bulletin to notify of this issue and the workaround should be generated, or to obsolete this is patch to prevent the issue at other sites.','null'),(209,'Bill Picardi','AS-OAM','2017-05-02','170502-629632','New York - Presbyterian','Call for assistance with A2 migration from Langley HT to IA-RMS.\r\nWhile performing steps 21-23, the backup fails to load and restore data to the new server, displaying the error:\r\nThe given directory \"/var/mcp/remote_backups\" does not contain any suitable files\r\nAfter server status checks and some investigation, we retried the procedure and it worked.  An incorrect option was previously entered.  After successfully completing the steps, GPS dropped the call.','null'),(210,'Oktay ESGUL','AS-OAM','2017-04-29','170429-629268','Canadian Imperial Bank (CIBC)','Chris Henwood paged me to report that Prepare DB screen failure at CIBC.\n\r\nUpgrade Path: \n\r\nFrom:18.0.26.2\n\r\nTo : 18.0.28.18\n\r\nPrepare db is the first screen of main upgrade steps. At this step wizards drop the replication between DBs .Yet this site was failing.\n\r\nFirst initial view was wizards fails due to ned issues .In order to fi this problem, I have restarted all server`s ned daemon.This did not help.\n\r\nThan, I  tried to stop/start oracles at primary db since secondary seems fine while primary was failing. This did not help neither.\n\r\nIn the time, opt directory of both db servers were over %80 which is major problem, also cleared dummy logs to free the disk size, no chance neither.\n\r\nLater on, in order to wipe out wizards related problem, I have tried to run corresponding script manually, that proved that primary db is not operating properly.\n\n\r\n##################\r\nleaning up primary DB replication\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170428_223917_zQdR | /opt/mcp/ned/bin/nedclient 10.28.0.24 4890\r\nCommand Output:\r\n> config ok\r\n> run ok 1\r\n> exited\n\r\nError occurred executing NE commands (see below):\n\r\nNE command exited with the value: 1\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20170428_223917_zQdR\r\nNot unlinking file\n\r\n> config ok\r\nNE command exited with the value: 1\r\n> exited\n\n\r\nFailed to cleanup replication on the server 10.28.0.24\n\r\nTerminated at  =>  Fri Apr 28 22:39:41 2017\n\r\n##################################\n\n\r\nAs a next action to clean up the stuck processes rebooted the primary db server, did not help.\n\r\nDB was getting stuck at quirsce mode at each replication drop attempt.ActivateDBRep was the solution to make the db writeable again.\n\r\nYet, when I tried to run direct cleanupReplication from primary db,while it failed at primary side, it completed the secondary. After this point, I could not activate the primary db again due to below deadlock erros.\n\r\n################\r\nTRUNCATE REP QUEUE\r\nDROP REPGROUP MCSDBSCHEMAREPGROUP\r\n   DECLARE\r\n*\r\nERROR at line 1:\r\nORA-04020: deadlock detected while trying to lock object MCSDBSCHEMA.DOMAINCOSINFO\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6470\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 6016\r\nORA-06512: at \"SYS.DBMS_REPCAT_UTL\", line 7027\r\nORA-06512: at \"SYS.DBMS_REPCAT_MAS\", line 2695\r\nORA-06512: at \"SYS.DBMS_REPCAT\", line 635\r\nORA-06512: at line 70\n\n\r\n################\n\n\r\nI have made a quick research at web and oracle forms yet could not figured out a helpful solution.\n\r\nLater on, since deadlock was blocking all db operations, we decided the reinstall the oracle at primary db server.\n\r\nChris uploaded the installer to customer side and I completed the oracle reinstallation.\n\r\nOnce oracle operation completed, we wanted to restore the daily backup of customer yet had slowness issue.\n\r\nRestore operation almost took 1 hour (This is one of known issue which fixed at 18.0.26.5)\n\r\nEven script seems running ,it was taking long time to proceed. We have applied below workaraund to proceed faster ..\n\r\n################\n\r\n1.Apply the below changes on this file => /var/mcp/run/MCP_XX.0/mcpdb_0/work/initmcpdb.ora\n\r\naq_tm_processes=0 should be commented out as below;\r\n#aq_tm_processes=0 and save the changes.\n\r\n2.Apply a db stop/start by using below commands.\r\n/etc/init.d/dbora stop\r\n  /etc/init.d/dbora start\r\n################\n\r\nAbove actions did not help much as expected, yet we completed the restore operation after a while.\n\r\nAs a last action, we setup db replication from scratch and made the system stable again.\n\r\nIn order to perform a stable upgrade after db restore and also as db backup did not has the latest screen of wizard,Chris started the upgrade from scratch.\n\r\nTill the Prepare_DB screen , I had waited stand by in case of any failure. This time, prepare db failed at first attempt again due to timer expiry,I just retry the wizards and it completed the screen succesfully.\n\r\nAfter all,I  let Chris continue with the rest of the upgrade,then dropped.\n\r\nThanks','null'),(211,'Oktay ESGUL','AS-OAM','2017-04-25','170424-628437','Global Tel*Link GTL','Glen paged me out to report neStop.pl fails due to disk space is allready full.\n\r\nIn order to stop/start problematic sm instance, I run neinit -p  to get process id.\n\r\nThen, kill the process via kill -9 \n\r\nAfter above actions, sm recovered and deleted files are gone.\n\r\nAlso, restarted PROV and AM instances to clean up their deleted files as well.\n\r\nCustomer performed tests and then we dropped.','null'),(212,'Oktay ESGUL','AS-OAM','2017-04-24','170424-628437','Global Tel*Link GTL','Glen Anderson NA GTS paged me out to reports critical  alarms at customer site.\n\r\nSystem Load: MCP_17.0.18.5\n\r\nGTS connected site and we take a look the alarms. \n\r\nThere were 10 critical disk threshold alarms on SESM and SM instances.\n\r\nActually, this should not be a pager call ,yet as  I have already connected site investigated the issue shortly.\n\r\nBasically,SM0 /var/mcp/ directory was full and due to this problem, sesm seems not able to transfer the spool files to SM periodically. As a results of these transfer issues, /var/mcp/ threshold increases to %100 on both Sesm instances.\n\r\nThis is one of the  known issue which fixed via AAK-42028. (Also this jira was mentioned at case description).\n\r\nThe issue is triggered by  below files at active sm server which do not free disk space ,even they are deleted.\n\r\n[root@GTWDSYS0 mcp]# lsof | grep \"/var/mcp\"|grep deleted \r\njava 2435 ntappsw 53u REG 9,7 25246848 1556482 /var/mcp/kpi/SM_0/20140828 (deleted)\r\njava 2435 ntappsw 86u REG 9,7 25246848 1556481 /var/mcp/kpi/SM_0/20140730 (deleted)\r\njava 2435 ntappsw 92u REG 9,7 25246848 1556499 /var/mcp/kpi/SM_0/20140731 (deleted)\r\njava 2435 ntappsw 139u REG 9,7 25246848 1557124 /var/mcp/kpi/SM_0/20140731 (deleted)\r\njava 2435 ntappsw 140u REG 9,7 25246848 1556505 /var/mcp/kpi/SM_0/20140731 (deleted)\r\njava 2435 ntappsw 142u REG 9,7 25246848 1557122 /var/mcp/kpi/SM_0/20140731 (deleted)\r\njava 2435 ntappsw 143u REG 9,7 25246848 1556501 /var/mcp/kpi/SM_0/20140730 (deleted)\n\r\nAs a workaround, sm  needs to be stopped/started to free disk .However, customer did not want to perform this action out of a MW. \n\r\nI have shared how to stop/start sm instance procedure in MW  with Glen and then dropped .','null'),(213,'Burak Biyik','AS-OAM','2017-04-21','170421-628171','NorthwestTel','Customer running 10.4 (17.0.22.11) reported that some of their siplines failed to make a call after swacting SESM instances. It looked like a IMDB-DB sync issue in the first place, but stopping both SESM instances and starting them one by one did not resolve the issue. Then, we noticed that a nil change from PROV (On User Portlet -> Links -> SIP Line -> SIP Line Service) made improvement. This is the portlet where users\' Sipline DN,Endpoint ID, VMG and Sip Profile are assigned.\n\r\nFurhter looking at the SIPLINEDATA table from database by comparing before&after nil change, we noticed that CLIENT_TYPE column is the key point. It is null for problematic users and filled with a SIP Profile after performing nil change on PROV. So, the root cause is having no sip profile data in SIPLINEDATA table.\n\r\nWe dumped all siplines that have null sip profile on SIPLINEDATA table and assigned A2PC profile to them by sql command. At this point, we expected SESM to get this data from database during initialization, however calls failed after starting SESM instances again. Apparently, there is a bug here that will be tracked later on by CallP GPS.\n\r\nIn order to recover the system, we dumped the list of siplines without sip profile (total of 176) and provided to the customer for applying nil change  from PROV one by one.\n\r\nA similar issue was found in sfdc (140919-494511) which explains the trigger point. When a sipline is provisioned from OSSGATE without specifying its sip profile (SIP_CLIENT_TYPE), sipline does not have sip profile defined in AS database. If SESMs are swacted at this point, then those users fail to make a call. \n\r\nThe solution is specifying SIP_CLIENT_TYPE while provisioning a new user with NEW command from OSSGATE.\n\r\nAgreed and dropped the call.','null'),(214,'Burak Biyik','AS-OAM','2017-04-20','170420-628006','Black Box Network Services','SWD reported that UW failed at System Validation screen during 10.4 MR Upgrade (17.0.22.11 -> 17.0.31.3). The failure was due to not being able to get current F/W levels of two BCP servers (IBM BCT Blades) \n\r\n#################################################\r\nLoad Lineup Validation : ............................................ [FAILED] \r\n- The Firmware Levels in the server could not be determined. \r\n- Following error(s) occured while querying the Firmware: \r\n-MM IP and/or MM Username and/or MM Password is missing \r\n-On MCP GUI, enter Chassis on the left panel, check if the blade center of this blade exists in Blade Center list \r\n-If it does not exist, add the blade center and enter the IP of MM in the pop-up window correctly.\r\n#################################################\n\r\nThis was pre-upgrade step so that it should have handled with a case under normal conditions. As said, the actual upgrade was scheduled for today since the previous pre-upgrade attempt delayed due to another issue resolved with the case: 170419-627798.\n\r\nAs applied earlier to other customers, this screen was skipped in debug mode not to block upgrade since this is not service impacting. If required, F/W upgrade of BCT blades can be done later on as a case work.\n\r\nAgreed and dropped the call.','null'),(215,'Oktay ESGUL','AS-OAM','2017-04-19','170419-627830','Alphawest Services P/L (Carr)','David Giomi paged me out to report that customer can not provision a new user from ossgate after they have updated prov admin users passwd.\n\r\nI asked him to try to login to prov via new credentials  and he reported the new credentials works fine at prov.\n\r\nAlso, he logged in to MCP gui as well via new credentials.\n\r\nAfter above tests, I let him to continue investigation with OSSGATE GPS since all credentials work fine with A2 perspective of view, then dropped the call.','null'),(216,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-04-13','170413-627124','Sejong Telecom (fka ONSE Telecom)','ER paged me to report that 50 Subscribers cannot register/make calls.\n\r\nFurther investigation showed that issue is same with the case 170221-620008.\r\nPASSWD_ENC_TYPE_FIELD on the USERDETAILS table should be 0 for surpass this issue.For our case it was 1 for the fail subscribers on the database.\n\r\nProvided the below action plan and migration team will be made this change 4 AM Seul time.\n\r\nChange PASSWD_ENC_TYPE_FIELD on Database as 0 with A2 converter tool.\r\nStop both SESM instance and start one by one.\n\r\nI have also requested to raise a RCA case for this along with the log collection\r\nThen dropped from call.','null'),(217,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-04-12','170411-626848','Bell Aliant','ER paged me to report that getting alarms about \"Application Blade Missing\" for slot 3 0 5 0. Site was having ethernet issues on the host12(STJHSUCA33HOST12) so that was impacting SM1\'s stability.\n\r\nBefore A2 GPS is paged,Genius GPS was worked on this site yesterday and reported the following action;\n\r\nGPS recommends re-seating the card.  However before proceeding all of the A2 services, database, etc. has to be moved off of this card.  (It is SM_1 and DB_1 (The standby DB).  SM_0 is active, so they don\'t need to move services, but should do graceful shutdowns from the OS to prevent corruption on the VMs or on the host OS). We will need to page A2 GPS and have them perform these actions:\n\r\nA2 GPS to perform these actions: \r\nTransfer or shutdown all A2 services,database, etc. from 3 0 5 0 to 0 0 5 0. SM_1 and DB_1 (The standby DB).SM_0 is active, so they don\'t need to move services, but should do graceful shutdowns from the OS to prevent corruption on the VMs or on the host OS. \r\nER to perform the following: \r\nha app deact 3 0 5 0\r\nReseat card in slot 3 0 5 0\r\nha app act 3 0 5 0\n\r\nAfter the upper action has been done, cli>FM ALARM START command executed on the NDM, returned no alarm.\r\nMonitored the site 10 min and then dropped from pager call.','null'),(218,'Emre OVA (NETAS External)','AS-OAM','2017-04-11','170411-626754','Bragg Communications (Eastlink)','FROM/TO LOAD: 18.0.20.6/ 18.0.28.1 \n\r\nThere was a failure to complete the \'restart SM 0\' step after switching activity to SM 1. \r\nSM 0 does show the right status in the MCP GUI as HOT STBY. \r\nSave and exit, then restarting the Upgrade Wizard has the same result. \r\nAttempted neinit restart on both SM servers. This did not help. \r\nAttempted to switch back to SM 0 but now SM 0 cannot take activity. \r\nSM 0 finally does take activity. Trying the Upgrade wizard again. \r\nSM 1 is now having difficulty taking activity this time but is does take activity. \n\r\nJar file has been provided for this issue and stability of both SMs has been provided. (18.0.20.6_AAK-47885_April11.jar) The problem has also permanently fix in 18.0.28 MR. \n\r\nUpgrade will be ongoing tomorrow.','null'),(219,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-04-10','170410-626547','BT PLC (Manchester)','ER paged me to report that BT PLC is having issues SM0 unit is active but can not be accessed with SSH session(unresponsible).Also see nothing on the SM0 server console.\r\nSM1 is the hot-standby unit\n\r\nThis is a SM/DB collocated server running with the below platform.\n\r\nSystem Type:     mcp_core_linux\r\nRelease Level:   17.0.7 (via install)\r\nHardware Env:    HP-CC3310\n\r\nAS release : 17.0.7.14\n\r\n------\n\r\nWhen I connected to the site, reboot is performed on the SM0 server and E1 is ended.\r\nI have made basic checks on the SM1 server.It was up for 320 days.I have requested reboot on the SM1 server also.After the server is rebooted,there was no issue with redundancy between SM instances.All instaces were raising a fake alarm about no connection to db instance 0. All instances are restarted and alarms got cleared.After the outage is restored, seen that EMServer1 which is holding the SM0/DB0 instances raised Non-sync partition alarm.\n\r\nsdb was the active disk and sda disk was the non-sync one.\r\nI have tried to perform sync between disks but failed with the Device \"/dev/sda\" either not defined or inaccessible error.\n\r\nI have requested the log collection from ER/GTS for RCA purposes.\r\nCustomer will be replacing the faulty disk for this server.','null'),(220,'Yunus Ozturk','AS-OAM','2017-04-05','170405-626019','Cleveland Clinic','Problem Description:\r\n====================\n\r\nER paged out OAM GPS for on going E1 situation on CallP side. They have informed us that they see some DB related SWERRs as follows;\n\r\nSM_0 SWERR 799 ALERT APR05 09:13:43:275 MCP_17.0.31.2\r\n                     pool failed to connect due to sql exception\r\n                     java.sql.SQLException: ORA-01017: invalid username/password; logon denied\n\r\nThis customer has recently applied the Hardware Migration procedure from Legacy Servers to Virtualized Servers. They have used the \"SSL/A2 Small Migration to Virtualized Experius on IA-RMS Servers\" IM document.. \n\r\nActions Taken:\r\n===============\n\r\nWe have accessed the site and noticed that there was a communication problem with DB1 instance and all the other instances on the system had DB1 connection alarms. \n\r\nThis issue does not actually have an impact on the ongoing CallP issue. It should be investigated separately by the CallP GPS Team (customer reports that their subscribers drop to fast busy and it is recovered by swacting the SESM units)\n\r\nFrom OAM perspective, we have noticed that Secondary DB Server IP was not properly configured on the installprops.txt file.. It was still pointing to the older EMServer2 IP address which was not correct.\n\r\nIt appears that they did not apply the Procedure 3 - Step 9 of the Migration document correctly or they missed it.. \n\r\nLogin to the new primary EM server as ntappadm\r\nEnter: /var/mcp/install/populateInstallpropsFile.pl\r\nDatabase Type (SINGLE/REPLICATED)? [SINGLE]: REPLICATED\r\nSecondary Database Host Address:  **See WARNING**\r\nPerform Database Backups (Y/N)? N\n\r\nOn this procedure, Secondary Database Host Address parameter should have been configured with the new DB Server 2 IP Address. \n\r\nWe have modified this parameter with the correct DB Server 2 IP address on the installprops.txt file of the new Primary EMServer and then executed some of the remaining steps of the migration document again as follows;\n\r\n- Updated the secondary DB instance in the database with ./updateDBServers.pl script (Procedure 3 - Step 11)\r\n- Executed the /var/mcp/install/setupDBReplication.pl script on the new Primary EM Server to deploy the files into new DB Server 2 and resynch the data from Primary DB to Secondary DB.. (Procedure 3 -  Step 22)\n\r\nOnce it is completed, all the DB related alarms are cleared.','null'),(221,'Yunus Ozturk','AS-OAM','2017-04-05','170405-625955','Cleveland Clinic','Problem Description:\r\n=====================\n\r\nCustomer was applying the SSL Hardware Migration Procedure from legacy Langley HT to Virtualized IA-RMS Servers and they had an issue with procedure 2 step 19 Method 74-3160 of SSL/A2 Small Migration to Virtualized Experius on IA-RMS Servers IM Document\n\r\nSite Release : 17.0.31.2\n\r\nsmDeploy.pl script failed at Procedure 2 step 19 as follows;\n\r\nFollowing error: \r\n[ntappadm@CC-J-B-EMS1 install]$ smDeploy.pl \r\nGlobal symbol \"$oldProps\" requires explicit package name at /var/mcp/install/NEMtcUtils.pm line 1163. \r\nCompilation failed in require at /var/mcp/install/smInstall.pl line 29. \r\nBEGIN failed--compilation aborted at /var/mcp/install/smInstall.pl line 29. \r\n[ntappadm@CC-J-B-EMS1 install]$\n\r\nActions Taken:\r\n==============\n\r\nTo fix this issue, we have manually modified the \"NEMtcUtils.pm\" file under the following directories on the corresponding EMServer; \n\r\n- /var/mcp/install/\r\n- /var/mcp/loads/MCP_17.0.31.2_2016-11-29-1729/install_scripts/bin/\n\r\nAt line 1163, the parameter \"$propFileOption.$oldProps.\" \".$oracleOption.\" \".\" was changed to $propFileOption.$neprops.\" \".$oracleOption.\" \". \n\r\n$oldProps parameter was replaced with $neprops parameter. \n\r\nThis is a known bug which is already fixed out with 17.0.31.3 Patch and upper releases with the JIRA issue (AAK-48065).','null'),(222,'Yunus Ozturk','AS-OAM','2017-04-05','170404-625946','TWT SpA','Problem Description:\r\n====================\n\r\nCustomer was applying the SSL Hardware Migration Procedure from legacy CC3310 to Virtualized IA-RMS Servers and they had an issue with procedure 3 step 18 Method 74-3160\n\r\nSite Release : 14.1.15.0\n\r\n Click on Network Elements->  ->  -> NE Maintenance\r\n Select the instance with ID 0.\r\n Stop the instance. You will lose connectivity to MCP GUI.\r\n Undeploy the instance ID 0.  <-- Failed Step\r\n Deploy the instance ID 0.\r\n Start the instance ID 0.\n\r\nWhen you try to undeploy the SM_0 instance via MCP GUI, it fails with the following error;\n\r\nUndeploy operation failed.\r\nCannot undeploy SM instance. Use undeploy script on server\n\r\nActions Taken:\r\n===============\n\r\nSince SM_0 instance cannot be undeployed via MCP GUI, the corresponding step should be performed manually with the smUndeploy.pl script on the server itself\n\r\nWe have accessed the site and executed the smUndeploy.pl / smDeploy.pl scripts on the server itself.\n\r\nSince this is not clearly documented on the IM document, customer tried to perform this action via MCP GUI and they were not aware how to do it with the script. \n\r\nSeems like we need to update the corresponding IM document and clearly explain the required steps to undeploy/deploy the SM_0 instance with the scripts.','null'),(223,'Yunus Ozturk','AS-OAM','2017-04-03','170402-625583','Timico','Problem Description:\r\n=====================\n\r\nER paged out GPS for an ongoing E2 situation for SM Servers. SM Unit 0 was at Active status but SM Unit 1 was not able to get into Hot Standby Status.. Additionally, all the instance states and alarm status were at gray out status on MCP GUI. So that, no maintenance activities can be performed through MCP GUI..\n\r\nActions Taken:\r\n==============\n\r\nWe have been informed that customer had a recent network problem on their routers and one of the Edge routers the A2 was connected to started causing packet loss. \n\r\nDue to this problem, we thought that all of the instances have flooded the SMs with logs and data. That overwhelms the SMs and contributes to the split brain situation. In order to clear that issue we have recommended the following action plan which should be applied during MTC window.. \n\r\n1-) Stop all the NE\'s in the system at the same time with neStop.pl script for all instances running on all servers on the sytem (SM, SESM, BCP, IPCM, etc (one at a time))\r\n2-) Clean the /var/mcp/spool directories of all the servers with the following command\r\nfind /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f -exec rm -f {} \\; ; find /var/mcp/spool/tmom -type f -exec rm -f {} \\; ; echo DONE\r\n3-) Start the instances starting with SM, SESM, BCP, IPCM, etc (one at a time) with neStart.pl script\n\r\nAdditionally, we have recommended to reboot the IPCM Servers as they have 1000 days uptime and customer was having some IPCM registration issues. \n\r\nER will be applying the steps during MTC window..','null'),(224,'Bill Picardi','AS-OAM','2017-03-27','170328-624892','TWT SpA','The customer was directed to page out to GPS to manually remove the current registrations within the application server database, to prevent registration conflicts after upgrade maintenance.  \n\r\n  This is a follow-up to case 170322-624215 BC:Wizard hung during upgrade from MCP14.1.0.12 to MCP14.1.15.  The previous night the customer was trying to upgrade from MCP14.1.0.12 to MCP14.1.15, but failed due to hardware disk failure.  They contacted ER and rolled back with GPS assistance.  During the upgrade/rollback SESM failovers events, the registration records got hung, possibly related to AAK-27617 Case 120319-322888.  The regdest info is not matching within the ACTIVE and HOT STANDBY units, and the HOT STANDBY is missing routelist \"path\" info. \n\n\r\n  At request of the customer during a specific point in the maintenance, the following was performed:\r\n   STOP the SESM instances\r\n   Delete DB regdest entries\r\n   Start SESM instances one by one\n\r\nWe remained on the call to check registration counts, to ensure these normalize to the previous value before maintenance, then ended the activities.\r\n-Bill','null'),(225,'Burak Biyik','AS-OAM','2017-03-26','170325-624700','Swisscom (Schweiz) AG','The trunk group issue experienced by Swisscom could not be identified by Design engineers for long hours (almost a day).\n\r\nThe agreement was rolling back the site to 18.0.24.2\n\r\nThe rollback was performed by GPS and took around 6 hours to be completed. The highlights:\n\r\n-> Rollback started with an hour delay since there were still some tests executed (deploying SESMs with 19.0.1.1 to rule out the possibility of bad 19.0.1.2 patch)\n\r\n-> There were JAR files on SESM and PROV to be re-deployed during the rollback process. Even with JAR files, long initialization time of SESM instances delayed the completion.\n\r\n-> Customer wanted to perform call/provisioning tests at half-rollback point. This was neither necessary (there is no way to go back to 12.0 release since installer files were already removed by the wizard) nor healthy (the system is not stable during rollback as documented already). This caused delay as well.','null'),(226,'Burak Biyik','AS-OAM','2017-03-26','170325-624700','Swisscom (Schweiz) AG','Following the 11.2 to 12.0 upgrade (18.0.24.2 -> 19.0.1.2) for Swisscom P-SYSTEM (LIVE), it was reported that TrunkGroup routing was not working properly for SA SIPPBX Extension users.\n\r\nSESM does not seem to care what provisioned on the Prov. Client, yet it always uses to default trunk group id \"100\"\n\r\nBefore Upgrade: INVITE sip:+41218049820;tgrp=101;trunk-context=a.617315.gate.swisscom.ch@10.83.0.25:5060;nt_info=proxy SIP/2.0 ---> tgrp = 101 \r\nAfter Upgrade : INVITE sip:+41218049822;tgrp=100;trunk-context=a.617315.gate.swisscom.ch@10.83.0.25:5060;nt_info=proxy SIP/2.0 ---> tgrp = 100 \n\r\n-> I connected to the database and verified that tgrp id 101 exists for the given domain\r\n-> SESM IMDB did not have this data after running audit as well as instance restart\n\r\nThis was clearly a problem with SESM-DB synchronization. I let CallP GPS to continue their investigation since this problem was taking the customer to rollback decision.\n\r\nIt was weird that this issue hadn\'t been identified during half-upgrade steps so that SWD would have performed half-rollback to make the system stable again.\n\r\nOnce being in this position, CallP GPS continued working with DS/Design to identify and fix the problem before customer came up with rollback decision.\n\r\nI dropped the call since it was no longer OAM issue.','null'),(227,'Burak Biyik','AS-OAM','2017-03-24','170324-624560','Axtel','SM_1 was failing to be HOT STANDBY as reported by ER for one of Axtel sites running 17.0.22.6.\n\r\nIts operational state was bouncing between the following:\n\r\nSM.1: Operational State: Warm Standby -> Synchronizing \r\nSM.1: Operational State: Synchronizing -> Warm Standby \r\nSM.1: Operational State: Warm Standby -> Synchronizing \r\nSM.1: Operational State: Synchronizing -> Warm Standby \n\r\nSM1 seemed to produce high number of core files and related logs into the instance\'s work directory. I copied them into a temporary folder for RCA and followed undeploy/deploy/start operations respectively after making sure that there is no JAR file applied. This did not change the situation.\n\r\nSwacting SM instances double times resolved the issue.\n\r\nER engineer will collect requested logs and attach to the RCA case.','null'),(228,'Burak Biyik','AS-OAM','2017-03-23','170322-624215','TWT SpA','Customer was doing an 8.0 SP1 upgrade from 14.1.0.12 to 14.1.15.X to get the last MR of SP1 release before moving to R18 along with H/W migration since they currently have HP-CC3310.\n\r\nThe failing wizard screen was \"Patching Primary Server Platforms\" screen. So, it was just a beginning of the UPGRADE steps after switching service to secondary instances. EMServer1 apparently failed to boot up after OS patching whereas it worked fine for SESMServer (not patch related!).\n\r\nWe had to bring that server up to proceed with the upgrade. We asked customer to perform some actions such as power cycling, disk swap, using only one disk etc. This took around 1,5 hours for customer to get the site and perform such actions. The server was up after removing the disk \"sdb\".\n\r\nThere were 2.5 hours left for MW to be finished. I told the customer that this time is enough for the completion of the upgrade, but half upgrade tests should have been considered as well.\n\r\nCustomer preferred rollback the system and re-schedule the upgrade (possibly for tonight) after placing new disk into the server and making sure that server is bootable with new disk inserted.\n\r\nI rolled back server platforms, switched service to primary instances, started db/server monitors and setup db replication.\n\r\nCustomer performed few tests and we dropped the call after agreement.','null'),(229,'Yunus Ozturk','AS-OAM','2017-03-09','170308-622226','Global Village Telecom (GVT)','Problem Description:\r\n====================\n\r\nUpgrade Path : 14.1.15.0 to 18.0.28.1\n\r\nAfter patching the EMServer1, the server rebooted itself as expected. However, SM_0 instance could not recover itself and stayed at Online - Unavailable status..\n\r\nWizard stayed stuck at Step 29.. \n\r\nActions Taken:\r\n==============\n\r\nWe have checked the logs and they were also telling us that SM_0 instance should be at ONLINE - HOT STANDBY status..\n\r\n2017-03-08 01:30:31,145 ERROR PatchPrimaryServerPlatformsController - State of primary SM is incorrect : Online - Unavailable. It should be ONLINE - HOT STANDBY.\n\r\n- Accessed the MCP GUI and killed the instance manually but Start button did not take any action and the instance stayed at Offline - Unavailable after killing it.. Start button never worked for some reason..\n\r\n- Tried to start the instance manually with the scripts on the server itself (neStop.pl/neStart.pl, smStop.pl/smStart.pl, smUndeploy.pl/smDeploy.pl, neinit restart and reboot). None of these actions changed the status of the SM_0 on MCP GUI.. It stayed stuck at Offline - Unavailable status.. \n\r\n- We also noticed that the status of the SM_0 instance was shown as HOT STANDBY status when we check the status of the instance with getInstance.pl script. This was weird.. The server was showing the correct instance status but MCP GUI not.. \n\r\n- Since we had limited time left on customer maintenance window, we made some of the upgrade actions manually with some scripts (dbInstall.pl -fo, mcpUpgrade.pl, patchPlatform.pl)\n\r\n- After completing the SM/DB Upgrade screens manually with the scripts, we have launched the wizard again and have the wizard to complete the rest of the upgrade.','null'),(230,'Oktay Esgul','AS-OAM','2017-03-02','170301-621117','BT MSL','We had shared the MOP for reinstallation of the problematic SESM blade.\r\nGTS had to assist customer to get the load.\n\r\nRodney paged me due the configNDM script failure.\n\r\nWhen I connected to site, realized that there is not installer file at NDM , they uploaded the patch iso instead of installer file.\n\r\nI have copied the corresponding iso file to my pc yesterday in case of it required.\n\r\nThen, I have sent the correct file to customer via secure file transfer which took almost 2 hour to upload/download/transfer.\n\r\nOnce the load is transfered, I have applied below procedure\n\n\n\r\ncreate_fs -m /opt/corp/a2 -s 5000\r\nmount_fs -m /opt/corp/a2\r\nexport_fs -a /opt/corp/a2\n\r\nha app-blade deactivate 0 0 6 0\r\nha app-blade offline 0 0 6 0\n\r\nhardware app-blade activate 0 0 6 0\n\r\nhardware app-blade online 0 0 6 0 \n\r\nhardware app-blade show 0 0 8 0 detailed\r\nhardware app-blade lock 0 0 8 0\r\nhardware app-blade modify 0 0 8 0 logical-blade-type openslot-0\r\nhardware app-blade modify 0 0 8 0 logical-blade-type openslot-1\n\n\r\nhardware app-blade show 0 0 8 0 detailed\n\r\nmkdir mnt\r\nmount o loop mcp_core_linux_ple2-17.0.18.iso mnt \r\ncp mnt /opt/corp/a2/mnt/extra/pxe/*  \n\r\n./configNDMforA2E.pl -blade 0 0 6 0 script and  completed installation steps through the console connection screen.\n\r\nPlatform installation took almost 1 hour due to vpn connection drop issues.\n\r\nOnce the server installed, I have deployed the sesm instance from MCP gui.\n\r\nServer recovery completed.','null'),(231,'Burak Biyik','AS-OAM','2017-03-01','170228-621107 ','Vodafone (New Zealand)','Vodafone reported provisioning outage after rebooting primary EM Server as a part of the 170212-618673 investigation. Primary database was somehow in QUIESCED state resulted in loss of provisioning. \n\r\nFollowing scripts were run respectively with ntdbadm user for the recovery:\n\r\n- activateRepDB.pl\r\n- cleanupReplication.pl\n\r\nThen, customer reported login issues to PROV Client. Login credentials were somehow not working. I reset credentials first to default, then what was before. Still, I was not able to get in. After restarting each PROV instance, login issue was resolved.\n\r\nAs final step; The \"setupDBReplication.pl\" were run and failed with timer expired error as in the original case. Then;\n\r\n- took current backup from primary db and transferred it to the secondary EM server\r\n- restored that backup to secondary db\r\n- run \"resync.pl\" from secondary to primary\n\r\nAll db related alarms were cleared on MCP GUI.\n\r\nAgreed and dropped the call.','null'),(232,'Oktay Esgul','AS-OAM','2017-03-01','170301-621117','BT MSL','Rodney from ER paged me to report BT MSL SESM1_1 was down .\n\r\nSystem Load: MCP 17.0.22.8\r\nPlatform : 17.0.18 ple2\r\nHardware: Atca I7 (Nehalem)\n\r\nPer initial reports, seems customer replaced the front card when they realied the applicaiton is down.\r\nAfter their replacement, since they could not boot the server up, they paged ER.\n\r\nI have connected site and connected to NDM to take a look at boot status of the server.\n\r\nServer boot was failing due to below error:\n\n\r\n##############\r\nIntel(R) Boot Agent PXE Base Code (PXE-2.1 build 086)\r\nCopyright (C) 1997-2007, Intel Corporation\n\r\nPXE-E61: Media test failure, check cable\r\nPXE-M0F: Exiting Intel Boot Agent.\n\r\nNo partition active\n\r\nNo partition active\n\r\nPXE-E61: Media test failure, check cable \r\nPXE-M0F: Exiting Intel Boot Agent. \n\n\r\n##############\n\r\nWe have tried to deactivate/activate the blade for another reboot attempt, yet the ha app-blade deactivate command failed with restore process error, I have recovered this problem by running  \"software backup archieve abort\"\n\r\n###############\r\ncli>ha app-blade deactivate 0 0 6 0  \r\n%Error[0x18]: Command is not allowed during data restore process. \n\r\nData restore process issue got cleared after aborting a stuck backup process\r\nCustomer was able to issue maintenance commands successfully\r\ncli>software backup archive abort---------------aborted the backup process \r\n%Warning: If a backup is in progress, this command will abort it. Do you want to continue? \r\nconfirm (y/n)>y\r\n###############\n\r\nThen, I have jumped to bios setups of the blade. Disks were not present at boot options list. \n\r\nRecommended customer to reseat the front card again, they did but it did not help.\n\r\nThen I recommended, to re-seat both front/rear card, which did not help neither.\n\r\nAll night , we have several attempts to re-seat cards including rear card replacement .\n\r\nIn the meantime, customer reported they have an alternative rear card as well. \n\r\nOnce they replaced the rear with brand new one, \"no partition active\" errors are cleared.\n\r\nHowever, since the both rear/front card are replaced, server needed to be reinstalled.\n\r\nCustomer did not have the installer file.Also , they reported the MW should be ended.\n\r\nIn conclusion, I let the customer know how to proceed with this recovery and dropped the call.','null'),(233,'Burak Biyik','AS-OAM','2017-02-22','170212-618673','Vodafone (New Zealand)','I was paged again regarding the previous pager call. This time ER provided remote access details.\n\r\nI check database instances and \"Oracle Broken Replication\" alarms were still there but I logined to Provisioning Client to test db availability.\n\r\nI was able to do some provisioning through Provisioning Client so that db was writable.\n\r\nI told ER to troubleshoot existing alarms with a case.\n\r\nAgreed and dropped the call.','null'),(234,'Burak Biyik','AS-OAM','2017-02-22','170212-618673','Vodafone (New Zealand)','Based on the reference case 170201-616840, GTS were provided following procedure to cleanup db related alarm: `Oracle Replication Link Deferred Transactions\'. The system is running 8.0 SP1 release.\n\r\n-	Login to Primary DB Server and navigate to /var/mcp/run/MCP_X/mcpdb_0/bin/util/ directory \r\n-	Run the script ./cleanupReplication.pl with ntdbadm user \r\n-	Login to Primary EM Server and navigate to /var/mcp/install directory \r\n-	Run the script ./setupDBReplication.pl with ntappadm user \n\r\nWhile running ./setupDBReplication.pl, GTS faced with a failure and suspected that db remained in QUIESCED mode that would block provisioning.\n\r\nI asked ER to provide site access to check the system, but ER did not seem to have the latest remote access details.\n\r\nI told ER to update the case with remote access so that I can investigate accordingly.\n\r\nAgreed and dropped the call.','null'),(235,'Burak Biyik','AS-OAM','2017-02-20','170220-619880','Singtel Optus Pty Ltd','It was reported that \"dbBackupAndFtp.pl\" was taking more than 15 hours during HP-CC3310 TO IA-RMS H/W migration for one of the Optus sites (running 14.1.15.X).\n\r\nThis was a part of pre-migration steps (should not be a pager call indeed) and Zelko told that scheduled maintenance window had already been aborted since this step took so much time.\n\r\nOnce being called, I provided the workaround solution (it is a known issue for Oracle11g and was fixed in later releases) to be used in the next MWs.\n\r\n#############################################\n\r\n1.      Kill the current dbBackup process\n\r\n1.	Open the file \"/var/mcp/run/MCP_XX.0/mcpdb_0/work/initmcpdb.ora\" with vi editor \n\r\n2.	Comment out the parameter aq_tm_processes=0 as #aq_tm_processes=0 and save&exit.\n\r\n3.	Restart Oracle database with:\n\r\n         /etc/init.d/dbora stop\r\n         /etc/init.d/dbora start\n\r\n4.	Retry taking db backup\n\r\n#############################################\n\r\nAgreed and dropped the call.','null'),(236,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-02-17','170217-619582','Alphawest Services P/L (Carr)','David Giomi from Network Integration is paged me in order to report that he has missed the upgrade for PA servers. Upgrade was already completed via Wizard.\n\r\nSince this is a lab environment, I have informed him about we do not provide pager support to this issue and requested to look with case work.\r\nCase is dispatched to GPS A2 OAM queue.','null'),(237,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-02-17','170217-619560','Unitymedia','ER paged me report that Upgrade wizard hung at screen 23 - where it is patching primary host servers.UW was not able to connect Host2Server1 because it could not be UP after rebooting the server.The error message was \"unable to connect to daemon\"\n\r\nUpgrade path : 17.0.12.20 to 17.0.31.2 \n\n\r\nFor the recovery action,First I have tried to deactivate/activate command on the NDM.\r\nAfter the command execution is completed, the server became UP and asked to customer click Retry button on the UW.\r\nThen screen is passed successfully.\n\r\nFor the root cause investigation, I have requested related logs from ER to see if this is fixed on latest release of A2.','null'),(238,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-02-14','170214-618964','Alphawest Services P/L (Carr)','David Giomi paged me in order to report that UW was failing due to cannot find the related ASU load in the /var/mcp/media directory.\n\r\nSince this is pre-upgrade step of UW, do not provide pager support to this issue and requested to look with case work.\n\r\nThen dropped the call.','null'),(239,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-02-16','170216-619454','CIBC','SWD paged me out in order to report that resync.pl was giving HUP interruption caught error.\n\r\nSince this is a lab environment of CIBC , I have asked to forward this case to GPS A2 OAM queue and informed that we will look that with case work.','null'),(240,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-02-14','170213-618911','Paltel','ER paged me to report that UW is stuck at Preparing Upgrade Report.(post upgrade steps)\n\r\nI have informed ER about not giving pager support to post upgrade steps.\r\nWe will investigate this issue with case work.\n\n\r\nNote:\r\nUpgrade was already performed to 17.0.31.2 AS release successfully.','null'),(241,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-02-14','170214-618934','Unitymedia','ER paged me to report MCP Upgrade stuck on upgrade DB&SM screen.\n\r\nUpgrade Path: 17.0.12.10 => 17.0.12.20\n\r\nThe issue is that UW was not able to monitor the progress bar although step is successfull.(verified from the smUpgrade logs)\n\r\nAsked customer to click save&exit and re-open the wizard.\r\nAfter customer opened the UW, screen was monitored successfully by UW and customer passed the screen by clicking Next button.\n\r\nRCA will not be provided since 10.3 is EOL.','null'),(242,'Yunus Ozturk','AS-OAM','2017-02-10','170210-618477','Bragg Communications (Eastlink)','Problem Description\r\n====================\n\r\nGENBAND was on site performing card upgrades as they had bad releases on this site.. They are changing out their GBRY52LA cards from release 1 to release 8\n\r\nAfter making this change, customer lost connection to MCP GUI and SM could not get the Service IP.\n\r\nActions Taken:\r\n===============\n\r\nGPS connected the site and noticed that SM instance was not in running state.. GPS started the SM instance manually as follows and the issue is resolved. \n\r\n[root@HLFX0A2SM11 ~]# cd /var/mcp/run/MCP_17.0/SM_0/bin/\n\r\n                   [root@HLFX0A2SM11 bin]# ./getInstanceState.pl \r\n                   [root@HLFX0A2SM11 bin]# su ntappadm\r\n                   [ntappadm@HLFX0A2SM11 bin]$ ./neStart.pl \n\r\n                   Starting the SM\n\r\n                   [ntappadm@HLFX0A2SM11 bin]$ ./getInstanceState.pl \r\n                   UNAVAILABLE\r\n                   [ntappadm@HLFX0A2SM11 bin]$ ./getInstanceState.pl \r\n                   ACTIVATING\r\n                   [ntappadm@HLFX0A2SM11 bin]$  ./getInstanceState.pl \r\n                   ACTIVE\n\r\nAfter SM was restarted GUI was able to be launched successfully and customer was able to update the                   certificates needed to complete the hardware upgrade activity they were performing at the site.','null'),(243,'Burak Biyik','AS-OAM','2017-02-01','170201-616825','Vodafone (UK)','ER reported that \"Patching/Migrating Oracle database on the primary server\" step failed during the upgrade from 18.0.0.4 to 18.0.26.1.\n\r\n\'re-try\' was performed two times by ER. Looking at the latest produced wizard logs, the oraUpgradeTool.pl script was complaining about CHECK_STATUS step of the migration, which basically checks the db availablity.\n\r\n##########################\r\n oraUpgradeTool.pl   Started at  =>  Wed Feb  1 00:14:18 2017 by ntdbsw\r\nInput Parameters received:\r\n   newOraRelease = 11.2.0.4\r\n   operation = CHECK_STATUS\r\n End of parameters.\n\r\nCan\'t call method \"getName\" on an undefined value at /usr/share/perl5/Error.pm line 38\r\n..............\r\n#######################\n\r\nThen we realized that oracle migration had made progress up to Oracle11g installation and failed at Oracle 11g installation.Since, Oracle was not installed properly, every \"re-try\" were failing with status check due to unavailable database.\n\r\nAfter analyzing the logs, it was found that \"oraclePatch.pl\" script was still using \'FROM LOAD\' load and it was searching some of the Oracle subdirectories (apex,owb,ctx,assistants) in the wrong path.\n\r\n############\r\nScript invoked with: -primary -v 11.2.0.4-7 -l MCP_18.0.0.4_2014-07-21-1928\r\n############\n\r\nThis was strange because the scripts must have already been updated with \"TO\" load until this screen.\n\r\nTo recover the failure:\n\r\n-> Uninstall Oracle 11g \r\n-> Run mcpInstallFirstLoad.pl to update scripts with new load\r\n-> Install Oracle 11g\r\n-> Restore the db backup (taken dynamically on this screen) manually:\n\r\n/var/mcp/run/MCP_17.0/mcpdb_0/bin/util/migrationScripts/oraUpgradeTool.pl -lk RESTORE 11.2.0.4-7\n\r\n-> Continued with SM&DB upgrade\r\n-> Agreed and dropped the call','null'),(244,'Burak Biyik','AS-OAM','2017-01-31','170131-616785','Singtel Optus Pty Ltd','Customer failed to update OMI/OPI configuration on CMT after doing re-IP for AS EM/DB Servers.\n\r\nFollowing is the IM prepared for ATO Optus (running 14.1.15.3) re-IP activity:\n\r\nhttps://portal.genband.com/sites/Svcs/GlbMthdRTls/Mthds/GIS/Transformation%20Methods/Module%2085%20-%20Custom%20Methods/0000007-85-3296.pdf#search=IM%2085%2D3296\n\r\nWhile applying step 14 on the page 41, the following error was received:\n\r\n-------------------------------------------\r\nIs the information correct? (y or n) : y \r\nRegistering CMCLLI to A2 ... \r\nRegistering CLLI to A2 ... \r\nCallServerOmiIf::loginOmi enter. \r\nCallServerOmiIf::loginOmi - got response \r\nresponse = 0 \r\nresponse.getSessionID = 795737425677648045 \r\nCallServerOmiIf::getCallServerByCLLI enter. \r\nERROR: The following exception occured during configuration of OMI! \r\ncom.nortelnetworks.mcp.ne.sm.base.svc.event.RequestException: Request not authorized\r\n---------------------------------------\n\n\r\nDuring reconfiguration, I suspected if OMI user was assigned to the authorized role to login to MCP GUI. After checking the role of admin user created for OMI interface, I saw it had no access. I gave \"admin\" role to it and re-run the configuration script on CMT.\n\r\nThen, we had successfull response:\n\r\n-------------------------------\r\nRegistering CMCLLI to A2 ...\r\nRegistering CLLI to A2 ...\r\nCallServerOmiIf::loginOmi enter.\r\nCallServerOmiIf::loginOmi - got response\r\nresponse = 0\r\nresponse.getSessionID = 8191458048946995507\r\nCallServerOmiIf::getCallServerByCLLI enter.\r\nThe CLLI \'CO19INTL\' has been registered into A2. The Call Server Name is \'CS-1\'\r\nPlease Lock/Unlock CMTg Units or swact CMTg to activate changes. \r\n----------------------------\n\r\nAgreed and dropped the call.','null'),(245,'Yunus Ozturk','AS-OAM','2017-01-26','170126-615974','Data Access Communications Inc','Problem Description:\r\n=====================\n\r\nSWD paged out GPS regarding a manual simplex A2 Lab Upgrade problem..\n\r\nUpgrade Path : From 17.0.7.13 to 17.0.31.2 \n\r\nAfter upgrading the system to the 17.0.31.0 load, applicator goes to run the \'mcpPatch.pl\' command and get the following error: \r\n============================================================== \r\n[ntappadm@SNLB0em11 install]$ mcpPatch.pl \r\nPING 10.75.1.153 (10.75.1.153) 64(92) bytes of data. \n\r\n--- 10.75.1.153 ping statistics --- \r\n2 packets transmitted, 2 received, 0% packet loss, time 999ms \r\nrtt min/avg/max/mdev = 0.048/0.049/0.051/0.007 ms \n\r\nLoadname was not entered, will derive Base loadname from installprops.txt \n\r\n----------------------------------- \r\nManagement (SM) Information: \r\nHost: 10.75.1.153 (local host) \r\nPort: 12100 \r\nNE Name: SM \r\nInstance: 0 \r\nLoad: MCP_17.0.31.2_2016-11-29-1729 \r\nConfig: A2-Demo_Small_4GB2Core \n\r\nDatabase Information: \r\nHost: 10.75.1.153 \r\nNE Name: mcpdb \r\nUserName: mcsdbapp \r\nType: SINGLE \r\nPerform Backups: N \n\n\r\nOperation being performed: PATCH \n\r\nContinue with these settings?(Y/N)[N]: Y \r\n--- Patching the installation scripts \r\nUpdated /var/mcp/install/installprops.txt with new Load => MCP_17.0.31.2_2016-11-29-1729 \n\r\n--- Patching the Database --- \n\r\nDeploying files to 10.75.1.153... \n\r\nUpdating schemas in the DB... \n\r\nDB Operation Completed. \n\r\n--- DataBase Patch Complete --- \n\n\r\n--- Patching the SysMgr (SM) --- \r\nGlobal symbol \"$oldProps\" requires explicit package name at /var/mcp/install/NEMtcUtils.pm line 1163. \r\nCompilation failed in require at /var/mcp/install/smPatch.pl line 21. \r\nBEGIN failed--compilation aborted at /var/mcp/install/smPatch.pl line 21. \r\n*** SM Patch FAILED *** \r\nSee SM Patch log for posible details \n\r\nSee log file for possible details: /var/mcp/run/install/logs/_mcpPatch.log.20170126_012546 \n\r\n*** SM/DataBase Patch FAILED *** \r\nSee log files in /var/mcp/run/install/logs/ for possible details \n\r\nSee log file for possible details: /var/mcp/run/install/logs/mcpPatch.log.20170126_012525 \n\r\n[ntappadm@SNLB0em11 install]$ \r\n===================================================================== \n\r\nActions Taken:\r\n===============\n\r\nSince the corresponding problem was related to Lab Upgrade, GPS rejected to work on this issue during the pager call and asked the customer to raise a case.\n\r\nGPS worked on this issue during business hours and the issue is resolved.. \n\r\nTo fix this issue, we have modified the \"NEMtcUtils.pm\" file under the \"/var/mcp/loads/MCP_17.0.31.2_2016-11-29-1729/install_scripts/bin\" directory.. \n\r\nAt line 1163, the parameter \"$propFileOption.$oldProps.\" \".$oracleOption.\" \".\" is changed to $propFileOption.$neprops.\" \".$oracleOption.\" \". \n\r\n$oldProps parameter is replaced with $neprops parameter. \n\r\nThis is known bug which is already fixed out with 17.0.31.3 Patch and upper releases with the JIRA issue (AAK-48065). \n\r\nThis issue only exists for the manual upgrades to 17.0.31.2 Patch Release. Upgrade wizard does not have this problem. \n\r\nSince this is a simplex lab upgrade and wizard is not being used, this problem was observed..','null'),(246,'Oktay Esgul','AS-OAM','2017-01-29','170129-616385 ','Liberty Global Europe B.V.','Brent Combs from ER paged me to report call failures at Liberty Gloabals.He informed that they restarted the sesm instances which solved the issue for a while,yet the issue resurfaced.\n\r\nRELEASE:\n\r\nMCP_17.0.22.15\n\r\nI have logged in to the site and tried to connect to SM instance,yet even em1 was pingable we could not connect it.\r\nMCP gui could not be launched as well even there is not  generated error at MgmtConsole logs at pc.\n\r\nHowever, there was no terminal access to the primary EM so that customer sent a technician to the site for hard reboot.\r\nIt took almost 90 mins to hard reboot the server.\n\r\nOnce the hard reboot completed, sm/db became active again and I could logged in to MCP gui.\r\nFor a fresh start, I have rebooted the secondary EM as well.\n\r\nIn the meantime,observed several broken link errors between primary/secondary db, so then cleaned this error via viewRepConflicts/delRepConflicts scripts.\n\r\nAll sesm were alarm free after sm/db recovered,yet after a while we observed below critical overload alarms which will be followed by a new case .Case will be dispatched to CallP queue.\n\r\n########################\r\n0] Sun Jan 29 11:12:30 CET 2017 (1485684750892) ***INBOUND*** \r\nSource: IPDestination  [INETADDR: 10.31.103.236][PORT:5060][TRNSPRT: UDP] \r\nSIP/2.0 500 Internal Server Error \r\nFrom: ;tag=7227 \r\nTo: \"SIPLineUser SIPLineUser\" \n;tag=7-4917560-5040656-e9e4db14-000000c6 \r\nCSeq: 70913 INVITE \r\nCall-ID: 20a36b9e953b0ebe4412fc97d1560bd37b23d6bd@212.142.24.34 \r\nVia: SIP/2.0/UDP 212.142.24.34:5060;branch=z9hG4bK-c86-30eedc-16462828 \r\nContent-Length: 0 \r\nRetry-After: 5 \r\nserver:  CBN-CH7465LG-NCIP-4.50.18.22m1a-NOSH\r\n#######################\n\r\nAs the system became stable except sesm critical alarm, we dropped the call.','null'),(247,'Oktay Esgul','AS-OAM','2017-01-28','170128-616374','Canadian Imperial Bank (CIBC)','Chris paged me again to report sm/db upgrade failure.\n\r\nUpgrade Path:\n\r\nFrom:17.0.22.15\r\nTo: 18.0.26.2\n\r\nI have take a look at the smUpgrade logs and verified that the sm/db upgrade completed succesfully. Yet ,screen was stuck.\n\r\nChecked the  traces and observed below exception which is same with 17.0.22.15.\n\r\nI had jar file for 18.0.26.2 as well and applied it to primary sm.However, system was not stable.\n\r\nSo then, I have run smUndeploy/Deploy for a fresh start for SM and applied the jar. These actions solved the issue and MCP gui could be launched succesfully.\n\r\nChris tried to launch the wizard and get OMI WEB Service failed error.Researched the similar cases and rebooted the server for a recovery action per case notes which did not help.\n\r\nThen, run the mcpInstallFirstLoad per another case which did not help either. \n\r\nIn the meantime, I just recommend swd to launch the wizard from web link instead of downloaded jnlp.\n\r\nWhen we tried this ,wizard could be launchED and sm/db upgrade completed successfully .','null'),(248,'Oktay Esgul','AS-OAM','2017-01-28','170128-616362','Canadian Imperial Bank (CIBC)','Ralph Posthumus paged OAM GPS to request the jar file to complete the upgrade as the customer escalating.\n\r\nUpgrade Path:\n\r\nFrom:17.0.22.15\r\nTo: 18.0.26.2\n\r\nWe have involved the desing support team to prop back the fix to 17.0.22.15 patch.\n\r\nDS provide the jar and I applied it to both of the SM unit,this action solved the issue and both unit start to able to get the service ip.\n\r\nOnce we completed the validation, Chris started to upgrade from Prepare DB screen.\n\r\nThe first attempt failed again, so then I have run the activateRepDB at /var/mcp/run/MCP_17/mcpdb_0/bin/util.\n\r\nAnd stop/start the oracle at secondary db .\n\r\nAfter above items, I have retried the wizards and prepareDB screen completed successfully.\n\r\nChris will continue the upgrade.\n\r\nThanks','null'),(249,'Oktay Esgul','AS-OAM','2017-01-28','170128-616362','Canadian Imperial Bank (CIBC)','Chris Henwood from SWD paged me to report that PREPARE DB screen failure during  CIBC upgrade.\n\r\nUpgrade Path:\n\r\nFrom:17.0.22.15\r\nTo: 18.0.26.2\n\r\nI have connected site and take a look at the prepare db failure logs.\n\r\nInitial view per prepare db log that the step was failing due to ned connection issues,however at wizard screen there was a pop up which prompts \"db monitor can not stop null error.\"\n\r\n==> I have restarted the ned daemon of EM1-EM2-DB1-DB2 servers one by one, this action did not help.\n\r\n==>As a next step, I have tried to start/stop db monitors from MCP gui.Yet, get the same failure and sm prompted db monitor can not stop null error like wizard. All actions were failing even stop/start the instances.\n\r\n==> Based on previous item, I though a SM failover might be helpfull to check the sm stability,that s why activated the secondary sm.\n\r\n==> Same issues observed at secondary sm as well .\n\r\n==>As a next step, I have tried to add a dummy sm user to test the db and it failed too. \n\r\n==>I have stop/start the oracle first by running /etc/init.d/dbora stop/start . Once the oracle started, attempted to add a user again and this time   got the db is quisece mode error.\n\r\n==>Then, I have run /var/mcp/run/MCP_17.0/mcpdb_0/bin/util/activateRepDB.pl script at primary DB which solved the quisece mode problem and let me add a user and start/stop the db monitors.\n\n\r\nOnce the above issues solved, in order to able to launch the wizard (as the wizards requires the primary sm status as active) I have performed another failover from secondary sm to primary one. Yet , primary SM could not be activated while getting service ip.\n\r\n==>Investigated the work logs and observed below errors:\n\r\n#########\r\nNov 20, 2016 5:11:06 AM org.apache.coyote.http11.Http11Protocol init\r\nSEVERE: Error initializing endpoint\r\njava.net.BindException: Address already in use :12120\r\nat org.apache.tomcat.util.net.JIoEndpoint.init(JIoEndpoint.java:549)\r\nat org.apache.coyote.http11.Http11Protocol.init(Http11Protocol.java:176)\r\n##########\n\r\n==>This was one the known issues which the fix is available at 17.0.22.20 release that is newer version than the customer load (AAK-47780)\n\r\n==>In addition, observed below error logs as well which the fix is submitted to newer version than customer load. (AAK-45665)\n\r\n#########\r\nFatal Error: Failed to start PHP [php-cgi, -v], reason: java.io.IOException: Cannot run program \"php-cgi\" (in directory \"/home/ntappsw\"): java.io.IOException: error=2, No such file or directory\r\nCould not start FCGI server: java.io.IOException: PHP not found. Please install php-cgi. PHP test command was: [php-cgi, -v]\r\nTimeout waiting for PHP FastCGI daemon\r\nphp.java.bridge.http.FCGIConnectException: Could not connect to server\r\nat php.java.bridge.http.SocketChannelFactory.test(SocketChannelFactory.java:58)\r\nat php.java.bridge.http.FCGIConnectionPool.(FCGIConnectionPool.java:175)\r\n#########\n\r\nI have tried several actions reboot of the servers included to activate the primary SM ,all failed.\n\r\nSince we lost almost 4 hours of MW during investigation, we decided to abort the upgrade till we deliver a jar file to fix primary sm problem.\n\r\nAs a rollback action, I just activated the DB monitors and validated the system is alarm free. (Prepare db is the first step of the main upgrade steps which means there were no upgrade related change at system currently )\n\r\nBC case raised to track the jar process.\n\r\nThanks','null'),(250,'Oktay Esgul','AS-OAM','2017-01-26','TBD','New York University','Dean Gilbert paged me to report Upgrade wizard failure at system validation step due to java version mismatch.\n\r\nUpgrade Path:\n\r\nNot Known yet.\n\n\n\r\nCustomer is performing upgrade on their own and they are performing prep steps which is out of scope for pager support.\n\r\nI have informed Dean to let the customer know to raise a case and dispatch to A2 OAM queue for initial investigation, then dropped the call.\n\r\nThanks','null'),(251,'Oktay Esgul','AS-OAM','2017-01-24','170118-614636','OPTUS','Tom paged me again , as customer would like to continue.\n\r\nCustomer Load:\r\nMCP_14.1.15.3\n\r\nOnce I have connected to server, realized that the working disk of old server was already inserted to new server.That s why ,server platform and oracle was restored automatically.\n\r\nHowever, server was not reachable,the only option was terminal connection.\n\r\nInvestigated the network for a while ,yet even though etho and eth1 were up ,s erver was not reachable.\n\n\r\nIn the meantime, we collected arp logs and Ken realized that eth0 has issues.\n\r\nThen, we down the eth0 interface and activate the eth1 ,this action solved the network issue.\n\n\r\n[root@O3FBSSLM02 bin]# cat /proc/net/bonding/bond0 \r\nEthernet Channel Bonding Driver: v2.6.0 (January 14, 2004)\n\r\nBonding Mode: fault-tolerance (active-backup)\r\nPrimary Slave: None\r\nCurrently Active Slave: eth1\r\nMII Status: up\r\nMII Polling Interval (ms): 100\r\nUp Delay (ms): 0\r\nDown Delay (ms): 0\n\r\nSlave Interface: eth0\r\nMII Status: down\r\nLink Failure Count: 1\r\nPermanent HW addr: 00:0e:0c:5d:77:95\n\r\nSlave Interface: eth1\r\nMII Status: up\r\nLink Failure Count: 0\r\nPermanent HW addr: 00:0e:0c:5d:77:94\n\r\nCurrently both of SM unit are active, customer will need to get a new license for new server.\n\r\nHowever, they will need to investigate eth0 port at router side.','null'),(252,'Oktay Esgul','AS-OAM','2017-01-24','170118-614636','OPTUS','Tom paged me to report that OPTUS replaced the secondary EM server as the disks were faulty .(This issue was investigated last week via another pager case).\n\r\nCustomer Load:\r\nMCP_14.1.15.3\n\r\nI have conected to ER vm ,yet console port was down to secondary EM server. \n\r\nTom pinged the customer ,yet could not get response.\n\r\nWe informed the customer via case comment + email to arrange a MW with corresponding loads for reinstallation.\n\r\nThanks','null'),(253,'Oktay Esgul','AS-OAM','2017-01-24','170124-615509','Data Access Communications Inc','Donnell paged me again to report seconday server OS patching screen failure.\n\r\nEven three of the guest on secondary host was completed sucessfully, just the PAServer was failing.\n\r\nUpgrade Path:\n\r\nFrom: 17.0.7.13\r\nTo: 17.0.31.2\n\n\r\nI have connected site via teamviewer again, tried to launch the virt-manager which took a  long time due to slow network.\n\n\r\nIn paralell, Donnell established another vpn from er vm pc.\r\nIt was faster and I started the investigation by launching virt-manager.\n\r\nThe server was in a weird condition , it was not booting up and just generating udev errors which blocks any action can be done for recovery.\n\r\nTried  several maintenance activies via virt-manager at hardware level ,yet none of them help to solve the issue.\n\r\nIn the meantime, I have skipped the wizards screen to complete the rest of the upgrade while investigating PA issue.\n\r\nThen, I suggested to customer a fresh install at PA2 as there is nothing to do for recovery .\n\r\nCustomer aggreed this option and I have reinstalled the server from scratch.\n\r\nAs a last step, deployed the secondary PA instance with new load.\n\r\nUpgrade completed succesfully.\n\r\nThanks','null'),(254,'Oktay Esgul','AS-OAM','2017-01-24','170124-615483','Data Access Communications Inc','Donnell paged me to report SM/DB upgrade failure during MR upgrade.\n\r\nUpgrade Path:\n\r\nFrom: 17.0.7.13\r\nTo: 17.0.31.2\n\r\nI have connected site and take a look the SM failure logs.\n\r\nIt was failing due to below error:\n\r\nUpdating SM instance(s) load data to MCP_17.0.31.2_2016-11-29-1729 in the database.\r\n/opt/mcp/java/MCP_17.0/jre/bin/java -Doracle.jdbc.timezoneAsRegion=false -classpath /var/mcp/run/MCP_17.0/SM_0/jars/delta_mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/3rdParty.jar:/var/mcp/run/MCP_17.0/SM_0/jars/jhall.jar -DpropFile=/var/mcp/run/MCP_17.0/SM_0/data/neprops.txt -Doracle.jdbc.thinLogonCapability=o3 com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData -u SM 0 MCP_17.0.31.2_2016-11-29-1729 ONLINE  > /var/mcp/run/MCP_17.0/SM_0/work/SMDataUpdateStdOut 2> /var/mcp/run/MCP_17.0/SM_0/work/SMDataUpdateStdErr\r\n        Failed to update the SM instance loads.\r\ncom.nortelnetworks.mcp.share.validator.ValidationException: No validator found for parm NetworkInterface.InterfaceRefreshPeriod\r\n        at com.nortelnetworks.mcp.ne.sm.svc.parm.impl.ParmDataValidator.validateData(ParmDataValidator.java:39)\r\n        at com.nortelnetworks.mcp.ne.sm.base.svc.reg.DataObjectClassMap.validateData(DataObjectClassMap.java:95)\r\n        at \n\n\r\nIn order to retry, we have redeploy the sm with old loads and retried the wizards, yet it did not help.\n\r\nEncountered several errors during retry attempts.\n\r\nIn order to skip the screen, I have run ut_mcpUpgrade script manually with corresponding parameter which completed the sm/db upgrade succesfully.\n\r\nLogs will be attached to case for rca investigation.\n\r\nThanks','null'),(255,'Yunus Ozturk','AS-OAM','2017-01-19','170118-614636','Singtel Optus Pty Ltd','Problem Description:\r\n=====================\n\r\nA2: One unit is out-of-service\n\r\nER reported that customer had a problem with the Secondary EM Server and it never booted up successfully. It was stuck at SCSI Menu all the time. \n\r\nCustomer Load : 14.1.15.3\r\nHardware : CC3310\n\r\n[root@O3FBSSLM01 root]# showversion.pl\r\n                   --------------------------------------------------------------------------------\n\r\n                   System Type:                  mcp_core_linux\n\r\n                   MCP Platform Release Level :  14.1.14 (via patching)\n\r\n                   MCP Platform Hardware Type :  HP-CC3310\n\r\n                   NED Version:                  Version: 14.1.4 (Built on Apr 15 2013 18:33:35)\n\r\n                   JRE Version:                  1.6.0_20\n\r\n                   Linux Kernel Version:         2.4.21-47.ELsmp\n\r\n                   Oracle Version:               11.2.0.4.0\r\n                   Oracle Patch Level:           2\n\r\n                   NIF Version:                  mcp-nif-fw-utils-14.1-1\n\r\n                   JBoss Version:                mcp-version=/main/mcp_install_12.0_int/mcp_install_12.0_sp_int/5\n\r\nActions Taken:\r\n==============\n\r\n- Accessed the problematic server through Terminal Console connection and checked the failures during the reboot process\r\n-  Noticed the following error before the server switched to SCSI Menu\n\r\nPXE-E61: Media test failure check cable\r\nPXE-M0F: Exiting Boot Agent\n\r\nThe error above indicates a bad hardware (disks or server chassis)\n\r\n- Accessed the BIOS Menu of the server and checked the BIOS configurations. Everything was fine.\r\n- Tried to boot the server with 1 disk only. Nothing changed.\r\n- Asked the customer to replace the server with a spare\r\n- Customer is looking for a spare server\r\n- Server will be re-installed from scratch (OS Platform, Application, Oracle, etc..)','null'),(256,'Oktay Esgul','AS-OAM','2017-01-18','170118-614480','Sasktel','Dean Gilbert from ER paged me to report upgrade failure  that is being performed by customer at Sasktel.\n\r\nUpgrade Path:\n\r\nFrom: MCP_14.1.10.3\r\nTo :  MCP_18.0.26.5\n\r\nSM/DB upgrade screen was failing via below error.\n\r\n##################################\r\nInvalid Load name given. Must have be of the form \".zip\" \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/mcpUpgrade/ut_mcpUpgrade.EMS1_0f163d1e-2a88-1b21-aca9-000e0cf5b2d0_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20140427_025614.log \r\n#################################\n\n\r\nI have connected site and take a look at the /var/mcp/loads directory.\r\nThere was just based load zip file and patched zipped file was not exist.\n\r\n*Zipped the destination load and arranged its permissions as below using the following commands \n\r\n\"zip -9 -r MCP_17.0.7.13_2014-03-17-1238.zip MCP_17.0.7.13_2014-03-17-1238\" \r\n\"chmod 660 MCP_17.0.7.13_2014-03-17-1238.zip\" \r\n\"chown ntappsw:ntappgrp MCP_17.0.7.13_2014-03-17-1238.zip\" \n\r\nOnce the above commands completed, I have retried the wizards and the screen completed succesfully.\n\r\nThank you','null'),(257,'Kemal AYDEMIR (NETAS External)','AS-OAM','2017-01-14','170114-613929','Alphawest Services P/L (Carr)','Problem Description:\n\r\nDavid Giomi paged me in order to report that Hardware Migration to IA-RMS is failed at Procedure 2 -Step 26 while starting the SM0.\n\r\nA2 release was 14.1.10.1\r\nPlatform release was 14.1.5\n\n\r\nActions Taken:\n\r\nSince there is no vpn&remote access allowed during the work , I have passed the basic mtce commands to David via IM and waited to see the results with screenshots.\n\r\nThere was no SM_0 directory under the MCP_14.1 directory.Further investigation showed that DB and SM is deployed to DB server although they are seperated.Then I have requested from David to start to migration-steps from stratch and waited to start SM0 but it is failed one more.\n\r\nDavid has suspected about the previous procedure 2-step 19 Installing getMAC.sh script content to SM VM.\r\nThat step was failed after scp operation.Error message was saying No such file or directory.\r\nWe also executed the getMAC.sh on the SM0 VM but it hanged.\n\r\nAfter the successful transfer operation of the script, it was still hang.\r\nThis script should be worked successfully in order to apply half-migrated license key to the system\n\r\nSince there was 75 min. left, customer decided to rollback and then site is rolled back successfully.\n\r\nWe are currently performing root cause analysis on this case to understand what was the main issue behind of this.','null'),(258,'Burak Biyik','AS-OAM','2017-01-04','170103-612408','Axtel','ER reported that the HP3310 server hosting standby SESM instance (MCP_17.0.22.X) was rebooting itself and there were non-synced partitions in RAID table. \n\n\r\n| | | | Sync & Recovery \r\nMD Device | | Member-0 | Member-1 | Speed Finish Done \r\nName Size(MB)| Usage | Name Flg | Name Flg | Mode (MB/s) (min) (%) \r\n--------------+------------+-----------+-----------+-------------------------- \r\nmd0 101.9 | /admin | sda1 U | . _ | . . . . \r\nmd1 101.9 | /boot | sda2 U | sdb2 U | . . . . \r\nmd2 2047.2 | swap | . _ | sdb3 U | . . . . \r\nmd3 2047.2 | swap | . _ | sdb9 U | . . . . \r\nmd4 2047.2 | / | sda8 U | . _ | . . . . \r\nmd5 6141.9 | /opt | sda6 U | . _ | . . . . \r\nmd6 5122.2 | /var | sda7 U | . _ | . . . . \r\nmd7 52297.4 | /var/mcp | sda5 U | . _ | . . . . \n\n\r\nThe resolution was already in the case 151021-552879. After running \"mdadm\" commands to recover the partitions, I got \"not all drives are accessible by the system\" error which indicates faulty disk.\n\r\nThe action plan was re-seating existing drives and identify which one is faulty and then replace it with the spare one.\n\r\nI handed over the rest of the actions to Mark Zattiero (Rodney was working on another pager call) to work with the on-site engineer.\n\r\nBased on the case updates, all of the partitions were in sync after the actions taken.','null'),(259,'Burak Biyik','AS-OAM','2016-12-31','161226-611734','Maxcom Telecomunicaciones SAB de CV','I was paged by GTS related to the case (161226-611734). GTS was provided a procedure to apply security certificate on MEP before it expires, however, got confused when deciding which file to choose as the certificate.\n\r\nEven though MEP pagers are not covered by A2 OAM/GW team, I asked what the confusion is to see if it is an easy one.\n\r\nGTS was confused with the file extensions and told her to choose \".cert\" file to continue the rest of the procedure.\n\r\nAgreed and dropped the call.','null'),(260,'Oktay Esgul','AS-OAM','2017-01-03','170102-612207','Vodafone (UK)','Tom Drapper paged me in order to report unstable SESM instances at two sites of Vodafone.\n\r\nIssue seems started after the leap second insertion at new year eve,yet  somehow they paged ER to get support today. \n\r\nPrior paging A2 OAM , Tom informed that GWC-SST-Core support teams  had investigated their sides and reported all is good.\n\r\n###########################\r\nSystem Info:\n\r\nMcp Release: MCP_18.0.0.4\n\r\n[ntsysadm@ZZ5brck0a2ss31 ~]$ mcpRelease.pl  \n\r\n        *** MCP Platform Release *** \n\r\nSystem Type:     mcp_core_linux_ple4\r\nRelease Level:   18.0.1 (via install)\r\nHardware Env:    RedHat-KVM \n\r\n[ntsysadm@ZZ5brck0a2ss31 ~]$ cat /etc/redhat-release \r\nRed Hat Enterprise Linux Server release 6.2 (Santiago)\r\n############################\n\n\r\nUnder normal circumstances, we do not consider leap second issues in A2 system as we are using NTP to sync server dates from a ntp server. \r\nYet,  even though EM/DB servers were synced from their hosts ,all sesm guests were not synced , even one of the sesm guests  on the same host with EM/DB.\r\nProbably, there might be a network issue as the sesm guests are using isolated callp network instead of oam network.Need to investigate.\n\r\nIn the meantime,I have take a look at the server monitorings and observed below cpu occupancy alarms on all sesm guest servers.\n\r\n##############################\r\nAlarmName: CPU Occupancy Threshold \r\nTimeStamp: Mon Jan 02 17:47:52 EST 2017 \r\nFaultNumber: 401 \r\nShortFamilyName: SRVR \r\nLongFamilyName: SERVER \r\nSeverity: MAJOR \r\nProbableCause: threshold crossed \r\nDescription: CPU Occupancy has reached or exceeded the defined threshold level of 90%. \r\nCorrective Action: Ensure the alarm threshold value is appropriate for the operating environment. High traffic may cause levels to peak. Contact next level of support if service is impacted.\r\n##############################\n\r\nThis indicator is one of the expected results of leap second insertion which is reported by red-hat as well at their official page.\r\nhttps://access.redhat.com/solutions/154793\n\r\nIssue\r\nAfter insertion of leap second several processes, notably java, reporting high CPU usage\r\nLogged messages shows back trace from ktime_get\r\nHigh CPU, non-responsive VMs\r\n##################\n\r\nPlatform level of customer system is subjected to leap second problem which can see below analysis in the leap second 2016 bulletin that is not released officially.\n\r\n\"Analysis: \n\r\n  The EXPERiUS AS is executed on multiple reference and hardware freedom platforms. \r\n  Only the 8.0 SP1, 10.0, 10.1, 10.2, and 11.0 supported release PLE4 (x64) platform for the IA_RMS or HardWare Freedom (HWF) Hypervisors are subject to leap second issue. \"\n\r\nThe bulletin recommends to perform EXPERiUS AS Leap Second Kernel Update Security Patch and then the reboot the server.\r\nPrior starting to apply this procedure, I have give a try to just reboot the guest servers which cleared out all cpu alarms and made the all sesm instances\r\nstable.So then, I have rebooted the all 12 sesm servers one by one at two different sites.\n\r\nOnce the reboots are completed , all cpu alarms cleared and sesm instances became stable.\n\r\nA follow up case will be dispatched to OAM queue for further analysis.','null'),(261,'Burak Biyik','AS-OAM','2016-12-27','161226-611732','ITian Corporation','After trying to resynchronize two databases (related with the case:161226-611732, increasing number of deferred transactions), Oracle was failing to start as reported by ER.\n\r\nBunch of failures was printed on the screen during the Oracle initialization. The following is one of them:\n\r\n##################################\r\nERROR at line 1:\r\nORA-06550: line 1, column 7:\r\nPLS-00201: identifier \'DBMS_SHARED_POOL.KEEP\' must be declared\r\nORA-06550: line 1, column 7:\r\nPL/SQL: Statement ignored\r\nSQL> BEGIN DBMS_SHARED_POOL.KEEP (\'DIANA\', \'P\'); END;\r\n################################\n\r\n-> remembered this error (due to missing redo.log files) as the same db was recovered few months ago by my teammate.\n\r\n-> This time all of the redo.log files were missing under \"/var/mcp/oradata/mcpdb\"\n\r\n-> tried the recovery action plan as given in the case: 120521-337003 to see if it works.\n\r\n-> failed with the last step of the recovery plan:\n\r\n#############################\r\nSQL>  alter database open;\r\nalter database open\r\n*\r\nERROR at line 1:\r\nORA-01589: must use RESETLOGS or NORESETLOGS option for database open\r\n#############################\n\r\n-> tried some recommended solutions as searched on the internet\n\r\n#################################\r\nSQL> alter database open resetlogs;\r\nalter database open resetlogs\r\n*\r\nERROR at line 1:\r\nORA-01195: online backup of file 1 needs more recovery to be consistent\r\nORA-01110: data file 1: \'/var/mcp/oradata/mcpdb/system01.dbf\'\n\r\nSQL> recover database using backup controlfile until cancel;\r\nORA-00279: change 2260455797 generated at 12/28/2016 03:16:57 needed for thread\r\n1\r\nORA-00289: suggestion : /opt/mcp/oracle/admin/mcpdb/arch/arch_1_10046.arc\r\nORA-00280: change 2260455797 for thread 1 is in sequence #10046\n\r\nSpecify log: {=suggested | filename | AUTO | CANCEL}\n\r\n\'/opt/mcp/oracle/admin/mcpdb/arch/arch_1_10046.arc\'\r\nORA-27037: unable to obtain file status\r\nLinux Error: 2: No such file or directory\r\nAdditional information: 3\r\n#################################\n\r\n-> Since there was no redo.log file left, we thought that recovery would take so much time if we go through the solutions one by one.\n\r\n-> Fortunately, customer found the Oracle (9i) installer CDs.\n\r\n-> Installed oracle software from the scratch\n\r\n-> resynchronized two databases again\n\r\n-> monitored the number of deferred transaction for a while and saw that they are not increasing any longer.\n\r\n-> no RCA will be provided as it is retired release (6.0 SP1)\n\r\n-> agreed and dropped the call.\n\r\nThanks Kemal and Garrett for the recovery assistance.','null'),(262,'Burak Biyik','AS-OAM','2016-12-27','161226-611732','ITian Corporation','ER paged me to report loss of db communication alarms on NEs for the customer running retired release (6.0SP1, MCP_10.3.x.x).\n\r\n-> created a bridge with the customer and got the details of the actions taken previously\n\r\n-> It all started with replacing faulty primary EM server with spare one. DBMNS826 alarms were raised indicating that the replication queue for link [ SSDVDBREPGROUP ] is not being serviced due to the increasing amount of deferred transactions.\n\r\n-> Customer had tried to resync two databases (details not known), but this did not clear the high number of deferred transactions and additionally raised DBCOMM alarms for a period.\n\r\n-> There were no DBCOMM alarm raised after I connected to the system, no call failures, no provisioning outage.\n\r\n-> I requested the reboot of the primary EM server and then try resync again, but customer told that MW was over so it had to be run in the next MW.\n\r\n-> Customer will apply the action plan in today\'s MW (00:00 GMT+9) and pager ER in case of any failure.\n\r\n-> Agreed and dropped the call.','null'),(263,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-12-23','161223-611613','Nuvia','Problem Description:\n\r\nBrad from ER paged me in order to report that host server where the MAS VM is deployed went nut poor quality so Phil switched to second but then found the CPU was high on the host.\r\nThen,Phil rebooted the host because there were a ton of \"power_saving\" processes. \n\r\n44867 root      -2   0     0    0    0 D 99.5  0.0  37:23.40 power_saving/13\r\n44758 root      -2   0     0    0    0 D 97.2  0.0  37:23.11 power_saving/1\r\n44878 root      -2   0     0    0    0 D 97.2  0.0  37:22.77 power_saving/23\r\n...\n\r\nAfter reboot everything was fine then 2hrs later both SESMS were active on the A2 system and he had seen the power_saving processes on the host server again.\n\r\nThe host server was Dell R620.\r\nRHEL was 6.7 Santiago\n\r\nActions Taken:\n\r\n1)Investigated the issue on the web and found that this is a known bug about acpi_pad driver on Dell R620.\r\n3)Added the acpi_pad.disable=1 parameter to grub.conf in the /boot/grub directory to force loading of acpi_pad driver.Then made a reboot on the host server.\r\n4)Monitored the site more than 1 hour and seen that no more power_saving processes on the host server.\r\n5)Requested from Phil to schedule BIOS updates and firmware updates for this server in order to prevent the occurence of this issue again.\n\r\nThen dropped from call.','null'),(264,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-12-11','161211-609924','Kapsch','Problem Description:\n\r\nTom from ER paged me to report that the alarms below has come back after we performed the action plan on the previous outage.\r\nCustomer is reported that some registrations are failing, but not all.\n\r\nAlarmName: Database Overload Control,OVLD402 (CRITICAL)\r\nAlarmName: IMDB_Resync, IMDB701 (MAJOR)\n\r\nSite is running on 17.0.22.11 A2 load.\n\r\nActions Taken:\n\r\n1) I have investigated the OSS and work logs of SM and SESM and DB logs.\n\r\n2) Found the below log on the SESM OSS logs.\n\r\nSESM1_0 DBCOMM 205 INFO DEC11 13:56:18:514 MCP_17.0.22.11\r\n  Open connections to DB Instance 0:\r\n    x1873eb2, since=12/11/16 7:53:41 AM, owner=DB Instance 0 (heartbeat connection)\r\n    x12c010c, since=12/11/16 1:56:18 PM, owner=DELETE FROM VMMSGWAITING WHERE RSRC_ID = ? \n\r\nI have suspected possible REGDEST issue when I see the above \"open connections to DB log\". Requested the page CallP GPS.\n\r\n3) Ozgur collected SIP trace and found that a huge amount of NOTIFY requests for 2 spesific user.Further investigation from his side showed that NOTIFY requests consists corrupted data(0.0.0.0)\n\r\n4)Checked the DB and seen that corrupted data was in REGDEST table.Cleared the corrupted data for that 2 spesific user.Then Ozgur performed a full SESM stop/start.\n\r\n5)In a one hour, SESM\'s has lost their redundancy again.We have logged out that 2 spesific user from PROV.However,this action did not cleared the corrupted data on IMDB because system is overwhelmed by internal NOTIFYs so DB and IMDB couldn\'t synced.Later then ,We have requested to perform a full SESM stop/start to clear the corrupted data on IMDB.\n\r\n6)Ozgur is requested to delete this 2 spesific users and we made a full SESM stop/start again.Ozgur is taken a SESM trace one more time and could not see the corrupted data(0.0.0.0) this time. \n\r\n7)Monitored the site more than 1 hour and since the system is stable, CallP GPS is requested data collection plan from ER to avoid further registration issues in future.','null'),(265,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-12-11','161211-609924','Kapsch','Problem Description:\n\r\nBrad paged me to report that customer is experienced SIP registration failures and seeing the below alarms on the active SESM instance of a2 system.\n\r\nAlarmName: Database Overload Control,OVLD402 (CRITICAL)\r\nAlarmName: IMDB_Resync, IMDB701 (MAJOR)\n\r\nActions Taken:\n\r\n1)Looked at the corrective action of alarm.There was no hint about how to clear this alarms.\r\n2)Investigated similar cases on SFDC.Found that similar issues have been resolved with dbora stop/start and SESM double swact action.\r\n3)Asked to customer to apply the action plan on the second matter.\r\n4)I have stopped/started DB and then double swact\'ed the active SESM instance and alarms got cleared on the SESM instance.\r\n5)Since the system is stable more than 15 min asked to make test calls.Customer reported that tests calls are successfull.\n\r\nGiven the data collection plan to the ER in order to understand what is behind of this issue.Then aggreed to drop the call.','null'),(266,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-11-25','161125-608018','Jet Infosystems','Problem Description:\n\r\nER has paged me in order to report that MR upgrade is failing at Secondary oracle database migration screen.\n\r\nHe informed me that when the screen above is opened , wizard has given the below error:\n\r\n=> Error occurred during monitoring script on EMS1 server. Please click to Save&Exit and relogin to the wizard.\n\r\nCustomer has tried to relogin to the wizard but all the time taken the same error.The screen is not started due to this error message.\n\r\nUpgrade path: 14.1.0.8 to 14.1.15.1\n\r\nActions taken:\n\r\n1)Requested the wizard logs on the local PC.After investigation, wizard logs did not indicate what actions can be done related with this issue.\r\n2)Tried basic mtce operations;Double swacted the SM.Reboot on EMS1.Ned restart.\r\nNevertheless taken the same error.\r\n3)Executed the script manually which is running on this step.\n\r\n./ut_oraclePatch.pl -nc -secondary -v 11.2.0.4-2 -l MCP_14.1.15.1_2015-05-13-1548\n\r\nAfter script is completed, started the wizard again and observed that it has opened from the next screen which is upgrading secondary NEs.\n\r\n4)Waited to be completed of upgrading secondary NEs screen and made a save&exit and passed the UW to the customer back.\n\r\nRCA: Since 8.0sp1 is unsupported release, we will not give RCA for this issue.','null'),(267,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-11-24','160914-597842','BT MSL','Problem Description:\n\r\nKevin has paged me to get further support about restoration process of SESM4_0 instance.\r\nAfter they have installed the platform to SESM rear-blade successfully from NDM server, the server was trying to boot up from DHCP due to wrong boot order on the BIOS.\n\r\nCustomer was running on 17.0.22.8 AS load.\n\r\nActions Taken:\n\r\nAfter we have placed the HardDisk at the top on the BIOS, this issue has been resolved and we continued with deploy&start the Session Manager instance. \r\nChecked the alarms for this instance and server on the MCP and all was green.\r\nDeleted the extracted files for the installation process on the NDM server then dropped from bridge.','null'),(268,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-11-23','161122-607727','Alphawest Services P/L (Carr)','Problem Description:\n\r\nDavid was performing migration from HT to IA-RMS.\n\r\nAsk for further help about the IM 34-3160 procedure 2 step 28.\r\nHe wanted to clarify from GPS if we just so Primary PROV only. \n\r\nActions Taken:\n\r\nStated that perform the primary EM/DB servers migration as written on the IM.\n\r\nIM has a good example about this like below;\n\r\nwhere  is any instance running on the migrated primary server.\r\nfor example: where  is Provisioning Manager, Accounting Manager (if any), Application Manager (if any), and Fault Performance Manager (if any).\n\n\r\nRecommended to make this step also for primary PROV instance.','null'),(269,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-11-23','161122-607727','Optus','Problem Description:\n\r\nCustomer were added SESM2 and SESM3 to Production IMS.\r\nThey can provision subscribers and no alarms but those subscribers can\'t make calls. It doesn\'t look like SESM IMDB is populated.\n\n\r\nActions Taken:\n\r\nInvestigated the case description and connected to the site.\r\nDetermined that this is CallP related issue and recommended to page A2 CallP GPS for further support.','null'),(270,'Ken Johnson','AS-OAM','2016-11-22','161123-607765','Alphawest','AS migration from HT to IA-RMS failed to launch SM on new IA-RMS.  Verified license key MACs and found that the key Bernhard provided was correct. The investigation turned to the new SM0 on the IA_RMS and found that an executable we manipulate to perform the migration was done so incorrectly. This left the executable with an incorrect user, group, and permissions.  This prevented the SM from validating its license key.  After correcting the permissions, owner, and group the SM could then importing the license successful.\r\n  Verified IM 74-3160 commands are correct.\r\n  Updated the migration tool package to prevent future occurrences.','null'),(271,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-11-22','161122-607611','Tbaytel','Problem Description:\n\r\nSWD paged me to report that had issues installing the certificates on GVM and CMTg.\r\nSite has been upgraded to 18.0.26.1 from 17.0.31.0.\n\r\nError message is below;\n\r\nroot@typhoon-base-unit1:/run/opt/corp/gvm/2.0.0/conf> /usr/java/default/bin/keytool -import -alias mcs_cert -keystore /run/opt/corp/gvm/2.0.0/conf/server.truststore -storepass genband -file /tmp/cert.cer \r\nkeytool error: java.io.FileNotFoundException: /tmp/cert.cer (No such file or directory)\n\r\nActions Taken:\n\r\nGVM:\n\r\n- Asked to SWD that which document he is following. [NN10324-510 04.18]\r\n- Looked at the exception.Checked that file was not located at the tmp directory.\r\n- ASked to SWD that if he had performed the previous step(\"Retrieving the Certificate from AS MCP Console\") on the document before applying the GVM certs.\r\nAfter performed the steps and put the cert to the GVM host server, installing the AS cert on the GVM application host was successful.\n\r\nCMTg:\n\r\n-SWD has issues with the fetch command which is given below\r\n[echo | openssl s_client -connect $OMI_IP:$OMI_PORT 2>&1 | sed -ne \'/-BEGIN\r\nCERTIFICATE-/,/-END CERTIFICATE-/p\' > /tmp/mcsOMIcert.pem]\n\r\nDue to the wrong OMI_IP is entered , command was fetching null data of the file -mcsOMIcert.pem;O KB\n\r\n-After executed the correct command with OMI IP, issue is resolved.','null'),(272,'Yunus Ozturk','AS-OAM','2016-11-19','161119-607339','Paltel','Problem Description:\r\n====================\n\r\nUpgrade Wizard Step 47 - Preparing Upgrade Report failed with with an unexpected error. \n\r\nActions Taken:\r\n==============\n\r\n- When Upgrade Wizard tool is initiated on a Local PC, it should be completed on the same local PC until the end of upgrade process. If it is Saved & Exited in the middle of the upgrade process and the Upgrade Wizard tool is opened on another Local PC which is what we have done during this upgrade, the tool cannot retrieve the upgrade report properly at the end of the upgrade process as the upgrade wizard logs are stored on the local PC and if the tool is closed on that Local PC and opened on another local PC, the local stored upgrade wizard logs are corrupted and it cannot generate the summary at the end. This is Design Intend.','null'),(273,'Yunus Ozturk','AS-OAM','2016-11-19','161119-607338','Paltel','Problem Description:\r\n====================\n\r\nUpgrade Wizard Step 44 - Post Upgrade Installations failed for PA1Server1 for Online Help File and ASU Updates.\n\r\nActions Taken:\r\n==============\n\r\n- We have re-tried the same screen for a few times on the wizard and passed the screen.\r\n- Will check the RCA and if ASU files have been updated successfully.','null'),(274,'Yunus Ozturk','AS-OAM','2016-11-19','161119-607337','Paltel','Problem Description:\r\n====================\n\r\nUpgrade Wizard Step 33 - Upgrading Primary Network Element Instances failed for all instances immediately. Wizard stopped all the instances successfully but the status of all the instances switched to \"Failed\" for some reason.\n\r\nActions Taken:\r\n==============\n\r\n- Re-tried the same screen but nothing changed. The screen failed again.\r\n- Saved & Exited the wizard and re-launched it again. This time step completed successfully.\r\n- Logs will be investigated for RCA.','null'),(275,'Yunus Ozturk','AS-OAM','2016-11-19','161119-607335','Paltel','Problem Description:\r\n====================\n\r\nUpgrade Wizard Step 33 - Patching Operating Systems of the Secondary Host Servers failed for Host1Server2. It showing 100% completed but status failed.\n\r\nActions Taken:\r\n==============\n\r\n- Checked the referenced logs, noticed that the Operating System of the server was successfully completed in the background. \r\n- So that, we have re-tried the same screen on the wizard and passed the screen.','null'),(276,'Yunus Ozturk','AS-OAM','2016-11-18','161118-607225','Tbaytel','Problem Description:\r\n======================\n\r\nUpgrading SM/DB Upgrade Screen failed at Step 26\n\r\nActions Taken:\r\n================\n\r\n- As per the referenced logs, we noticed that SM_0 instance could not communicate to DB Server for some reason it caused the SM Upgrade to be failed. \n\r\nUpdating SM instance(s) load data to MCP_17.0.31.0_2016-10-17-0641 in the database.\r\n/opt/mcp/java/MCP_17.0/jre/bin/java -Doracle.jdbc.timezoneAsRegion=false -classpath /var/mcp/run/MCP_17.0/SM_0/jars/delta_mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/3rdParty.jar:/var/mcp/run/MCP_17.0/SM_0/jars/jhall.jar -DpropFile=/var/mcp/run/MCP_17.0/SM_0/data/neprops.txt com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData -u SM 0 MCP_17.0.31.0_2016-10-17-0641 ONLINE  > /var/mcp/run/MCP_17.0/SM_0/work/SMDataUpdateStdOut 2> /var/mcp/run/MCP_17.0/SM_0/work/SMDataUpdateStdErr\r\n	Failed to update the SM instance loads.\r\ncom.nortelnetworks.mcp.ne.base.db.fw.DBConnectionPoolException: No boot db instance available\r\n	at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.createBootDBInstance(DBConnectionManagerBase.java:320)\r\n	at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.bootstrap(DBConnectionManagerBase.java:216)\r\n	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initDBLayer(InstallationUtilitiesBase.java:158)\r\n	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initPersistenceFramework(InstallationUtilitiesBase.java:109)\r\n	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.initialize(UpdateSMData.java:295)\r\n	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.executeUpdate(UpdateSMData.java:390)\r\n	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.execute(UpdateSMData.java:459)\r\n	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.main(UpdateSMData.java:485)\n\r\nSM upgrade unsuccessful. Failed to update the SM instance loads.com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionPoolException: No boot db instance available	at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.createBootDBInstance(DBConnectionManagerBase.java:320)	at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.bootstrap(DBConnectionManagerBase.java:216)	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initDBLayer(InstallationUtilitiesBase.java:158)	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initPersistenceFramework(InstallationUtilitiesBase.java:109)	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.initialize(UpdateSMData.java:295)	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.executeUpdate(UpdateSMData.java:390)	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.execute(UpdateSMData.java:459)	at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.main(UpdateSMData.java:485)\n\r\n- Appears that we have a possible bug here.\r\n- Accessed the MCP GUI and checked the status of SM_0 instance. We noticed that the SM_0 instance was deployed and started with the new release but wizard failed on this screen for some reason.\r\n- To be on the safe side, we have killed/un-deployed the upgraded SM_0 instance on MCP GUI and deployed/started back with the older release and have the wizard to try the same upgrade screen again and it successfully upgraded the SM_0 instance','null'),(277,'Yunus Ozturk','AS-OAM','2016-11-19','161118-607334','Paltel','Problem Description:\r\n======================\n\r\nUpgrading SM/DB Upgrade Screen failed at Step 26\n\r\nActions Taken:\r\n================\n\r\n- As per the referenced logs, we noticed that SM_0 instance could not communicate to DB Server for some reason it caused the SM Upgrade to be failed. \n\r\nUpgrading Engineering parameters to MCP_17.0.22.20_2016-05-30-1352 in the database.\r\nCommand = /opt/mcp/java/MCP_17.0/jre/bin/java -Doracle.jdbc.timezoneAsRegion=false -classpath /var/mcp/run/MCP_17.0/SM_0/jars/delta_mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/3rdParty.jar:/var/mcp/run/MCP_17.0/SM_0/jars/jhall.jar -DpropFile=/var/mcp/run/MCP_17.0/SM_0/data/neprops.txt com.nortelnetworks.mcp.ne.sm.base.dbio.fw.SMUpgrade MCP_17.0.22.20_2016-05-30-1352 > /var/mcp/run/MCP_17.0/SM_0/work/SMUpgradeOut_NpHL\r\nUpgrade Config Parms was successful\n\r\nUnlinked /var/mcp/run/MCP_17.0/SM_0/work/SMUpgradeOut_NpHL\n\r\nUpdating SM instance(s) load data to MCP_17.0.22.20_2016-05-30-1352 in the database.\r\n/opt/mcp/java/MCP_17.0/jre/bin/java -Doracle.jdbc.timezoneAsRegion=false -classpath /var/mcp/run/MCP_17.0/SM_0/jars/delta_mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/mcp.jar:/var/mcp/run/MCP_17.0/SM_0/jars/3rdParty.jar:/var/mcp/run/MCP_17.0/SM_0/jars/jhall.jar -DpropFile=/var/mcp/run/MCP_17.0/SM_0/data/neprops.txt com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData -u SM 0 MCP_17.0.22.20_2016-05-30-1352 ONLINE  > /var/mcp/run/MCP_17.0/SM_0/work/SMDataUpdateStdOut 2> /var/mcp/run/MCP_17.0/SM_0/work/SMDataUpdateStdErr\n\r\n        Failed to update the SM instance loads.\r\ncom.nortelnetworks.mcp.ne.base.db.fw.DBConnectionPoolException: No boot db instance available\r\n        at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.createBootDBInstance(DBConnectionManagerBase.java:320)\r\n        at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.bootstrap(DBConnectionManagerBase.java:216)\r\n        at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initDBLayer(InstallationUtilitiesBase.java:158)\r\n        at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initPersistenceFramework(InstallationUtilitiesBase.java:109)\r\n        at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.initialize(UpdateSMData.java:295)\r\n        at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.executeUpdate(UpdateSMData.java:390)\r\n        at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.execute(UpdateSMData.java:459)\r\n        at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.main(UpdateSMData.java:485)\n\r\nSM upgrade unsuccessful. Failed to update the SM instance loads.com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionPoolException: No boot db instance available   at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.createBootDBInstance(DBConnectionManagerBase.java:320)  at com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionManagerBase.bootstrap(DBConnectionManagerBase.java:216)     at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initDBLayer(InstallationUtilitiesBase.java:158)  at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.InstallationUtilitiesBase.initPersistenceFramework(InstallationUtilitiesBase.java:109)     at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.initialize(UpdateSMData.java:295)     at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.executeUpdate(UpdateSMData.java:390) at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.execute(UpdateSMData.java:459) at com.nortelnetworks.mcp.ne.sm.base.dbio.fw.UpdateSMData.main(UpdateSMData.java:485)\n\r\n- Appears that we have a possible bug here.\r\n- Accessed the MCP GUI and checked the status of SM_0 instance. We noticed that the SM_0 instance was deployed with the new release but SM_0 NE was showing older release. So, the deployment was unsuccessful. \r\n- We have killed/un-deployed the upgraded SM_0 instance on MCP GUI and deployed/started back with the older release and have the wizard to try the same upgrade screen again and it successfully upgraded the SM_0 instance','null'),(278,'Yunus Ozturk','AS-OAM','2016-11-18','161118-607328','Paltel','Problem Description:\r\n====================\n\r\nOracle Patch/Migration step failed with the following error;\n\r\n2016-11-18 19:08:14,529 DEBUG ScriptCommonManagerImpl - Monitor script response \r\n: \r\nScript Name:\'ut_oraclePatch.pl\' \r\nServer Name:\'EMS1\' \r\nResult: Last Modified Time:\'Fri Nov 18 19:08:08 GMT 2016\' \r\nState:\'Failed\' \r\nCurrent Total Steps:\'5\' \r\nStart Time:\'Fri Nov 18 18:53:36 GMT 2016\' \r\nStop Time:\'Fri Nov 18 19:08:08 GMT 2016\' \r\nDescription:\'Determining Which Operation To Perform... -Oracle Migration Operation In Progress- -Steps: Backup DB->Uninstall Old Oracle->Install New Oracle->Restore DB- Taking backup of the database in server 82.213.24.154 ... Backup database of server 82.213.24.154 is completed. Uninstalling oracle from the server.. Oracle uninstalled from 82.213.24.154 Installing new Oracle to the server. This operation can take some time.Please wait... Error executing Oracle install commands. See log file for possible details: /var/mcp/upgrade_tools/logs/oraclePatch_logs/ut_oraclePatch.EMS1_38b0af1a-2b46-1b21-9174-00e0ed21a624_ut_oraclePatch.pl_PATCH_PRI_ORACLES_0.20161118_205335.log \' \n\r\nError:\'Operation Error\' \n\r\n2016-11-18 19:08:14,530 ERROR PanelController - Operation had been failed : 2016-11-18 19:08:14 \r\nEMS1 \r\n: \r\nDetermining Which Operation To Perform... \r\n-Oracle Migration Operation In Progress- \r\n-Steps: Backup DB->Uninstall Old Oracle->Install New Oracle->Restore DB- \r\nTaking backup of the database in server 82.213.24.154 ... \r\nBackup database of server 82.213.24.154 is completed. \r\nUninstalling oracle from the server.. \r\nOracle uninstalled from 82.213.24.154 \r\nInstalling new Oracle to the server. This operation can take some time.Please wait... \r\nError executing Oracle install commands. \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/oraclePatch_logs/ut_oraclePatch.EMS1_38b0af1a-2b46-1b21-9174-00e0ed21a624_ut_oraclePatch.pl_PATCH_PRI_ORACLES_0.20161118_205335.log \n\r\nOperation Error \n\r\nActions Taken:\r\n===============\n\r\n- Based on the showversion.pl output, the oracle patch/migration did seem to be performed successfully as it was showing the new version of the oracle.\r\n- We have tried to Save&Exit the wizard and re-launch it to see the results but it did not come up due to loss of Primary DB communication. \r\n- Appears that we have a bug on the showversion.pl script. It should not show the new version of the oracle even if the oracle patching/migration is not completed successfully. We need to find out how/why it has generated the new version of the oracle. \r\n- When we have checked the referenced logs of the wizard, it was showing that oracle patch/migration failed due to the existence of /var/mcp/mcpdb/ directory. This directory should have been removed on the previous Upgrade/Rollback attempt on this site. Seems like something went wrong on the previous Rollback process and this mcpdb directory was not removed and it prevented the \"oraclePatch.pl\" script not to work properly and took the Primary DB down. \r\n- Appears that we have a possible bug on this script as it should overwrite or remove the /var/mcp/mcpdb/ directory during the upgrade.\r\n- We have removed this directory and tried to run \"oraclePatch.pl\" script manually with the required parameters but it failed again. \r\n- After further investigations, we noticed that the monitor state file parameter that we have used with the oraclePatch.pl script was preventing the script to work properly as the monitor state file was showing the newer version of oracle and it was at \"Failed\" status.\r\n- We have engaged the Design Team and have them to modify the oraclePatch.pl script temporarily to pass the monitor state file check. \r\n- After disabling the monitor state file check of the oraclePatch.pl script, we were able to run this script manually and recover the Primary DB and completed Oracle Patch/Migration step. \r\n- Appears that we need an improvement on the oraclePatch.pl script and we might have a parameter option to run this script with skipping the monitor state file parameter.','null'),(279,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-11-21','161121-607381','Alphawest Services P/L (Carr)','Problem Description:\r\n=====================\n\r\nDavid Giomi paged me to report that they are preparing for a A2 HW migration to IARMS and running with IM 74-3160 and on the prep migration steps(step 22&23)they cannot get boot prompt on Virtual machine manager.\n\r\nDavid Giomi is performing this action because SIF did not complete step 22&23\n\r\nActions taken: \r\n===============\n\r\nSince this is the Prep steps of migration, I did not give page support and asked to investigate this case during business hours.\r\nI also recommended that to increase the priority of this case to Business Critical to take daily update.\n\r\nThen dropped the call.','null'),(280,'Yunus Ozturk','AS-OAM','2016-11-20','161120-607353','BT TELECOMUNICACIONES S A','Problem Description:\r\n=====================\n\r\nSecond MW of the BT Spain migration from HP3310 to IA-RMS servers. The MW has been completed but some problematic scenario has been found during customer testing related with SM redundancy. \n\r\nReboot of active system manager doesn\'t swact the MCP to the other SM. Now primary SM is active, when SM is rebooted, secondary SM doesn\'t take the System Manager ownership and when the primary is rebooted this server is still active in MCP Console. \n\r\nIf the active SM (this case primary) is shutdown. Secondary SM never takes the ownership and so the MCP console and OAM management is lost. \n\r\nActions taken: \r\n===============\n\r\n- virtual machine reboot or console reboot has the same effect \r\n- both system manager primary and secondary have been undeployed, deployed and started again from scratch and this didn\'t solve the issue. \r\n- service package is downloaded from the swacted SM but the MCP doesn\'t take service. \r\n- The service IP is not hosted in the swacted SM. The problem seems to be that the secondary SM takes the service IP for a while but then releases it. \r\n- Sometimes when the active SM is rebooted the service IP is jumping from one SM to the other, until the primary takes the ownership again.\n\r\nSince this issue was not a outage/pager issue, and it is being reproduced only after rebooting the Primary EM Server, we have asked the customer to raise a case and we will be working on it during business hours.','null'),(281,'Yunus Ozturk','AS-OAM','2016-11-18','161118-607292','Alphawest Services P/L (Carr)','Problem Description:\r\n=====================\n\r\nCustomer was performing Re-IP of SM using IM 85-3296 procedure 3 step 14 and they failed this step with the following error. \n\r\nIs the information correct? (y or n) : y \r\nRegistering CMCLLI to A2 ... \r\nRegistering CLLI to A2 ... \r\nCallServerOmiIf::loginOmi enter. \r\nCallServerOmiIf::loginOmi - got response \r\nresponse = 11 \r\nCallServerOmiIf::getCallServerByCLLI enter. \r\nERROR: The following exception occured during configuration of OMI! \r\ncom.nortelnetworks.mcp.ne.sm.base.svc.event.RequestException: Invalid session \n\r\nCallServerOmiIf::addCallServerByCLLI enter. \r\nERROR: The following exception occured during configuration of OMI! \r\ncom.nortelnetworks.mcp.ne.sm.base.svc.event.RequestException: Invalid session \n\r\nRegister CLLI to A2 failed.\n\r\nSolution:\r\n==========\n\r\nSince this issue was not pager call and it was related to GVM side, we have rejected this pager call and asked the customer to contact GVM GPS if needed.','null'),(282,'Senem Gultekin','AS-OAM','2016-11-16','161111-606239','Tbaytel','Problem Description:\n\r\nWe were paged by customer for a BCP server not coming up problem.\r\nI was already working on the case, which is related to the pre upgrade steps. BCP2 was failing because there was no connection to server.\r\nIve provided reseat and replacement actions, however they have reported that BCP2 was still down.\n\r\nCustomer Release: 17.0.12.20\r\nBCP Platform Level: 17.0.10\n\r\nSolution:\r\n-	Accessed to the site and tried to check BCP2 from remote console connection, however I was not able to reach it.\r\n-	Restarted Management Module.\r\n-	We were able to see the server starting, but since there was no operating system running on it, suggested them to put 17.0.10 installer CD.\r\n-	Customer couldnt find the CD on site, and they didnt know where ESD files were located.\r\n-	Customer wanted wait until their day started, so the people come to the office to locate 17.0.10 installer iso file. All instructions and steps provided to the customer.\r\n-	Later that day, installation has been done with GPS assistance.\r\n-	Afterwards new issues occurred, and they have been recovered with case 161117-607023.','null'),(283,'Yunus Ozturk','AS-OAM','2016-11-17','161111-606239','Tbaytel','Problem Description:\r\n====================\n\r\nAfter re-installing the BCP Server, customer was not able to deploy and start the new BCP instance on MCP GUI. The status of the BCP instance was as follows on MCP GUI;\n\r\nAdmin status was Online, Link status was Down and Oper was Unavailable\n\r\nSolution:\r\n==========\n\r\nWe have accessed the site and performed the Kill - Undeploy - Deploy - Start actions for the corresponding instance and the instance was started up properly.\n\r\nSince E2 situation was recovered, we dropped the call.','null'),(284,'Oktay Esgul','AS-OAM','2016-11-07','161107-605499','Tbaytel','Greg Trimeloni paged me in order to report secondary oracle step failure  during first MW of 3310 to IA-RMS migration period.\n\r\nMCP Release: MCP_17.0.12.20\n\r\nGreg had been following the IM 74-3160 procedure 3 step 10,when he faced with the issue.  \n\r\nI have connected site via teamviewer and taken look at the status. Even though secondary DB server seems installed and up, server could not be reachabled\r\nfrom network.\n\r\nAs there are several network related backup/restore steps during migration, I suspected that some missed steps during MW.So that, as a first action,\r\nI have run below command to restore DBServer2 network backup at secondary host server which DB1Server2 is located.\n\r\nvirsh define /var/mcp/install_tools/work/DB1Server2.xml\n\r\nIt did not help to solve, then checked the network interfaces defined at guest server and realized that there are NIC interfaces which are applicable with bridge\r\nnetwork.  Yet, the IA-RMS host servers are  installed  via VF conf which proved that there is an interface mismatch issue at DB1Server2.\n\r\nThen , to ensure I have double checked the primary DB and verified that it is installed via VF as well.\n\r\nAs recovery action ,\n\r\n1. I have deleted both of the existing NIC interfaces.\r\n2. Run below command to list the available PCI devices for  eth0 and eth2 which are used for OAM network.\n\r\n[root@vkcs2khost05 bin]# ls -ld /sys/class/net/eth0/device/virtfn* |sort |cut -f 2 -d \'>\' | cut -f 2 -d \'/\' |tr :. __\r\n0000_81_10_0\r\n0000_81_10_4\r\n0000_81_11_0\r\n0000_81_11_4\r\n0000_81_12_0\r\n0000_81_12_4\r\n0000_81_13_0\r\n[root@vkcs2khost05 bin]# ls -ld /sys/class/net/eth2/device/virtfn* |sort |cut -f 2 -d \'>\' | cut -f 2 -d \'/\' |tr :. __\r\n0000_81_10_2\r\n0000_81_10_6\r\n0000_81_11_2\r\n0000_81_11_6\r\n0000_81_12_2\r\n0000_81_12_6\r\n0000_81_13_2\r\n[root@vkcs2khost05 bin]#\n\r\n3. Then, assinged 0000_81_12_0 + 0000_81_10_6 PCI interfaces to DB1Server2.\r\n4.Then , poweroff and powerup the guest server.\n\r\nOnce the above actions completed, server became reachable.Then , I have handed over to Greg to continue with rest of migration steps .\n\r\nThank you','null'),(285,'Yunus Ozturk','AS-OAM','2016-10-22','161021-603365','BT TELECOMUNICACIONES S A','Problem Description:\r\n=====================\n\r\nUpgrade Path : FROM: 17.0.7.13 TO: 17.0.22.20 \n\r\nStep 27 : Upgrading primary network element instances \n\r\nUpgrade operation failed on Network Element Instance:  \r\nValidating Network Element Instance : Network Element Instance Administrative state should be Online. Network Element Instance Operational state should be Active. \r\nPlease wait until validation is finished. If operation has failed, please contact with next level of support. \n\r\nSolution:\r\n==========\n\r\n- Saved & Exited the Upgrade Wizard\r\n- Tried to re-launch the Upgrade Wizard. At that time, we got a warning and it was saying that SM_0 instance should be Active instead of SM_1\r\n- Connected both EM Servers and stopped the Active SM_1 instance manually with ./neStop.pl to switch the activity to the Hot Standby SM_0 instance. \r\n- Launched the MCP GUI and manually started the SM_1 instance.\r\n- Re-launched the Upgrade Wizard and Step 27 successfully passed','null'),(286,'Yunus Ozturk','AS-OAM','2016-10-20','161018-602646','Paltel','Problem Description:\r\n======================\n\r\nUpgrade Path : From 17.0.7.13 to 17.0.22.20 \n\r\nAs per Bills analysis during pre-ugprade steps, it seems that missing installconfig.xml file is preventing mount operation at Step 5 of the wizard.\n\r\nError code 1! \r\nERR: /var/mcp/install_tools/data/installconfig.xml does not exist  \n\r\nSolution:\r\n============\n\r\nSince this was a pre-upgrade step problem, we rejected to investigate this issue on pager call.\n\r\nThe day after, when we have discussed this issue internally, we noticed that this site has been migrated before and this was the first customer upgrade with CD/DVD device after this migration process. As a part of this migration document, the following manual steps should be performed at the end of this migration process; \n\r\nVirtual guest servers are created manually during migration, in order to avoid DVD mount problem prior next upgrade, we need to create installconfig.xml file manually.\n\r\n Login to primary host server as ntsysadm\r\n su  root\r\n cd /var/mcp/install_tools/data directory\n\r\nCreate installconfig.xml file\n\r\n touch installconfig.xml\n\r\nChange permission and owner\n\r\n chmod 775 installconfig.xml\r\n chown ntappsw:ntappadm installconfig.xml\r\n vi installconfig.xml\r\n Copy/paste below section (modify it with your primary EM1Server1 IP)\n\r\nInsert mode on VI\n\n\n\n\n\nEM1Server1\n\n\n\nEMServer1 IP ADDRESS\n\n\n\nVirtualized-Guest\n\n\n\n\n\r\n Save and exit (Press ESC then type :wq!)\n\r\nWe were suspicious that this step was missed at the time of the migration and it was causing the problem that we were having.\n\r\nThis installconfig.xml file under /var/mcp/install_tools/data/ directory should be created manually and the above section should be added with IP Address of EMServer1 IP address on the Physical Host Server itself where the EMServer1 is running on. \n\r\nAfter applying the steps above, the issue is resolved and GPS completed the pre-upgrade steps during business hours as per customer\'s request.','null'),(287,'Yunus Ozturk','AS-OAM','2016-10-18','161017-602636','TurkNet','Problem Description:\n\r\nCustomer (TurkNet) contacted ER reporting that step 40 procedure 7 of Retro cap 44-3591 was failing and the blade lost the network connection after replacing the front card of the ATCA blade.  \n\r\nCustomer was unable to ping and ssh to the new card in slot 5 after replacing the front card of the ATCA blade. Genius GPS was engaged first and after reviewing the status, Genius GPS noted that A2 GPS needed to be engaged since this was an open slot TYPE-1 card and the Genius platform only supplies power, and network. Genius GPS verified that part was ok. \n\r\nSolution:\n\r\nAs per our investigation, we found out that wrong MAC address mappings were observed after rebooting the server. The expected interfaces should be expected as eth6 / eth7 and their MAC addresses. However, we have seen that eth16 / eth18 interfaces and their MAC addresses have been mapped to the bond0. So that, there were no incoming/outgoing packets through bond0 and we were not able to see the eth6/eth7 interfaces on ifconfig output. \n\r\nWe have removed the \"70-persistent-net.rules\" file under /etc/udev/rules.d/ directory and then rebooted the server to fix this issue.\n\r\nIssue is found to be the same issue reported with AAK-36683. For the releases older than 18.0.3 ple4 (which are not supported), \"70-persistent-net.rules\" file should be deleted before replacing the front card of ATCA blades. Customer was running on 17.0.18 ple4. \n\r\nFor the releases 18.0.3 and above, net rules generator is already disabled and front card replacement is not causing any problem regarding network access availability.\n\r\nInformed the customer that they should remove the \"70-persistent-net.rules\" file under /etc/udev/rules.d/ directory before replacing the front card of the next ATCA blade.','null'),(288,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-10-15','161015-602376','Kandy JP','Problem Description :\r\n=====================\n\r\nI have been paged by Martha Foster to report that they have seen 6 red API alarm(subscriber/SPIDR subscriber) and test calls are not completing.\r\nWhen I involved to the outage, they have solved the registration problem(E1 to BC) but due to known pending expiry devices problem, subscriber limit is over.\n\r\nThe SPIDR was running with 9.1.2.ap68(4.1.2) load.\n\r\nActions Taken :\r\n==============\n\r\nI have connected to the site and applied the below procedure on the both app tiers in order to prevent the registration issue in future.\n\n\r\n1. Change the values in the /etc/wae/config file as below.\n\r\n  if [ -z \"$SESSION_AUDIT_DEVICE_LIMIT\" ]; then         SESSION_AUDIT_DEVICE_LIMIT=50   else     is_integer \"$SESSION_AUDIT_DEVICE_LIMIT\" || log_fatal \"Invalid SESSION_AUDIT_DEVICE_LIMIT: \'$SESSION_AUDIT_DEVICE_LIMIT\'. It must be integer.\"     if [ \"$SESSION_AUDIT_DEVICE_LIMIT\" -lt  1  ] || [ \"$SESSION_AUDIT_DEVICE_LIMIT\" -gt  100000  ]; then         log_fatal \"Invalid SESSION_AUDIT_DEVICE_LIMIT:$SESSION_AUDIT_DEVICE_LIMIT. It should be min  1  and max  100000 \"     fi   fi    if [ -z \"$SESSION_AUDIT_INTERVAL\" ]; then         SESSION_AUDIT_INTERVAL=10   else     is_integer \"$SESSION_AUDIT_INTERVAL\" || log_fatal \"Invalid SESSION_AUDIT_INTERVAL: \'$SESSION_AUDIT_INTERVAL\'. It must be integer.\"     if [ \"$SESSION_AUDIT_INTERVAL\" -lt  1  ] || [ \"$SESSION_AUDIT_INTERVAL\" -gt  6000  ]; then         log_fatal \"Invalid SESSION_AUDIT_INTERVAL:$SESSION_AUDIT_INTERVAL. It should be min  1  and max  6000 \"     fi   fi \n\r\n2. Add the below two lines to the /etc/wae/wae.conf file:\n\r\nSESSION_AUDIT_DEVICE_LIMIT=150\r\nSESSION_AUDIT_INTERVAL=3\n\r\n3.Change the device limit count under 10 at ADMIN GUI for Restful Client service.\n\r\n4. Restart the app tiers.\n\r\nAfter restarted the app tiers, test calls are passed successfully and registrations were successull.\n\r\nThen we all agreed to drop the call.\n\r\nNote 1: Corrective fix of this issue has been delivered to the future releases of SPIDR.\n\r\nNote 2:Above workaround procedure has been applied to all Kandy sites before except this one.','null'),(289,'Oktay Esgul','AS-OAM','2016-10-14','160926-599542 ','Shaw','John Shamer paged me out in order to report that a disk issue at customer . He informed that while customer was replacing faulty, by mistake they replaced the working\r\none so that new disk synced from the faulty one. As a results of this, below error observed at terminal of the server.\n\r\n                   cpd2sem1no login: raid1: sda: unrecoverable I/O read error for block 77184\r\n                   raid1: sda: unrecoverable I/O read error for block 78720\r\n                   raid1: sda: unrecoverable I/O read error for block 80128\r\n                   raid1: sda: unrecoverable I/O read error for block 16768\r\n                   raid1: sda: unrecoverable I/O read error for block 77184\r\n                   raid1: sda: unrecoverable I/O read error for block 78720\r\n                   raid1: sda: unrecoverable I/O read error for block 80128\n\r\nAs  SM/DB are running on this faulty server, we focused on the recovery to avoid any reinstallation from scratched.\n\r\nThen , we recommended customer to power off the server and take out   both of the disk.Then locate the working one first and then the new disk. \r\nServer booted up successfully then we added new spare disk to raid.\n\r\nOnce the server disk synched and application started succesfully, we dropped.','null'),(290,'Oktay Esgul','AS-OAM','2016-10-12','160914-597842','BT MSL','Mark paged me out to report that Atca Blade installation failure. They were working on recovery case for SESM4_0 which is not live (frendly traffic running on this blade).\n\r\nI have connected and checked NDM logs and everything seems correct and installation seems completed as ndm reboots the blade via successfully installed messages.\n\r\nYet, even though installation successfull, blade was stuck at boot screen via below error:\n\r\n###################################\r\n Intel(R) Boot Agent PXE Base Code (PXE-2.1 build 086)                           \r\n                   Copyright (C) 1997-2007, Intel Corporation                                      \n\r\n                   CLIENT MAC ADDR: 00 D0 1C 07 9D 89  GUID: FFFFFFFF FFFF FFFF FFFF FFFFFFFFFFFF  \r\n                   CLIENT IP: 172.31.0.103  MASK: 255.255.255.0  DHCP IP: 172.31.0.58              \r\n                   TFTP.                                                                           \r\n                   PXE-T01: File not found                                                         \r\n                   PXE-E3B: TFTP Error - File Not found                                            \r\n                   PXE-M0F: Exiting Intel Boot Agent.\n\r\n#################################\n\r\nIn order verify that boot order, I have connected BIOS of the blade yet somehow, there was not any disk  present in boot list which we can set as a primary boot option which makes us to suspect a disk issue on the blade.\n\r\nIn the  meantime, since the MW time completed, customer force us to to stop working and any operation requests was rejected.\n\r\nMark will dispatch the case to our queue for further investigation and we ll provide and action plan to customer for next MW.\n\r\nThank you','null'),(291,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-10-10','161009-601522','Cleveland Clinic','Problem Description :\r\n=====================\r\nI have been paged by ER about the java applications(SM0,PROV10,SESM10) on the EMServer1 can not be active state.\n\r\nSite was running on 14.1.10.3 A2 load.\r\nServers were running on 14.1.15 ple2 platform load (HT Langley)\n\r\nActions Taken :\r\n==============\n\r\n1) Connected to the VM and checked the error while SM is initializing.\r\nWork log is indicating that interface information parsing is failed.\r\n Caused by: java.io.IOException: Failed to parse interface information string.\n\r\nExecuted getIfInfo.pl manually on the server and taken the following exception below;\n\r\n[ntappadm@CC-J-B-EMS1 bin]$ /opt/mcp/nif/bin/getIfInfo.pl\n\r\n                   Use of uninitialized value in split at /opt/mcp/nif/bin/../lib/NifUtils/Linux.pm\n\r\n                   line 878.\n\r\n2) Executed mcpRelease.pl command on the EMS1 and seen that Hardware Env is returning null.Then checked the .hardware_environment file on the /admin directory and all the files are missing on EMS1.\n\r\n3)Tried to mount the /admin directory which is locating on /dev/md0 with fsck -fv /dev/md0 command but this action pulled the files on the /admin directory without the true data inside them.\n\r\n4)Checked the patch logs and verified that previous platform image on the server is 14.1.13 . Executed patchPlatform -rs to restore with 14.1.13.After restore is successful ,java app. were able to become online/UP/active state.\n\r\n5)Given a procedure to the ER to inform the customer about 14.1.15 platform patches should be applied on EMS1.\n\r\n6)Aggreed on dropped the pager call since the A2 system is stabile.\n\r\nNote : Customer will open a seperate case for follow up the platform patch issue.\r\nNote 2: As far as customer said to the ER, there is nothing performed manually to delete the files on /admin directory.\r\nSince this is EOL load, we will not give RCA for this issue.','null'),(292,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-10-08','161008-601490','SaskTel','Problem Description :\r\n=====================\r\nI have been paged by ER for EM1 Server unresponsive issue.\r\nInvestigated the logs on the case and seen disk error on the sda.\n\r\nraid1: Disk failure on sda9, disabling device.\r\nraid1: Disk failure on sda5, disabling device.\r\nraid1: Disk failure on sda10, disabling device.\r\nraid1: Disk failure on sda6, disabling device.\r\nraid1: Disk failure on sda11, disabling device.\r\nraid1: Disk failure on sda1, disabling device.\r\nraid1: Disk failure on sda8, disabling device.\r\nraid1: Disk failure on sda12, disabling device.\r\nRemounting filesystem read-only\r\nEXT3-fs error (device md10): ext3_journal_start_sb: Detected aborted journal\r\nEXT3-fs error (device md10): ext3_get_inode_loc: unable to read inode block - inode=3\n\r\nNote: md10 is holding the data for /var/mcp partition.\r\nNote: Site was working on A2 18.0.26.1 load.\n\r\nActions Taken :\r\n==============\r\nGiven an action about to change sda disk on the HT Langley.\r\nCustomer has changed the disks and I have started the sync process between disks.\r\nAfter completed successfully the operation, server became unresponsive again.\n\r\nThere is nothing we can do on this case other than the server replacement.\r\nCustomer has pulled off the disks from old one,changed the server and completed restore process.\r\nThen, Ken Johnson has created license key for the LKEY754 alarm which is LKEY_HARDWARE_MISMATCH_ERROR_CRITICAL_754\n\r\nCustomer will apply the license key to the A2 system in the mtce window.\r\nSince we have seen that the EM1 server became stabile, dropped the call.','null'),(293,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-10-08','161008-601466','BT Spain','Problem Description :\r\n=====================\r\nNagore has paged me about the license key mismatch issue during the half-migration of their system.\r\nThey were migrating their servers from HP CC3310 to IA-RMS.\r\nThe error message on the MCP GUI was indicating that license key does not match with the hardware.\n\r\nActions Taken :\r\n==============\r\nI have decrypted the half-migrated license key if the given MAC addresses by Nagore(old legacy system MAC ,New host1 MAC) matches with the license key file.Confirmed that MAC addresses were matched and during this time I have informed Bill about the issue.\r\nHe specified that we should only see bond0 and bond1 links on each IA-RMS host.\r\nBut we were seeing vnet interfaces on the host1.Also he indicated that hosts may be using NAT and Bridge style interfaces with the guest virtual machines, which may alter the results of getMAC.He wanted to restart the host and execute \r\nthe getMAC.sh script again.\r\nI have checked that IA-RMS was configured with VF network type.\n\r\nbond0     Link encap:Ethernet  HWaddr 00:E0:ED:2E:52:EA\r\nbond1     Link encap:Ethernet  HWaddr 00:E0:ED:2E:52:EB\r\nvirbr0    Link encap:Ethernet  HWaddr 52:54:00:A0:2C:73\r\nvnet2     Link encap:Ethernet  HWaddr FE:54:00:07:F1:01\r\nvnet3     Link encap:Ethernet  HWaddr FE:54:00:E0:6A:88\n\r\nAfter restarted the IA-RMS host,the results were the same and this action did not work.We have taken the same error when applying the license key.\n\r\nNagore has collected the hardware logs and then started the rollback process and\r\ncompleted it successfully.\r\nWe do not know the root cause of this issue yet but will update the RCA case as soon as we determined.','null'),(294,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-10-05','161005-601000','Unitymedia','I have been paged by CallP GPS for the synchronization issue between A2 Database and SESM IMDB.\n\r\nI have connected to VM then executed the SQL command to check total count of REGDEST table on the DB side and also to check the total registered user count on SESM IMDB and verified the mismatch.\n\r\nIn order to start sync correctly, deleted the data on the REGDEST table and stopped/started each active and hot standby SESM instance on A2 system but this action did not solve the issue.\n\r\nAfter monitoring site a while, we have seen imdb sync alarms and db overload alarm on the SESM instances.\r\nTo clear the alarms, I have applied the below mtce action on the site.\r\n-stopped/started oracle on the DB0 and DB1 server one by one\r\n-then swacted the SESM instances.\n\r\nThis action has cleared the alarms and we have continued with registration issue.\n\r\nWe have suggested 2 option to the customer not to wait expiry time of registrations.\r\n1)Wait for the next audit between DB and SESM IMDB.\r\n2)Stop all the SESM instances,clear the regdest and start them simultaneously to prevent existing registrations.\n\r\nCustomer has come up with an idea which involves logout of all users on the regdest table.\r\nWe have accepted this action.After the customer logout all users via bulk prov tool issue has been solved.','null'),(295,'Burak Biyik','AS-OAM','2016-09-28','160928-599901','Global Village Telecom (GVT)','SWD reported that \"Step 27: Switching Service to Secondary SM Instances\" failed due to SM in wrong state during AS Major upgrade (14.1.15.0--> 17.0.22.20)\n\r\nI connected to the system and verified that the service was already switched to the secondary System Manager as shown in MCP GUI (SM1 was ONLINE-UP-ACTIVE and SM0 ONLINE-UP-HOT STANDBY)\n\r\nEven though they were in correct states, wizard were failing to verify their state so that the screen was showing \"in progress\" state all the time.\n\r\nWe had to double swact the SM instances and then re-launch the wizard to pass the screen.\n\r\nA jira will be opened and linked to the case in order to investigate wrong wizard behavior.\n\r\nAgreed and dropped the call.','null'),(296,'Oktay Esgul','AS-OAM','2016-09-23','160923-599285','Sasktel','Robert Johnson from ER paged me to report upgrade wizards  failure at  Secondary Server platform failure for Sasktel.\n\r\nUpgrade Path:\n\r\nFrom : 14.1.10.3\r\nTo: 18.0.26.4\n\r\nAfter I completed the primary site upgrade , handed over the wizard to customer and they performed pause point tests. Once they started to secondary site upgrade,\r\nwizard failed at os patching screen . We tried to reach to EMServer2 and SesmServer2 ,yet it failed since servers were not reached after reboot even patch completed succesfully.\n\r\nThen , customer hard rebooted the server to recover which solved the issue for EM2 ,yet SESM2 was not recovered. Then, customer unplug/plug the power cables,then booted up the server which recovered the problem.\n\r\nIn order to be safe side, I continued upgrade and completed the rest part myself.\n\r\nWe can say that upgrade performed by GPS instead of customer!\n\r\nThank you','null'),(297,'Oktay Esgul','AS-OAM','2016-09-23','160923-599285','Sasktel','Kyle Mawst from ER paged me to report upgrade wizards  failure at  Primary Servers OS Patch screen.\n\r\nUpgrade Path:\n\r\nFrom : 14.1.10.3\r\nTo: 18.0.26.4\n\r\nCustomer was performing the upgrade on their own.Wizards failed step 29 with following error.\n\r\n Patching Operation failed, \"OMI Webservice failed\".\n\r\nEr connected site and we took a  look at the problem, even the servers OS were patched succesfully, somehow wizards failed due to OMI connection issue.\r\nRecommended customer to save&exit the wizards then re-launch, yet wizards failed since wrong sm instance active while re-launching.\r\nThen , tried to activate the primary SM, yet it failed since EM1 was not reachable. Since there is no terminal, customer power cycled the server which solved\r\nthe access issue.\n\r\nThen, we re-launch the wizards successfully.\r\nIn order to ensure the wizards complete the primary site upgrade properly, I have continued the upgrade till the pause point where primary site upgrade completed.\r\nAfterward,hand over the wizards to customer to complete the rest of the upgrade.','null'),(298,'Oktay Esgul','AS-OAM','2016-09-07','160907-597013','Kandy','Gerry paged me to report registration failures at Kandy EU. \n\r\nWhile I was connecting site, I have informed that Ken had already involved and figured out that registration failing due to db corruption.\n\r\nThere is an outstanding upgrade activity on this site and  all upgrade files still located at admin servers. Apparently, due to these files disk capacity was overloaded and cause db errors which had triggered registration failures. \n\r\nAfter cleaned up the corresponding upgrade files, system became stable.','null'),(299,'Oktay Esgul','AS-OAM','2016-09-07','160907-596916','BT TELECOMUNICACIONES','David Barlett paged me to report that he has several issue while doing DBMock prior upgrade steps.\n\r\nI let him know that DBMock step do not need pager support since actual upgrade steps are not started yet,then recommended him to raise a new case and dispatch to OAM queue.','null'),(300,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-08-26','160823-594746','Optus','Problem Description :\r\n=====================\n\r\nER has paged me to get help about platform installation on the SESM10 server.\r\nI had assisted him with my action plan yesterday.\n\r\nToday,The OS started loading after entering install-kvm but then stopped and the screen is frozen.\n\r\nSite was working on 8.0 SP1\n\r\nActions Taken :\r\n==============\r\nI have connected to VM23 and investigated the issue.\n\r\nSince we have connected to this blade from external, I have requested site tech attempt the OS install to see if the screen locks up when attempted locally.\n\r\nSite tech has started the OS install and it is progressed normally.\r\nThe OS install is completed and then I have executed deploy&start from MCP GUI on the SESM10 instance.\r\nIt is come to Hot-Standby state and I have checked if they have JARs on the SESM11.\n\r\nInformed to ER about following jars will need to be applied.\r\n14.1.15.3_AAK-43644_19_Jan.jar and 14.1.15.3_AAK-43644_19_Jan.jar \n\r\nThen dropped from call.','null'),(301,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-08-25','160825-595255','ITian Corporation','Problem Description :\r\n=====================\r\nER has reported that secondary database is down at Itian.\r\nI have looked at ER timeline, connected to the site and then started to investigate the issue.\n\r\nCustomer has already rebooted the server where the secondary database is installed and also performed a stop/start on DB instance 2 but this actions did not help to get rid of the issue.\r\nCustomer was working on MCP 9.1 (Oracle 9i)\n\r\nActions Taken : \r\n=====================\r\n1)Manually tried to start database on the server and taken this error below while oracle is initializing;\n\r\nERROR at line 1:\r\nORA-06550: line 1, column 7:\r\nPLS-00201: identifier \'DBMS_SHARED_POOL.KEEP\' must be declared\r\nORA-06550: line 1, column 7:\r\nPL/SQL: Statement ignored\r\nSQL> BEGIN DBMS_SHARED_POOL.KEEP (\'DIANA\', \'P\'); END;\n\r\n2)Investigated the issue on the web(PLS-00201) and found a solution which explains to execute dbmspool.sql to get rid of this situation.\n\r\n3)Tried to execute sql as sysdba user below but my attempts were not successful.\r\na)\r\nSQL> @d:\\oracle\\ora9i\\rdbms\\admin\\dbmspool\r\nSP2-0310: unable to open file \"d:oracleora9irdbmsadmindbmspool.sql\"\r\nb)\r\nSQL> exec @d:\\oracle\\ora9i\\rdbms\\admin\\dbmspool\r\nBEGIN @d:\\oracle\\ora9i\\rdbms\\admin\\dbmspool; END;\n\r\n      *\r\nERROR at line 1:\r\nORA-06550: line 1, column 7:\r\nPLS-00103: Encountered the symbol \"@\" when expecting one of the following:\r\nbegin case declare exit for goto if loop mod null pragma\r\nraise return select update while with \n  <<\r\nclose current delete fetch lock insert open rollback\r\nsavepoint set sql execute commit forall merge\n pipe\r\nc)\r\nSQL> conn sys/password as sysdba\r\nConnected.\r\nSQL> @$ORACLE_HOME/rdbms/admin/dbmspool.sql\r\ncreate or replace package dbms_shared_pool is\r\n*\r\nERROR at line 1:\r\nORA-01109: database not open\n\r\n4)I have remembered that I have seen this issue => ORA-01109: database not open on my previous pager 160613-584936.I have tried to execute alter database open command to see if it works and finally it showed me where the issue is.\n\r\nSQL> alter database open;\r\nalter database open\r\n*\r\nERROR at line 1:\r\nORA-00320: cannot read file header from log 5 of thread 1\r\nORA-00312: online log 5 thread 1: \'/var/mcp/oradata/mcpdb/redo05.log\'\n\r\n5)Checked the directory /var/mcp/oradata/mcpdb/ but redo05.log were missing.\r\n6)Started to recovery procedure like the case 160613-584936 and started the secondary oracle instance successfully.\r\n7)Database alarms on the system is gone after waited a while.\n\r\nThen dropped the bridge.\n\r\nRCA: we are not giving RCA to this issue since this release is EOL.','null'),(302,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-08-25','160823-594746','Optus','Problem Description :\r\n=====================\n\r\nKyle from ER has paged me to get help about platform installation on the SESM10 server.\r\nHe informed me about the disks of this server has replaced but he cannot see the platform installation screen.\n\r\nI have connected to the VM23 and checked the issue.\r\nThe server type was HP CC3310 and A2 system was working on AS 8.0 SP1.\n\r\nActions Taken :\r\n==============\n\r\nSince it is a MD hardware, it was hard to find the related procedure.\r\nFinally, I have found and sent the IM procedure to the ER and informed them to use 32 bit OS install.\n\r\nI have also advised to ER about changing the boot priority on this server. (CD/DVD must be at top) \r\nAfter customer inserted the DVD to the server, we cannot get into the BIOS.\r\nCustomer has removed the DVD from the server and then able to enter to the BIOS. Checked the Boot priority and it was correct.\n\r\nWe have performed required procedure for the installation on the BIOS and inserted the CD back but could not see the platform installation screen once more.\r\nAt the time, we have noticed that while the OS is booting, there is an error message appeared as below;\n\r\nPXE-E61: Media test failure, check cable                                        \r\nPXE-M0F: Exiting Intel Boot Agent.    \n\r\nI have recommended to customer to look for a spare server and burn the installation CD once more.\r\nCustomer responded that they will have the spare server 20 hours later beginning from now.\r\nSince the action plan is addressed about the installation, we agreed to leave from bridge.','null'),(303,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-08-25','TBD','Baylor University','Problem Description :\r\n=====================\r\nSWD paged me to report upgrade wizard did not detect the Physical Media (CD/DVD) on Media type selection screen.\n\r\nUpgrade Path : AS 10 to AS 11\n\r\nActions Taken :\r\n==============\n\r\nSince this is a pre-upgrade screen, I have told to the SWD that I cannot give pager support to this issue.\n\r\nRequested to create a new case for this issue along with the log collection and dispatch to the A2 GPS Queue in order to investigate the problem.\r\nDuring that time, recommended to perform upgrade via ESDs until we address the problem on this.\n\r\nThen dropped from call.','null'),(304,'Senem Gultekin','AS-OAM','2016-08-16','160816-593738','SaskTel','Problem Description:\n\r\nER paged A2 OAM GPS upgrade failure at Sasktel. Customer was performing the upgrade.\r\nDuring oracle patch/migration of the secondary server, wizard failed due to disk usage.\r\nDisk Partitions Validation Failed \"\n\r\nUpgrade path: 14.1.10.3 to 18.0.26.2\n\r\nSolution:\n\r\n	Accessed to the site via customer PC, checked the error, and it was requesting at least 40%  free space for /opt. However secondary db had already almost %59 free space.\r\n	Ive checked primary DB server and it had %29 free space. So it seemed like the script was checking primary db server instead of secondary db server.\r\n	After checking the history of this issue, found that its a known issue and already fixed in higher releases. So we will have to submit the same fix into 18.0 and 17.0 latest releases.\r\n	Since this is a bug, I had to continue manually to complete the secondary database patching/migration steps from command line.\r\n        *Uninstall 10g Oracle on secondary Server.\r\n        *Install 11g Oracle on Secondary Server\n\r\n	After the above steps were completed, Ive hit retry on wizard, and the screen passed successfully. Its doing release check before performing oracle patch/migration. Since it was already on 11g, it turned to success.\r\n	Customer continued with the upgrade.\n\r\nNote: Jira is already opened for this bug. It will be fixed in 11.2 with Jira: AAK-46413','null'),(305,'Senem Gultekin','AS-OAM','2016-08-16','160816-593710','SaskTel','Problem Description:\n\r\nER paged A2 OAM GPS upgrade failure at Sasktel. Customer was performing the upgrade.\r\nDuring platform patch of the primary host servers (HOS1S1), script has failed in the background and wizard failed as well.\n\r\nUpgrade path: 14.1.10.3 to 18.0.26.2\n\r\nSolution:\n\r\n	Accessed to the site via customer PC, checked HOS1S1. There was only one MAS guest server running on it.\r\n	Checked the platform patch logs under /var/mcp/os/logs/patch_logs\r\nThere was error due to not enough memory on the Host Server.  I do not have the logs on my PC right now, but ER will attach them to the case late for detailed info.\r\n	Server was up for 323 days, to clear stuck processes Ive rebooted the server.\r\n	However after reboot, server didnt come up, customer had to perform hard reboot and it was up. Server is IA-RMS. Customer said that they had to do hard reboot usually.\r\n	Ive hit retry on wizard for patching operating system of primary host servers screen. Since its a major upgrade, it took some time to complete the screen.\r\n	After screen was complete customer wanted to continue by himself.\r\n	Dropped the call.','null'),(306,'Oktay Esgul','AS-OAM','2016-08-11','160810-592941','TELUS','\"Patching/Migrating Oracle Database on Secondary Database Server\" screen failed during the MR Upgrade (17.0.18.4 -> 17.0.22.20) with the following error: \"Disk Partitions Validation Failed \"\n\r\nLooking at the details of the failure from /var/mcp/upgrade_tools/logs/ directory, available space for /opt directory did not seem to meet validation limit (at least 40% should be free for /opt).\n\r\nHowever, there was already more available space than 40% on secondary DB Server. The validation was apparently running on the primary even though the subject db is the secondary one.\n\r\nThis was found as a known issue fixed in later releases. GPS will contact DS to expedite 10.4 prop-back operation.\n\r\nAs manual steps,\n\r\n-> Uninstall Oracle 10g\r\n-> Install Oracle 11g\n\r\napplied to pass the screen.Yet the first secondary oracle install attempt failed since there are several unrelated files at /var/mcp/db/install directory of secondary steps.\n\r\nSo then, I have cleaned up the corresponding directory, then started the second attempt of reinstallation which completed successfully.\n\r\nOnce the oracle migration completed,I have clicked retry button of Upgrade wizards, and it automatically passed that screen wiht oracle is allready migrated message.\n\r\nAfter that, I handed over the wizards to Tony to complete the rest of the upgrade.\n\r\nHe will page CALLP gps to apply sesm jar file after upgrade completed.\n\r\nThen ,I dropped.','null'),(307,'Oktay Esgul','AS-OAM','2016-08-10','160810-592941','Telus','CallP GPS was paged due to SIPPBX outage due to AYT audit were failing after the primary side upgrade. We had suspected that the issue triggered by\r\nAYT options audit success/failure responses changes which delivered with 17.0.18.5 and later release.\n\r\nUpgrade path:\n\r\nFrom :17.0.18.4\r\nTo : 17.0.22.20\n\r\nCallP and OAM gps investigated the issue together. As a first action, we have modified existing AYT Options profile like we did at another Telus site \r\nafter upgraded to same load. Yet, it did not help, somehow even the PBX status are UP at Link Maintenance Screen,they were going down after a while later.\n\r\nIn order to narrow down the AYT Options issue, we have created a new profile that we added all response messages (4XX,5XX,6XX) to success list yet this did not help neither.\n\r\nCallP team had worked to collect traces, but could not see any ayt mesages sesm traces, so that we collect tcpdump traces for investigation. In the pcap file,\r\nthere were incoming OPTIONS to SESM , yet there were not any outgoing OPTIONS from SESM which is not normal,customer also informed that they do not get any message from Sesm.\n\r\nThen, to clarify whether the issue a software or network problem, we just stopped the upgraded primary SM and SESM instances and make the old release loaded secondary SM and SESM active.\n\r\nOnce we activated the secondary side, customer reported that all calls are working and they see PING/OPTIONS sent by SESM.\n\r\nThis pointed as this may be a software issue and the customer was insisting on rollback,we started to prepare rollback, in the meantime since SWD engineer\r\n did not have the old version oracle installer (as this site is upgrade to 10.4 which\r\noracle is migrated from 10G to 11G, old release load should be available in case of \r\nemergency rollback, this is noted many places in upgrade documents) we spent time to found out the load and transfer it to customer site, the estimated time\r\nfor transfer was ~2 hours.\n\n\r\nIn paralell to our rollback preparation, customer reported that call failures are observed again even though the sesm and sm were running with old load, which made the rollback meaningless.\n\r\nThen, we had suspected the applied jar file during upgrade (telus has particular sesm jars),that s why redeployed the primary sesm instance with new load to \r\nclean up the jars and make a fresh start .\n\r\nWhen we completed redeployment to new load without any jar file applied, calls started to worked fine and we did not see any call failures .The symptoms proves there may be\r\na network issue on site out of application load issue.\n\r\nWe monitored site almost 1 hour and no call failures reported,so that we decided to leave site as how it is (half upgraded  to 17.0.22.20 ) and continue to\r\nupgrade at next MW since MW time was allready over.\n\r\nTonight MW,SWD will complete the upgrade and paged the CALLP GPS to apply the jar file together to avoid any user fault.\n\r\nCurrently,system is working fine  without any call failures at primary side Upgrade pause point.\n\r\nThank you','null'),(308,'Burak Biyik','AS-OAM','2016-08-07','160807-592581','Liberty Global Europe B.V.','ER paged GPS to report the SESM servers in overload condition and registrations failing in the Amsterdam SSL (AMSTNLUP06C) which is running 17.0.22.15.\n\r\nThe same site had suffered from the same issue in January 2016 (referenced case:160103-562118). To sum up, all SESM instances failed to talk to primary DB for some reason (this is the point where GPS could not find the RCA last time) and raised following alarm:\n\r\n------------------------------------------\r\nEvent : An IEMS critical event was received \r\nAlert Description: DB Overload \r\nResource Description: IEMS=212.142.25.230-A2E-Mgr-SESM1_1;Server=SESM1;Instance=1;Software=OVLD \r\nAdditional Information: OVLD;402 \r\n(info for NMS team below) \r\n--------------------------------------------\n\n\r\nSESMs failed to access REGDEST table to update/delete/insert registration entries. Then, they started to handle registrations within their IMDB, but they started responding 503 to REGISTER messages after certain point.\n\r\nThe primary EM server hosting primary DB was pingable but we were getting following failure when trying to make ssh connection:\n\r\n-------------------------------------------------\r\n[ntsysadm@NLssw2-SLM1 ~]$ ssh 212.142.25.228 -v\r\nOpenSSH_4.3p2, OpenSSL 0.9.8e-fips-rhel5 01 Jul 2008\r\ndebug1: Reading configuration data /etc/ssh/ssh_config\r\ndebug1: Applying options for *\r\ndebug1: Connecting to 212.142.25.228 [212.142.25.228] port 22.\r\ndebug1: Connection established.\r\ndebug1: identity file /home/ntsysadm/.ssh/id_rsa type -1\r\ndebug1: identity file /home/ntsysadm/.ssh/id_dsa type -1\r\ndebug1: loaded 2 keys\r\nssh_exchange_identification: Connection closed by remote host\r\n---------------------------------------------------\n\r\nAfter a quick search on the web, there might have been several reasons for this failure such as missing file dependencies, corrupted fingerprints etc., but most matching one was heavy server load, probably due to overloaded db.\n\r\nThere was no action left other than rebooting primary EM server. \n\r\nThe outage was recovered after server reboot (it took around 2.5 hours for technician to arrive to the site and power-cycle the server) which reset SESM-DB connection and passed failing registrations.\n\r\nWe had found some suspicious entries in SESM oss logs last time but due to lack of logging, we could not find RCA. The site is now running 10.4 with extra logging and OAM-CallP GPS will work together to come up with an acceptible RCA.\n\r\nAgreed and dropped the call.','null'),(309,'Burak Biyik','AS-OAM','2016-08-03','160802-592082','Verizon Communications','I was paged by ER to report an issue with SESM. However, the issue was with SESM in CMT, not Session Manager on AS. There was a misunderstanding of SESM GPS obviously.\n\r\n-> Routed pager to correct team after informing them.\n\r\n-> Dropped the call.','null'),(310,'Burak Biyik','AS-OAM','2016-08-03','160803-592117','TELUS Communications Company','\"Patching/Migrating Oracle Database on Secondary Database Server\" screen failed during the MR Upgrade (17.0.18.4 -> 17.0.22.20) with the following error: \"Disk Partitions Validation Failed \"\n\r\nLooking at the details of the failure from /var/mcp/upgrade_tools/logs/ directory, available space for /opt directory did not seem to meet validation limit (at least 40% should be free for /opt).\n\r\nHowever, there was already more available space than 40% on secondary DB Server. The validation was apparently running on the primary even though the subject db is the secondary one.\n\r\nAfter a quick discussion with colleagues, this was found as a known issue fixed in later releases. GPS will contact DS to expedite 10.4 prop-back operation.\n\r\nAs manual steps,\n\r\n-> Uninstall Oracle 10g\r\n-> Install Oracle 11g\n\r\napplied to pass the screen.\n\r\nAgreed and dropped the call.','null'),(311,'Burak Biyik','AS-OAM','2016-08-03','160803-592102','TELUS Communications Company','During the MR Upgrade (17.0.18.4 -> 17.0.22.20), wizard failed at \"Switching Service to Secondary System Manager\" screen just before upgrading primary side.\n\r\nSM_1 was stuck in ACITVATING state\n\r\n[ntappadm@CLGRAB21A2SM2 bin]$ ./getInstanceState.pl \r\nACTIVATING \n\r\nwhereas SM_0 could not change its state to Hot Standby\n\r\n[ntappadm@CLGRAB21A2SM1 bin]$ ./getInstanceState.pl \r\nUNAVAILABLE \n\r\nLooking at the work logs of both SM instances, several Perfect Channel exceptions were observed but there was no time to check them deeply to keep up with Maintenance Window.\n\r\n-> Killed both instances and start them manually from cli\r\n-> Even though SM_1 became ACTIVE, SM_0 failed to start up as HOT STANDBY.\r\n-> realized that the servers were responding slow so that checked uptime (reached 550 days for both)\r\n-> performed reboot for both servers and we were able to change SM instances\' states properly this time.\r\n-> We left SM_0 ACTIVE and SM_1 HOT STANDBY to let wizard perform switch operation\r\n-> passed the screen\r\n-> agreed and dropped the call.','null'),(312,'Burak Biyik','AS-OAM','2016-08-03','160803-592101','BSkyB','During the patch application (17.0.22.8 -> 17.0.22.16), the wizard failed at \"PrepareDB\" screen while trying to take database backup.\n\r\nSWD seemed to retry the same screen few times but had the following failure:\n\r\n-------------------------------------------------\r\nEstimate in progress using BLOCKS method... \r\nProcessing object type SCHEMA_EXPORT/TABLE/TABLE_DATA \r\nTotal estimation using BLOCKS method: 2.427 GB \r\nProcessing object type SCHEMA_EXPORT/USER \r\nProcessing object type SCHEMA_EXPORT/SYSTEM_GRANT \r\nProcessing object type SCHEMA_EXPORT/DEFAULT_ROLE \r\nProcessing object type SCHEMA_EXPORT/TABLESPACE_QUOTA \r\nProcessing object type SCHEMA_EXPORT/PRE_SCHEMA/PROCACT_SCHEMA \r\nProcessing object type SCHEMA_EXPORT/TYPE/TYPE_SPEC \r\nProcessing object type SCHEMA_EXPORT/DB_LINK \r\nProcessing object type SCHEMA_EXPORT/SEQUENCE/SEQUENCE \r\nProcessing object type SCHEMA_EXPORT/TABLE/TABLE \n\r\nExport> UDE-00005: unexpected End-Of-File encountered while reading input \r\n---------------------------------------------------\n\r\nThe export of \"MCSDBSCHEMA\" tables was failing somehow due to reported error.\n\r\nThe issue did not happen when I launched wizard on my PC and we passed the screen successfully.\n\r\nSearching the error on the web, several reasons might have caused this failure (having a busy db during backup time or full disk etc.)\n\r\nGPS will contact the customer if there is a known db activity at that time and also investigate alternative RCA of this failure in parallel.\n\r\nAgreed and dropped the call.','null'),(313,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-07-26','160726-591036 ','Guyana Telephone and Telegraph Company','Problem Description :\r\n=====================\r\nSWD paged me to report upgrade wizard failed due to network connection error on Upgrade DB&SM screen.\n\r\nUpgrade Path : 17.0.22.1 to 17.0.22.16\n\r\nActions Taken :\r\n==============\n\r\n1)Connected to the site and checked the mcpUpgrade logs on the server.\r\nSeen that the screen has been completed successfully.\n\r\nDatabase --- \r\ndbInstall command: /var/mcp/upgrade_tools/bin//ut_dbInstall.pl -p /var/mcp/install/installprops.txt -upg -vof -nc -m EMS11_88367fe0-2b2f-1b21-a9da-ec9ecd0a4e2e_ut_mcpUpgrade.pl_UP \r\nGRADE_DB_SM_0 -s 172.31.13.90 \n\r\n--- DataBase Upgrade Complete --- \n\r\n--- Upgrading the SysMgr (SM) --- \r\nsmUpgrade command: /var/mcp/upgrade_tools/bin//ut_smUpgrade.pl -p /var/mcp/install/installprops.txt -vof -rboff -primarySM -nc -m EMS11_88367fe0-2b2f-1b21-a9da-ec9ecd0a4e2e_ut_mcp \r\nUpgrade.pl_UPGRADE_DB_SM_0 -s 172.31.13.90 -start \n\r\n--- SM Upgrade Complete --- \n\r\nDB/SM Upgrade Completed Successfully \n\r\n3)Closed the UW and re-opened it.Seen that the screen is completed successfully.\n\n\r\nI have requested upgrade wizard logs on the local PC and EMServer1.\r\nI will investigate the logs in order to find the RCA on this issue.\n\r\nAfter that dropped the call.','null'),(314,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-07-26','160726-591016','Boston College','Problem Description :\r\n=====================\r\nSWD paged me to report upgrade wizard fails on PrepareDB screen.\n\r\nUpgrade Path : 17.0.22.16 to 17.0.22.20\n\r\n1)Connected to the site and checked the dbBackup logs on the server.\r\nSeen the issue below.\n\r\nError occurred executing NE commands (see below):\n\r\nNE command exited with the value: 1\r\nUnlinked /var/mcp/upgrade_tools/work/dbBackup//init_ned_cmd_20160726_011400_MjGI\r\nNot unlinking file\n\r\n> config ok\r\n> run output  failReason=7\r\n> run output  ERROR: To restrict multiple DB scripts from being executed simultaneously,\r\n> run output  a Lock is required for the execution of this script.  With that, this script\r\n> run output  cannot be executed at this time because the Lock has already been acquired by\r\n> run output   =>  dbBackup.pl\r\n> run output \r\nNE command exited with the value: 1\r\n> exited\n\r\nError: Database backup is failed.\n\r\n2) Checked uptime of the server. It was up for 426 days.\n\r\n3) Rebooted the EMServer1 and then tried retry button on UW. Then upgrade wizard passed the screen successfully.\n\r\n4) Agreed with the SWD and then dropped the call.\n\r\nNote : It is quite normal that since the server is up more than 1 year, stuck processes and memory &CPU allocation can prevent or slow down dbBackup action.\n\r\nI will discuss with the Design Support internally about what can be developed on this kind of issues.','null'),(315,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-07-26','160726-591018','Guyana Telephone and Telegraph Company','Problem Description :\r\n=====================\r\nSWD paged me to report upgrade wizard fails on PrepareDB screen.\n\r\nUpgrade Path : 17.0.22.1 to 17.0.22.16\n\r\n1)Connected to the site and checked the dbBackup logs on the server.\r\nSeen the issue below.\n\r\n> run err Error! Timer expired while executing: perl\r\nUnlinked /var/mcp/upgrade_tools/work/dbBackup//init_ned_cmd_20160726_001846_ugys\r\nNot unlinking file\n\r\n2) Restarted ned client and tried \"retry\" button on UW. Retry action on UW has failed with the same reason above.\n\r\n3) Checked uptime of the server. It was up more than 1 year.\n\r\n4) Rebooted the EMServer1 and then tried retry button on UW.Then upgrade wizard passed the screen successfully.\n\r\nAgreed with the SWD and then dropped the call.\n\r\nNote : It is quite normal that since the server is up more than 1 year, stuck processes and memory &CPU allocation can prevent or slow down dbBackup action.\n\r\nI will discuss with the Design Support internally about what can be developed on this kind of issues.','null'),(316,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-07-25','160725-590837','Shaw CableSystems','Problem Description :\r\n=====================\r\nI have been paged by ER due to license key apply fails on the A2 system.\r\nCustomer was performing hardware migration on their A2 system.\r\nCustomer reported that yesterday, halfMigrated license key for the primary side is applied to their system successfully.\r\nToday, they were applying  fullyMigrated license key for the secondary side of system to clear LKEY_HARDWARE_MISMATCH_ERROR.(LKEY754)\n\r\nCustomer Load:18.0.1.0\n\n\r\nActions Taken :\r\n==============\n\r\n1) Connected to the site and tried to apply the license key manually.\r\nAltough I can apply the license key successfully from MCP GUI, the alarm was still there. Then I have performed double swact/undeploy/deploy mtce operations to clear the alarm on the A2 system but this action did not help to me.\n\r\n2) Investigated the OSS/work logs and seen this log below;\n\r\nLKEY/LKEYSYS 734 ALERT  JUL25 02:58:41\r\nError occured validating license key. License key is not valid on this hardware. Error Code: 2113\n\r\nLKEY/LKEYSYS 754 CRITICAL  JUL25 02:58:41\r\nLicense key problem detected: License key does not match hardware.\n\r\n3)Decoded the delivered license key to compare with the MAC addresses of the existing EM servers.\n\r\n4)SM0:\r\n[root@vpd6em10no ~]# /usr/local/bin/getMAC.sh\r\neth0      Link encap:Ethernet  HWaddr EC:9E:CD:0D:86:44  \r\neth1      Link encap:Ethernet  HWaddr EC:9E:CD:0D:86:45  \r\nbond0     Link encap:Ethernet  HWaddr EC:9E:CD:0D:86:46  \r\neth2      Link encap:Ethernet  HWaddr EC:9E:CD:0D:86:48  \r\neth3      Link encap:Ethernet  HWaddr EC:9E:CD:0D:86:49  \n\r\nSM1:\r\n[root@vpd7em10dr ~]# getMAC.sh \r\neth0      Link encap:Ethernet  HWaddr EC:9E:CD:0B:04:0A  \r\neth1      Link encap:Ethernet  HWaddr EC:9E:CD:0B:04:0B  \r\nbond0     Link encap:Ethernet  HWaddr EC:9E:CD:0B:04:0C  \r\neth2      Link encap:Ethernet  HWaddr EC:9E:CD:0B:04:0E  \r\neth3      Link encap:Ethernet  HWaddr EC:9E:CD:0B:04:0F  \n\r\nLICENSE KEY:\nEC9ECD0D8644\nEC9ECD0D8645\nEC9ECD0D8646\nEC9ECD0D8648\nEC9ECD0D8649\nEC9ECD0B040A\nEC9ECD0B040B\nEC9ECD0B040C\nEC9ECD0B040D\nEC9ECD0B040F\n\r\n5)EC:9E:CD:0B:04:0D should be EC:9E:CD:0B:04:0E on the LK file.\n\r\n6)Requested from Ken Johnson to generate new License Key(fullyMigrated) with the information above.\n\r\nI have informed the ER that once I have received the new LK file, I will lead for applying it to the A2 system. Then dropped from call.','null'),(317,'Oktay Esgul','AS-OAM','2016-07-14','160714-589364','Singtel Optus Pty Ltd','Gary Norwood paged OAM pager in order to report one of sesm pair was unreachable at MCP gui.\n\r\nKemal had answered the call since I am not available for awhile,then I have involve to recovery part.\n\r\nMCP Load: MCP_17.0.22.7\n\r\nSince the secondary sesm instance is not reachable even the server is up, we suspected a ned issue causing this issue, so that as a first action ned restart performed by running neinit restart.\n\r\nOnce the ned restart completed, we attempted to start problematic instance from gui and it became hot-standby after synchronising successfully.\n\r\nCustomer performed basic test calls those all passed without any issue.\n\r\nIn the meantime, customer complaint in regards below critical diameter alarm.\n\r\n AlarmName: Startup Failure\r\n                   TimeStamp: Sun Jun 19 23:25:12 EST 2016\r\n                   FaultNumber: 101\r\n                   ShortFamilyName: DMTR\r\n                   LongFamilyName: DIAMETER\r\n                   Severity: CRITICAL\r\n                   ProbableCause: underlying resource unavailable\r\n                   Description: Peer is not working. Group Name: RFcdfg\r\n                   Peer Address: 10.194.20.162\r\n                   Peer Name: RFcdf2\r\n                   Peer Host Name: MM1-o3cai1mm000\r\n                   Corrective Action: Check for peer.\n\r\nI have checked diameter settings from configuration parameter field of SESM and notice that the parameter was set as FALSE which means diameter(offline charging is not being used for this site, customer confirmed this as wel) is not activated.\n\r\nI have given more details in regards diameter service functionality to ER and customer,then we agreed to raise a new case to investigate diameter alarms seperately out of pager since it is not actively used service.\n\r\nThen, customer requested sesm failover to perform call tests for newly activated sesm instances and all tests passed again.\n\r\nThen we aggreed and dropped the call.\n\r\nThank you Kemal for cooperation.','null'),(318,'Oktay Esgul','AS-OAM','2016-07-13','160713-589114','Singtel Optus Pty Ltd','John Shamer paged me out since Chris paged him for Optus upgrade which was left at pause point last night to report several\r\nBcps were unstable which made customer concerned for the rest of upgrade.\n\r\nUpgrade Path:\n\r\nUpgrade Path: 14.1.0.12 --> 14.1.15.3\n\r\nJohn shared site access over vm ,then firstly I have checked status of BCPs ,even the servers of blades are upgraded ,application\r\nload was not which make application unstable,Furthermore,secondary SM was active with old load which is another trigger of the issue.\n\n\r\nFirstly, I have performed failover betweeen SMs and activated the primary one, then changed corresponding BCP loads from MCP gui and redeploy all of them\r\none by one.When the load changed all blades were activated successfully except one which we could not figure out reason yet.Even applcation seems working fine\r\nfrom server,at MCP gui application state do not update somehow, furthermore when we tried to attempt perform any action like delete/add instance,all system goes down which we can recover\r\nby sm failover.This blade will be investigated with a new case.\n\r\nIn paralell to our BCP operations, Chris continued the upgrade and wizard failed at secondary patch screen after completing secondary servers patching\r\nwithout any issue. I have save&exit wizards and retry since patch logs proves timeout error due to network latency for file transfer.\r\nRetry helped and wizards passes oracle screen.\n\r\nIn next screen where secondary instance re-deployed with new loads, wizards stuck due to problematic blade I mentioned previously.In order to proceed ,I have launch the wizard in debug\r\nmode and skip that screen.\n\r\nSetupReplication screen took almost 1 hour to complete successfully(seems ned was stuck, restarted ned at secondary server and script completed then) ,afterward, dbs and system became stable.\n\r\nNo upgrade related alarms left at MCP gui and problematic BCP killed to avoid it to cause voice quality issues.\n\r\nThen,I have handed over the wizard back to Chris to complete post upgrade steps.\n\r\nSince this site is running on SP1, we ll not work on RCA.','null'),(319,'Burak Biyik','AS-OAM','2016-07-13','160713-589066','Singtel Optus Pty Ltd','After completing SP1 MR half-upgrade (14.1.0.12 -> 14.1.15.3), one-way speech issue was reported by Optus for one of their BCP cluster (1+1).\n\r\nThe actions taken during the investigation:\n\r\n-> Checked problematic BCP instance state from MCP GUI and realized that it failed to be deployed with new load (14.1.15.3) and stuck in \"CONFIGURED\" state\r\n-> It was probably the active node of the cluster so that causing speech path problems.\r\n-> Failed to deploy the instance with new load from MCP GUI.\r\n-> Kill the instance pid from cli and remove load folder (var/mcp/run/MCP_14.1) from the server manually (rm -rf var/mcp/run/MCP_14.1)\r\n-> Deployed the instance with new load from MCP GUI and verified the creation of load folder on EM Server (to verify SM-BCP connection)\r\n-> Started the instance from MCP GUI\r\n-> Even though it (BCP and HAL processes) was successfully started (verified from work logs and \"neinit -p\" output), instance state was \"none-offline-down-unavailable\"\r\n-> Checked \"instanceState\" file under var/mcp/run/MCP_14.1//work directory and it was showing ACTIVE as well (./getInstanceState.pl returns different results for ntappadm and root users for SP1 release but skipped this behavior since it is EOL release)\r\n-> Verified that \"mptool -n\" returns current calls for active BCP whereas it does not work for standby BCP (as designed).\r\n-> Customer confirmed that no longer one-way speech was being reported.\r\n-> Agreed to drop the call and let SWD to perform the rest of the uprade.\n\r\nNote: \'Why BCP failes to write its state to database\' will be investigated under a new case as soon as the upgrade is completed.','null'),(320,'Oktay Esgul','AS-OAM','2016-07-13','160712-589058','Mobistar','David paged me again since platform patching fails for all primary servers ,connected again and checked /var/mcp/os/logs/patch_logs\n\r\nUpgrade Path:\n\r\n14.1.15.2->17.0.22.16\n\r\nPatching was failing with below error at 17.0.9 level just before 17.0.10 target level.\n\r\n#####################################################\r\n[*P-Info*]       >>17.0_fileset_10/bash-2.05b-41.7.nt.mcp.1.i386.rpm<<\r\n[*P-Info*]       >>17.0_fileset_10/open_src_rpms.txt<<\r\n[*P-Info*] Installing new RPM: bash\r\n[*P-Info*]    Executing command: \"(cd /var/mcp/os/install/workdirs/temp/resources/tar.bz2/17.0_fileset_10 && /bin/rpm -Uvh --force bash-2.05b-41.7.nt.mcp.1.i386.rpm >> /tmp/bashrpm.out 2>&1) 2>&1\"\r\n[*P-Error*]    [*P-Error*]    Retcode is     >>0x0100<<\r\n[*P-Error*]    Signal part is >>0x00<<\r\n[*P-Error*]    Main part is   >>0x01<<\r\n[*P-Error*]    System return strings:\r\n[*P-Error*]       None.\r\n[*P-Error*] Error installing RPM package.\r\n[*P-Info*] --------------------------------------------------------------------------------\r\n[*P-Error*]\r\n[*P-Error*] Patch part #1 exited with an error (2).\r\n[*P-Error*]\r\n[*P-Error*] **********************************************************************\r\n[*P-Error*] *                  PATCH INSTALLATION ERROR                          *\r\n[*P-Error*] *                                                                    *\r\n[*P-Error*] * One or more error(s) have been detected during the installation of *\r\n[*P-Error*] * this patch. To avoid any further configuration errors on this      *\r\n[*P-Error*] * system, please contact your next level of support to resolve this  *\r\n[*P-Error*] * current installation before proceeding with additional patching.   *\r\n[*P-Error*] * Once corrected, re-start the patching process, starting with this  *\r\n[*P-Error*] * patch.                                                             *\r\n[*P-Error*] **********************************************************************\r\n[*P-Error*]\n\r\n####################################################\n\r\nI have tried to re-launch wizards after save&exit, it did not help.\n\r\nThen, tried to run patchPlatform.pl manually instead of wizards from server, yet it did not work neither.\n\r\nTried to run the failed step of server patching manually , it did not help.\n\r\nTo ensure that there is not any hung process, I have rollbacked with patchPlatform -rs then reboot the server,and\r\nretry to patch via patchPlatform.pl,did not help.\n\r\nIf there was any server patching success, I would suspect server specific issues, yet all of the servers had been failing which made me suspicious\r\nabout the patch iso. So that we tried to find new load and David downloaded new esd and upload the patch file to server\r\nthat took almost more than one hour.\n\r\nFirstly roll-back again, and give another try with new esd file by running patchPlatform.pl and it failed again at same step.\r\nIn the meantime,error message changed.\n\r\n######################################\r\n-------------------------------------------------------------------------------\r\nApplication part #1\r\nAppl Target: Update Bash RPM\r\nCopying BZ2 archive \"17.0_fileset_10.tar.bz2\"\r\nCopying file \"../resources/17.0_fileset_10.tar.bz2\" to \"/var/mcp/os/install/workdirs/temp/resources/tar.bz2\"\r\nChanging mode of \"/var/mcp/os/install/workdirs/temp/resources/tar.bz2\" to \"600\".\r\nUncompressing archive; this may take a few minutes.\r\nUntar\'ing archive.\r\nInstalling new RPM: bash\r\nError installing RPM package.\r\n--------------------------------------------------------------------------------\n\r\nPatch part #1 exited with an error (2).\n\r\n**********************************************************************\r\n*                  PATCH INSTALLATION ERROR                          *\r\n*                                                                    *\r\n* One or more error(s) have been detected during the installation of *\r\n* this patch. To avoid any further configuration errors on this      *\r\n* system, please contact your next level of support to resolve this  *\r\n* current installation before proceeding with additional patching.   *\r\n* Once corrected, re-start the patching process, starting with this  *\r\n* patch.                                                             *\r\n**********************************************************************\n\n\r\nError (2) executing patch \n\r\n#####################################\n\n\r\nSomehow,error message changed yet the issue persisted, in paralell our investigation customer started to push us for rollback since MW becoming end.\n\r\nEventually, we started roll-back , as server platform patch is one of initial upgrade steps, roll-back completed shortly without any issue.\n\n\r\nCurrently,system is roll-backed and stable , just one of SesmServer is too slow which customer is allready aware.\n\r\nRCA case is raised and we ll investigate the issue with design to provide a solution asap.','null'),(321,'Oktay Esgul','AS-OAM','2016-07-13','160712-589056','Mobistar','David paged me out to report prepare db failure during 8.0 SP1 to 10.4 upgrade.\n\r\nUpgrade Path:\n\r\n14.1.15.2->17.0.22.16\n\r\nPrepare db screen was failing with below error:\n\n\r\n###########################\r\nQuiescing replication groups\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd\r\n_20160712_224234_0UaE | /opt/mcp/ned/bin/nedclient 10.0.98.35 4890\r\nCommand Output:\r\n> config ok\r\n> run err Error! Timer expired while executing: perl\r\n> exited\n\r\nError occurred executing NE commands (see below):\n\r\n> run err Error! Timer expired while executing: perl\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20160712_2\r\n24234_0UaE\r\nNot unlinking file\n\r\n> config ok\r\n> run err Error! Timer expired while executing: perl\r\n> exited\n\r\nquiecseRepDB was not successful\r\nIt may have timed out due  to a large queue.\r\nThus, will continue with cleanup Replication operation\r\n####################################\n\r\nI have connected site directly with contivity and launch the wizards on my own pc and it failed again with same error.\n\r\nAs a first action, I have restarted ned daemon on first EMServer yet it did not help, then restarted the secondary EMServer ned as well ,not help neither.\n\r\nFurthermore, wizards started to failed with duplicate scripts running error since each retry start new cleanupreplciation process.\n\r\nIn order avoid duplication process, I have rebooted both of EMServes after save&exit the wizards.\n\n\r\nWhen the both of the servesr and instances become up, restarted the wizard again and this time prepare db screen completed successfully.\n\r\nThen, hand over the wizard to David to continue upgrade and drop the call.','null'),(322,'Oktay Esgul','AS-OAM','2016-07-12','150306-520520','OPTUS','Chris Henwood paged me out to report wizards failure while patching servers platform.\n\r\nUpgrade Path: 14.1.0.12 --> 14.1.15.3\n\r\nThere are almost 40 servers,in the system which makes wizards very slow at server patching screen and 5 of BCPs were failed with primary EM servers.\n\r\nA site technician sent to site to power cycle , in the meantime, I have tried to patch primary EM Server. It failed due to ned error , when I checked uptime\r\nIt was up for 365 days .So that, I have rebooted the server first,it did not help ,patching failed with ned error.As a next step, I have stopped the ned and tried again, it failed again.\r\nThen, started the ned again, and give another patchPlatform try and it worked this time.\n\r\nAfter servers patch completed, we launched the wizard again and it passed the server platform screen without any issue.\n\r\nSince this site is running on SP1 , no rca will be provided.\n\r\nThank you','null'),(323,'Oktay Esgul','AS-OAM','2016-07-12','160712-588825','Brown University','Peter Maloney from SWD paged me out to report setup DB replication failure while performing MR upgrade.\n\r\nUpgrade Path: \n\r\nFrom :14.1.8.2\r\nTo   :14.1.15.0 \n\r\nReplication was failing since wizard could not get oracle version of secondary database with below error.\r\n\"DB replication failed: \n\r\nError can not get Oracle version \r\n\"\n\r\nI have connected teamviewer session and checked the servers.The site was small deployment and there were just 2 EMservers.\r\nFirstly check platform version, which is OKEY.\r\nThen run showversion.pl which proved that secondary database version is not aligned with primary. (While the primary is running with secondary oracle patch, secondary was not patched)\r\nBased showversion.pl output, I have checked oraclePatch logs and realized that even though secondary oracle was not able to patched by wizards due to ned error,\r\nsomehow, wizards skipped that oracle patching screen.\n\r\nSo,I have tried to patched oracle manually by running oraclePatch.pl -secondary, yet it failed as well since script could not find version data of secondary oracle.\r\nWhen  I checked, /opt/mcp/db/data directory ,version file were not there with several other missed files which forced me to oracle re-install instead of patching.\n\r\nIn order to proceed, I uninstalled the secondary oracle via oracleUninstall.pl -secondary command,then attempted to install it.Yet,Re-installation failed with below error.\n\r\n#############################################\r\nINSTALL, for Oracle on 172.17.4.28 ... \r\nInstalling Oracle...please be patient \r\nError occurred executing NE commands (see below): \n\r\nError: when execute above command => 1 \r\nNE command exited with the value: 1 \n\r\n******************************************************************************* \r\nOracle installation has failed. Please resolve the underlying issue that \r\ncaused the failure. Once the issue is resolved \r\nRemove the existing Oracle installation by executing \r\n./oracleUninstall.pl -secondary (in the /var/mcp/install/ directory) \r\nThen re-execute this script again to attempt the install again. \r\n******************************************************************************* \n\r\nError executing Oracle install commands. \r\n############################################\n\r\nI have researched the similar cases and found out one which refered /tmp directory disk utilization may cause this type of issue, then checked the secondary EM servers\r\nand tmp/ was %100 full with oracleInstall file ,possibly the first oracle install attemp file was not deleted even it was failed.\n\r\nAfter deleting all useless data from /tmp directory, I have re-try to oracle installation again and it completed successfully.\n\r\nAfterward, oracle issues solved we attempted to setup replication again by clicking re-try button of wizards and it completed replication without any issue.\n\r\nAs this site is running on SP1 which we do not provide RCA ,rca case will not be raised to follow up.','null'),(324,'Tugkan INCE','AS-OAM','2016-06-27','150428-528456','Swisscom','I was paged by ER related with an unstable System Manager issue reported from Swisscom Ittigen Site.\n\r\nBoth system managers were failing to get active, which was preventing MCP GUI to be launched.\n\r\nI have connected to the site and checked the states of the SM instances and verified that they were failing to get active. I took a quick look to the oss logs in the system and according to the last available oss logs, there were plenty of socket exceptions reported from SM instances which is indicating a possible network problem.\n\r\nSo I asked Thomas to check with the customer if there were any network issues experienced on Ittigen site. Thomas verified with the customer that last week they had networking issues on the site.\n\r\nThen suspecting of the \"SM Spool Issue\" I have counted monitored element number in the site which was 48 although System Managers do not support monitoring of more than 40 elements.\n\r\nSo I checked the spool directories of the servers in the system and verified that there are more than 1000 files on almost each server, which was explaining the SM problem observed on the site. \n\r\nTo summarize briefly as a quick RCA, since the system managers were unable to be reached due to networking problems observed on the site and started stacking spool files to be processed once the SMs are back online.\n\r\nHowever, on the site there are more than 40 MEs without an FPM deployed which is unfortunately not supported and causing SM a workload which it can not handle.\n\r\nSo once SMs become online, all the instances and monitored elements on the site tried to send their spool files to SM to be processed yet due to the load of the spool files SM can not even get activated properly which is causing a System Manager outage until its resolved.\n\r\nHowever the workaround resolution of the problem requires a service outage for a while(around 30 minutes-1hour depending on the system speed). So before applying the workaround we asked customer to arrange a maintenance window for the application of the procedure to prevent a service outage in active work hours.\n\r\nAs a workaround resolution, following steps are explained in detail within a MOP and sent to ER mail alias:\n\r\n1) Take unit 1 of all HA nodes down via cli via ntappadm user (neStop.pl) \r\n2) Take BCP blades and unit 0 of all HA node down via cli via ntappadm user (neStop.pl) \r\n3-) Take remaining SESM and SM servers down via ntappadm user\r\n4-) After all nodes are down - Issue find commands to remove all files in the log, om, and tmom dir on EACH server in the system with root user\r\n5-) Bring SM 0 up via cli (neStart.pl) via ntappadm user - start MCP GUI \r\n6-) Bring up SM1, BCP blades, and all unit 0s of all HA nodes via MCP GUI\r\n7-) Bring up all unit 1s of HA nodes via MCP GUI \n\r\nI have informed Thomas regarding if anything goes wrong or if there is any point unclear in the MOP, ER team can page us again.\n\r\nER team was waiting for customer feedback regarding scheduling the maintenance window. I left the pager call.\n\r\nIn the ultimate end, to prevent re-occurance of the problem, customer should upgrade their system from 10.2 to 10.4 and deploy at least 1 pair of FPM\'s to balance the monitoring/logging load on System Managers. This situation and recommendations will be evaluated and discussed with the customer during RCA case investigation','null'),(325,'Yunus Ozturk','AS-OAM','2016-06-24','160624-586762','Ziggo','Problem Description:\r\n=====================\n\r\nZiggo called into ER reporting an outage where they have seen their SESM nodes were flapping. \n\r\nActions Taken:\r\n================\n\r\nGPS logged into node and verified that the outage was seen on both SESM servers earlier but now everything was stable. Both SESM instances were up and running. \n\r\nGPS notified continues exceptions \"com.nortelnetworks.ims.mw.vqreport.base.VQEventException: Misplaced Unknown header IF PFC :0\" on SESM work logs and these exceptions were generated on the Active SESM instance. As we thought that this bulk of exceptions on Active SESM caused the bounces on SESM nodes, we have engaged the Callp GPS.\n\r\nAsked the customer to collect the logs for RCA analysis.','null'),(326,'Yunus Ozturk','AS-OAM','2016-06-24','160624-586753','Singtel Optus Pty Ltd','Problem Description:\r\n=====================\n\r\nER paged out GPS for SESM E1/E2 situation. \n\r\nActions Taken:\r\n=================\n\r\nCustomer experienced an E1 outage situation. At the time of outage SESM3_1 instance was the Active instance and SESM3_0 was at Unavailable state. SESM3_1 was recovered after rebooting the server. \n\r\nSESM3_0 was recovered after plugging out both disks and plugging in one of the disks back. \n\r\nGPS verified that SESM3_1 is Active and SESM3_0 is HotStandby. \n\r\nCustomer will replace the disk of SESM3_0. \n\r\nGPS provided the steps of adding a new disk to the server.. \n\r\nSince customer was running on 8.0 SP1 Release which is EOL, GPS will not provide an RCA for this problem.','null'),(327,'Senem Gultekin','AS-OAM','2016-06-18','160618-585872','UPC Nederland','Problem Description:\n\r\nER paged OAM GPS for a wizard upgrade failure. Upgrade path: 17.0.7.0 to 17.0.22.15.\n\r\nCustomer was performing the upgrade themselves, and they have paged out for a BCP failure on wizard screen even though it was completed in the background.\n\r\nSolution:\n\r\n	Until I accessed to the site customer already was on DB&SM Upgrade screen in debug mode. He told me that he switched to debug mode once he saw the BCP upgrade failure. Normally debug mode is only to skip the problematic screen with GPS approval, not to continue for all screens. \r\n	Once I logged in I saw that DB&SM upgrade was completed successfully, however the next button was not available in debug mode.\r\n	During the debug mode we have seen \"OMI web service failed\" failures. \r\n	Ive passed the screen by clicking force next and in the next screen we switched back to the normal mode, which is the official way of A2 upgrade.\r\n	Customer completed the upgrade steps until the half upgrade step (pause point). \r\n	They mentioned that maintenance window was over, which is very strange because DB&SM upgrade is actually beginning of the actual upgrade steps, and it shouldnt take that long. Will understand the reason for this.\r\n	Next day customer continued with the upgrade.\r\n	Site is running on 17.0.22.15 now.','null'),(328,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-06-19','160618-585891','UPC Nederland','I have been paged by ER due to Setup DB replication step is failed on the upgrade wizard.\n\r\nUpgrade path : 17.0.7.0 to 17.0.22.15\n\r\n1)I have checked resync logs and seen this issue below;\n\r\nERROR at line 1: \r\nORA-24099: operation not allowed for 8.0 compatible queues \r\nORA-06512: at \"SYS.DBMS_AQADM_SYS\", line 9386 \r\nORA-06512: at \"SYS.DBMS_AQADM\", line 1374 \r\nORA-06512: at line 57\n\r\n2)Taken TW access details from customer and checked showversion.pl outputs of the primary and secondary database servers.\n\r\nPrimary DB was at Oracle 10g  and secondary DB was at Oracle 11g.\n\r\n3)From the beginning of the upgrade, Customer was performing the upgrade in Debug mode and mentioned that he has used force back&next button without GPS recommendation.So I suspected that he had used these buttons during Migration steps of Upgrade Wizard.\n\r\n4)To recover the primary side with oracle 11g  and continue the remaining steps via UW, I have taken a DB backup on primary database(Primary database holds WIZARD_STATE table).\r\nI have followed the Oracle Migration document and uninstalled oracle 10g and then installed oracle 11g to primary database server with the same oracle patch level of secondary database server. Then restored with DB backup which I have taken before starting to the recovery.\n\r\nRestored the database and requested from customer to open UW.\r\nSetup DB Rep Screen has been completed successfully \n\r\nI will ask to ER to create a follow-up case to identify if force next is used during the upgrade.\r\nThen dropped from the call.','null'),(329,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-06-19','Not known yet','Liberty Global Europe B.V.','Next Day:\n\r\nI have paged by ER due to customer has observed unexpected error on the Wizard again after clicked the next button on the Upgrade Wizard which were Pause Point screen before.\n\r\nSince I could not troubleshoot this issue although I have investigated the similar issues on the SFDC , The only option left is to continue this upgrade manually.\n\r\nIn a sequence(platform patch, oracle Patch ,Upgrade of NEs) I have upgraded the secondary side of A2 system successfully.\n\r\nThen I have requested from ER to create a new case for post upgrade steps and dropped from call.','null'),(330,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-06-18','160618-585873','Liberty Global Europe B.V.','I have been paged by ER due to customer could not make or receive calls during the upgrade of A2.\n\r\nThe customer was upgrading the A2 from 14.1.0 MR to 14.1.10 MR via upgrade wizard in debug mode and I warned him about not to perform the upgrade in debug mode.\n\r\n2016-06-18 00:10:47,652  INFO UpgradeMainController - Wizard load: MCP_14.1.0.3_2011-10-07-1802\r\n2016-06-18 00:10:47,652  INFO UpgradeMainController - Wizard version: 0\r\n2016-06-18 00:10:47,652  INFO UpgradeMainController - Wizard started in debug mode.\r\n2016-06-18 00:10:47,652  INFO UpgradeMainController - Starting screen: WELCOME\n\r\n1)\r\nI have connected to site and seen that all BCPs are down.\r\nCustomer informed me that their calls are dropped while the Wizard is performing SM and DB upgrade screen.In the meantime, He dropped from UW due to customer has observed \"OMIWebServiceFailed\" error on the Wizard.\n\r\n2016-06-18 05:34:24,956 ERROR UpgradeWizard - Following error occurred while starting upgrade wizard : fail:WebServiceFailed:null\r\n2016-06-18 05:34:24,956 DEBUG MessageView - Error Dialog : Starting Wizard Error, Following error occurred while starting upgrade wizard.WebServiceFailed\n\r\n2) Checked the UW logs and seen that DB and SM are upgraded successfully.\r\nDouble checked from MCP GUI that SM0 was new load and SM1 was old load.(expected situation)\n\r\n3)\r\nAll BCP instances was deployed on the primary servers. \n\r\n2016-06-18 00:18:07,394 DEBUG UpgradePanelController - Primary servers are \'[BCP07, BCP08, BCP05, BCP06, MASS00, BCP03, BCP04, BCP01, SES2S1, BCP02, SES3S1, BCP09, SES5S1, BCP10, EMS1, SES4S1, BCP00, SES1S1]\'.\n\r\nUW has given the services to secondary side in order to upgrade primary side but after that,there were no active BCP on the A2 system.\r\nIn order to finish outage situation, checked the BCPs platform level and seen that all of them were upgraded.\r\nAdvised to customer that to start all BCP instances with new load. E1 is ended after BCPs started.\n\r\nMeanwhile, I have focused the UW issue which customer has taken.\r\nCustomer tried to login to UW but this attempt was failed with same reason.\n\r\n2016-06-18 04:06:57,898 ERROR UpgradePanelController - Error during omi relogin: fail:WebServiceFailed:null\n\r\nI have double swacted the SMs and also rebooted the SM_0 server.Then we are able to login to UW but this time we have taken Unexpected error on the UW and it was not processing although it seems working.\n\r\nI have performed the rest steps manually in order to reach to Pause Point screen since we have limited time for mtce window.\r\nLater then, I have set the UW screen as Pause point and informed the customer that continue to the upgrade via UW.\n\r\nI will contact with ER to create a follow-up case to determine the RCA about UWs behaviour (WebServiceFailed,unexpected error ) and also about why all the BCPs are deployed on the primary servers on this A2 system.','null'),(331,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-06-14','160614-585314','Kontron AG','I have been paged by SWD due to Upgrade Wizard is failing at Step 35 which is Patching/Migrating Oracle database on the secondary database server.\n\r\nFROM/TO LOAD: 17.0.12.17 -> 17.0.22.19\n\r\nSince this is a migration path (from Oracle 10g to 11g), I have connected to the site and investigated the UW logs deeply.\n\r\n1) UW logs referred the oracle Patch logs on the EMS1. Opened the oracle patch logs and seen this issue below. Confirmed from the Oracle Migration document that it has failed during the Pre-Check steps of Migration on the Server Side.\n\r\n   Disk Partitions Validation Failed\r\n        -Below partition does not have enough disk space:\r\n         /opt should have at least 40% free disk space\r\n        -Recommended Action:\r\n         Please clean up unrequired data in this partition.\r\n         If you are not sure how to do it, please contact next level of support.\n\r\n2) Executed df h command on the primary database server.\n\r\n[ntappadm@kntrna2em01 bin]$ df h\r\n/dev/mapper/vg00-opt  6.0G  3.8G  1.9G  67% /opt\n\r\nTried to reduce the disk usage under 60% on the opt directory. For this purpose, I have executed du sh * command on the each directory under opt but there is nothing to delete.\n\r\n3) Tried to execute the command below manually on the EMS1.It has given more detail this time.\r\n./ut_oraclePatch.pl -nc -secondary -v 11.2.0.4-2 -l MCP_17.0.22.19_2016-05-03-1612\r\n...\r\n...\r\n...\r\ninflating: /var/mcp/loads/MCP_17.0.22.19_2016-05-03-1612/db/tempDirName4Unzip/data/create_tables.sql\r\n  inflating: /var/mcp/loads/MCP_17.0.22.19_2016-05-03-1612/db/tempDirName4Unzip/data/create_triggers.sql\r\n  23:48:32 Could not access state file Unable to open >><< to read\r\n   Disk Partitions Validation Failed\r\n        -Below partition does not have enough disk space:\r\n         /opt should have at least 40% free disk space\r\n        -Recommended Action:\r\n         Please clean up unrequired data in this partition.\r\n         If you are not sure how to do it, please contact next level of support.\n\n\r\n4)To solve this issue,The only thing left is uninstall the oracle 10g and then install the oracle 11g on the secondary database server.Performed one by one and finally oracle has been migrated. \n\r\nSince this screen has been completed successfully, Informed the SWD about skip this step and continue on the UW. I have collected the necessary logs to find RCA and also possible fixes on this issue and dropped from call.','null'),(332,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-06-13','160613-584936','BT PLC (Manchester)','Mark paged me out to report that after SM_1 is rebooted, it could not be Online-Up-Hot-Standby on the A2 system. The swact was already performed when he paged me. He informed that both SMs were up over 300+ days and had a high CPU utilization alarms. Also SESM1_1 and SESM1_0 instance are having a IMDB701 alarm due to swact of SM. Customer was running on AS 10.2 release .(17.0.7.14)\n\r\nI have taken access details and connected to the A2 system via Bomgar.\n\r\n1)\r\nInvestigated the work logs of SM_1 and found this one below;\n\r\nDBCM/DBCOMM 204 ALERT  JUN13 11:18:20\r\nNo suitable database instance is available for initialization\n\r\n2) Connected to the SM_1 server and executed stopDatabase.pl and then startDatabase.pl.\n\r\nstartDatabase.pl script has given this error below:\n\r\nORA-01033: ORACLE initialization or shutdown in progress\n\r\n3) Investigated the startDatabase log and the problem behind this ,missing redo log files on the database server;\n\r\nERROR at line 1:\r\nORA-00313: open failed for members of log group 2 of thread 1\r\nORA-00312: online log 2 thread 1: \'/var/mcp/db/data/mcpdb/redo02.log\'\n\r\n4) I have checked redo logs on the referred directory which is /var/mcp/db/data/mcpdb.\r\nredo02.log and redo03.log were not exist.\n\r\n5) Investigated similar cases on SFDC and found the recovery plan on these cases 120521-337003,120521-337021. I have applied the recovery plan to both EM server and then started the database instances successfully. After that, I have started SM_1 instance as well which was initially reported as a down instance.\n\r\n6) Monitored the system for 10 minutes if it is clear Oracle Broken Job and Oracle Replication Link Deferred Transactions alarms on the DB0 instance and IMDB701 alarm on the SESM1_0 and SESM1_1 instance. \n\r\n7) 10 minutes passed and all the alarms have been cleared by A2 system.\n\r\nI have asked from ER to create follow-up case to understand how these log files are deleted.\r\nAlso customer would like a RCA about high CPU utilization alarm on the SM.\r\nI have given the log collection plan to ER for both cases and then dropped the call.','null'),(333,'Oktay Esgul','AS-OAM','2016-06-11','TBD','OneConnect','Brad Hetzel from ER paged me out to  report SESM overload alarm observed at Oneconnect after an internet outage on site.\n\r\nHe informed that they observed similar issue(160510-580444) which handled by CallP GPS in the beginning of May and issue was recovered by restarting sesm instances.\n\r\nSince the first issue solved by CallP GPS actions, I recommended him to page CallP to get their comments first, and if they request OAM involment page me again.Then we dropped.','null'),(334,'Burak Biyik','AS-OAM','2016-06-08','160607-584286','BSkyB','BSkyB (running EOL release, 8.0 BRC with HP-CC3310s) failed to launch MCP GUI after updating CA-signed TLS/SSL certificates. The error \"Path does not chain with any of the trust anchors\" was being raised. \n\r\nThe customer had reported the same problem last year during certificate renewal and the outage recovered by first applying old certificate from database and then re-applying new ones.\n\r\nInstead of modifying related tables from database, restoring an old db backup was preferred to move the system into its old state with \"certificate expiration\" alarms and launchable GUI. Mostly likely due to high uptime values, EM Servers were responding too slow.\n\n\r\n  [root@ssl-em1 root]$ uptime\r\n 08:46:17  up 370 days, 22:37,  2 users,  load average: 16.08, 15.52, 12.81\n\r\n [root@ssl-em2 ntdbadm]$ uptime\r\n 08:24:31  up 306 days,  9:23,  3 users,  load average: 50.15, 49.97, 47.69\n\r\nDB Restoration failed in the first attempt due to a deadlock in database. Instead of focusing deadlock condition, I tried to get primary db out of this state. First oracle restart and then server reboot did not help to put primary db into stable state.\n\r\nAs a last action, restoring empty db and then restoring old backup worked. At the end, we had a system with launchable MCP GUI and critical cerfiticate alarms. Based on alarm description, there was 1 day left for expiration, so that I asked ER to end pager call since this is not an outage anymore and the re-application of CA-signed certificates can be done with a BC case by reviewing the steps followed.\n\r\nAgreed and dropped the call.','null'),(335,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-06-07','160606-583929','BT MSL','BT MSL reported that they cannot able to launch MCP GUI and cannot make SSH to SM0.Since SM0 server is pingable but unresponsible during the issue occurred, Our A2 system was trying to open MCP GUI over SM0.\n\r\nOn the last occurrence of this issue, I have undeployed and then deployed the SM instances then we monitored the site and cannot see this issue for 3 months.\r\nRCA investigation on the follow-up case was on track and I have specified that since I have seen hardware related errors on the logs below and suspected about network related issue, I have given my action plan to the customer about card replacement and re-installation of the problematic blade(front+rear blade) which is located at 0 0 8 0 position on the ATCA chassis.\n\r\nHardware errors on the kernel logs;\r\nsd 0:0:1:0: SCSI error: return code = 0x08000002\r\nsdb: Current: sense key: Hardware Error\r\nAdd. Sense: No defect spare location available\r\nInfo fld=0x1256cac\r\nend_request: I/O error, dev sdb, sector 19229868\n\r\n2016-06-02T04:02:03.408735+01:00 LDB0A2SM10 SEL: 007f 01/01/70 06:38:31 BMC  0f System Firmware #06 POST Code 6f [c2 08 ff]\r\n2016-06-02T04:02:03.437328+01:00 LDB0A2SM10 SEL: 0080 01/01/70 06:38:31 0033 0f System Firmware #00 POST Error 6f [c2 08 ff]\n\r\nConsequently, I have performed re-installation of the EMS1 from stratch regarding to the MOP I have submitted to the customer, and at the final, all calls were passed successfully.\n\r\nI will monitor this A2 system for next 2 weeks while each SM instance is active and get in touch with customer.','null'),(336,'Oktay Esgul','AS-OAM','2016-06-02','160601-583364','BT MSL','While performing action plan which we provided last night, ER and customer had issues with deactivating/activating blade from ndm.As a results of this, they  could not perform redeploy operation for sm instances.\n\n\r\nThomas paged me out, I connected site and joint to conf bridge with customer to respond their questions.\n\r\nThen, in order to recover I had re-run below commands from NDM \n\r\n==>ha app-blade deactivate 0 0 8 0 ==>Waited for a while,then run following one\r\n==>ha app-blade activate 0 0 8 0\n\r\nThis time, above command worked fine and problematic blade recovered successfully.\n\r\nAs a second step of action plan, I have launched the GUI and redeploy the primary SM instance without any issue. Then, switch over to primary one by stopping the active secondary one.And redeploy the secondary as well.\n\r\nAfterward, customer had run several test calls and all tests were OKEY.\n\r\nEr case will be closed off and a new follow up case will be dispatched to oam queue and we ll prioritize rca investigation to share update asap.\n\r\nThank you','null'),(337,'Oktay Esgul','AS-OAM','2016-06-01','160601-583364','BT MSL','Robert paged me out in order to report SM0 is down at BT MSL which one of known issues of this site that RCA investigation is ongoing. \n\r\nWhile he is trying to connect site,customer have informed that any maintenance activity is not permitted till 2:00 AM local time.So that, we have stopped working on the issue.\n\r\nThey ll page us again at 2:00 am to proceed with workaraund solution,however gps engineer who is working on rca investigation will participate maintenance window to investigate further.\n\r\nThank you','null'),(338,'Senem Gultekin','AS-OAM','2016-05-15','160514-581247','Bell Aliant','Problem Description:\r\nER paged OAM GPS for a SM service IP issue.\r\nCMT and SPFS GPS were working on a provisioning issue seen on their side, and they requested assistant from A2 OAM GPS to reach 10.92.126.14 (SM Service) IP.\r\nA2 Release: 18.0.20.x\n\r\nSolution:\r\nAccessed to the site over VMs, we were not able to open MCP GUI. ER mentioned a case(160415-577190) where this issue was seen before.\r\nWe have swacted SMs from the server and MCP GUI was able to launch back again.\r\nSPFS GPS was able to reach the SM service IP as well. They have continued with their work.\r\nSince MCP GUI is not reachable for the second time, there should be a follow up case for this issue.\r\nDropped the call.','null'),(339,'Tugkan INCE','AS-OAM','2016-05-15','TBD','Axtel','I was paged by ER regarding the verification of Hot-Standby SESM recovery actions including linux platform installation and application deployment for secondary sesm server which had disk issues the previous day.\n\r\nI have requested some data from the server from Dean and evaluating the results, we have verified that the server is installed successfully and the instance itself is running without any issue with an operational state of Hot_Standby.\n\r\nHowever checking the other servers in the system we have analysed that Axtel needs to platform patch their other servers to the 17.0.19 ple2 patch to provide consistency with the newly installed one and allign with the platform version in the loadlineup.\n\r\nAfter specifying the next action plan, I dropped the call.','null'),(340,'Senem Gultekin','AS-OAM','2016-05-14','160514-581237','Axtel','Problem Description:\r\nER paged OAM GPS for a disk issue seen at SESM server. Customer reported that they have removed sda but coulndt boot up the server again.\r\nMCP Release: 17.0.22.6\r\nServer Type: HP CC3310\n\r\nSolution:\r\nPerformed the following actions to determine the actual reason.\n\r\n-	Swap sda and sdb\r\n-	Boot up the server only with sda\r\n-	Boot up the server only with sdb\r\n-	Replace sda with spare disk\r\n-	Swap spare disk and sdb \n\r\nNone of the above actions worked, server was not able to boot up with the following failure;\n\r\npartition error:  partition table invalid or corrupt\n\r\nBoth sda and sdb disks were problematic, requested from the customer to use two new disks and reinstall the operating system on it.\r\nProvided customer the installation procedure for CC3310 and the needed ordercode (A2I0M170.170.R.NCL.NAP.NPV.12.D.tar)\r\nThey will perform the installation in the next maint window with ER online.\n\r\nEnded the call.','null'),(341,'Tugkan INCE','AS-OAM','2016-05-12','160512-580806','Iberbanda Madrid','I was paged by software David Bartlett from Software Delivery team for a problem experienced after DB Mock Upgrade Attempt on Iberbanda Madrid site for the upgrade path 17.0.7.14 to 17.0.22.16.\n\r\nDavid reported that DB Mock Upgrade took longer than 1.5 hours and the screen failed with the timeout error.\n\r\nI explained him that the DB Mock Upgrade failures were not under pager support coverage. However he said that the reason why he called was not to pass the screen but to recover the site since after db mock upgrade started, dbcomm alarms are raised for all the instances located on EMServers.\n\r\nSo I connected to the site and analysed that the instances were raising alarms only for secondary db instance. And db mock upgrade operation session appeared to be hung. \n\r\nAlthough the current status of the system was not service impacting, Since I was connected, I took a look at the secondary EMServer and analysed that secondary database was not functional. So I restarted the secondary database and waited for DBMN102 and DBMN 826 alarms to clear.\n\r\nDBMN102 alarm cleared in seconds after db restart however it took around 20-30 minutes for DBMN826 alarm to clear due to queued replication operations.\n\r\nAfter the alarms are cleared, we agreed on leaving the pager call and start second db mock upgrade attempt within the next maintenance window as part of the upgrade since the maintenance window was closed.','null'),(342,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-04-28','160428-578878','Princeton University','Problem Description :\r\n=====================\n\r\nI have been paged by Chris Henwood due to upgrade fails at PrepareDB screen on Upgrade Wizard.\r\nUpgrade Path: 17.0.22.8 to 17.0.22.16\n\r\nActions Taken :\r\n==============\n\r\nI have taken the Teamviewer access from SWD and investigated the upgrade wizard logs.\r\nWizard was failing when taking the db backup on the PrepareDB screen.\r\nThe error message was referring that below;\n\r\nERROR: To restrict multiple DB scripts from being executed simultaneously,a Lock is required for the execution of this script.  With that, this scrip cannot be executed at this time because the Lock has already been acquired by  dbBackupAndFtp.pl.\n\r\nI have executed ps ef  command in order to find and kill the running process which is dbBackupAndFtp.pl.\r\nKilled the process manually and clicked retry button on the UW but this time it has failed due to timer is expired.\n\r\n> run err Error! Timer expired while executing: perl\r\nUnlinked /var/mcp/upgrade_tools/work/dbBackup//init_ned_cmd_20160427_214855_HA36\r\nNot unlinking file\n\r\nAfter investigated the issue on SFDC , there is a lot of cases similar to this issue. \r\nRestarted the primary database server then I was able to complete prepareDB screen on UW.\r\nI have requested to attach the UW logs to the case from SWD.\r\nI will create a JIRA to this issue and link to the case to prevent occurrence once again.','null'),(343,'Oktay Esgul','AS-OAM','2016-04-21','160421-577890 ','Bell Aliant','Josh Kishner from ER paged me out in order to report Bell Aliant patch upgrade failed just before pause point. Customer was performing on their own instead of swd.\n\r\nFirstly, we though that there was a monitoring issue ,yet when we checked the system again ,we noticed that FPM instances were down somehow.\n\r\nUpgrade Path :\n\r\n	From Load:	MCP_18.0.20\r\n	To Load  :  MCP_18.0.25\n\r\nBoth of FPM instance were bouncing and going down afterward ~10 sec become active.\n\r\nHere is the actions\n\r\n1.Redeploy the instances and start, they failed again.\r\n2.Checked the work logs ,notice there are memory related errors generated asa instance activated like below.\n\r\n	maybeGrowTheCache\r\n	Thread id : 1\r\n	Thread stack trace : \r\n		java.lang.Thread.getStackTrace(Unknown Source)\r\n		com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionCacheImpl.maybeGrowTheCache(DBConnectionCacheImpl.java:118)\r\n		com.nortelnetworks.mcp.ne.base.db.fw.DBConnectionCacheImpl.connectionCacheParametersUpdated(DBConnectionCacheImpl.java:71)\n\r\n	|\r\n	|\n\r\n	java.lang.OutOfMemoryError: Java heap space\r\n	at com.nortelnetworks.mcp.ne.share.format.ossdirmgmt.OSSDirectoryManager$OSSReadOnlyRecordable.(OSSDirectoryManager.java:602)\r\n	at com.nortelnetworks.mcp.ne.share.format.ossdirmgmt.OSSDirectoryManager.createReadRecSys(OSSDirectoryManager.java:280)\n\n\r\n3.In order to check the system stable with old load, redeployed the instances with 18.0.20.0 , they worked without any issue.This test proved that \r\nwe have application problem at 18.0.20.5\n\r\n4.Then, as a workaraund , I have increased java memory size at jvm conf file like below.\n\r\n	a.vi /var/mcp/run/MCP_18/FPM1_0/data/jvm.conf\r\n	b.Modify the memory sizes:\r\n		Before:\r\n				         #GC Options\r\n                   OPTION=-XX:NewSize=32m\r\n                   OPTION=-XX:MaxNewSize=32m\r\n                   OPTION=-XX:MaxTenuringThreshold=0\r\n                   OPTION=-Xms256m\r\n                   OPTION=-Xmx256m\r\n                   OPTION=-XX:+UseConcMarkSweepGC\r\n                   OPTION=-XX:+UseParNewGC\r\n                   OPTION=-XX:+PrintClassHistogram\r\n                   OPTION=-XX:CMSInitiatingOccupancyFraction=68\r\n                   OPTION=-XX:+UseCMSInitiatingOccupancyOnly\r\n                   OPTION=-XX:+UseMembar\r\n		After:\r\n				         #GC Options\r\n                   OPTION=-XX:NewSize=32m\r\n                   OPTION=-XX:MaxNewSize=32m\r\n                   OPTION=-XX:MaxTenuringThreshold=0\r\n                   OPTION=-Xms512m\r\n                   OPTION=-Xmx512m\r\n                   OPTION=-XX:+UseConcMarkSweepGC\r\n                   OPTION=-XX:+UseParNewGC\r\n                   OPTION=-XX:+PrintClassHistogram\r\n                   OPTION=-XX:CMSInitiatingOccupancyFraction=68\r\n                   OPTION=-XX:+UseCMSInitiatingOccupancyOnly\r\n                   OPTION=-XX:+UseMembar\n\r\nAfter memory size increased, both of instances became active/hotstandby as expected with 18.0.20.5 load. Customer performed the test calls and everything passed.\n\r\nWe ll follow up the issue to figure out if there is any new changes that may cause memory issues at 18.0.20.5 patch.\n\r\nThank you','null'),(344,'Senem Gultekin','AS-OAM','2016-04-13','160412-576727 ','Axtel','Problem Description:\n\r\nER paged OAM GPS for SM0 unreachable issue. Also MCP GUI was not reachable.\n\r\nCustomer is running on 17.0.22.6. Server type HP-CC3310 (17.0.10)\n\r\nSolution:\n\r\n-	Ive accessed to the site over VPN, however I wasnt able to ssh to EM0 and I was not able to launch MCP GUI.\r\n-	I was able to ssh to  EM1, it had high CPU usage and  over %80 partition usage /opt directory. Ive decreased usage to %72 by deleting old traces.\r\n-	Talked with Enrique Leija from customer. Ive logged into his local PC via teamview. MCP GUI was up, and both EM servers were reachable. Explained to him that this issue is not A2 related to and it seems like a problem for the people who accessing over Axtel VPN.\r\n-	Enrique and site engineers performed other tests.\r\n-	Later on realized that qdn and qsip responses were very slow.\r\n-	Rebooted EM1 server which had high CPU usage (around %95)\r\n-	However after reboot nothing changed.\r\n-	Found an old solution(00010966) for HP-CC3310 servers. However after restarting IMS, CPU get back up to %95 usage.\r\n-	Stopped IMS at last, SM1 instance up and running properly, qsip and qdn commands are working properly as well. IMS service will stay stopped for now, it doesnt seem that has an impact to the system for so far. All tests successful.\r\n-	Customer needs to plan a maint window to reinstall EM1 server.\n\r\nIm also sharing the old solution for the CPU spikes for reference. \n\r\n00010966 - SOLUTION\n\r\n=============================== \r\nProblem Description \r\n=============================== \r\nWith HP-CC3310 servers , it is seen CPU utilization issues and CPU gets over %80 or %90. When the processes are checked it is observed that dmi2snmpd process is using CPU at most and causing the problem. \n\r\ndmi2snmpd process is specific to CC3310 servers and a known issue by HP. This process is part of the suite of ISM services (Intel Server Management) that allows the system to provide the system information from the DMI data with SNMP to the management system. The DMI data is defined as the information provided from the System Monitor BIOS (SMBIOS) that is an extension of the BIOS and represents a logical instance that works as an interface between the System Components with the Operating System. \n\r\n=============================== \r\nProblem Resolution \r\n=============================== \n\r\n*** Please be aware that these actions should be run in a maintenance window**** \n\r\n1. Login as root on the Server in trouble. \r\n2. Validate Status of Services currently running (service --status-all) \r\n3. Get a copy of temp directory and all its content. \r\n4. Remove the entire /var/dmi/temp directory \n\r\n[root@vop-sslm-bem root]# cd /var/dmi \r\n[root@vop-sslm-bem dmi]# ls -lrt \r\ntotal 8476 \r\n-rwxrw-rw- 1 root root 1325 Aug 23 2000 dmi2snmpd.conf \r\ndrwxrwxrwx 3 root root 4096 Nov 3 2009 mifs \r\ndrwxrwxrwx 2 root root 4096 Nov 3 2009 mifdb \r\ndrwxrwxrwx 2 root root 4096 Oct 22 13:31 bin \r\ndrwxrwxrwx 2 root root 8646656 Oct 25 10:58 temp \r\n[root@vop-sslm-bem dmi]# cd temp \r\n[root@vop-sslm-bem temp]# rm * \r\n[root@vop-sslm-bem temp]# cd .. \r\n[root@vop-sslm-bem dmi]# rmdir temp \n\r\n5. Stop ISM/SNMP suit of services as below sequence (Alarms are expected to be seen on the MCP GUI at the moment due to lack of SNMP agent conflict): \n\r\n-service dmi2snmpd stop \r\n-service ism stop \r\n-service dmispd stop \r\n-service portmap restart \n\r\n6. Recreate the /var/dmi/temp directory considering the appropriate owner and group defined (chown root:root /temp) and the appropriate rights over the directory (chmod 777 /temp) \n\r\n[root@vop-sslm-bem root]# cd /var/dmi \r\n[root@vop-sslm-bem dmi]# mkdir temp \r\n[root@vop-sslm-bem dmi]# chmod 777 temp \r\n[root@vop-sslm-bem dmi]# chown root:root temp \n\r\n7. Start the ISM/SNMP suit of services as per the below sequence: \n\r\n-service dmispd start \r\n-service ism start \r\n-service dmi2snmpd start \r\n-service portmap stop \n\r\n8. Validate Status of Services currently running (service --status-all). \n\r\nNOTE: Alarms might take ~20 mins to be cleared from the MCP GUI after ISM/SNMP services are restarted. \n\r\n9. Monitor CPU usage and performance of the Server \r\n===============================','null'),(345,'Yunus Ozturk','AS-OAM','2016-03-28','160327-574326','Shaw CableSystems','Problem Description:\r\n====================\n\r\nER paged out GPS to get assistance for ATCA Blade re-installation activity\n\r\nActions Taken:\r\n===============\n\r\n- On the previous pager, we have advised the following actions to the customer;\n\r\n- Burn the iso file that we have already provided into a DVD and insert the DVD into an extenal DVD drive\r\n- Prepare a terminal server access for the problematic blade through NDM\r\n- Replace the front blade first to see if it will fix the issue. If it works fine, no need for re-installation\r\n- If front blade replacement does not fix the issue, replace the RTM (disc) of the blade which will require fresh Operating system installation \r\n- Re-install the Operating system of the Host Server\r\n- Manually create Guest Servers (PA2_0, AM2_0, SESM3_0, and SESM4_0)\r\n- Re-install the Operating systems of the Guest Servers with the same DVD\r\n- Deploy and start the instances on MCP GUI\n\r\nBased on these action items, customer first replaced the front blade and this action fixed the server reboot problem. However, this time we had network connection problem on this blade. \n\r\nAs per our investigations, we found out that wrong MAC address mappings were observed after rebooting the server. The expected interfaces should be expected as eth6 / eth7 and their MAC addresses. However, we have seen that eth16 / eth18 interfaces and their MAC addresses have been mapped to the bond0. We have tried to modify the ifcfg-eth files under under /etc/sysconfig/network-scripts/ directory along with the files under /etc/udev/rules.d/ directory but we were not able fix the issue.. Afterwards, we have rebooted the server again and the interface connections and MAC address mappings have been corrected.\n\r\nAfter recovering the network connection, we have modified the BIOS settings of the server and disabled the O/S Watchdog parameter since the front blade is replaced.. \n\r\nLastly, we have deployed/started the instances..','null'),(346,'Yunus Ozturk','AS-OAM','2016-03-27','160327-574326','Shaw CableSystems','Problem Description:\r\n====================\n\r\nER paged out GPS for a problematic ATCA blade problem. \n\r\nThe card 0-0-10-0 was continually going into a poweroff status, and was bouncing. The SCX links that are reported in the alarms are to card 0-0-10-0. The PA2_0, AM2_0, SESM3_0, and SESM4_0 are all in a \"online/down/unavailable\" state on MCP GUI. \n\r\nActions Taken:\r\n================\n\r\n- Asked the customer to re-seat the problematic blade physically on the site.\r\n- Customer sent a technician to the site to re-seat the blade but this did not fix the issue. We have seen solid red led lights on the blade itself which was an indication of bad hardware.\r\n- Asked the customer to replace the front blade first and then RTM if it does not fix the issue.\r\n- If RTM changes, the blade will have to be installed from scratch. So that, we asked the customer if they have A2D ple4 installation DVD for this re-installation activity. \r\n- As customer did not have this DVD, we had to find out the 18.0.2 ple4 installation iso file and sent it out to the customer. This part took longer as we could not find out the required iso file for some time. \r\n- After finding this iso file, we have asked the customer to upload this file to NDM server after creating the /opt/corp/a2 directory manually as we though that this was an ATCA Nehalem hardware. \r\n- Then, after checking out the specbook of the customer, we noticed that customer was using SB-ATCA Emerson hardware instead of ATCA Nehalem hardware.\r\n- Then we have changed the action items and asked the customer to burn the iso file into DVD as the ATCA Emerson blades are being installed with the DVD, not through NDM. \r\n- When we have tried to reach the problematic ATCA Emerson blade, we were not successful. So that, the problem was on the Host Server and therefore the Guest Servers running on this Host Server were down (PA2_0, AM2_0, SESM3_0, and SESM4_0)\r\n- Following actions have been provided to customer for recovery;\r\n- Burn the iso file that we have already provided into a DVD and insert the DVD into an extenal DVD drive\r\n- Prepare a terminal server access for the problematic blade through NDM\r\n- Replace the front blade first to see if it will fix the issue. If it works fine, no need for re-installation\r\n- If front blade replacement does not fix the issue, replace the RTM (disc) of the blade which will require fresh Operating system installation \r\n- Re-install the Operating system of the Host Server\r\n- Manually create Guest Servers (PA2_0, AM2_0, SESM3_0, and SESM4_0)\r\n- Re-install the Operating systems of the Guest Servers with the same DVD\r\n- Deploy and start the instances on MCP GUI\n\r\nThe site is scheduled to contact ER tonight @2300MDT, 0100EDT for the listed actions above for the recovery of the blade 0-0-10-0. \n\r\nCustomer requested online assistance for the actions above. So that, ER/GPS will be assisting the customer if needed.','null'),(347,'Yunus Ozturk','AS-OAM','2016-03-24','160324-574018','Canadian Imperial Bank of Commerce (CIBC)','Problem Description:\r\n=======================\n\r\nThe upgrade wizard failed on step \"Upgrade Database and System Manager\"\n\r\nUpgrade Path : 17.0.18.2 to 17.0.22.15 \n\r\nActions Taken:\r\n==================\n\r\n- Checked the wizard logs and noticed the following errors for the failed screen;\n\r\nERROR PanelController - Operation had been failed : 2016-03-23 21:53:39\r\nValidating the scripts and files required for upgrade.\r\nUpgrading the Database\r\nDeploying Files to Primary DB\r\nUpdating schemas in the Primary DB\r\nError at Updating schemas in the DB.\r\nUpdating schemas in the DB FailureDB & SM upgrade failed.\n\r\n- Checked the dbInstall logs and noticed the following errors;\n\r\nUpdating schemas in the DB...\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/dbInstall/startCmds.txt.20160323_205856 | /opt/mcp/ned/bin/nedclient 10.240.0.24 4890\r\nCommand Output:\r\n> config ok\r\n> run output \r\n> run output  DbSetup.pl   Started at  =>  Wed Mar 23 20:59:27 2016 by ntdbsw\r\n> run output \r\n> run output \r\n> run output Could not chdir to directory test:\r\n> run output \r\n> run output  ERROR: DbSetup.pl Terminated at  =>  Wed Mar 23 20:59:27 2016\r\n> run output \r\n> run output      Reference the following log file for additional details:\r\n> run output       /var/mcp/run/MCP_17.0/mcpdb_0/installLogs/DbSetup_17.0.22.15_2016-02-05-1141.log\r\n> run output \r\n> run ok 1\r\n> exited\n\r\nError occurred executing NE commands (see below):\n\r\nNE command exited with the value: 1\n\r\nError at Updating schemas in the DB.\n\r\n- Restarted the NED (neinit restart) on both EM/DB Servers but this did not help out\r\n- After some further investigation, found out that someone manually created a directory named as \"test\" with \"root:root\" permissions under \"/var/mcp/run/MCP_17.0/mcpdb_0/bin/util/\" directory on Primary DB Server and this was causing the problem as the /var/mcp/run/MCP_17.0/mcpdb_0/bin/util/ directory and its sub-directories are working with the permissions \"ntdbsw:ntdbgrp\"\r\n- When we checked the content of this \"test\" directory, we have seen the \"restoreConfigData.pl\" script inside of it. This script should actually be located under \"/var/mcp/run/MCP_17.0/mcpdb_0/bin/util/\" directory. So that, we did not understand why this \"test\" directory is manually created and why the \"restoreConfigData.pl\" script is located under this directory. \r\n- We thought that it might have been created before as a part of the L3 Backup/Restore Procedure from MCC to SCC site.\r\n- To fix the issue, we have changed the permissions of this \"test\" directory as \"ntdbsw:ntdbgrp\" and retried the same screen and it passed out successfully.\r\n- We will be discussing the reason of this manually created \"test\" directory with the customer and asking them to remove this directory permanently.','null'),(348,'Burak Biyik','AS-OAM','2016-03-19','160319-573381','NUVIA','SWD paged me to report that FPM instance failed to be upgraded in 20th screen of the wizard (Upgrading Primary NE Instances and Secondary Instances of Redundant NEs) for the 10.4 patch upgrade (17.0.22.10 --> 17.0.22.16).\n\r\nWhen SWD retries the screen, the following output was observed in the wizard window. The wizard seemed to deploy the FPM1_0 instance with the old load even if the first attempt (expected) was done for the target/new load.\n\n\r\nFPM1_0: Maintenance State: None -> Stopping \r\nFPM1_0: Operational State: Active -> Deactivating \r\nFPM1_0: Operational State: Deactivating -> Shutdown \r\nFPM1_0: Admin State: Online -> Offline \r\nFPM1_0: Maintenance State: Stopping -> None \r\nFPM1_0: Operational State: Shutdown -> Unavailable \r\nFPM1_0: Connection State: Down \r\nFPM1_0: Admin State: Offline -> Configured \r\nFPM1_0: Load Name Change: MCP_17.0.22.16_2016-02-12-1214 \r\nFPM1_0: Maintenance State: None -> Deploying \r\nFPM1_0: Load Name Change: MCP_17.0.22.10_2015-09-17-1702 \r\nFPM1_0: Admin State: Configured -> Offline \r\nFPM1_0: Maintenance State: Deploying -> Starting \r\nFPM1_0: Admin State: Offline -> Online \r\nFPM1_0: Maintenance State: Starting -> None \r\nFPM1_0: Operational State: Unavailable -> Initializing \r\nFPM1_0: Connection State: Up \r\nFPM1_0: Operational State: Initializing -> Synchronizing \r\nFPM1_0: Operational State: Synchronizing -> Hot Standby \r\nFPM1_0: Operational State: Hot Standby -> Activating \r\nFPM1_0: Operational State: Activating -> Active \n\r\nI launched the wizard from my local PC and this behaviour did not show up anymore. FPM1_0 was deployed/started with the new load and the screen was passed successfully.\n\r\nThe case (160319-573381) will be used for tracking the RCA of this odd behavior of the wizard. I asked SWD to attach wizard logs to the case for further investigation.\n\r\nAgreed and dropped the call.','null'),(349,'Kemal AYDEMIR (NETAS External)','AS-OAM','2015-03-12','160312-572435','Liberty Global','Problem Description :\r\n=====================\n\r\nI have been paged by ER due to A2 upgrade fails at PrepareDB screen on Upgrade Wizard.\r\nUpgrade Path: 10.2 to 10.4\r\nER has created a conference call with customer and I joined.\n\r\nActions Taken :\r\n==============\r\nI have connected to VM and investigated the upgrade wizard/cleanupreplication/prepareDB/ned  logs on the EM1S1 server.\n\r\nPrepareDB logs below show that cleanupReplication.pl script timed out while executing based on the ned daemon error.\n\r\n> run err Error! Timer expired while executing: perl \r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20160312_013452_uAPm \r\nNot unlinking file \r\n> config ok \r\n> run err Error! Timer expired while executing: perl \r\n> exited\r\nFailed to cleanup replication on the server 80.110.1.49\n\r\nI have recommended to customer these actions below to recover this issue;\r\n1)Save&exit on the upgrade wizard \r\n2)Executing neinit restart on the EMS1.\r\n3)Run this cleanupReplication command below manually on the EMS1 which is executing on prepareDB screen by the upgrade wizard ;\n\r\n/var/mcp/upgrade_tools/bin/ut_cleanupReplication.pl -nc -m EMS1_22421994-2b14-1b21-889a-001b218864f8_ut_prepareDB.pl_PREPARE_DB_0 -s 80.110.1.49\r\nAfter executed this command above manually, Replication has dropped between databases successfully.\r\n4)Open the upgrade wizard again.\n\r\nThen Customer has passed prepare DB screen successfully and we all agreed to drop to call.','null'),(350,'Kemal AYDEMIR (NETAS External)','AS-OAM','2015-03-11','160311-572264','University of Alabama at Birmingham','Problem Description :\r\n=====================\n\r\nI have been paged by SWD due to PROV1_0 is remained stuck at configured state although has been upgraded to new load when upgrading primary network elements screen on the upgrade wizard.\r\nWizard did not fail, but never progressed. \n\r\nUpgrade path=> FROM/TO LOAD: 17.0.22.8 / 17.0.22.15 \n\r\nSWD told me that he had performed deploy operation about PROV1_0 on MCP GUI but did not take response. Also he had performed manuel stop script which is neStop.pl but this action failed with the following error below;\n\r\nError occurred stopping PROV1 \r\n> stop err Error! Could not stop instance PROV1_0@MCP_17.0 (1581): Unknown error 4294967295 \n\n\r\nActions Taken :\r\n==============\n\r\nPROV1_0 was deployed on EM1Server1.\r\nSince I am suspecting with the error output that can be ned daemon issue on the EM1Server1,I have executed \"neinit restart\" command first of all.\r\nThis action has been resolved the issue and I was able to perform mtce operations on the MCP GUI.\r\nI have killed PROV1_0 then deployed and started it.\r\nWizard validated that screen and we agreed to drop the call.','null'),(351,'Kemal AYDEMIR (NETAS External)','AS-OAM','2015-03-09','160309-571852','Liberty Global','Problem Description :\r\n=====================\n\r\nI have been paged by ER due to SESM3_1 server could not be up after reboot.\r\nThe site was preparing to upgrade for an upcoming upgrade and noticed the SESMs were running 900+ days of uptime.\r\nER has created a conference call with customer and I joined the call.\n\r\n*Site is running with 8.0 SP1 A2 load.\r\n*SESM3_1 server was standing on  1 0 10 0 slot.\n\n\r\nActions Taken :\r\n==============\n\r\nI have tried to ping SESM3_1 server and could not get response.\r\nThen I have performed these actions below to make the server UP again;\n\r\n1)Attempted to hard reboot on 1 0 10 0 from NDM then we have observed these output below and could not get any more response on the terminal server.\n\n\r\nCLIENT MAC ADDR: 00 D0 1C 07 C5 24  GUID: FFFFFFFF FFFF FFFF FFFF FFFFFFFFFFFF  \r\nCLIENT IP: 172.22.4.104  MASK: 255.255.255.0  DHCP IP: 172.22.4.59 TFTP.                                                                           \r\nPXE-T01: File not found                                                         \r\nPXE-E3B: TFTP Error - File Not found                                            \r\nPXE-M0F: Exiting Intel Boot Agent.  \n\r\nIntel(R) Boot Agent GE v1.3.27.1 (GE-IP)                                        \r\nCopyright (C) 1997-2008, Intel Corporation                                                       \r\nIntel(R) Boot Agent PXE Base Code (PXE-2.1 build 086)                           \r\nCopyright (C) 1997-2007, Intel Corporation                                      \n\r\nInitializing and establishing link...                                          \n\n\r\n2) Attempted to re-seat the blade. After re-seated the blade, server was accessible and up but then somehow after a few minutes,it remained stuck and pings are failed.\r\nAt this point, I have performed hard reboot on the 1 0 10 0 slot from NDM server.Pings were stable on the server after this action.\n\r\n3) I have connected to MCP GUI and killed&&started the SESM3_1 instance and recovered it.SESM3_0 is active, SESM3_1 is hot-standby now.\n\r\nSince the A2 system is running with EOL 8.0 SP1 load, I have informed ER about not to create RCA case for this hardware issue.\n\r\nWe agreed and dropped the call.','null'),(352,'Oktay Esgul','AS-OAM','2016-02-25','160224-570005','Jet Infosystems','Alexander requested collaboration for 160224-570005 KZT Astana :Upgrade wizard stuck on step 27: SESM1_0 bouncing case. \n\r\nUpgrade Path:\n\r\nFrom : MCP_17.0.18.5_2015-02-051611\r\nTo :   MCP_17.0.22.15_2016-02-05-1141\n\r\nEven though SM/DB seems upgrade successfully , sesm was bouncing with below exceptions while initializing.\n\r\n############################\r\nSubsystemManager Failed: VoiceMailSubsystemManager\r\ncom.nortelnetworks.mcp.ne.base.parm.ParmNotFoundException: com.nortelnetworks.mcp.base.collections.NotFoundException: key does not exist:\r\nHistoryInfo.InitiateVoicemailIfHIExistsParm Name: HistoryInfo.InitiateVoicemailIfHIExists\r\n|\r\n|\r\nSubsystemManager Failed: AccntSubsystemManager\r\ncom.nortelnetworks.mcp.ne.base.parm.ParmNotFoundException: com.nortelnetworks.mcp.base.collections.NotFoundException: key does not exist:\r\nAccounting.UseUserIdentityOnAccountingParm Name: Accounting.UseUserIdentityOnAccounting\r\n|\r\n|\r\nSubsystemManager Failed: CD3SubsystemManager\r\ncom.nortelnetworks.mcp.ne.base.parm.ParmNotFoundException: com.nortelnetworks.mcp.base.collections.NotFoundException: key does not exist:\r\nMobileConverged.WaitingResponseTimeParm Name: MobileConverged.WaitingResponseTime\r\n	at com.nortelnetworks.mcp.ne.base.parm.ParmManager.getParm(ParmManager.java:50)\r\n############################\n\r\n1.I have connected MCP gui and verified that SM/DB is running at correct load.\r\n2. Then checked the upgrade wizards logs and observed that sm/db upgrade screen retried 2-3 times and latest attempt was succesffull.Yet, this made us suspicious that there might some manual action when the issue was observed first.(I have checked history of the server and observed several smStart/smStop commands).\n\r\nHere is the first failure at sm/db upgrade screen:\n\r\nm.nortelnetworks.mcp.ne.sm.base.dbio.fw.SMUpgrade MCP_17.0.22.15_2016-02-05-1141 > /var/mcp/run/MCP_17.0/SM_0/work/SMUpgradeOut_X_ss failed\r\nUnlinked /var/mcp/run/MCP_17.0/SM_0/work/SMUpgradeOut_X_ss\r\n        Unable to upgrade the Engineering parameters in the database\n\r\nSM upgrade unsuccessful. Unable to upgrade the Engineering parameters in the database\n\r\nTerminated at  =>  Thu Feb 25 01:47:01 2016\n\n\r\nThen we decided to add missed config paramaters to db manually in order to complete the upgrade successfully.\n\r\nHere is the list of missed config parameters:\n\r\n HistoryInfo.InitiateVoicemailIfHIExists\r\n Accounting.UseUserIdentityOnAccounting\r\n MobileConverged.WaitingResponseTime\n\r\nAfterward adding this parameters,sesm initialized successfully and became hotStandby as expected.\n\r\nCustomer will continue upgrade to complete the rest of steps.\n\r\nThank you','null'),(353,'Oktay Esgul','AS-OAM','2016-02-15','160215-568315','Teleset','Alexander Orlow paged me to report that upgrade wizard stuck at SM/DB  upgrade screen.\n\r\nUpgrade Path: \n\r\nFrom Load:MCP_14.0.16.5.2014.04.25.1212\r\nTo Load: MCP_17.0.22.15.2016.02.05.1141\n\r\nIt took too long to connect to site due to network issues at customer side.Afterward connection established,I have checked the logs and observed that wizards had completed the screen succesfully.Yet, even though screen success as the secondary sm has ned issue,wizards seems could not swact secondary sm to primary one that is why failing as correct sm instance is not active.\n\r\nAs a corrective action, I have swacted SMs manually and wizard skipped the screen. \n\r\nThen, customer kept continued the ugprade.','null'),(354,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-02-03','160129-566197','Bermuda Telephone Company','Problem Description :\r\n=====================\n\r\nI have been paged by Haydee Hernandez due to write failure issue on the SM0.On this case CallP GPS was already working on an issue which is unable to register any new SIP line customers.\r\nThe site was running on 17.0.7.13 A2 load and 17.0.7 platform release level on 3310 server.\n\r\n[root@HMTNBMSEM0 mcp]# mcpRelease.pl \r\n        *** MCP Platform Release ***\r\nSystem Type:     mcp_core_linux\r\nRelease Level:   17.0.7 (via patching)\r\nHardware Env:    HP-CC3310\n\r\nActions Taken :\r\n==============\n\r\nI have connected to the site and looked SM_0 instance alarms on MCP GUI.It was raising write failure issue due to /var/mcp partition is reached 100%.\n\r\nAlarmName: Write failure\r\nTimeStamp: Wed Feb 03 16:19:49 EST 2016\r\nFaultNumber: 213\r\nShortFamilyName: RECS\r\nLongFamilyName: RECSYS\r\nSeverity: MAJOR\r\nProbableCause: file error\r\nDescription: System /var/mcp/oss/om/SM/OTP/csv/PROV1_0 could not write to the active file.\r\nReason: java.io.IOException: No space left on device\r\nCorrective Action: Check directory permissions or partition free space.\n\r\n[root@HMTNBMSEM0 root]# df -k\r\nFilesystem           1K-blocks      Used Available Use% Mounted on\r\n/dev/md4               2063440    933876   1024748  48% /\r\n/dev/md1                101018     15147     80655  16% /boot\r\n/dev/md5               6190592   2051492   3824636  35% /opt\r\nnone                   2049696         0   2049696   0% /dev/shm\r\n/dev/md6               5162728     74592   4825880   2% /var\r\n/dev/md7              52711412  50033784         0 100% /var/mcp\r\n/dev/md0                101018      4206     91596   5% /admin\n\r\nI have been informed by CallP GPS that DEBG logs have been left verbose on the SM0 and I think this problem is related with spool partition at first.\n\r\n-There was 275 directories, 6564 files spool files.\n\r\nAlthough I delete the spool files on EM server0,still /var/mcp partition is full.\r\nAt this point, I have determined to look each partition under /var/mcp.\n\r\nAfter my analyze on the output , I have clarified that the issue is originated by server backups which is the known problem on HP-3310 server type.[ 8.9G upgrade_bkups ]\n\r\nI have recommended to GTS that this backups should be stored on the DVD or external hard drive.\r\nAfter pre/post backups are stored, These backups can be deleted.\r\nI have requested to open a new case for this issue and we will continue to monitor this system for a week.\n\r\nWe agreed and dropped the call.','null'),(355,'Senem Gultekin','AS-OAM','2016-01-30','160130-566304','Telus','Problem Description;\n\r\nER paged A2 OAM pager for a provisioning issue seen on ossgate side. Customer Release: 10.4.\n\r\nError was;\n\r\nEndPoint can not be added to GateWay. SystemGWEMProxy; EndPoint data can not be added to GateWay.; Reason Problem encountered when processing request on MCS-EM for GWEP VMGDNNCCA01SEAF/000/0/0593 Details Username \"u5147874938\" matches an existing username, directory number, user alias or service alias. Please enter another value. > on Switch:DONNACONA\n\r\nSolution:\n\r\n-	Asked ER to check if both PROVs are up on MCP GUI.\r\n-	Asked ER to check the problematic user to search from Provisioning Client. He couldnt find it. Which means it does not exist on A2 database.\r\n-	Needed more information from customer. How they are removing users, do they have existing user in core side, and are they seeing this issue for every provisioning.\r\n-	ER agreed and end the phone call with me to talk with customer for more details, he was supposed to page me back if he needed more help.\r\n-	From the case notes it seems issue was recovered by working with CMT/OSSGATE GPS with ptmctl restart on their side.','null'),(356,'Senem Gultekin','AS-OAM','2016-01-27','160127-565675','OPTUS','Problem Description:\n\r\nER paged me for 200 missing subscribers at Optus after a resync procedure. Customer is running on 17.0.22.7 Release.\n\r\nProblem Description:\n\r\n-	Discussed with ER about the history. Customer had database related alarms and learned that they have ran Resync from Secondary to primary instead of Primary to Secondary. Since secondary database did not have the newly provisioned data,  reverse resync caused 200 missing subscriber issue.  \r\n-	Accessed to the site and checked their latest backup, it belongs to 20 January. Also verified from the logs that they have ran the resync from secondary to primary. Provided action plan to customer, they will perform it within today on their maint window;\r\n-	Here are the main steps that they will perform.\r\n1-	Drop Replication\r\n2-	Restore database backup\r\n3-	Setting up the replication\n\r\n-	Customer will apply the steps and if anything goes wrong they will contact ER and GPS again.','null'),(357,'Kemal AYDEMIR (NETAS External)','AS-OAM','2016-01-04','160102-562115','Unitymedia','Problem Description :\r\n=====================\n\r\nI have been paged by Thomas Godwin due to inactive SESM11(SESM2_1 instance) unable to access to server and hot-standby unit is unavailable.\n\r\nSystem was running on 10.2 AS load.\n\r\nER reported that which at least restored the SSH access but then it was flapping between Unavailable to Initializing in MCP GUI (closed case 151221-561273).\r\nER also reported that he can able to ping this SESM instance.\n\r\n[ntsysadm@sslsesm10 ~]$ ping 80.69.110.11 \r\nPING 80.69.110.11 (80.69.110.11) 64(92) bytes of data. \r\n72 bytes from 80.69.110.11: icmp_seq=1 ttl=64 time=1.14 ms \r\n72 bytes from 80.69.110.11: icmp_seq=2 ttl=64 time=1.10 ms \n\r\nHe had not yet tried to reboot the server via MCP nor directly on site, in case there are known steps or if there is something that can be checked/investigated before reboot to try and recover the server.\n\n\r\nActions Taken :\r\n==============\n\r\nI have tried to SSH to this SESM server in order to recover it.\r\nwhen I tried, I have taken this output below.\n\r\ncbm00-unit0(active):/cbmdata/users/mon03> ssh ntsysadm@80.69.110.11 \r\nssh_exchange_identification: Connection closed by remote host \n\n\r\nI have tried to SSH from other SESM instance(Sesm2_0) but this operation has failed with same error.\n\r\nSince we could not open MCP GUI on VM,Before we have someone reboot the server, I have requested from customer to kill the instance on MCP GUI and undeploy/deploy/start the instance.\r\nWhen customer is clicked kill button on MCP GUI, it has stated \"killing\" output on MCP GUI and this action failed.\n\r\nAfter this SESM server is power cycled, I can able to make SSH to server and \r\nI have started the instance successfully which was down within this period. \n\r\nSince the issue is resolved, we agreed with ER and dropped the call.','null'),(358,'Burak Biyik','AS-OAM','2016-01-03','160103-562116','Liberty Global Europe B.V.','As reported by Tom Draper,Liberty Global Europe (LGE) had contacted GENBAND ER to report the SSL servers in overload condition and registrations were failing in the Amsterdam SSL (AMSTNLUP06C) site which was running 17.0.7.13 (10.2) \n\r\nThere were following alarms reported for the most of the SESM instances:\n\r\n-------------------------------------------------------------------------\r\nbodyText Location: 212.142.25.230-A2-Mgr-FPM1_0;212.142.25.233 Notification Id: 33 State: Raised Category: qualityOfService Cause: congestion Time: Jan 03 09:38:30 2016 Component Id: Server=SESM8;Instance=0;Software=OVLD Specific Problem: OVLD;402 Description: DB Overload \r\nspecificProblem OVLD;402\n\n\r\nbodyText Location: 212.142.25.230-A2-Mgr-FPM1_0;212.142.25.233 Notification Id: 18 State: Raised Category: processingError Cause: corruptData Time: Jan 03 04:36:10 2016 Component Id: Server=SESM3;Instance=0;Software=IMDB Specific Problem: IMDB;701 Description: Resync with DB failed: pooled_services \r\nspecificProblem IMDB;701\r\n--------------------------------------------------------------------------\n\r\nLooking at the conditions that could raised these alarms, it seems that SESM IMDB somehow had failed to synchronize its data with database and new registrations failed due to overloaded db transactions.\n\r\nThe outage had been recovered by power-cycling SM0 (Active) and force all db activity over to SM1.\n\r\nER had already collected some logs for the RCA but wanted to me check if further logs were needed. In addition to SM-related logs, I asked to collect SESM logs (oss,work etc.) since the alarms were initally raised by SESM instances.\n\r\nThe RCA will be tracked by follow-up case:160103-562118\n\r\nAgreed and dropped the call.','null'),(359,'Kemal AYDEMIR (NETAS External)','AS-OAM','2015-12-29','151229-561828','Axtel','Problem Description :\r\n=====================\n\r\nI have been paged by Mark Zattiero due to inactive SM-1 fails to come Hot Standby state. \r\nSite was preparing to Switch Over the SMs  to replace power supply in SM-0 due to Java issues when attempting to launch the MCP GUI.\n\r\nThis site was running on 17.0.22.6 A2 release.\r\nEM server type : HP CC3310\n\r\nAfter started SM1 instance, It was remaining initializing state. Customer has already tried to stop / restart SM1 instance and reboot SM-1 server but it did not help to recover this SM instance. \n\r\nActions Taken :\r\n==============\n\r\nI have connected to site and opened MCP GUI. It was running very slow. Alarm details were not coming momentarily.\n\r\nI have investigated SM1 OSS/work logs in order to catch if there is an exception/SWERR on logs.\n\r\nSM1 OSS logs showing;\n\r\nSM_1 SWERR 799 ALERT DEC29 02:13:07:471 MCP_17.0.22.6\r\nTask Runner exception:\r\npriority=10\r\ntask=Task: com.nortelnetworks.mcp.ne.base.sync.CheckpointSender@a40f0e\r\nState: run\r\njava.lang.NullPointerException\r\nat com.nortelnetworks.mcp.ne.base.sync.CheckpointSender.handleConnectConfirmEvent(CheckpointSender.java:383)\r\nat com.nortelnetworks.mcp.base.io.tcp.ConnectConfirmEvent.handle(ConnectConfirmEvent.java:34)\r\nat com.nortelnetworks.mcp.base.task.SimpleTask.handle(SimpleTask.java:26)\r\nat com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:709)\r\nat com.nortelnetworks.mcp.base.task.Task.run(Task.java:543)\r\nat com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:152)\r\nat java.lang.Thread.run(Unknown Source)\n\r\nSM1 Work logs showing;\n\r\njava.net.SocketException: Socket Closed\r\nat java.net.PlainSocketImpl.setOption(Unknown Source)\r\nat java.net.Socket.setSoLinger(Unknown Source)\r\nat com.sun.net.ssl.internal.ssl.BaseSSLSocketImpl.setSoLinger(Unknown Source)\r\nat com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession.closeSocket0(TCPRunnableSession.java:324)\r\nat com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession.closeSocket(TCPRunnableSession.java:312)\r\n        at com.nortelnetworks.mcp.base.io.tcp.TCPRunnableSession$MessengerRunnable.run(TCPRunnableSession.java:595)\r\n        at java.lang.Thread.run(Unknown Source)\n\n\r\nI have applied these maintenance operations  below one after another to get the redundancy between SMs.\r\n-Checked the jars on SM1 then Undeploy/deploy/start => same issue occurred on SM1.\r\n-Checked the memory and CPU usage on SMs =>the usage was normal on SMs.\r\n-Checked hardware logs, executed mcpSwRaid.pl status on both EM server=> outputs are okay.\r\n- Swacted SMs to see while SM1 active, SM0 will be hot-standby or not. => While SM1 is active,SM0 is dropped  during  activating state .\r\n-EM0 has been running for 216 days.\r\n[root@MTYSSLM00 work]# uptime\r\n03:56:29  up 216 days,  4:04,  3 users,  load average: 1.80, 1.97, 2.04\n\r\nWith customer permission, I have stopped SM processes , rebooted the EM0 server and started them one by one but this did not solve the issue also\n\r\nAt this point, We tried to open Alarm details once again on MCP GUI .\r\nAfter waited for a few moments,  We have seen that BCPs were showing spool alarms like the case ID : 140502-470710\n\r\nAlarmName: AboveTHLD\r\nTimeStamp: Tue Dec 28 05:05:00 EST 2015\r\nFaultNumber: 401\r\nShortFamilyName: THLD\r\nLongFamilyName: THRSHLD\r\nSeverity: CRITICAL\r\nProbableCause: threshold crossed\r\nDescription: /var/mcp/spool/tmom/MCP_17.0/TIJB6_0:closedFileCount exceeds threshold value of 200\r\nCorrective Action: If any debug log on this instance is verbosed, set it OFF. Contact the next level of support for any further help.\n\r\nWhen BCP instances try to be up, SM was not capable of processing so many spool files so the initial problem was network or any other thing but the resolution was deploying FPM but we are seeing again that even if FPM is deployed .\r\nWe are working close with Design on this issue since the problem has been addressed.\n\r\nCustomer has arranged maintenance window  for tomorrow  2 AM CST to clear this spool logs and get redundancy with SMs.\r\nWe will perform this action plan below with ER tomorrow;\r\n1.	stop all NEs\r\n2.	clear spool logs on BCPs one by one\r\n3.	start BCP instances one by one.\r\n4.	Start SM instances.\n\r\nSince we agreed with ER ,we dropped the call.','null'),(360,'Fatih Cakir ( NETAS External )','AS-OAM','2015-12-21','151221-561251','BT MSL','Tom Draper reached me over e-mail about a case for BT MSL. They were not able to launch MCP GUI. Customer had faced this issue in the past and the moment I have investigated their case in the past, there was high memory usage and spool log alarms. I had informed customer that high memory usage and spool alarms seems to be the reason of this issue. After that, they told me that they cleaned spool logs.\n\r\nWe had reached an agreement that they would collect heap dump and run \"top\" command in order to catch if there is a process that leading this high memory usage on EM servers when the problem seen again.\n\r\nBT MSL was using MCP_17.0.22.8. \n\r\nTom Draper informed me about the issue. I have connected to the site with the help of ER over VMN15. We could able to connect to the EM0 over NDM but we could not connect to the EM1. While trying to get into EM1 over NDM, the server printed \"remote connection denied\"\n\r\nSince I have seen high memory usage in the last investigation related to this issue, as first approach, I had monitored top command output for a while and there was not any process that running with high memory. \n\r\nOn the primary EM server, I have run \"neinit -p\" command in order to see whether SM is up or down. After running that command, the server could not complete the process and stucked. I have run getInstanceState.pl script under /var/mcp/run/MCP_17.0/SM_0/bin directory and the output was \"ACTIVATING\" for a while.\n\r\nAfter restarting ned process, I was able to see \"neinit -p\" command properly but the instance was still \"ACTIVATING\"\n\r\nI have checked oss logs but there was neither SM_0 log nor SM_1 log. \n\r\nAfter deactivate/activate the blade from the NDM, the problem has been solved\n\r\nI asked ned, oss, work, messages logs in order to investigate further.','null'),(361,'Senem Gultekin','AS-OAM','2015-12-21','151221-561273','Unitymedia','Problem Description:\r\nER paged OAM GPS for a SESM2_1  flapping issue. Instance was in continuously in Unavailable to Initializing state. Customer was not sure when this issue started.\r\nMCP Release is 17.0.7.18.\r\nServers are HT Langley and platform level is 17.0.14 ple2.\n\r\nSolution:\r\n-	Accessed to the site and checked the work logs for SESM2_1 instance, the error was as following;\r\ncom.nortelnetworks.ims.app.sessmgr.system.main.SessMgrBootstra\r\npper.main(SessMgrBootstrapper.java:40)\r\nCaused by: java.io.IOException: Failed to parse interface \r\ninformation string.\r\n  Info String:  Error parsing bonding inforation file\n\r\n-	Checked the configuration of SESM2_1 Server and realized that there were odd outputs;\n\r\n[ntappadm@sslsesm11 bin]$ ./getIfInfo.pl\r\nError parsing bonding inforation file\r\n   bond mii status: >>up<<\r\n    mii status: >>mii-null<<\r\n    mii status: >>mii-null<<\r\n   state:           >>2<<\r\neth0 80.69.110.11 255.255.255.192 80.69.110.63 UP bond0 ACTIVE  \r\n00:15:17:E7:23:1C\r\neth1 80.69.110.11 255.255.255.192 80.69.110.63 UP bond0  \r\nINACTIVE 00:15:17:E7:23:1C\n\r\n-	Ive compared SESM2_0 and SESM2_1 config files and realized that /admin directory was empty for SESM2_1. This is a really odd situation.\r\n-	Admin partition was not existing as well. \r\n[root@sslsesm11 admin]# ll\r\ntotal 0\n\r\nDefined disks:  sda=146813(MB)  sdb=146813(MB)\n\r\n                ***** Software RAID-1 Devices *****       \n\r\n              |            |           |           |     Sync  \r\n& Recovery      \r\n  MD Device   |            |  Member-0 |  Member-1 |       \r\nSpeed Finish Done \r\nName  Size(MB)|   Usage    | Name  Flg | Name  Flg | Mode  \r\n(MB/s) (min)  (%)  \r\n--------------+------------+-----------+----------- \r\n+--------------------------\r\nmd0      94.0 | .          | sda1  U   | sdb1  U   |  .    .    \r\n   .     .    \r\nmd1     101.9 | /boot      | sda2  U   | sdb2  U   |  .    .    \r\n   .     .    \r\nmd10 115976.9 | /var/mcp   | sda12 U   | sdb12 U   |  .    .    \r\n   .     .    \r\nmd2    4094.6 | swap       | sda3  U   | sdb3  U   |  .    .    \r\n   .     .    \r\nmd3    4094.6 | swap       | sda5  U   | sdb10 U   |  .    .    \r\n   .     .    \r\nmd4    2047.2 | /          | sda8  U   | sdb7  U   |  .    .    \r\n   .     .    \r\nmd5     501.9 | /home      | sda6  U   | sdb5  U   |  .    .    \r\n   .     .    \r\nmd6     501.9 | /tmp       | sda7  U   | sdb6  U   |  .    .    \r\n   .     .    \r\nmd7    6141.9 | /opt       | sda11 U   | sdb11 U   |  .    .    \r\n   .     .    \r\nmd8    2047.2 | /var       | sda9  U   | sdb8  U   |  .    .    \r\n   .     .    \r\nmd9    3074.8 | /var/log   | sda10 U   | sdb9  U   |  .    .    \r\n   .     .\n\n\r\n-	Since admin partition was gone, the only recovery is to reinstall the SESM2_1 Server.\r\n-	Provided Linux Installation Document to the customer with the needed ordercode information.\r\n-	After 1 hour customer was not able to locate the Linux Installer CD on their site. A2J00170  or A2J0M170.\r\n-	Transferred cd_mcpcore0170_linux_ple2.iso to our ftp server, customer downloaded to be burned on a CD.\r\n-	Customer decided to perform reinstallation tomorrow.\r\n-	Ended the conf.\n\r\nRCA: Tracked from server command history that there were some partition and disk actions were taken. However, cannot say 100% that this was the reason for admin partition to disappear. We will have to gain more information from customer about the activities done on this site in the past few weeks.','null'),(362,'Tugkan INCE','AS-OAM','2015-12-16','151216-560776','First Communications (fka Globalcom)','Customer Load: MCS 10.2.1.4\n\r\nI was paged by ER reporting that Globalcom was having issues with their call forwarding service and lost their redundancy for their Session Manager pair.\n\r\nAnalyzing that secondary Session Manager Instance was unavailable, we have tried killing/starting it on MCP GUI yet it did not work. Considering that the customer load was one of the oldest releases of the product, I thought that there might be an issue with MCP GUI on that release.\n\r\nSo I wanted to try starting the instance using the scripts located on the server and investigate the work logs in case of a fail scenario.\n\r\nHowever, we were not able to ssh or ping secondary sesm server. So we asked the customer to perform power cycle for the server.\n\r\nAfter server came up, we were agaşb not able to login the server since the passwords were not working and customer did not remember their correct password.\n\r\nSo understanding that we could not perform any operation for secondary sesm server unless the customer recovers/reset their password, I wanted to focus on Call Forwarding problem -related with IPCM instance.\n\r\nChecking the instance monitors, we have analysed that IPCM instances were utilizing 100% RAM on the server so I asked customer permission to restart the IPCM instances. Checking the old cases we have also seen that the same problem was resolved with restarting IPCM instances on the site for the same customer/site.\n\r\nHowever customer refused applying the action until their secondary SESM server is available.\n\r\nGlen Anderson from ER approved that I can leave the pager call and he would work on recovering the server passwords for secondary SESM instance and complete the action plan on IPCM instances after SESM is up.','null'),(363,'Tugkan INCE','AS-OAM','2015-12-15','151215-560613','Swisscom','Customer: Swisscom\r\nUpgrade path: 17.0.22.11 to 18.0.20.4\n\r\nWe were paged by Camila Guzzo via Yahoo Messenger IM due to an issue observed on SM/DB Upgrade screen during the patch upgrade from 17.0.22.11 to 18.0.20.4. Although the site on which the upgrade is being executed was not live, we decided to support as much as we can for the revenue impact of this particular upgrade.\n\r\nAccording to the logs and Wizard output, the database instances were upgraded successfully. However, upgrade wizard was failing to upgrade SM_0 instance. \n\r\nSM_0 was at Unavailable state when we engaged the site. So we tried stopping and starting SM_0 instance for wizard to let us retry the screen.\n\r\nIt failed with the same error again. Then we realized in the mcpUpgrade logs that the script was generating an Oracle error as below:\n\r\nORA-01882: timezone region not found\n\r\nIn the past, that error was being reported on the systems running on 10.4 with only UTC timezone and it was fixed.\n\r\nYet again we have suspected if there could be a similar specific error scenario for 11.2 and CET timezone and decided to try the workaround as we did in the past as below:\n\r\n-Implement the jvm parameter below in jvm config files of the instances \r\n-Include the parameter in javaexec process of Upgrade Wizard, edit the upgrade tools library ut_NEMtcUtils.pm in the primary EmServer to allow instance deployments in the system\n\r\n\"Parameter\"\r\n-Doracle.jdbc.timezoneAsRegion=false\r\n\"Parameter\"\n\r\nHowever, implementing the workaround cost us a lot of time due to networking problems with VPN connection. Logs and servers were all okay in terms of MCP GUI and Wizard functionality yet we were again not able to launch them due to network timeouts or server not responding errors. We had to try launching Wizard and MCP GUI dozens of times to be able to login\n\r\nAfter launching Wizard/GUI and adding the parameter to jvm.cfg file and script library, the screen passed successfully.\n\r\nHowever, additionally, we had to edit all the jvm.cfg file of the instances after they are deployed including secondary instances. So we continued and waited until the upgrade is completed to apply the workaround in particular instances failing to start.\n\r\nWhat we were planning for the other instances was to wait until they are deployed, edit jvm.cfg file and retry wizard in any case of screen failure.\n\r\nWe passed the screens between Secondary Instance Upgrade and Primary SM/DB Upgrade screens successfully, -despite receiving network connection timeout error and save&exiting in every screen-.\n\r\nOn Secondary Instance Upgrade screen we again were not able to launch wizard, due to network timeout issues. So we asked Camila to try launching the wizard.\n\r\nHowever when she managed to launch the wizard for secondary instance upgrades, wizard skipped secondary instance upgrade screen and jumped to \"Setup Database Replication\" screen due to an unexpected and undefined condition. So I had to upgrade the instances manually and apply the workaround at the same time after Setup Database Replication Screen was completed.\n\r\nAfter upgrading and starting secondary instances, we have verified that everything was okay on the site and upgrade was completed to the post pause point.\n\r\nWe agreed with Camila to end the pager support.\n\r\nWe will be creating a jira linked to the case to fix the jdbc timezone driver issue specifically observed on 18.0.20.4 with CET timezone and deliver it as soon as possible.','null'),(364,'Cigdem Vural','AS-OAM','2015-12-16','TBD','Bell Alliant','Mike pinged via YahooIM and Bell could not launch MCP GUI.\r\nNormally it was not an emergency call, but customer has scheduled a mtc to remove the JAR file that I provided tehm before. And to be able to go on with jar file remove, they would need MCP GUI.\n\r\nThey suspect it is because of Jar file, but jar file was applied at Dec 11 and GUI was launched after that.\r\nWhen I checked from logs it seems related with tomcat.\n\r\nI swacted the SM instances and GUI could be launched.\n\r\nAfter that emergency covered but I continue with Jar removal.\r\nI modified all jvm.cfg and jboss-service.xml files.\r\nRestarted SM and PROV instances.\n\r\nCustomer will do 5 SESM pair instance restarts durig the mtc window.','null'),(365,'Cigdem Vural','AS-OAM','2015-12-16','151216-560741','BlackBox-NY Pres. Hospital','Upgrade path: 14.1.10.3/17.0.22.11 \n\r\nUpgrade failed while doing oracle migration for Primary side.\r\n/opt directory was full, so I deleted the coredump files from that directory.\n\r\nManual ut_oraclepatch.pl did not success.\n\r\nWe performed manual oracle uninstall.\r\nAnd while doing install oracle with new load it failed.\r\nIt seems looking for older patch load directory as 14.1.10.3 for the dbsep3.ora file.\n\r\nWe run mcpInstallFirstLoad.pl script and re-run install, this time it passed.\r\nRestored the db manually and set wizard_state as \"UPGRADE_DB_SM\" at database.\n\r\nRe-launch the upgrade wizard and continue with DB and SM upgrade, but it failed with upgrade wizard screen with a known issue that we had fixed at later patch.\n\r\nPerformed the DB & SM upgrade screen manually with upgrade scripts:\n\r\ncd /var/mcp/upgrade_tools/bin\r\n./ut_runMonitoredScript.pl -prg ut_mcpUpgrade.pl -arg \"\" -m ut_mcpUpgradeStateFile -s 127.0.0.1\r\n./ut_mcpUpgrade.pl -primarySM -l .zip -noChgDbTyp -rboff -nc -m ut_mcpUpgradeStateFile -s \n\n\r\nThen upgrade continued until secondary DB migration screen.\r\n/opt was full again and install of oracle did not work.\n\r\nDeleted the coredump files from /opt directory, and retried but it failed again\r\nRun mcpInstallFirstLoad on secondary EM server and re run Oracle uninstall/install failed again with following error:\n\r\n> run output Error: Could not get lock\r\n> run ok 1\r\n> run ok 0\r\n> run output /bin/chown: `ntdbsw:ntdbgrp\': invalid user\r\n> run ok 1\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/apex\': No such file or directory\r\n> run ok 1\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/assistants\': No such file or directory\r\n> run ok 1\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/owb\': No such file or directory\r\n> run ok 1\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/ctx\': No such file or directory\r\n> run ok 1\n\r\nRebooted the server where secondary DB has been deployed and re-run oracle install at secondary DB, this time it passed.\r\nRe-launched the wizard and continue with the upgrade.\n\r\nUpgrade completed successfully.','null'),(366,'Tugkan INCE','AS-OAM','2015-12-16','151216-560709','NY Presbyterian Hospital','Upgrade Path -> from 14.1.10.3 to 17.0.22.11\n\r\nI was paged by Antti Kahkonen from Software Delivery team for a failure received in Patching Operating Systems of Primary Servers screen during the upgrade path execution of NY Presbyterian Hospital.\n\r\nEMServer was failing on even starting the platform patch with message \"cannot analyze image\". I first tried retrying the wizard. After receiving the same failure again, I suspected the loadlineup file might be unofficial or old so I established ssh connection on primary EMServer and checked the loadlineup file used in the upgrade.\n\r\nAfter verifying that the loadlineup file is correct, I have tried running platform patch manually on the server. It failed with the same error again. Eric Duke said that he uploaded the platform load to the server again to make sure that the loads are okay.\n\r\nSo I have extracted the platform load Eric transferred and tried executing the platform patch with that one and it failed with the same error again \"cannot analyze image\".\n\r\nThen I checked the operating system patch driver logs, no significant error message was again reported in the os driver logs.\n\r\nSo I decided to check the overall health of the operating system with executing basic commands such as top/df and etc.\n\r\nWhile running basic healthcheck commands, I have noticed that df command was not working reporting an error that filesystems and partitions are not able to be read.\n\r\nThen I checked the uptime of the server and saw that the server was up for around 500 days. After getting approval from the customer we have rebooted the server and after server came up, df command was functional again and platform patch completed successfully.\n\r\nWe have agreed with SWD to end the pager call after the screen is passed.','null'),(367,'Oktay Esgul','AS-OAM','2015-12-10','151210-559874','BlackBox','Peter Maloney paged me out in order to report Upgrade wizards failure step that is just the one of the first steps of the upgrade progress that failed due to primary SM instance admin state.\n\r\nUpgrade Path:\n\r\n14.1.8.2 TO 14.1.10.3\n\r\nUW was failing cos primary SM admin state was OFFLINE even it oper status ACTIVE.\r\nSince Peter could not stop the instance from MCP gui, he paged me out.\n\r\nI have connected site and ssh to both primary/secondary EMServers, then performed below steps in order to solve the issue.\n\r\n1. Run neStop.pl at EMServer1.\r\n2.Relaunch the MCP gui.\r\n3.ReStart the primary instance which was unavailable ,after this operation primary SM became hot-standby with correct admin state (Online)\r\n4.Then performed another SM swact to make the primary one active again.\r\n5.Then relaunch the gui and start the secondary instance to make it hot-standby.\r\n6.Retry the UW failed screen and it passed without any issue.\n\r\nAfter passing the problematic screen, we agreed to drop the call.\n\r\nPeter will continue with the upgrade.','null'),(368,'Fatih Cakir ( NETAS External )','AS-OAM','2015-12-07','151207-559329','Swisscom','Meraz Aziz from Software Delivery team paged me about an upgrade problem. Upgrade was from 10.3 to 10.4 A2 load. They had faced with SM Upgrade problem on Upgrade Wizard. Error logs stated \"Unable to upgrade the Engineering parameters in the database\"\n\r\nAfterwards, we realized that customer was trying this on LAB. Finally, we agreed with opening new case for this and keep investigating through the case.','null'),(369,'Fatih Cakir ( NETAS External )','AS-OAM','2015-12-07','151207-559236 ','UPC','Thomas Godwin paged me and stated that UPC was not able to do provisioning. Customer was using 17.0.22.6 A2 load.\n\r\nI have connected to the site and checked whether I really could able to add new data or not. Since SM GUI could not be opened for VPN slowness, I opened PROV GUI and tried to add new data. At that moment, I was able to add new data and see it was stored in the database.\n\r\nI checked A2 side for provisioning and everything seemed normal. I want ER to continue for further investigation with OSS Gate GPS since error logs were below: \n\r\nGetting NE response for prompt > \r\n>> 093552.710:Connection handler 172.16.73.208-42529:LOW :com.metasolv.cartridge.oss.nt_cs2k_sip_isn09.CS2KSIPLNPProv: \r\nGot NE response TABLE PNSCRN;VERIFY OFF^M \r\nTABLE: PNSCRN^M \r\nVERIFY OFF^M \r\n> \r\n>> 093552.711:Connection handler 172.16.73.208-42529:LOW :com.metasolv.cartridge.oss.nt_cs2k_sip_isn09.CS2KSIPLNPProv: \r\nSending CLI DELETE 238444379; \r\n>> 093552.711:Connection handler 172.16.73.208-42529:LOW :com.metasolv.cartridge.oss.nt_cs2k_sip_isn09.CS2KSIPLNPProv: \r\nPAUSE value not configured \r\n>> 093552.711:Connection handler 172.16.73.208-42529:LOW :<>: \r\nMsg logged for SRQ_ID:17526723, LOG:DELETE 238444379;','null'),(370,'Cigdem Vural','AS-OAM','2015-12-05','151205-559195','OneConnect','While SWD was performing upgrade they failed at Oracle Patch screen\r\nSWD got the error as \"Connection Loss\" and he did Save&Exit, after that could not login to wizard again.\n\r\nFROM/TO LOAD: 17.0.7.19 / 17.0.22.11 \n\r\nWhen I checked the showversion.pl, it showed the new Oracle version\r\nbut at the oraclePatch logs, there was an error while performing installation just after listener installation on new load.\n\r\nWhen I try to login database I got the error as below:\n\r\nORA-01034: ORACLE not available \r\nORA-27101: shared memory realm does not exist\n\r\nAnd the output of \"echo $ORACLE_HOME\" and \"echo $ORACLE_SID\" was empty.\r\nAt the  /var/mcp/db/logs/InstallOracle logs, the error was due to /tmp directory was full.\n\r\nSo we deleted the files from /tmp directory, and re-run the upgrade wizard script manually:\n\r\n\"./ut_oraclePatch.pl -nc -primary -v 11.2.0.4-2 -l MCP_17.0.22.11_2015-10-21-1141 -m 1111\" \n\r\nThat script did all the steps( uninstall oracle+install oracle+restore db backup) by itself, so there is no need to perform oracle uninstall/install/db restore manually.\n\r\nAfter the script completed successfully,wizard could be opened at DB&SM upgrade screen and passed that screen without any issues.','null'),(371,'Bill Picardi','AS-OAM','2015-12-03','151203-558955','Razorline, LLC.','GPS paged out to platforms support for an OS upgrade issue during the upgrade of IMM. The OS of the application servers (IMM/FAX) are upgrade from RHEL 5.5 to RHEL 5.11.  The script began to run upon the IMM application server to update rpm packages, but halted during the process.\r\n    It failed with an error for a dependency check or a rpm package upgrade, and with conflicts to existing RPM packages.  The IMM server has the legacy Dialogic IPMS media server onboard, and this is a greenfield installation with customer-based server hardware, and a custom Red-Hat OS installation for each IMM Application server.  Additional RHEL rpm packages are installed on the servers for the IPMS to function. \r\n   The upgrade script attempts to upgrade and freshen sendmail-8.13.8-8.1 and sendmail-cf-8.13.8-8.1, but cannot because the older packages (sendmail-8.13.8-8) is a prerequisite and needed by sendmail-devel-8.13.8-8 and sendmail-doc-8.13.8-8.  After verifying the old RPM packages are staged on the server in case we need to back out and down grade, we fixed this by updating all the packages to the 8-8.1 revision.\r\n  The upgrade also attmepted to update rpm-build and failed due to dependency revisions.  All required packages were updated and prerequisites installed.\r\n  The update failed due to update rhn-client-tools with a conflict against rhn-virtualization-host.  The conflicting packages were removed.\n\r\nThe upgrade of the OS inadvertently removed the LDAP configuration database by updating fedora directory server to 389 directory server:\r\n 389-ds  noarch  1.2.1-1.el5  epel  9.1 k\r\n     replacing  fedora-ds.i386 1.0.4-1.RHEL4\n\r\n  It was unclear if this was intended by the upgrade, or a problem.  The next step after the OS upgrade, the IMM software upgrade did not function, so we manually re-installed fedora directory server, restored the data from a backup, and eventually re-established LDAP replication.\n\r\n  After this was complete, we manually started LDAP service, and started each of the IMM services to see if they would function.  We also check the Dialogic IPMS media server software for issues, and it appeared to be fine after reviewing the log files.  Once all services proven to run successfully, we stopped and restarted IMM services on both servers to allow the IMM service to acquire the virtual IP service address, and the servers were processing calls again.\r\n   Since the OS upgrade took an extended amount of time, and the maintenance period ended, we stopped at this point of the upgrade, with the IMM platforms updated from RHEL 5.5 to 5.11 and the existing revision of the IMM software untouched.','null'),(372,'Burak Biyik','AS-OAM','2015-11-29','151129-558234','UPC Nederland','I was paged by ER to take a look \"DB Comm\" alarms raised on most of the NEs for secondary db after 17.0.22.6 upgrade.\n\r\nI had already been paged for the same site yesterday since some instances were not going beyond \"INITIALIZING\" state. We had found that the number of processes that Oracle 11g was accepting reached its max (300 sessions) so that NEs were not able to talk database instances and increased #process to 600 in \"/opt/mcp/db/admin/mcpdb/pfile/initmcpdb.ora\"\n\r\nResource usage before modification:\n\r\nRESOURCE_NAME                  CURRENT_UTILIZATION    MAX_UTILIZATION        INITIAL_ALLOCATION LIMIT_VALUE \r\n------------------------------ ---------------------- ---------------------- ------------------ ----------- \r\nprocesses                      297                    300                           300                300  \r\nsessions                       302                    308                           600                600  \r\nenqueue_locks                  310                    326                          6840               6840 \n\r\nSince this is oracle-level configuration file and we do not copy this file to secondary db while resync operation, the same operation had to be applied to ssdvdb_1 as well.\n\r\nAfter applying the same change, resource usage table showed the improvement:\n\r\nRESOURCE_NAME                  CURRENT_UTILIZATION    MAX_UTILIZATION        INITIAL_ALLOCATION LIMIT_VALUE \r\n------------------------------ ---------------------- ---------------------- ------------------ ----------- \r\nprocesses                      297                    300                           300                300  \r\nsessions                       302                    308                           600                600  \r\nenqueue_locks                  310                    326                          6840               6840 \n\r\nAll alarms were cleared.\n\r\nAgreed and dropped the call.','null'),(373,'Fatih Cakir ( NETAS External )','AS-OAM','2015-11-27','151127-558180','','Rodney Neese from ER paged me about Singtel Optus\' billing problem.\n\r\nHe reported that power was lost on their billing platform for the A2. Power has been restored. Customer is asking if any of the billing is stored on the A2 and if so can the files be moved to the billing server.\n\r\nCustomer was using Ericsson IMS to collect billing from the A2. Customer has already contacted Ericsson prior to calling Genband and they were unable to restore the missing billing files after power was restored to the IMS. They requested that this case be opened to see if the billing can be pulled from the A2.\n\r\nCustomer has fully recovered the Ericsson IMS and the links to the A2. All billing has been transferred from the A2 successfully. So, the problem has solved on Ericsson side.\n\r\nThen, we have ended the call.','null'),(374,'Kemal AYDEMIR (NETAS External)','AS-OAM','2015-11-26','151126-558053','Suddenlink','Problem Description :\r\n=====================\n\r\nI have been paged by Kyle Mawst due to troubleshooting an issue on SESM, Customer has tried to swact SESM1_0 (which was active) and  swact failed and SESM1_0 became down and unavailable . Although customer have tried to start button on MCP GUI, SESM1_0 remained stuck on MCP GUI as Starting. Also, Customer is unable to  SSH to unit 0.It was giving Connection closed  output on PuttY.\n\r\nActions Taken :\r\n==============\n\r\nWhen Kyle paged me, he warned me that customer had a problem with the hard drive on unit 0 several weeks ago.\r\nI have accessed to VM and tried to reach unit 0  from other SESM server. This action was failed ssh_exchange_identification: Connection closed by remote host\n\r\nThen I requested from ER to provide me SESMs terminal server access information after talk with customer. Despite ER provided to me Bomgar Session, I also experienced a connection issue on customer PC when accessing SESM1Server1s terminal server. The only option left is power cycling this SESM server.\r\nAfter customer  power cycled of SESM1Server1, the unit is back in service as hot standby.\n\r\nCustomer requested RCA about why swact failed and it took down unit 0.\r\nI told to ER that open a  follow-up case and dispatch it to PS A2 OAM queue as well as attached logs into case which I requested.\r\nIn addition to that, customer requested since this is a E2 case, remain open as they still consider this a loss of redundancy in that unit 0 may not take activity on a swact.\r\nI will update 151126-558053 as well after I determine the issue.\n\r\nSince the customer is satisfied, we agreed and dropped the call.','null'),(375,'Kemal AYDEMIR (NETAS External)','AS-OAM','2015-11-21','151120-557173','','Problem Description :\r\n=====================\n\r\nI have been paged by Bill Picardi due to some SESM guests could not install successfully through install wizard. It was a SIF case and we do not have time based on planned shipment to Verizon.\n\r\nActions Taken :\r\n==============\n\r\nI have connected to site and checked which guests could not install. There were 6 SESM guests on the system.SESM1 and SESM2 were installed successfully and able to SSH but the rest SESMs (3,4,5,6) was failed with Unable to PXE-Boot and install error message on IW.\n\r\nAs a first action, I have investigated why SESM install script failed on these guests.\n\r\nFor this purpose,I have looked into Install Wizard logs and saw that IW could not find referred .state file related with SESM3,4,5,6 under /var/mcp/install_tools/work directory. To understand main reason of this issue I will request guestInstall logs on each host server.\n\r\nSince we have no time on this critical issue, I have decided to proceed manual install those guests.\n\r\nConnected SESM 3,4,5,6 via virt-manager and performed fresh installation based on specbook information.\r\nAlthough installation was successfully on guest servers, I was unable to SSH to these guests.\n\r\nI have already 2 SESMs which I was able to SSH(SESM1,SESM2).Then I compared network configuration of SESM1 and SESM3 to find difference via virt-manager. found on SESM3 that its NICs source device was set as default NAT instead of host device bond0.\r\nI changed this on all NICs then rebooted SESM3 guest and this fixed the issue.\r\nI have performed same process on SESM4,5,6 and clicked retry button on IW and passed the screen.\n\r\nI have collected logs from /var/mcp/install_tools/work and /var/mcp/install_tools/logs directory on host1s1 in order to find RCA and what fixes can be put in place to improve outcome next time.\n\r\nSince the issue is resolved, I have informed Duncan Jefferson in order to continue installation where he left.','null'),(376,'Burak Biyik','AS-OAM','2015-11-23','151123-557295','Swisscom (Schweiz) AG','A Business Critical case from Swisscom was assigned to \"PS SIP Lines/SIP PBX\" queue today and customer initiated a mail loop to highlight the impact of the situation being experienced. \n\r\nBased on the complaints, end users were having one-way speech path for the incoming calls from PSTN and it was impacting 126 PBX.\n\r\nIt has been found that all of the location entries for \"None\" Routability Group (affected domain: \"system\") were unchecked from \"Routability Group\" portlet so that Media Portal insertion was not working for PSTN legs causing one-way speech problems.\n\r\nThis issue had been experienced by several customers before and there were two main triggers leading this problem. Basically, it may be related to CMT Audit in or lazy mechanism of SESM (Details can be found in formerly published bulletins).\n\r\nAccording to the customer, there were more than 400 locations used in the \"Routability Group\" portlet and they did not have any record/screenshot of those so that they could re-check the locations from Provisioning Client to get rid of outage condition quickly.\n\r\nThe only solution to return a healthy system was restoring lastly taken DB Backup which was done after stopping both Prov. instances to block possible provisioning. After navigating to \"Services-> Media -> Media Portal Routable Group\" and click on the \"Locations\" of problematic group name, all ticked locations were available once again and customer confirmed that no more failing calls were being reported.\n\r\nCustomer is running 17.0.7.13 release and this problem was fixed in 10.4 release. Regardless the frequency of this problem, upgrade was recommended to the customer.\n\r\nA list of checked locations were also taken not to apply \"DB Restore\" procedure every time in case of suffering the same issue since it takes longer than average time due to customer\'s db size as well as old H/W (HP-CC3310)\n\r\nAfter the customer accepted applied procedure and agreed to apply recommended workaround for the next failure, we dropped the call.','null'),(377,'Oktay Esgul','AS-OAM','2015-11-21','151120-557177-151120-557177','Codetel','---------------\r\nDay1:\r\n---------------\r\nI have paged by Gary Norwood out since customer had problem just one of their PBX was not acting properly after they performed CVM16-CVM17 previous day of the\r\noutage.\n\r\nSipPBX GPS was allready engaged prior ER paged me out ,yet they were still trying to collect traces to investigate the issue.However, customer and regional account \r\nmanagers had also involved the issue and there was a call conf that I attended as well.\r\n--------------------------------\r\nUpgrade Path:\r\nFrom : CVM16 -->MCP_14.1.10.3\r\nTo   : CVM17 -->MCP_17.0.22.8\r\n--------------------------------\n\r\nEventhough, initial investigations proved that this was not a total PBX outage as  there are many up/running pbxes without any problem.There was only 1(one) PBX \r\nhad problems. Furthermore,even there  was not anything  pointing the problem was triggered by A2 upgrade, customer was insisting to perform entire system rollback \r\nfrom CVM17 to CVM16.\n\r\nIn paralell to conf call with customer/er/account team, I have connected site and checked the system with A2 OAM perspective of view.There were none \r\nany critical alarms or other indication that may be trigger of the problem (the only alarm on MCP gui was platform passwd expiry alert), customer was insisting for rollback.In order to avoid rollback,\r\nI pushed back this request for a long time and let them know that till the problem was clearly addressed, I ll not rollback the system\r\n (The only focus was to keep rollback as a last option of recovery).\n\r\nFirst action requested by SipPBX GPS was SESM swact,yet customer did not allow us to perform this action for almost an hour even account team involved.In paralell\r\nthis request,pbx gps and er were trying to collect traces.During this time period I was requested to prepare for rollback and upload required\r\n files to server even I was not aggreed,started load transfer just in terms of requirement in case we could not convince the customer.\n\r\nAfter logs were collected properly,SipPBX team cleary addressed the issue and proved that the incoming request from problematic pbx to SESM  does not include \r\nip:port information of pbx, instead just dns name was being sent by PBX .\n\r\nPbx team shared the investigation results and explain to customer what they need to change on pbx side.Customer tried to change the configuration with \r\ntheir PBX vendor support team but could not succeed somehow, which is weird.\n\r\nEven the problem was totally addressed and proved the problem was configuration issue,customer was still insisting to rollback system as they though and believed, rollback to\r\nCVM16 will recover the problem.Then, pbx team had performed several basic tests on CVM16 loads in Netas lab with simulation tools and they proved that if they \r\nkeep that pbx configuration,rollback to CVM16 would  not recover the problem as SESM would keep rejecting the inbound messages from problematic PBX.\n\r\nEveryone from genband side ER/PBX/OAM GPS were aggreed regarding the problem addressed and needed to be solved at PBX side.Though,we could not convince customer.\r\nSo that,I invited my manager to join calls since rollback was not one of the desired option to be performed.We all attended the customer conf call and explained \r\neverything again and again.Unfortunately, technical explanation of the issue did not help us to convince customer to focus on pbx side.\n\r\nIn order to prove that rolling back to CVM16 would not recover this issue, we recommended to redeploy just secondary sesm instance with CVM16 load, but this\r\ndid not work due to platform/application between CVM16/CVM17.\n\r\nAs a second option, we recommended to rollback secondary side of the system just to show CMV16 does not work with that PBX setup even there is not any official\r\nprocedure to perform this.(Note:After entire system upgrade, the only official documented procedure is rolling back entire system, half rollback is not an option)\n\r\nI started the transfering rest of required files,  then started rollback. \n\r\nThe first issue observed at patchPlatform rolback of secondary sesm server.\n\r\nThe server was  getting stuck at patchrollback steps,yet when refreshed the ssh session and check the version with mcpRelease/showversion, I could see \r\nthat the server platform is rolled back.When platform and oracle rollback completed, I started to redeploy secondary SESM/SM/Prov. Spent alot time with java issues as well.\r\nWhen the MCP gui was launched with CVM16 load,the gui was totally grey out and no data was listed.I updated java , cleared cached it did not help,then changed the vm\r\nand verified that SM gui on CVM16 was acting properly.Then,attempted to deploy secondary sesm instance but it failed.\r\nThere were not any log generated under work or oss directories. Investigated this issue for a whiled\n\r\nWhile working on secondary sesm problem, customer informed that they want to stop maintenance in the middle of investigation \r\nsince  there are active call traficc and business hours started at Dominic Republic.\n\r\nEventually, first day was ended with aggrement to start MW at midnight of DR time again, IST 6:00 AM.\r\n---------------------------------------------------------------------------------------------------------------------------------------\n\r\n-------------------\r\nDay 2:\r\n-------------------\n\r\nThe arrangement was to start at 6 am ,yet ER paged me at 1:30 am since customer had troubles adding some testing purposes pbxes. I logined site again and performed several provisioning\r\ntesting to verify db/prov communication,everything was fine.However,even the admin state of pbx at MCP gui was OFFLINE, when they had attempted\r\nto delete a pbx node from PROV gui, was getting failure due to PBX admin state was not offline.I did not focus on this issue as I was aware that system was not stable \r\nas it was in the middle of a rollback cycle,though this kind of unexpected problem might be triggered.\n\r\nI started the arranged MW at 6 am with rolling back entire system target , yet the SESMSERVER2 platform problem blocked me for awhile.Since to keep continue rolbackk\r\nwithout solving this problem might cause total outage,I focused on the issue.After a while,realized that somehow ned version of the server was not rolled back to cvm16,even the server\r\nseems so.Reinstalled the ned and made it aligned with corresponding patch,but this did not help neither.There was a weird situation at the server,mcpRelease/showversion\r\noutputs were clearly showing server was rollbacked,yet /var/mcp/patchplatform directory was not empty .We could not address the issue .However, I just run patchPlatform.pl -rs\r\nand leave the server for a long time and joint discussion to clarify road map.During this time, script kept running and somehow after 100 minutes,patchrollback completed\r\nsuccessfully and I was able to deploy sesm with CVM16 load without any issue. \n\r\nAt this point, management team was in call with customer and they informed me that customer is working a sbc solution to manipulate sip header so that\r\nthey are convinced to keep system at CVM17 instead of rolling back to CVM16 which a disappointing information for me after solving the blocking problem.\n\r\nThen I stopped rollback and started looking for required loads to upgrade second side of the system back to CVM17.Transfering files took a long time ,then I started\r\nmanual upgrade of secondary side.Completed manual upgrade without any issue.\n\r\nIn paralell, customer informed that tests with header manipulation solution were passing so that they do not want to rollback to CVM16 again,at least for now.\n\n\r\nIn conclusion, I worked 2 days just to make customer satisfied .\n\r\nSpent 1 day to rollback even technically %100 proved that this would be waste of time  and 1 day to recover system and upgrading back to CVM17\r\n which they were running without any issue  when they paged us.\n\r\nCurrently, customer is running on CVM17 without any issue and testing with PBX is ongoing.\n\r\nI got approval that A2 OAM perspective of view everything was fine  , I dropped the pager .\n\n\r\nThank you','null'),(378,'Bill Picardi','AS-OAM','2015-11-01','51101-554230','','ER called in with a customer reporting that they are unable to provision. ALARM indicates PROV1_0:Data distribution Error, unable to distribute data. The following network elements are out of sync: PROV2.\n\r\n    The database appeared normal but with over 30,000 deferred transactions. SM1 was down and in a configured Hot standby state, and unresponsive to MCP GUI commands.  Stopped both databases then restarted each one by one to recover.  Killed and deployed and started SM1.  Provisioning still failed.  \r\n    Restarted each PROV instance one by one to resync, but the alarms back each time and provisioning still failed.  SM reports SESM sync alarms  \r\n    Stopped both SM elements and restarted each one at time.  Restarted each SESM, by restarting standby, swacting, and restarting the peer element.  Provisioning still failed and still displays alarms.  \r\n    PROV log analysis indicated database was quiesced, and checking the status from the cli confirmed the database confirmed a Quiesced state.  Ran activeRepDB.pl to place the database back online.  Restated the PROV instances again and provisioning was functional.\r\n    This issue followed a database resync that failed to complete, which likely caused the problem.','null'),(379,'Burak Biyik','AS-OAM','2015-10-26','151023-553198','Claro Codetel (Dominican Republic)','I have been paged by Mark Zattiero that both IPCM and Prov instances were in UNAVAILABLE state due to H/W errors in the server hosting these instances and GPS consultancy was needed to clarify what steps are needed to reinstall the platforms.\n\r\nThe customer was running EOL release 9.1.7.0 (MCS5200) on a different UNIX distribution (Solaris) than RHEL.\n\r\nKnown scripts (e.g. mcpRelease.pl, showversion.pl) were not available in the system so I checked \"/etc/mcp-release\" file to check platform level. See below;\n\r\nMCP_SYSTEM_RELEASE=mcp_core_solaris-9.1.7\r\nMCP_SYSTEM=mcp_core_solaris\r\nMCP_SYSTEM_RELEASE_NUMBER=9.1.7\r\nMCP_SYSTEM_RELEASE_MAJOR=9\r\nMCP_SYSTEM_RELEASE_MINOR1=1\r\nMCP_SYSTEM_RELEASE_MINOR2=7\r\nMCP_SYSTEM_SRC_CTL_TIMESTAMP=Thu-Feb-22-16:54:20-CST-2007\r\nMCP_SYSTEM_BUILD_TIMESTAMP=Thu-Feb-22-18:08:26-CST-2007\r\nMCP_SYSTEM_INSTALL_TIMESTAMP=Wed-May-23-03:59:51-AST-2007\r\nMCP_SYSTEM_UPGRADED=9.0.7\n\r\nThis information is enough for customer to contact with SWD/ account manager to get software and re-install the platform after the replacement of old servers/disks.\n\r\nI will also attach an Installation Method to the case so that it can be used for installing Solaris in NETRA240 hardware.\n\r\nAgreed and dropped the call.','null'),(380,'Fatih Cakir ( NETAS External )','AS-OAM','2015-10-22','151022-552952','Dacom','Problem Description:\r\n=====================\n\r\n- Customer was managing upgrade process from 17.0.18.5 to 18.0.20.x\n\r\n- ER reported that they were stucked on step-20 \"Prepare DB\" screen of Upgrade Wizard.\n\r\n- Upgrade wizard screen was showing \"An error occured during stopping DB monitor of {}\" \n\r\n- I have connected to the site and seen that mcpdb monitor was not working properly. On the alarm segment, db was invisible with grey color.\n\r\nActions Taken:\r\n==============\r\n- I have opened mcpdb monitor on SM GUI and seen that neither mcpdb_0 nor mcpdb_1 were able to start monitoring. At the moment of clicking \"start monitor\" button, a pop-up appears and show \"unexpected error\" \n\r\n- I have run \"neinit restart\" command on the database server but still mcpdb monitor was not under control.\n\r\n- I have stopped/started database but Upgrade Wizard was showing same error.\n\r\n- I have checked upgrade logs and seen that cleanup replication could not complete successfully. \n\r\n- I have run cleanupReplication script by manual and the script has run successfully this time.\n\r\n- After dropping replication between primary and secondary database, mcpdb monitor was able to work properly and upgrade continued from where it stayed.','null'),(381,'Fatih Cakir ( NETAS External )','AS-OAM','2015-10-21','151021-552704','Axtel','Problem Description:\r\n=====================\n\r\n- Customer was using 17.0.7.13 A2 load and has got issue with SM1 (secondary SM)\n\r\n- ER reported that SM1 was warm standy state and it could not be stable for a while. \n\r\n- A few days ago they started the MR 10.4 prep they are now on step 14 just before the DB_MOCK but everything was passing until they ran into this issue.\n\r\n- Customer has already performed power cycle of SM 1 but it did not work as ER reported.\n\n\r\nActions Taken:\r\n==============\n\r\n- I have checked oss logs of SM1 instance on active SM. When we tracked the logs we could not determine any specific reason that might be reason of this issue. \n\r\n- I have investigated work logs of problematic SM instance. However, it was not able to enlighten us regarding our issue.\n\r\n- I have suspected about spool logs might blow the hardware disk of the server but spool logs of SM1 was fairly normal.\n\r\n- I have run \"top\" command on the primary and secondary EM servers and observed that CPU and memory usage on the servers were quite usual.\n\r\n- Then, I have run \"neinit restart\" if ned session had problem on the server but after performing that command, state of SM1 was still warm standby.\n\r\n- We have tried stop/start instance of SM1 and tried undeploy/deploy SM1 respectively. Nevertheless, warm standby state of SM1 instance was resisted to be still warm-standby.\n\r\n- Finally, we have swacted SM0 and SM1 instances. So that, SM1 was active and SM0 was hot-standby in the end. \n\r\n- After customer request, we have swacted SM instances again. Hence, SM0 was active state and SM1 was hot-standby as requested\n\r\n- We have monitored SM instances for 15 minutes and ended up the call.','null'),(382,'Fatih Cakir ( NETAS External )','AS-OAM','2015-10-21','151021-552717','SaskTel','Problem Description:\r\n=====================\n\r\n- A2 migration to IA-RMS: Critical DB Communication Alarms \n\r\n- Genband migration engineer was doing migration to IA-RMS for A2. Just finished DB replication, reported it passed with no issues. But they had critical alarms in the system and were using IM 74-3160\n\r\n- Customer was using 14.1 A2 release and migration engineer was concerned about database related alarm on BCP servers and SESM servers during procedure3-step19 of the IM document.\n\r\nActions Taken:\r\n==============\n\r\n- I have seen \"database link error\" alarms for db0 and db1 on all BCP servers and SESM servers. I have doubted about eth1 and eth3 since these are related signaling. Already host1server1 and host1server2 had \"network interface failure\" for eth1 and eth3.\n\r\n- I have checked whether database server is able to ping one of SESM servers or BCP servers or not and it seemed that database server was able to communicate with them. \n\r\n- Since customer reported that calls are established successfully and there is no problem with calls, we thought the possibility of missing step according to the IM document.\n\r\n- We have realized that migration engineer did not perform the step of undeploy/deploy process of SESM servers and BCP servers.\n\r\n- After undeploy/deploy process of two SESM servers and four BCP servers, alarms that are regarding database links are cleared. Because; after redeploy process, servers (BCP and SESM) were able to recognize new database during migration.\n\r\n- Besides, we have reported that eth1 and eth3 interfaces of regarding two host server are down and need to be up before next maintenance window.\n\r\n- We had cleared the critical alarm and some major alarms related database and agreed to end up the call.','null'),(383,'Yunus Ozturk','AS-OAM','2015-10-09','151009-551205','Liberty Global (US)','Problem Description:\r\n=====================\n\r\nCustomer contacted ER to report that SESM2 Unit 1 had crashed twice in the past 12 hours and had to be power cycled to restore. They asked for assistance to figure out RCA before starting to upgrade their site\n\r\nActions Taken:\r\n==============\n\r\n- GPS checked the logs and noticed the following alarms during the problem occcurence;\n\r\nSESS4 SERVER 401 MAJOR OCT09 13:33:25:900 MCP_17.0.7.13 \r\nCPU Occupancy has reached or exceeded the defined threshold level of 90%. \n\r\n- Additionally on the SESM Servers, following alarms were observed;\n\r\nAlarmName: IMDB_Capacity \r\nTimeStamp: Fri Oct 09 13:36:01 CEST 2015 \r\nFaultNumber: 702 \r\nShortFamilyName: IMDB \r\nLongFamilyName: IMDB \r\nSeverity: WARNING \r\nProbableCause: resource at or nearing capacity \r\nDescription: Internal table [subscriber] is exceeding engineered capacity. \r\nEngineered Capacity : [35000] \r\nCurrent Element Count : [35309] \r\nUtilization : [95%] of actual maximum capacity [36843] \r\nThe internal table will continue to function even in excess of engineered capacity, however this may degrade overall server performance. \n\r\nAction: Decrease the number of subscribers provisioned against the domains hosted by this session manager instance to the engineered level or fewer. \n\r\n- Informed the customer that they are using their SESM Servers with over-sized capacity and this is causing the high CPU alarms on the corresponding server and getting the server crashed. \n\r\n- Customer will discuss this issue with their Engineering Teams and we told the customer to continue with their upgrade plan. \n\r\n- In any case, customer asked for an RCA case for this issue and we asked them to open that case for Callp GPS Team to see if anything can be done with the SESM configurations related to # of subscribers.','null'),(384,'Tugkan INCE','AS-OAM','2015-10-03','151003-550343','NUViA','Customer: Nuvia\r\nUpgrade Path: 17.0.22.8 -> 17.0.22.10\n\n\r\nI was paged by software delivery due to an issue observed on SM/DB Upgrade screen during the patch upgrade from 17.0.22.8 to 17.0.22.10.\n\r\nI was reported that the database upgrade was failing on the screen and retry operations did not work.\n\r\nBy the time I connected to the site, Chris from SWD was already checking the logs and the exception received during the upgrade:\n\r\nRUNNING SCRIPT SO_2_5_WebCollab_5.sql \r\n**************************************************** \r\nBEGIN mcsdb_utl.set_column_attr_not_null(\'DOMAINWEBCOLLABDATA\', \'PARAMETER\'); END; \r\nhenwoodc: * \r\nERROR at line 1: \r\nORA-02296: cannot enable (MCSDBSCHEMA.) - null values found \r\nORA-06512: at \"MCSDBSCHEMA.MCSDB_COMMON\", line 115 \r\nORA-06512: at \"MCSDBSCHEMA.MCSDB_COMMON\", line 129 \r\nORA-06512: at \"MCSDBSCHEMA.MCSDB_UTL\", line 771 \r\nORA-06512: at line 1 \n\r\nLog message itself was indicating a possible software bug related with the sql script, SO_2_5_WebCollab_5.sql. According to the logs, sql script was trying to add a not null constraint to a column yet it was failing due to present null values in the table.\n\r\nI have connected to customer database and checked the table DOMAINWEBCOLLABDATA regarding its contents and we have verified that the table was not empty. There were 3 records. I have analysed that to pass the screen we need to first delete the records in the table and add them back again once the upgrade is finished if required.\n\r\nWith SWD, we have applied following operations on the system:\n\r\n1-) Remove the records in the table and make the table empty\r\n2-) Retry the screen and it should success this time\r\n3-) Add the records back once the upgrade is completed if it is necessarry\n\r\nI have helped software delivery to build sql queries to find the particular domains which have domain data in the problematic table. \n\r\nAfter finding the problematic domains and clearing the domain data from prov, retry worked and screen passed successfully.\n\r\nWhile waiting for the screen to complete, Engin from design team verified that the issue is a software bug. New columns which are added during the upgrade path to the table was not assigning default values to the newly added columns of the records which is causing their parameters to be set as null initially (which is causing the next sql to fail during upgrade)\n\r\nAfter the screen is passed successfully, we agreed to end the pager call.\n\r\nWe will be creating a jira linked to the case to fix the problematic sql scripts and deliver it as soon as possible.','null'),(385,'Tugkan INCE','AS-OAM','2015-10-03','151003-550340','NUViA EMEA','Customer: Nuvia\r\nUpgrade Path: 17.0.22.8 -> 17.0.22.10\n\n\r\nI was paged by software delivery due to an issue observed on SM/DB Upgrade screen during the patch upgrade from 17.0.22.8 to 17.0.22.10.\n\r\nAccording to the logs and Wizard output, the database instances were upgraded successfully. However, upgrade wizard was failing to upgrade SM_0 instance. \n\r\nsmUpgrade script was not able to access database to query the required information from relevant tables. SWD reported that, he tried manual operations such as retry, deploy instance manually and reboot yet unfortunately none of them worked.\n\r\nI have connected to site to analyse upgrade tools logs. Correlating the logs, mcpInstall and smInstall logs were indicating no boot db instance available with the familiar exception we observed during the previous upgrades in the field. \n\r\nAlthough the previous issue should have been fixed with 17.0.22.10 patch(jdbc driver issue), I have tried workarounds to eliminate the possibility if the problem is the same(since the exception output and upgrade script logs were the same).\n\r\nLearning that we had around 4 hours remaining in the maintenance window, I have tried the actions below as known workaround to make sure the problem is not the same as the problem we observed in the past:\n\r\n1-) Undeploy the SM instance from the server.\r\n2-) Stop/Start oracle\r\n3-) Execute smUpgrade.pl script manually with parameters in upgrade tools logs.\n\r\nThe workaround has been tried twice and both failed.\n\r\nAnalysing that the workaround is failing I tried deploying SM manually to see if the problem is actually a database problem or a deploy problem. smDeploy itself executed successfully. Yet again, SM_0 instance failed to start properly due to no boot db instance available exception.\n\r\nSo I have decided/verified that the problem was again jdbc driver, however this time it was different from what we experienced and fixed in the past. \n\r\nBeing reported that there was a misunderstanding and, we had a maintenance window of less than 1.5 hours, I decided to consult design assistance to provide the fastest solution or workaround.\n\r\nWorking co-operatively with design, we have found that in the System Manager work logs, there is an exception in SM work logs with which we have not come across so far:\n\r\nORA-01882: timezone region not found\n\r\nPerforming a quick research on exception and testing it in our lab environment, we have decided to implement a jvm parameter in jvm config of the instance which is as following:\n\r\n-Doracle.jdbc.timezoneAsRegion=false\n\r\nAfter adding the parameter to jvm.cfg file of SM, the instance started successfully.\n\r\nTo include the parameter in javaexec process of Upgrade Wizard, we also had to edit the upgrade tools library ut_NEMtcUtils.pm in the primary EmServer to allow instance deployments in the system. After editing the library along with the jvm.cfg file of SM_0, the screen passed successfully.\n\r\nHowever, additionally, we had to edit all the jvm.cfg file of the instances after they are deployed. So I stayed up during the maintenance window throughout the upgrade execution to apply the workaround in particular instances failing to start.\n\r\nWhat we did for the other instances was to wait until they are deployed, edit jvm.cfg file and retry wizard in any case of screen failure.\n\r\nAgain during the upgrade, secondary BCP instance failed to be deployed and ned restart did not work. We had to reboot the server and unfortunately server did not come up successfully. So we have requested site engineers to hard reboot the server, which took around 2 hours for them to arrive to the site and reboot the server. \n\r\nAfter the hard reboot operation, server came up/deployed and started successfully.\n\r\nSWD reported that all the tests were successful after the upgrade is completed and we agreed to end the pager call.\n\r\nWe will be creating a jira linked to the case to fix the jdbc timezone driver issue and deliver it as soon as possible.','null'),(386,'Yunus Ozturk','AS-OAM','2015-09-15','150915-547783','Smart City Telecommunications LLC','Problem Description:\r\n====================\n\r\nSWD paged out GPS and reported an upgrade wizard problem. At step 26, SM & DB Upgrade screen failed with an unexpected error. There were no details regarding this error on the wizard screen. \n\r\nUpgrade Path : From 17.0.18.5 to 17.0.22.8\n\r\nActions Taken:\r\n=================\n\r\n- Checked the local upgrade wizard logs and noticed the following error:\n\r\n2015-09-15 06:01:52,160 DEBUG MainController - Starting screen from step : UPGRADE DB SM - \r\n2015-09-15 06:01:52,180 ERROR PanelController - Exception in start / resume\r\njava.lang.IllegalArgumentException: No enum const class com.nortelnetworks.mcp.client.upgrade.ui.control.UpgradeDBSMController$Step.\r\n	at java.lang.Enum.valueOf(Unknown Source) ~[na:1.6.0_30]\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.control.UpgradeDBSMController$Step.valueOf(UpgradeDBSMController.java:45) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.control.UpgradeDBSMController.resume(UpgradeDBSMController.java:425) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.share.ui.control.PanelController.start(PanelController.java:139) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.share.ui.control.MainController$4.run(MainController.java:587) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.base.ui.swing.SwingExecutor$Runner.run(SwingExecutor.java:56) [wizardws.jar:na]\r\n	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source) [na:1.6.0_30]\r\n	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.6.0_30]\r\n	at java.lang.Thread.run(Unknown Source) [na:1.6.0_30]\r\n2015-09-15 06:01:52,200 ERROR MainController - Error occured during starting panel : UPGRADE_DB_SM.\r\n2015-09-15 06:11:09,373  INFO MainController - Save and Exit is clicked.\r\n2015-09-15 06:11:15,563  INFO MainController - Save and Exit\n\r\n- Discussed these exceptions with the Design Support Team and they have informed me that someone has performed a manual operation on the SM instance at previous steps (SWD has manually started up the SM_0 instance on MCP GUI since after rebooting the EM1Server1 Guest Server (SM_0), the instance stayed at Online-Unavailable status instead of Online-Hot Standby).\n\r\n- Since someone set the \"autorestart\" feature of the SM_0 instance as \"off\" on the server itself, it could not started up automatically after reboot and stayed at Online-Unavailable status. We have corrected the status of the \"autorestart\" feature on SM_0 instance as follows;\n\r\n# neinit -autorestart=on\n\r\n- Even the status of the instance was corrected, we still received the corresponding unexpected error at this screen after Save&Exit and re-launching the wizard. \n\r\n- We have passed this screen manually with mcpUpgrade.pl script and will discuss the reason of this unexpected error with Design Support Team. The screen should not report an unexpected error even if the status of the instance was manually modified on the previous steps.','null'),(387,'Yunus Ozturk','AS-OAM','2015-09-15','150915-547771','Smart City Telecommunications LLC','Problem Description:\r\n====================\n\r\nSWD paged out GPS and reported a upgrade wizard problem. At step 23, after patching the Primary EM Server OS, it did not come up after reboot. \n\r\nUpgrade Path : From 17.0.18.5 to 17.0.22.8\n\r\nActions Taken:\r\n=================\n\r\n- Checked the local upgrade wizard logs and noticed the following error:\n\r\n2015-09-15 03:37:14,804 ERROR PatchPrimaryHostServerPlatformsController - State of primary SM is incorrect : Online - Unavailable. It should be ONLINE - HOT STANDBY.\r\n2015-09-15 03:37:14,804  INFO MainController - Next button is disabled.\r\n2015-09-15 03:37:14,814  INFO PanelController - Retry is enabled.\r\n2015-09-15 05:01:34,560  INFO MainController - Save and Exit is clicked.\r\n2015-09-15 05:01:34,560  INFO MainController - Save and Exit\r\n2015-09-15 05:01:34,570 DEBUG MainController - PreExit is started.\r\n2015-09-15 05:01:34,570  INFO UpgradeWizard - Pre exit from wizard.\r\n2015-09-15 05:01:34,570 DEBUG TopologyManagerImpl - Stopping topology monitor.\r\n2015-09-15 05:01:34,860 ERROR TopologyManagerImpl - Error occurred while starting topology monitor: {}\n\r\n- Connected to the Host Server where the Primary SM Server is running on with xming and noticed that the Guest Server was at Shutdown status. Tried to start the Guest Server and received the following error;\n\r\nit_manageGuest.pl script is started.\r\nParsing the given arguments.\r\nThe specified operation is: \"start\" and the specifed domain is \"EM1Server1\".\r\nstart operation will be performed on \"EM1Server1\" domain\r\nSucceeded: >>>/usr/bin/virsh autostart EM1Server1<<<\n\r\nFailed: >>>/usr/bin/virsh start EM1Server1<<<\n\r\nGuest Operation is failed:\r\nerror: Failed to start domain EM1Server1\r\nerror: internal error Did not find USB device 781:5575\n\r\n- Checked the details of the EM1Server1 Guest details and noticed that someone added a USB Device Hardware for this Guest Server and it was preventing the server to boot up properly. \n\r\n- Removed this manually added USB Device Hardware from this EM1Server1 Guest and then the server rebooted without an issue.\n\r\n- When the server is rebooted, Save&Exit the wizard and started it up again and passed the problematic screen.','null'),(388,'Fatih Cakir ( NETAS External )','AS-OAM','2015-09-08','150908-546807','Paltel','Problem Description:\r\n====================\n\r\nCustomer load:  MCP_17.0.7.13\r\nPlatform:       17.0.12 PLE4 SandyBridge RMS\n\r\n- After server replacement of HOST2S2, this server had got network problem.\n\r\n- Customer verified the settings when they installed the new server and made sure they were all set to the same as the previous working server.\n\r\n- HOST2S2 was just reachable via KVM.\n\r\n- Guests which are PA2,SESM3S2 and SESM2Server2 running on HOST2S2 was not pingable over HOST2S2\n\r\n- HOST2S2 was not able to ping its own gateway\n\r\n- Guests which are PA2,SESM3S2 and SESM2Server2 are not accessable via SSH\n\r\n- SM_0 had got NE communication alarms on it regarding PA2,SESM3S2 and SESM2Server2 \n\r\n- IEMS had got alarm as below for unreachable guests on HOST2S2:\n\r\nLocation: 82.213.24.145-A2-Mgr-SM_0;82.213.24.145 Notification Id: 1789 State: Raised Category: communications Cause: unspecifiedReason Time: Sep 08 12:38:53 2015 Component Id: Server=ss3s2;Software=SRVR Specific Problem: SRVR;101 Description: SNMP Request Timeout\n\r\nLocation: 82.213.24.145-A2-Mgr-SM_0;82.213.24.145 Notification Id: 1227 State: Raised Category: communications Cause: unspecifiedReason Time: Sep 08 08:50:34 2015 Component Id: Server=pa2s;Software=SRVR Specific Problem: SRVR;101 Description: SNMP Request Timeout\n\n\n\n\r\nActions Taken:\r\n===============\r\n- We have checked if host2s2 information on SMGUI matches with host2s2\'s userinfo.txt file on host2s2. These information was true.\n\r\n- var/log/messages has been checked whether any interface was down or not on the host2s2. There was not any faulty interface log. \n\r\n- ifconfig command output has been checked on the server whether any configuration was incoherent or not. It seemed okay.\n\r\n- Specbook information has been compared with host2s2\'s userinfo.txt file. There was not any difference.\n\r\n- Since customer was sure about the settings when they installed the new server and they were all set to the same as the previous working server, we have doubted about \"70-persistent-net-rules\" file which include a map where MAC addresses of the interfaces are matched.\n\r\n- We have investigated \"70-persistent-net-rules\" file under /etc/udev/rules.d and observed that MAC addresses was from other server left and that is why host2s2 was not able to map MAC addresses and interfaces on it and can not overcome network issue.\n\r\n- Customer has missed one of replacement procedure steps of HOST2S2 about changing NIC Configuration via BIOS. \n\r\n- In order to perform NIC configuration step, we have deleted 70-persistent-net-rules file from /etc/udev/rules.d directory to make forget old MAC addresses.\n\r\n- Also, we have disabled NIC Controllers for interfaces from BIOS settings and waited to server restart. \n\r\n- Then,server was up normally and network problem was solved. \n\r\n- We have activated PA2,SESM3S2 and SESM2Server2 guests on host2s2. So that alarms on SM_0 was cleared.\n\r\n- Regarding IEMS alarms, we have run \"service snmpd restart\" on SM_0 and cleared IEMS alarms.\n\n\r\nER and customer has agreed to end the call.','null'),(389,'Cigdem Vural','AS-OAM','2015-08-29','150828-545516','BT MSL','Gary paged and told Virgin Media has the load for oracle\n\r\nChecked the A2C00141 ESD file is under /var/mcp/extract\r\nRun \"mcpExtractContent.plf /var/mcp/extract/ at oracle_installer\"\r\noracle file is now under /var/mcp/media\r\nConnected primary SM with ntappadm\r\nRun \" oracleInstall.pl -secondary\"\r\nChecked \"showversion.pl\" and confirmed both SMs has the same oracle load and patch level\r\nCustomer had run prep steps for 10.4 upgrade so installation scripts were from Experius 10.4 load\r\nRun mcpInstallFirstLoad.pl with 14.1.10.3 load\r\nRun \"oracleUnInstall.pl -secondary\"\r\nand re-run \" oracleInstall.pl -secondary\"\r\nit completed without any issues\r\nLogin to primary EM with ntappadm\r\ncd /var/mcp/install\r\nrun dbInstall -fo\r\ncd /var/mcp/run/MCP_14.1/mcpdb_0/bin/util\r\nRun \"resync.pl\" and resync db from PRimary to Secondary\r\nAfter sync completed login to the secondary SM\r\ncd /var/mcp/install\r\nRun \"./smUpgrade.pl\"\n\r\nAll completed\r\nI made SM_0 active and SM_1 hot standby\r\nRun mcpInstallFirstLoad.pl script with 17.0.22.8 load not to affect upgrade steps.\n\r\nGUI access was not available so told them to deploy PROV2_0 at MCP GUI\n\r\nKill PROV2_0\r\nUndeploy PROV2_0\r\nDeploy PROV2_0\r\nStart PROV2_0\n\r\nThen dropped off','null'),(390,'Cigdem Vural','AS-OAM','2015-08-29','150828-545516','BT MSL','Kyle from ER paged for the Virgin Media issue\n\r\nWe had a open case and provide an action plan:\n\r\n1- Access to EMServer2 and run reboot \r\n2- Once the server is up try to run \'mcpSwRaid.pl -stat\' and \'mcpSwRaidCheck.pl\' scripts again. \r\n3- If the result is showing UU for both sda and sdb disks, issue has been solved. \r\n4- If the result is showing the same as the above results we need to add sda back; \r\nmcpSwRaid.pl -remove sda \r\nmcpSwRaid.pl -add sda \r\n*add section may take around 1-2 hours if there are no issues. If you would like to monitor it you can run \'mcpSwRaid.pl -mon\' \n\r\nBut when they try that action plan EM2 failed to boot because of the disk.\r\nI logged in and check the system and the only option was RTM replacement\n\r\nThe actions from here is belong to GTS actually but since GTS did not take the responsibility and did not trust their ability \r\nit again came to GPS.\n\r\nTold the instructions to the customer and ER to perform RTM replacement\n\r\no	Deactivate the blade;\r\nhardware app-blade deactivate 3 0 8 0\n\r\no	Take the blade offline\r\nhardware app-blade offline 3 0 8 0\n\r\no	Plug-out the old RTM unit\r\no	Plug-in the new RTM unit\r\no	Take the blade online\r\nhardware app-blade online 3 0 8 0\n\r\no	Activate the blade\n\r\nhardware app-blade activate 3 0 8 0\n\r\nStart the installation with the new RTM.\n\r\no	Go to /opt/corp/a2 file system on the NDM server.\r\no	Mount your ISO to `mnt` directory  as follow;\r\no	mount -o loop mcp_core_linux_ple2-14.1.15.iso /tftpboot/cnp/ssl/mnt/\r\no	cd /tftpboot/cnp/ssl/mnt\r\no	Copy the configNDMForA2E.pl and cliCommands.exp scripts from /tftpboot/cnp/ssl/mnt/extra/pxe/ directory to /tftpboot/cnp/ssl/ directory  \r\no	cd /tftpboot/cnp/ssl/mnt/extra/pxe/\r\no	cp  configNDMForA2E.pl / tftpboot/cnp/ssl/\r\no	cp cliCommands.exp / tftpboot/cnp/ssl/\r\no	cd /tftpboot/cnp/ssl/\r\no	chmod 775 configNDMForA2E.pl\r\no	chmod 775 cliCommands.exp\r\no	Run ./configNDMforA2E.pl -blade 3 0 8 0 script and  watch installation steps through the console connection screen\n\r\no	After installation completed, run the script ./configNDMforA2E.pl -blade 3 0 8 0 -d to delete the DHCP settings and delete the previously created file systems\r\no	export_fs -d /opt/corp/a2\r\no	umount_fs -m /opt/corp/a2\r\no	remove_fs -m /opt/corp/a2\n\r\nThen reset the blade and make it reboot\r\nAfter blade come up platform installation completed, we did application data restore from usb\n\r\n Insert the USB drive that contains the backup file into the server. \r\n Login to the server with an SSA (ntsysadm) role. \r\n Run the restore script. restoreSvr usbdrive \r\n Select the number of the backup file from the list from which to restore and press Enter. \r\n Enter Y at the prompts after verifying the settings. The system will indicate when the restore is successfully completed.\n\r\n Then we came to step to install oracle and resync from primary db\r\n But customer did not have the correct Oracle installer on their hand\r\n We need the oracle installer for \"installer-mcp-oracle-EE-10.2.0.4-23.LINUX64.tar\"\r\n So the only option was to transfer the file from Genband/Netas to customer.\r\n Because of network speed transfer the file may take 24 hours on my local\n\r\n Er find A2C00141.141.R.NCL.NAP.NPV.7.D.tar.gz    and according to NPV it has the oracle installer in it.\r\n It will take time to transfer the file to primary EM so I asked ping me again when the .23 oracle patch installer is ready.','null'),(391,'Cigdem Vural','AS-OAM','2015-08-26','150825-545084','Bell Alliant','Martha paged and told customer cannot change PMC password via GVPP.\r\nI was aware about the issue earlier and asked for CMT team\'s investigation as first step.\n\r\nCause there are two scenarios and only difference is on C20/CS2K side:\n\r\n1-Customer can run the same command on CS2K without any problem\r\n2-Customer gets error when they run the same command on C20\n\r\nSo there is no change on A2 side. I had told it might be related with the version of the connection that C20 and CS2K is sending to the A2 so the response is different.\n\r\nMartha told she paged CMT team and they told it is an A2 issue.She got a workaround. And I told her to get CMT ptmdebuglogs in VRB mode and OPI&DAL logs in verbose mode from PROV manager for same scenario both from C20 and CS2K.\n\r\nSince no outage, I will check the both CMT and A2 side in business hours and  tell where the problem is. If it is on A2 or CMT side.','null'),(392,'Cigdem Vural','AS-OAM','2015-08-22','150820-544443','Lime','GTS Jorge paged for customer Lime\r\nHe explained the situation as customer cannot do any provisioning an E2 call.\r\nI asked why there is no ER engaged and directly GPS engaged, he told executives asked so.\n\r\nDemetriou for Lime was on the call. \n\r\nCustomer does not have any passwords for their servers\r\nCustomer does not provide GUI access for Genband\r\nCustomer cannot launch MCP GUI because of Java version and PC spesific issues\r\nAll Ericsson and customer people does not familiar anything with the system, even manage to connect a monitor to the server.\n\r\nThey are running MCP_10.3 load which is very very old load.\n\r\nTheir original problem was could not get response to QSIP command via Ossgate and they have a solution to reset NCAS Link/SESM/PROV/SM instances.\r\nTo be able to perform that action plan MCP GUI should be launched or servers should be accessible.\n\r\nSo even all these should be provided by customer I provided pager support as GPS:\n\r\n1- MCP GUI launch issue: \n\r\nI could get teamviewer access to one of PCs customer try to launch MCP GUI\r\nThat PC even could not ping the SM IP, so I had to explain tehy should have a PC that is in teh same network :)\r\nWith second PC even I uninstall all Java versions and re-install 1.5_011 PC could not recognise it is installed on the PC.\r\nSince I am not a IT guy, I did not search on PC filesystems\r\nI download and install 1.6_021 of the JAva version to the PC and edited \"mgmtconsole-windows.jnlp\" file by deleting Java version line\r\nI had to do it by transferring the file on my local PC cause customer\'s PC even does not have wordpad.\r\nAfter editing jnlp file I could launch MCP GUI with Java version 6.\n\r\n2- SM1 is not reachable\n\r\nI reset the SM0 servers root password but SM1 was not reachable.\r\nASked to reboot the SM1 server since there is no access to it.\r\nAfter reboot SM1 server did not get up and stuck at booting.\r\nCustomer again does not have any console to see why it is stuck. Another hour passed to be able to provide console access.\r\nAt teh end we saw it is disk failure. \r\nI performed disk reseat, no help\r\nRemove disk1 off, put disk2 on disk1\'s place, an tried reboot again no help.\r\nSo disks are faulty and needs to be replaced.\n\r\nCustomer does not have any spare disks.\n\r\nWe discussed the option to perform procedure by having only one side active(SM0)\r\nBut i told them the risks as we are in an E2 condition. Customer team told they could do provisioning.\r\nSo I told not take risk of losing SM0 also. If we restart and lose SM0 then there will not be any active SM.\n\r\nCustomer and Ericsson management team discussed and did not want to take the risk of losing SM0 also since they could do provisioning.\n\r\nI explained the move forward action plan:\n\r\n1- Need spare disks to recover SM1\r\n2- Need installation CDs for the load of the customer is using.\r\n(Platform laod 10.1.10, MCP load is MCP_10.3)\r\n3- The PC needs to be reachable that I could manage to launch MCP gUI with java version 6 and edited the jnlp file.(That is needed to be applied the Knova solution for NCAS link)\n\r\nWithout having these , it is not possible to recover the SM1.\n\r\nSo they decide to wait for another site technician that will come to island tomorrow around 1pm and then  will prepare the requirements and there will be another maintenance window for server recovery.','null'),(393,'Burak Biyik','AS-OAM','2015-08-20','150817-543835','BT PLC (Manchester)','I was paged by ER to assist customer for a previously known SM recovery action. OAM team had been paged for the same issue on Monday (please check the case: 150817-543835 and it was found that primary EM Server was continuously rebooting itself so that the server hosting both primary SM and DB instance were unreachable, which causes loss of redundancy as well as provisiong incapability.\n\r\nH/W type: HP-CC3310\r\nCustomer Release: 17.0.7.14 (10.2)\n\r\nDuring case investigation, it was found that the server fails to boot up properly due to faulty disks and we had given some actions such as re-seating,swapping,placing only one of the disks. EMEA GTS Paul Simpson assisted customer to run these tests and all actions did not help for a smooth reboot.\n\r\nWhen I was paged, customer had already placed two spare disks into server and found required installer CDs to start recovery action. Customer did not have any KVM environment on site so that the only option was using terminal server to establish a serial connection to primar EM Server (site did not have any KVM switch (e.g. Raritan) to provide remote access to server over a web page).\n\r\nIt took more than half an hour for the customer to find terminal server credentials and access to primary EM server console. I asked on site engineer to insert platform installer CD to CDROM and reboot the server. Then inital installation screen was shown on the terminal server;however when I typed \"install-kvm\" to trigger installation, unreadable charachters were displayed on the screen and I was not able to identify what prompt I was currently on. We retried rebooting server several times but this did not change the behavior. It might have resulted from having different BAUD rates between ssh client and the server itself (since they do not have KVM, we could not access BIOS to check BAUD rate).\n\r\nThe only applicable solution seems to have KVM environment on the site. Customer will take this action as soon as possible.\n\r\nWe will provide the platform installation steps to customer so that on site engineer can perform the installation on his own and let GPS proceed with recovery procedure.\n\r\nThe case will be updated by ER once the customer have KVM on site.\n\r\nWe were agreed and I dropped the call.','null'),(394,'Fatih Cakir ( NETAS External )','AS-OAM','2015-08-17','150817-543797','BT PLC (Manchester)','Problem Description:\r\n====================\r\nCustomer load: MCP_17.0.7.14\n\r\n- GVP Genesys calls failing (Pisces)\r\n- Genesis agents were unable to properly log into the EXPERiUS/A2 system.\r\n- SESM1_1 was always active. But, SESM1_0 was unstable. \r\n- SESM1_0 had \"Memory Overload Control\" alarm.\r\n- SM1_0 situation was unavailable and it was even not pingable.\n\r\nActions Taken:\r\n===============\r\n- I have connected the site and I have truly realized that EM server which has got SM, PROV and DB nodes on it was not pingable. \n\r\n- I asked somebody from site to check to regarding server if they can console access or power cycle\n\r\n- While somebody trying to get the site for EM server, I have focused on SESM server problem. Since SESM_1_0 was unstable and had \"Memory Overload Control\" alarm. I have rebooted SESM_1_0 server and waited it to be up properly. 5 minutes later, SESM_1_0 was up and in HotStandby mode. So that, SESM1 server has started to work stable as redundant.\n\r\n- Case:150817-543829 has been opened as follow-up case for that SESM issue.\n\r\n- After SESM issue resolved, ER reported that customer had rebooted the EM server over the weekend and it stays in a continuous boot loop. Their H\\W type is HP-CC3310. Primary EM Server seems to be unreachable for last two days due to continuous reboot  so that there is no redundancy on customer\'s system since primary DB and SM instance is collocated. Customer is not able to perform provisioning due to E2 condition.\n\r\n- We gave some actions to understand EM server\'s problem more detailed as below:\n\r\n1- Turn off the server. Re-seat all disks on it. Turn on the server again.\r\n2- If it does not work, turn off the server, change disks places with each other. Turn on the server again\r\n3- They can take all disks from that server and use these disks in a spared server and check if it is work.\r\n4- Connect the server and record a video and send initializing screen and let us investigate further and see error messages.\n\r\n- Case:150817-543835 has been opened to follow-up this SM issue. \n\r\n- GPS already informed ER to drive customer communication about the urgency of the issue. As long as primary EM Server is reachable after corrective actions (including server replacement in worst case), recovery procedure will be applied accordingly.','null'),(395,'Yunus Ozturk','AS-OAM','2015-08-14','150815-543737','NetFortris','Problem Description:\r\n====================\n\r\nUpgrade Path: 17.0.22.8 to 18.0.20.1\n\r\nChris Wadden (Account Team) pager out GPS and asked for an assistance to check if the TDK parameter was selected as Standard or Custom TDK during the Pre-Upgrade steps. If it was selected as Standard, she wanted us to change this to Custom TDK option since she told us that customer was using Customized TDK and they do not want their Custom PCC loads to be overwritten after the upgrade with Standard PCC load. \n\r\nActions Taken:\r\n===============\n\r\n- Upgrade Wizard logs showed us that the corresponding parameter was as Standard.\r\n- We also checked this parameter from DB with the query \"select state from wizard_state;\"\n\ntrue --> Means Standard PCC option was selected. \n\r\n- Since SWD Team completed the Pre-Upgrade steps and started the upgrade, we had to correct this parameter manually from DB.\r\n- Engaged Design Support Team and they have prepared a manual sqlplus script to update this parameter. \r\n- As we had to use the sqlplus (customer did not have sqldeveloper tool installed on the Remote PC), we had some format problems with the prepared script. Therefore, Design Support Team had to update the script several times to be able to run it properly with sqlplus.\r\n- When the script is corrected after several attempts, the parameter on DB was updated properly \n\nfalse --> Means Customized TDK option was selected.','null'),(396,'Yunus Ozturk','AS-OAM','2015-08-14','150815-543770','NetFortris','Problem Description:\r\n=====================\n\r\nSWD paged out GPS and reported that root password on one of the two SESM units requires a reset as customer does not find out the correct password. \n\r\nActions Taken:\r\n===============\n\r\n- Since the customer had a limited time to start the upgrade and they had to apply jar files to the SESM Servers at the end of the upgrade, we had to reset the password through NED process over another server (which is not an official method and not recommended to customers)\r\n- Customer should be using the official password recovery method if needed.','null'),(397,'Yunus Ozturk','AS-OAM','2015-08-16','150815-543769','Bell Aliant','Problem Description:\r\n=====================\n\r\nER paged out GPS and reported that customer had a provisioning outage on their on 2 MAS Servers. \n\r\nPROV/PA instances were generating the following alarms as well;\n\r\nAlarmName: Media Application Server Unreachable \r\nTimeStamp: Sat Aug 15 15:37:24 ADT 2015 \r\nFaultNumber: 103 \r\nShortFamilyName: MASM \r\nLongFamilyName: MEDAPSVR \r\nSeverity: CRITICAL \r\nProbableCause: underlying resource unavailable \r\nDescription: The connection with Media Application Server has been lost. As a result, subscriber data on the MAS may not be current. Details of the MAS and the affected domains are as follows: MAS IP Address: 10.92.125.71 , [Domain: ucpoc1.ca , aliant506.ca , aliant902.ca , fornebulumber.com , learningbar.com , sancton.com , mbc506.ca , harbourstn.ca , xstratazinc.ca , andmctague.com , maximconstruction506.ca , bluedrop.ca , caa506.ca , adts506.ca , acdlab506.ca , acadie.com , ah2inc.com , beaubear.ca , foulem.com , arrowco.ca , nbu.ca , harveys.com , bellatl.ca , alliance.com , aliant709.ca , uc.jdirving.com , acpi.com , saintjohn.ca , acura.ca , bellgrant506.ca , coastalbp.ca , sykes.ca , bellaliant.ca , voiplab.net , uclab.ca , sigmans.ca , sigmape.ca , sigmanl.ca , , Pooled Entity Name: site_nb_meetme] \r\nCorrective Action: Verify MAS IP Addresses are configured correctly in the pooled entities assigned to the domains by using the Provisioning Client. Verify that the configured MAS is up. Verify network connectivity. If all the configurations and network connectivity seems OK, reboot the related MAS server(s). Failing that, contact your next level of support.\n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and checked the corresponding alarms. \r\n- Prov / PA elements were generating alarms for 2 MAS Servers (10.92.125.68 / 10.92.125.71)\r\n- Checked these 2 MAS Servers and they were up and running.\r\n- There was no network communication problem between the PROV/PA elements and these 2 MAS Servers\r\n- These MAS Servers were the ones having the Primary/Secondary Roles (storing the CStore Database)\r\n- Thought that some process on these MAS Servers got stuck for some reason before and this was preventing to provision new data to MAS CStore Databases of these servers. \r\n- First, asked for restarting the PROV/PA instances generating the alarms. However, customer told us that they already restarted them.\r\n- Then, asked for rebooting these 2 MAS Servers to clear out stuck processes.\r\n- Rebooting these 2 MAS Servers has fixed the provisioning issue and alarms cleared.','null'),(398,'Yunus Ozturk','AS-OAM','2015-08-12','150812-543202','Unitymedia','Problem Description:\r\n=====================\n\r\nUpgrade Path : 14.0.16.3 TO 17.0.7.14 \n\r\nScript distribution step (Screen 9) of 17.0.7.14 upgrade preparation failed for 5 out of 6 BCP blades with the following error: \n\r\nScript hasn\'t completed in predefined time interval. \n\r\nPossible reasons are : \n\r\n- Network speed is slower than expected. \r\n- There is not enough available network bandwidth for the system. \r\n- Script is stuck due to an unexpected scenario. \n\r\nUse retry button to resolve the problem. \n\r\nIf you get a lock message on the screen after pressing retry button, please continue pressing Retry button at regular intervals (i.e. 2-3 times in 5 minute intervals). \n\r\nIf the problem is still present , then contact next level of support. \n\r\nActions Taken:\r\n===============\n\r\nAS this was a Pre-Upgrade Step problem, we have rejected this pager call and asked the SWD to continue to work on this problem during the business hours through a case. \n\r\nThe possible reasons of this issue were as follows;\n\r\n- A network speed problem on the customer site \r\n- Time/date difference between the servers on the system \n\r\nFollowing actions have been recommended to fix this issue;\n\r\n- SSH to the BCT00BLADE1 (working one) with root user and check the userinfo.txt file\n\r\no cat /admin/userinfo.txt\r\no Check the clockIPs part to see the NTP Server configuration of the blade\r\no Run the date command to see the current date/time data of the server \n\r\n- SSH to failed BCP Blades with root user and verify that they have the same NTP Server configuration with BCT00BLADE1  on the userinfo.txt file. \n\r\n- If they have the same NTP Server configuration, manually synch the date/time data with the NTP Server on the failed BCP Blades as below;\n\r\no Login the BCP Server with root\r\no Change directory to /usr/sbin/ --> cd /usr/sbin/\r\no Run the script --> service ntpd stop\r\no Run the script --> ./ntpdate   -> This should be a real NTP Server \n\r\n->  An output should be seen something like 12 Aug 15:57:14 ntpdate[31749]: step time server 10.254.1.1 offset 3164.283064 sec after running the script above. \n\r\no Run the script --> service ntpd start\n\r\n- After synching the date/time data manually, run the date command all on servers and ensure that they have the same date/time data. \n\r\nSince customer\'s NTP Server was not a real NTP Server, the \"ntpdate\" script did not work properly on the problematic servers. At that point, we have recommended the following actions;\n\r\n- The time of the servers can be changed with the date -s command\n\r\no For example, to set the new data to 13 Aug 2015 18:00:00, type the following command as root user:\r\no date -s \"13 Aug 2015 18:00:00\"\n\r\n- Customer was able to correct the time/date configuration of the servers \r\n- After launching the Upgrade Wizard and clicking on the Retry button again, the screen passed.\n\r\nCustomer completed the upgrade successfully.','null'),(399,'Yunus Ozturk','AS-OAM','2015-08-14','150814-543556','Unitymedia','Problem Description:\r\n=====================\n\r\nUpgrade Path : 17.0.7.14 to 17.0.7.16 \n\r\nER paged out GPS to report a problem on SESM2_1 unit. When SESM2_1 unit was patched, the instance could not be deployed properly and stayed at Unavailable status for some reason. \n\r\nThe following logs observed during this process;\n\r\n04:25:41.187 [pool-1-thread-2] DEBUG PanelController - NEI Validator is set to com.nortelnetworks.mcp.client.share.ui.control.NEIOperationsController$5@10dbef1 \r\n04:25:41.187 [pool-1-thread-2] DEBUG PanelController - Starting monitoring : SESM2_0 \r\n04:25:41.187 [pool-1-thread-2] DEBUG PanelController - Upgrading NE : SESM2_0 with load MCP_17.0.7.16_2014-10-23-0939 \r\n04:25:41.327 [pool-1-thread-3] ERROR NEIOperationsManagerImpl - Error occurred during operation : SESM2 (or one of its instances) is already under maintenance. \r\n04:25:41.337 [AWT-EventQueue-0] ERROR PanelController - Upgrading SESM2_0 is not completed : NEI_UPGRADE_FAILURE \r\n04:25:41.337 [AWT-EventQueue-0] INFO PanelController - SESM2_0:Upgrade operation failed on Network Element Instance: SESM2. \r\n04:25:41.337 [AWT-EventQueue-0] DEBUG PanelController - Operation completed for : SESM2_0 \r\n04:25:42.248 [AWT-EventQueue-0] DEBUG PanelController - SESM2_0\'s state hasn\'t been stable yet.\n\r\nActions Taken:\r\n===============\n\r\nAs we have thought that the NED process was somehow at Stuck status after patching the unit, we have restarted the NED process on SESM2_1 as follows;\n\r\n- neinit restart\n\r\nTried to deploy the unit again on MCP GUI, it was successfully deployed. \n\r\nCustomer continued the upgrade process and it was completed without any further issue','null'),(400,'Fatih Cakir ( NETAS External )','AS-OAM','2015-08-04','150804-542133','','Robert Redman paged me and he mentioned about he was on the call with CallP GPS-Ertugrul SERT too and they are trying to solve SESM unstable situation.\n\r\nCustomer-TeleGuam was using 18.0.1.0 A2 load and 18.0.2-sandy bridge-emerson-ATCA-7370-PLE4 as host server.\n\r\nWhen we connected the site, we observed Partition Utilization Threshold alarms on EM and SESM servers. Then, we deeply took a look at EM and SESM servers and observed that /var/mcp/spool directory on SESM server reached to %100 level since EM server\'s Partition Utilization Threshold reached to %100 level as well.\r\nSESM was not able to send its spool logs to EM server.\n\r\nWe checked the logs that are being generated by SESM server and observed the exception that was printed over and over again as below:\n\n\r\nSESM1_1 SWERR 799 ALERT JUL31 13:47:13:801 MCP_18.0.1.0\n\r\n  java.lang.StringIndexOutOfBoundsException: String index out of range: 0\r\n      at java.lang.String.charAt(String.java:729)\r\n      at com.nortelnetworks.ims.mw.base.subr.SubrMgrImpl.signalingBitmapParsing(SubrMgrImpl.java:1255)\r\n      at com.nortelnetworks.ims.mw.base.subr.SubrMgrImpl.addEndpoint(SubrMgrImpl.java:1206)\r\n      at com.nortelnetworks.ims.mw.base.subr.SubrMgrImpl.getSubscribers(SubrMgrImpl.java:965)\r\n      at com.nortelnetworks.ims.mw.imdb.tables.BaseSubsCacheImpl.downloadSubscriberData(BaseSubsCacheImpl.java:153)\r\n      at com.nortelnetworks.ims.mw.imdb.tables.SESMSubsCacheImpl.handleDomainsFinishedLoading(SESMSubsCacheImpl.java:80)\r\n      at com.nortelnetworks.ims.mw.imdb.tables.IMSubscriberTable.receiveUpdate(IMSubscriberTable.java:579)\r\n      at com.nortelnetworks.ims.foundation.imdb.mgmt.ManagedCache.notifyDependents(ManagedCache.java:682)\r\n      at com.nortelnetworks.ims.mw.imdb.tables.IMRegDestRouteTable.receiveUpdate(IMRegDestRouteTable.java:440)\r\n      at com.nortelnetworks.ims.foundation.imdb.mgmt.ManagedCache.notifyDependents(ManagedCache.java:682)\r\n      at com.nortelnetworks.ims.base.imdb.domain.IMDomainInfoTable.tblMgrPeriodicAuditHandler(IMDomainInfoTable.java:592)\r\n      at com.nortelnetworks.ims.foundation.imdb.mgmt.TableManager.initSynchronizationAndPeriodicAudit(TableManager.java:195)\r\n      at com.nortelnetworks.ims.mw.imdb.manager.IMDBManager.initialize(IMDBManager.java:1073)\r\n      at com.nortelnetworks.mcp.ne.base.subsystem.InitializeRequest.handle(InitializeRequest.java:50)\r\n      at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemManager.handle(SubsystemManager.java:58)\r\n      at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemTask.dispatchSubsystemRequest(SubsystemTask.java:100)\r\n      at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemRequestEvent.dispatch(SubsystemRequestEvent.java:47)\r\n      at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemTask.dispatch(SubsystemTask.java:366)\r\n      at com.nortelnetworks.mcp.ne.base.subsystem.SubsystemTask.handle(SubsystemTask.java:80)\r\n      at com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:709)\r\n      at com.nortelnetworks.mcp.base.task.Task.run(Task.java:543)\r\n      at com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:152)\r\n      at java.lang.Thread.run(Unknown Source) \n\n\r\nBesides, Ertugrul SERT from CallP team started to investigate that exception details. \n\r\nSince regarding servers have got problems with disk space and logs were growing rapidly, we directly deleted spool logs on SESM servers and oss logs from EM Servers. Even though we removed the logs from servers to create disk space, both EM and SESM servers kept creating logs rapidly approximately 50 MB per minute.\n\r\nCallP GPS and Design Support engineers suggested a database operation to fix the reason causing the exception. The workaround they suggested will be explained in detail in CallP GPS pager report. After applying their workaround, the customer clients were able to register and calls started to be established successfully.\n\r\nYet even after applying the workaround or even the secondary SESM instance was not up, the instance continued generating the same exception logs and the operation states of the instances was not stable.\n\r\nWhile Ertugrul SERT continue investigating SESM logs with details, I have checked SM logs and checked CPU and memory usage on EM server1 that has active SM on it. Afterward, since EM and SESM have high CPU and high memory usage, I realized that customer servers might be experiencing \"leap second\" symptoms. \n\r\nI have decided to perform leap second steps on 12 servers of customer to basically eliminate the possible impacts of the leap second (even though their kernel version is not vulnerable against the leap second insertion). By keeping customer service up and by not to interrupt anything on the system, I have started to perform leap second steps as below:\n\r\n- Setting date of the servers manually after stopping NTP communication\r\n- Reboot of the servers (in case of freezing, hard-reboot is required)\n\n\r\nUnfortunately the site\'s timezone (TZ) is ChST and the date command of the Linux was not accepting ChST timezone as a parameter. So we decided to use CHUT for the district GUAM as CHUT is within the same timezone with ChST. As a result, TZ in the A2 changed from CHST to CHUT via tzConfig.pl. Then we were able to run the commands below to apply leap second workaround:\n\r\n-service ntpd stop\r\n-date -s \"`LC_ALL=C date`\" \r\n-service ntpd start\n\n\r\nWithin an hour, I have completed leap second steps on 12 servers. After performing leap second steps, servers stopped to generate excessive logs on the servers. Considering that the system was already running on a kernel which was not vulnerable against leap second insertion, we are not quite sure if rebooting the whole system or the workaround itself solved the issue.\n\r\nUnluckily, SESM, SM and AM was not able work redundant. Those 3 nodes was working with its just one instance. For example; one of SM instances was ACTIVE and one of SM instances were down. While trying to up the down instance, both instances were getting ACTIVE-ACTIVE and one of them is getting down again and within short time they were working ACTIVE-UNAVAILABLE. Several times, I have tried make them ACTIVE-HOT_STANDY to work redundant. But, SM, AM and SESM were getting ACTIVE-UNAVAILABLE status anyway. Since one of instances is always up, customer system has not problem with provisioning and active calls.\n\n\r\nWhile we are checking various logs on A2 server, we have realized that var/log/messages logs on Host1S1-10.38.1.142 were including \n\r\nAug  2 04:23:20 agan0host11 kernel: ixgbe 0000:0f:00.1: eth7: NIC Link is Up 10 Gbps, Flow Control: RX/TX\r\nAug  2 04:23:20 agan0host11 kernel: ixgbe 0000:0f:00.0: eth6: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n\r\nServer logs showed that since July 28, the link seemed to be going down and coming up constantly for that blade. The current situation of the blade is causing our application to run in an unstable state and we suspected that SCX which is being used by our blade might be the cause of the issue.\n\r\nGenius Platform GPS involved to support the outage and confirmed interface problem. After their involvement and investigation, the impacts on A2 is resolved and the system is stabilized','null'),(401,'Bill Picardi','AS-OAM','2015-07-15','150715-539612','Cable & Wireless (CALA) Ericsson (Jamaica)','#HISTORY\r\n  The Customer contacted GB ER to report that an undetermined number of users could not register or make / receive calls. The customer provided a list of 172 trouble tickets but the actual number of registration failures was not known. ER investigated with the customer and found that a large number of the users on the list eventually could register. Being that all lines affected appeared to be ONT lines off multiple Calix E720s, Calix Support was engaged. Calix Support was able to determine that the ONTs were sending up registration requests but were not immediately receiving a response from the Call Server / A2. \r\n# GPS recovery of services\r\n  ER engaged A2 Platform GPS and found that active SESM1_1 was in extreme overload due to the /var/mcp dir being filled up and the mate server (SESM1_0) being in a deallocated state. GPS made room in the /var/mcp dir on SESM1_1 but the server remained in overload. We recovered SESM1_0 and swacted to make SESM1_0 active. This action appears to have cleared all residual registration issues but we ran into issues recovering SESM1_1 after the swact to restore the SESM to redundancy. \r\n# GPS recovery of redundancy\r\n  Further investigation showed that active SM 0 had also filled up the /var/mcp dir which would not allow us to bring SESM1_1 back in service or swact the SMs. GPS deleted files to SM 0 to make space in the dir. This allowed us to stop / start SM 1, then swact to make SM 1 active. \n\r\n#  Conclusion.\r\nOnce this action was done, we could bring SESM1_1 back in service. Apx 500 lines were affected for 4 hours 35 minutes.  Follow-up cases were created to resolve the event and data file creation filling up the servers cauing the outage. Recommend preventive measure from previous cases (140503-470766 and 140502-470710) to help with the issue of excess files generated on servers','null'),(402,'Senem Gultekin','AS-OAM','2015-08-01','150731-541865 ','Banner Health System','Problem Description\n\r\nTom paged me for a oracle Installation failure during server migration procedure(IBM to IA-RMS). Customer running on 17.0.22.3 Load with 17.0.23 platform MR.\n\r\nFailure;\n\r\n> run output /bin/mkdir: cannot create directory `/var/mcp/mcpdb\': File exists\r\n> run ok 1\r\n> run ok 0\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/apex\': No such file or directory\r\n> run ok 1\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/assistants\': No such file or directory\r\n> run ok 1\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/owb\': No such file or directory\r\n> run ok 1\r\n> run output /bin/cp: cannot stat `/opt/mcp/db/product/11.2.0/ctx\': No such file or directory\n\r\nSolution:\n\r\n-	Accessed to the site and checked the new servers running on IA-RMS, the platform levels were 17.0.21 ple4 (17.0.23 Platform MR).\r\n-	Checked the oracle installer they are using, and it was 10g oracle. 17.0.22(10.4) uses 11g oracle level. Therefore oracle installer was incorrect.\r\nActions;\r\n-	Uninstall oracle: oracleUnsintall primary\r\n-	Remove 10g oracle file from /var/mcp/media directory\r\n-	Transfer 11g oracle file to /var/mcp/media\r\n-	Install Oracle: oracleInstall primary\n\r\nIt failed in the first attempt with already existing file\n\r\n> run output /bin/mkdir: cannot create directory `/var/mcp/mcpdb\': File exists\r\n> run ok 1\n\r\nRan uninstall again and double checked that mcpdb was removed. Ran install again and it worked properly.\n\r\nLater on there was also a problem with the License Key applying. Ken Johnson gave support to that.\n\r\nMonitored rest of the migration processes until the resync was completed successfully.','null'),(403,'Oktay Esgul','AS-OAM','2015-07-24','150724-540816-150326-523855','Hong Kong Broadband','As per customer request,CallP and OAM GPS had audit the HKBN system which is running MCP_17_0 releases. Several cases were raised according to audit and GPS provided action plan to GTS team in order to apply in a MW.\n\r\nEven the detailed procedure was allready provided , GTS paged me in order to perform entire procedure.I discussed with GTS team and advice to apply the proceudre if he encounter any problem during progress,page me again but he insisted to get online gps support in order to apply procedure.\n\r\nSince GTS does not feel safe himself to apply the procedure, I connected site and perform the below procedure that is quite clear to avoid any outage on customer site during maintenance operation as gts .\n\r\nHere is the procedure provided by GPS.\n\r\n\"1-) Reboot the Secondary EMServer(Virtual Machne) wait until the server is booted up successfully.\r\n2-) Reboot the Secondary Database(Virtual Machne)  server and wait until the server is booted up successfully.\r\n3-) Reboot the Primary EMServer(Virtual Machne)  wait until the server is booted up successfully.\r\n4-) Reboot the Primary Database(Virtual Machne)  server and wait until the server is booted up successfully.\r\n5-) When the reboot operations are completed, secondary side of the system should be Active, so using MCP GUI, service should be swacted to primary side via Stopping/Starting Secondary instances located on EMServer (System Manager and Accounting Manager instances)\r\n\"\n\r\nI have connected and performed the steps without any issue.Then aggreed with Jackson to drop call.\n\r\nIn addition , he called me again 10 minutes later and reported there is a hardware alert on one of host server, potentially fake HWERR problem already solved with 17.0.18 MR and later versions. Since the this problem is out of scope of this maintenance and problem solution requires upgrade of system, I recommended to open a new case to follow up.\n\r\nThanks','null'),(404,'Oktay Esgul','AS-OAM','2015-07-08','150708-538562','Suddenlink','Keith Marshall from SWD team paged me since SM/DB upgrade screen failed during major upgrade from 14.1.10.3/17.0.22.6.This problem was known issue and\r\nwe had a procedure that includes oracle logs collection for the investigation by oracle support team.\n\r\nUpgrade Path:\n\r\nFrom : 14.1.10.3\r\nTo: 17.0.22.6\n\r\nI have connected site and checked the failure logs and verified that the problem is similar issue which design support,desing and oracle support team\r\nare working on.\n\r\nHere is the error logs that causes failure.\n\r\nERROR at line 1:\r\nORA-00054: resource busy and acquire with NOWAIT specified or timeout expired\r\nORA-06512: at \"MCSDBSCHEMA.MCSDB_UTL\", line 407\r\nORA-06512: at line 16\n\n\r\nAudit/Monitor the database since upgrade failed ..\r\n   exec MCSDB_UTL.SET_NOTIFY_FILE(\'DbSetup_notice_17.0.22.6_2015-05-07-1216\');\r\n   exec MCSDB_COMMON.DEBUG(TRUE,\'DbSetup_debug_17.0.22.6_2015-05-07-1216\');\r\n@/var/mcp/run/MCP_17.0/mcpdb_0/data/upgrade/rel17.0_prov_part_0.sql;\r\n   exec MCSDB_COMMON.DEBUG(FALSE);\n\r\n   exec MCSDB_UTL.CLOSE_NOTIFY_FILE;\n\n\r\nsqlplus  -S fails at /usr/lib/perl5/site_perl/mcsBase/SysUtl.pm line 400\r\n        mcsBase::SysUtl::pipedCmd(\'mcsBase::SysUtl=HASH(0x1cf39c20)\', \'sqlplus  -S\', \'ARRAY(0x1d8d2880)\', \'sys\') called at /usr/lib/perl5/site_perl/mcsBase/SysUtl.pm line 445\r\n        mcsBase::SysUtl::pipedSysCmd(\'mcsBase::SysUtl=HASH(0x1cf39c20)\', \'sqlplus  -S\', \'ARRAY(0x1d8d2880)\') called at \n\n\n\r\nFirst of all, I have verified that oracle 11g migration completed successfully, after that start to apply procedure.\n\r\nHere is the procedure I have applied:\n\r\n1.Run the restoreEmptyDB.pl script under /opt/mcp/db/bin directory by ntdbadm user.\r\n2.Restore the db with preupgrade backup\r\n3.Enable oracle logs by connecting   sqlplus and running below instructions:\r\n	3.1- ALTER SYSTEM SET events 54 trace name systemstate level 258;\r\n4.Run mcpUpgrade.pl in order to perform upgrade manually and reproduce the issue.\r\n	Note1: We were assuming that the mcpUpgrade would fail but it did not, it completed upgrade successfully.This may be cos of the \r\n	 failed db table during first upgrade attempt was not same with the previous one.That s why we did not need to extend the ddl_lock_timeout parameter \r\n	 by running \"\"ALTER SYSTEM SET ddl_lock_timeout=30;\":\"\r\n5.Since mcpUpgrade completed successfully and wizards state was  removed after restoring the db with preupgrade backup, I have updated wizard state by running\r\n  UPDATE WIZARD_STATE SET SCREEN=UPGRADE_DB_SM; command.\r\n6.As a last step, I have relaunch the wizards with debug mode in order to pass the failed SM/DB upgrade since upgrade performed by mcpUpgrade.pl script.\r\n7.Afterward stepping to next screen, I relaunched the wizards with normal mode.Then delivered the wizards to Keith to continue upgrade.\n\r\nOracle logs and failured dbInstall logs are collected ,Investigation for this problem will continue with design support and oracle support team.\n\r\nThank you','null'),(405,'Burak Biyik','AS-OAM','2015-07-05','150704-538128','Axtel','Bradley paged me once again to report that provided procedure failed to put System Manager into stable state and SM instances were still trying to be ACTIVE-ACTIVE (Axtel-Torreon Site activity).\n\r\nTaking into account other Axtel sites with the same procedure applied, this result was unexpected and ER was likely to do something wrong while applying the procedure. Possible scenarios coming to my mind:\n\r\n1. Forgot to stop all NEs before starting var/mcp/spool clearance of all instances\r\n2. Failed to track NE state after running ./neStop.pl script and ensure that instance was really down (kill operation needs to be performed for that scenarios)\r\n3. Failed to follow proper order of bringing up the instances.\n\r\nSince we were running out of time and customer started to get nervous, I stop discussing ER which step possibly they must have mistaken. \n\r\nI followed the same action plan by myself and SM was stable. It took more than an hour to establish ssh session to over 15 servers and apply the steps. When the activity was over, we nearly passed scheduled time but MCP GUI was reachable and the customer was happy.\n\r\nCustomer performed test calls and provisioning for half an hour.\n\r\nSince all tests were successful, we agreed to drop the call.','null'),(406,'Burak Biyik','AS-OAM','2015-07-05','150704-538128','Axtel','I was paged by Bradley Hetzel regarding Axtel Maintenance Window scheduled for recovering their MCP GUI which was totally grayed out on Saturday (check previous pager report).\n\r\nI was surprised to be paged due to this activity since the steps were provided to ER a day ago and ER had applied the provided procedure several times before.\n\r\nThe question was how to make following script work:\n\r\n# find /var/mcp/spool/log -type f -exec rm -f {} \\; ; find /var/mcp/spool/om -type f - exec rm -f {} \\; ; find /var/mcp/spool/tmon -type f -exec rm -f {} \\; ; echo DONE \n\r\nThe script was used for the reference case (140502-470710) before for the second step of the action plan. Let me provide them again:\n\r\n1-) Stop(kill in case of failure) all the NE\'s in the system at the same time\r\n2-) Clean the /var/mcp/spool directories of the servers\r\n3-) Start the instances starting with SM, SESM, BCP (one at a time)\n\r\nI told Bradley that the idea in the second step was emptying the content of var/mcp/spool directory no matter which method is used and there is no meaning to stuck with that command. They could have used basic \'rm\' command for delete operation.  He agreed and completed the second step by using \"rm -rf /var/mcp/spool*\" command for all servers.\n\r\nLater I discovered that there was a little typo in the command that \"tmom\" folder was typed as \"tmon\". I would expect ER to navigate var/mcp/spool directory, list the content and find out that it was a humon error in the script.\n\r\nSince ER was able to delete var/mcp/spool content, we were agreed to end the pager call.','null'),(407,'Burak Biyik','AS-OAM','2015-07-04','150704-538128','Axtel','I was paged by Bradley Hetzel from ER team for an E2 outage on Axtel site running 17.0.7.13 load. \n\r\nBoth System Manager instances were in ACTIVE-ACTIVE state and all undeploy/deploy/restart operations failed to put SM instances into stable state.\n\r\nMCP GUI was totally grayed out and reporting no alarm of NEs. This known issue had been experienced in other Axtel sites due to overloaded spool files and there are excessive number of spool files for some NEs on this site too.\n\r\nWhen I asked if there is any major change in the system, I was told that Axtel had a maintenance window to perform some changes in their transport network and when they made a change on their cisco router they lost all visibility to their network. When they backed out the changes that were made but now all elements are not visible in their GUI.\n\r\nThere are 42 managed elements on this site (Firstly, I suspected SM instability issues experienced some other Axtel sites due to having so many network elements) but observed issue was most likely resulted by this network outage causing instances not to be able to communicate with SM and create high number of spool files. When network outage was over, SM was not able to process those files, which caused SM to lost its stability.\n\r\nTo recover the system, the following steps have to be applied (referenced from previously opened case due to same reason: 150704-538128) \n\r\n1-) Kill all the NE\'s in the system at the same time\r\n2-) Clean the /var/mcp/spool directories of the servers\r\n3-) Start the instances starting with SM, SESM, BCP (one at a time)\n\r\nCustomer arranged a maintenance window (2:00 AM CST)for ER to apply given changes.\n\r\nWe agreed with customer & ER and ended the pager call.','null'),(408,'Fatih Cakir ( NETAS External )','AS-OAM','2015-07-01','150701-537614','Hong Kong Broadband','I was paged by Thomas Godwin since SESM2_0 instance state was not stable in HKBN after leap second incident was introduced the system. He also stated that SESM2_0 was restarted by him manually but unstable situation continued for SESM2_0.\n\r\nThomas already knew about leap second issue and he pointed out that SESM2_0 server\'s date was changed manually to prevent leap second.\n\r\nThere were two recommendations to recover the leap second issue:\n\r\n- Setting date of the servers manually after stopping NTP communication\r\n- Reboot of the servers (in case of freezing, hard-reboot is required)\n\r\nCustomer was on 17.0.9 platform and 10.1 A2 load. We had already published a bulletin to take preventative actions against leap second. For those running virtualized platform release below 17.0.15 and 18.0.1, instability of ple4 servers (guests & hosts) might have been experienced.\n\r\nSince ER reported that SESM2_0 server date already set by manual, we have tried to reboot the host server which is hosting SESM2_0 guest. After host server up again, we have tried to make SESM2_0 up over \"virt manager\". While trying to up SESM2_0 guest, we have faced \"Unable to allow access for disk path /dev/cdrom\" output on the host server for guest. That output leaded us to possible forgotten media device on guest. \n\r\nWe have connected SESM2_0 guest over virt manager and confirmed that media device was connected to SESM2_0 guest. In case of connected devices to guest, guest server is not able to be up. We have ejected media device from SESM2_0 and with customer permission, we have rebooted the host server again.\n\n\r\nHowever, we were still facing same error \"Unable to allow access for disk path /dev/cdrom\" while trying to make SESM2_0 up. After that, we have decided to remove regarding guest server disk by \"it_manageGuest.pl -remove \" command. Then, we created the same guest by following command:\n\r\n\"virt-install --name=SESM2Server1 --ram=32768 --vcpus=16 --cpu=host --cpuset=5,21,6,22,7,23,8,24,9,25,10,26,11,27,12,28 --disk path=/dev/vg01/SESM2Server1,bus=virtio --boot cdrom,hd --vnc --noautoconsole --disk device=cdrom --network bridge=br_bond0,model=virtio,mac=52:54:00:6d:03:f1 --network bridge=br_bond0,model=virtio,mac=52:54:00:7a:e7:2d<<<\n\r\nAgain, we were having the same \"/dev/cdrom\" error. Then we run the same command without \"--disk device=cdrom\" parameter. Then we were able to create the guest and make the SESM2_0 instance up.\n\n\r\nWe have monitored SESM2_0 state transition over MCP GUI for a while. Unfortunately, SESM2_0 was still unstable. Then, we have seen CPU alarms for SESM as well as host servers.\n\r\n\"top\" command output on the hypervisor were showing that CPU usage of \"qemu-kvm\" process was unrealistic (which was around 900%). The SESM guests servers did not seem to be stable after leap second was introduced to system.\n\r\nAs last mission, we have decided to run commands on SESM servers which includes host and guest as below to prevent leap second: \n\r\n-service ntpd stop\r\n-date -s \"`LC_ALL=C date`\" \r\n-service ntpd start\n\r\nAfterward, CPU alarms are cleared. SESM servers start to be stable.\n\r\nIn conclusion, we have monitored approximately 40 minutes SESM servers situation and they were still stable. I have explained the solution for this issue to customer while monitoring process. Then, we have decided to end up the call between me, customer and ER.','null'),(409,'Burak Biyik','AS-OAM','2015-07-01','150701-537613','IDT/HINET','I was paged by Kenny Chi that all SESM instances were bouncing in every 5 min after leap second introduced by 23:59:59.(Detailed information about leap second event can be found in the bulletin \"EXPERiUS AS Leap Second Kernel Update Patch\") \n\r\nCustomer was running 17.0.7.5 release with 3 pairs of SESM.\n\r\nWhile SESM instances restarting themselves, CPU occupancy alarms were observed for these instances in the MCP GUI. \"top\" command output on the hypervisor were showing that CPU usage of \"qemu-kvm\" process was unrealistic (which was around 900%) . The SESM guests servers did not seem to be stable after leap second was introduced to system.\n\r\nWe had already published a bulletin to take preventative actions against this incident (mentioned above). For those running virtualized platform release below 17.0.15 and 18.0.1, instability of ple4 servers (guests & hosts) might have been experienced.\n\r\nIf kernel update patch is not applied, there were two recommendations to recover the situaion:\n\r\n1. Setting date of the servers manually after stopping NTP communication\r\n2. Reboot of the servers (in case of freezing, hard-reboot is reqiured)\n\r\nWhile I was trying to connect the site, customer informed me that they had already applied option 1 and the system was stable at that moment.\n\r\nThere are other recommendations from RedHat, which can be found in the case 150626-537012.\n\r\nWe monitored the site for a while and then I dropped the call.','null'),(410,'Burak Biyik','AS-OAM','2015-06-19','150619-535984','Suddenlink','Keith Marshall from SWD paged me to report a failure of Suddenlink 10.4 Upgrade. I had paged in their first upgrade attempt on Tuesday and we had to roll back the system to 14.1.10.3 load. Since we were not able to reproduce the failure of the previous incident, we wanted customer to perform upgrade once again. \n\r\nUpgrade path: 14.1.10.3 --> 17.0.22.6 \n\r\nThis time Upgrade Wizard failed on Screen #31 while upgrading DB & SM.It has been identified that oracle 11g install was failing due to the presence of /var/mcp/mcpdb directory which normally should not have been stored in the systems running Oracle 10g. \n\r\nActions taken \r\n=============\r\n1.       As a first action, GPS tried to launch Upgrade wizard and retry the same screen, but the same error displayed. GPS/Design confirmed that the folder should not have been there since rmoracle script introduced with the Oracle 11g should have removed the specified folder during uninstall operation as part of previous rollback.\n\r\n2.    GPS run the oracleUninstall.pl command  manually and completed the operations which were being performed on Oracle Migration screen. Hence, Oracle 11g was installed and database was restored to the pre-upgrade backup successfully. Then Wizard was able to pass the screen and proceed until SM&DB Upgrade screen.\n\r\nGPS suspects that this issue might be related to half-rollback operation that needs to be investigated further. A Jira will be opened to track the issue if this is a SW issue or a documentation issue or an operator problem.\n\r\nWe wanted to monitor next screen (SM & DB Upgrade) which had failed on Tuesday after surpassing Oracle Migration step.\n\n\r\nAfter successfully migrating the oracle 11g, Screen #31 failed while upgrading DB & SM database (same screen at 1st attempt). GPS observed that the error message was the same with the error received on Tuesday, however, the failing .sql script is different from the .sql which was causing upgrade to fail on Tuesday. \n\r\n****************************************************\r\n****************************************************\r\nRUNNING SCRIPT GPOR_387217_Intelligent_Messaging_Manager_2.sql\r\n****************************************************\r\nCreating IMM_INFO table.....\r\nDECLARE\r\n*\r\nERROR at line 1:\r\nORA-00054: resource busy and acquire with NOWAIT specified or timeout expired\r\nORA-06512: at \"MCSDBSCHEMA.MCSDB_UTL\", line 407\r\nORA-06512: at line 18\n\r\nActions taken \r\n=============\n\r\nGPS/Design attempted to run mcpUpgrade.pl manually to see if succeeds or fails again. The manual execution of mcpUpgrade.pl script also failed, yet again the .sql causing the script to fail was completely different and the previous problematic .sql was executed successfully according to the logs.\n\n\r\n****************************************************\r\n****************************************************\r\nRUNNING SCRIPT A00027789_IntercomTerm_1.sql\r\n****************************************************\r\nCreating ICTERM_DOMAINDATA table.....\r\nCreating USERIPL table.....\r\nDECLARE\r\n*\r\nERROR at line 1:\r\nORA-00054: resource busy and acquire with NOWAIT specified or timeout expired\r\nORA-06512: at \"MCSDBSCHEMA.MCSDB_UTL\", line 407\r\nORA-06512: at line 17\n\r\n3.       Analyzing that the error referenced below were being reported for different .sqls for each attempt, GPS/Design changed their focus from CUG_PROV.3969_1.sql, to a general resource lock/timeout problem on Oracle 11g. As a possible and expected solution to the busy resource problem observed on oracle, increasing the DDL_LOCK_TIMEOUT variable of the Oracle 11g (a new parameter in 11g) was tried (by default it was 0, changed to 30  in seconds). \n\r\n4.	After modifying the parameter and running the command mcpUpgrade.pl manually, Database and SM instances upgraded successfully at first attempt. GPS proceeded with the upgrade and the upgrade completed successfully without additional failures.\n\r\nGPS suspects that problem #2 might be related to DDL_LOCK_TIMEOUT default value being 0, however considering that the tables were not even created by the time the errors received, there should not be any interaction that would allocate resource for those non-existent tables. Hence the issue needs to be investigated further with the oracle support team.\n\n\r\nJira will be opened to analyze and track the issue. It probably needs to be investigated by Oracle Support Team by making a deep analysis for DDL_LOCK_TIMEOUT and table creations.','null'),(411,'Burak Biyik','AS-OAM','2015-06-16','150616-535403','Suddenlink','I got paged by SWD that Suddenlink (production site) A2 8.0 SP1 to 10.4 upgrade failed in SM&DB Upgrade Screen.\n\r\nUpgrade path: 14.1.10.3 --> 17.0.22.6 \n\r\n==================================\r\n1.	As a first action, I tried to launch Upgrade wizard and retried the same screen, however got a permission error. Then, I realized that System Manager was not able to retrieve role definition of wizard user from the database. I was able to verify this from MCP GUI too that the database didnt seem to be very stable at that moment.\n\r\n2.	Then, I continued the investigation by looking at upgrade_tools/logs. While deploying new load database scripts, it seemed that something interrupted database while CUG_PROV.3969_1.sql  script was trying to create new tables (The script was added before as part of A2 16.0 - 9.1 load).\n\r\nSCRIPT A00027853_AccountCodes_2.sql COMPLETED\r\n****************************************************\r\n****************************************************\r\nRUNNING SCRIPT CUG_PROV.3969_1.sql\r\n****************************************************\r\nCreating CUG_GROUP table.....\r\nCreating CUG_USER_GROUP_MAP table.....\r\nDECLARE\r\n*\r\nERROR at line 1:\r\nORA-00054: resource busy and acquire with NOWAIT specified or timeout expired\r\nORA-06512: at \"MCSDBSCHEMA.MCSDB_UTL\", line 407\r\nORA-06512: at line 17\n\r\nAudit/Monitor the database since upgrade failed .. \r\n   exec MCSDB_UTL.SET_NOTIFY_FILE(\'DbSetup_notice_17.0.22.6_2015-05-07-1216\');\r\n   exec MCSDB_COMMON.DEBUG(TRUE,\'DbSetup_debug_17.0.22.6_2015-05-07-1216\');\r\n@/var/mcp/run/MCP_17.0/mcpdb_0/data/upgrade/rel16.0_prov_part_0.sql;\r\n   exec MCSDB_COMMON.DEBUG(FALSE);\n\r\n   exec MCSDB_UTL.CLOSE_NOTIFY_FILE;\n\n\r\n3.	Then, oracle stop/start operations were tried but failed too. Since I was not able to login Upgrade Wizard, I decided to continue upgrade by running upgrade scripts manually. mcpUpgrade.pl script also failed while running the same sql script.\r\n4.	Lastly, I restored previously taken database backups and then proceeded but not even able to restore db backups taken from primary EM Servers. It was failing with the identical  ERROR line.\r\n5.	Since we were running out of time in the maintenance window, the customer asked to roll back the system and the rollback completed successfully. Now the customer is running on top of 8.0 SP1 again.\n\r\nBased on initial discussions,we suspect its related to CUG_PROV.3969_1.sql  that needs to be investigated further. Also, it might be a one-shot problem as we had 8.0 SP1 to 10.4 upgrades previously at AXTEL lab and Suddenlink lab.\n\r\nAction Plan to move forward:\r\n======================\r\n1.	CUG_PROV.3969_1.sql  script will be investigated in detail by Design Support/Design teams.\r\n2.	GPS is going to take backup of current database backup (A2 8.0 SP1) and apply the same upgrade path here in the lab in order to see if this is reproducible or a one-shot issue.','null'),(412,'Oktay Esgul','AS-OAM','2015-06-03','150530-532737','LIME Cable & Wireless (CALA)','Harun Caliskan my manager paged me as Chakradhar Vinnakota called him for support BC ossgate provisioning problem at Lime.\n\r\nMCP LEVEL: MCP_9.1\r\nOssgate Level : Unknown.\n\r\nCustomer had been trying to perform new provisioning from ossgate but even they were performing the provisioning without issue on friday, thay said that\r\nthey were not be able to provisiong after latest saturday due to below error.\n\n\r\n> new $ 3459469151 ibn resgrp1 0 0 SIP0 01 3 06 90 dgt SIP_DATA SIP_PACKAGE sl_silver SIP_URI p3459469151@siplines.ky SIP_CLIENT_TYPE sip_line SIP_LOCATION cayman SIP_PASSWD fc292c77 SIP_SUBDOMAIN resgrp.siplines.ky $ DPL Y 2 $\n\r\nSystem:LineProv; EndPoint can not be added to GateWay.\r\nSystem:GWEMProxy; Provisioning is not supported on Gateways containing this version of load.; Reason:\r\nProblem encountered when attempting to connect to the remote MCSEM server.\r\nEnsure that the MCSEM connection data has been configured correctly using SESM s configure tool.\r\nFirst EM URL connection failure cause: Invalid username or password. Please provide valid credentials. Access to this account will be blocked after 3 invalid attempts. This lockout condition will automatically clear after 120 seconds.\r\nSecond EM URL connection failure cause: Invalid username or password. Please provide valid credentials. Access to this account will be blocked after 3 invalid attempts. This lockout condition will automatically clear after 120 seconds. \n\r\nAs seen in error response problem is obviously provision manager login credentials. Customer did not know admin password of provision manager that is why they were looking for support.\n\r\nApparently, firstly ER provided a recovery procedure to customer on sunday but they were not  be able to get any feedback from customer for a while , then downgrade the case severity to BC from E2.\n\r\nSince the release level is MCS_9.1 which we do not have resetAdminPasswd script that we use passwd recovery in current released, the procedure provided to customer includes copying encrypted passwd from another MCS system,and update the problematic side prov passwd from DB by manual sql instructions.\n\r\nOn the other hand, as the customer do not have sqlplus username/passwd they had not be able to perform procedure. \n\r\nWhile discusing with customer engineer, she said that she had another user to login prov out of admin user. She login and create admin level new user \r\nsysprov/sysprov. After this progress,I recommended to update ossgate setting via new username/passwd instead of working on admin user passwd recovery since it is  faster recovery option. I logined the ossgate but as per their MCP level, ossgate was running an unknown release for me so that I could not perform the changes.\n\r\nAs we need ossgate gps involvement to perform changes, I assigned case to Jorge GTS engineer of case in order to work with ossgate gps.\n\r\nAfterward they performed changes, if the problem persist and ossgate gps verify the everything fine on ossgate side, I will involve  again in order to investigate,old MCS_9.1 release side.\n\r\nThanks','null'),(413,'Yunus Ozturk','AS-OAM','2015-05-28','150528-532415','Alphawest Services P/L (Carr)','Problem Description:\n\r\nUpgrade Path = 14.1.10.1 to 14.1.13 Platform Only MR\n\r\nBCP Fails at Load Audit Screen of the wizard with the following errors;\n\r\nBCPBUWDMTGR0002 : \r\nCommand exited with error. \r\nCommand: perl /var/mcp/upgrade_tools/bin/ut_mcpSystemValidation.pl -v loadLineup fw -b IBM-HS21 10.131.247.4 USERID Mgm7U$3r -l 7e0d7c8c6ea5daa4c50e1e58ac5cd5a6 14.1.13 -r 10.65.248.132 -m MTS002_164f3336-2ada-1b21-aad0-000e0cb8bf9d_ut_mcpSystemValidation.pl_RUN_AUDITS_0 \r\nExit code: 1 \r\n-------------------- MCP SYSTEM VALIDATION TOOL - Ver 1.7 -------------------- \n\r\nExecution Time: 05/28/2015 19:48:48 \n\r\nHardware Environment: IBM-HS21 \r\nPlatform Environment: ple3 \n\r\n------------------------------------------------------------------------------ \n\r\nqueryBCTFirmware received error(s)... \n\r\nLoad Lineup Validation : ............................................ [FAILED] \r\n- The Firmware Levels in the server could not be determined. \r\n- Following error(s) occured while querying the Firmware: \n\r\nError: Authentication failed. MM IP address or username or password is wrong. \n\r\nSee logfile /var/mcp/os/logs/ut_queryFirmware.pl.0528154850.log for more details \n\r\n------------------------------------------------------------------------------ \n\r\nut_mcpSystemValidation result status FAILED is written into the status file /var/mcp/upgrade_tools/work/ut_mcpSystemValidationStatus \n\r\nDetails are in /var/mcp/upgrade_tools/logs/system_validation/ut_mcpSystemValidation.MTS002_164f3336-2ada-1b21-aad0-000e0cb8bf9d_ut_mcpSystemValidation.pl_RUN_AUDITS_0.20150528194848.log on server with IP: 10.131.247.21\n\r\nSolution:\n\r\nSince this is a Pre-Upgrade step issue, we have informed the customer that we do not support the Pre-Upgrade steps on pager call. Pre-upgrade steps would need to be carried out 10 days prior the actual upgrade schedule. We have requested further logs from the customer.\n\r\nWe will continue to work on this issue during business hours with a support case.','null'),(414,'Oktay Esgul','AS-OAM','2015-05-07','150507-529719 ','MDNX (fka Viatel)','Thomas Godwin from ER paged me in order to report UCD call outage in MDNX since the problem was observed yesterday and solved after sm redeployment.\n\r\nI have connected site and checked the sm instances again, both of the instances were up and runnning without any issue.There were just several thresholds alerts on active instance.In order to clear alerts I have swacted SM to secondary instance then alerts are gone.\n\r\nEven the operations those I have performed do not seems to be related UCD call recovery, after customer relogined the devices , ucd call outage recovered and call test were passing.\n\r\nIf the issue reoccurs  again, ER will page CallP gps for detailed CallP investigation of this intermittent UCD problem.\n\r\nThanks','null'),(415,'Oktay Esgul','AS-OAM','2015-05-06','150506-529436','MDNX(Viatel)','Rodney Neese from ER paged me in order to report that customer has UCD outage and MCP gui launch problems.\n\r\nMCP Load: MCP_16.0.0.4_2012-07-30-1145\n\r\nI have connected site and checked the system , btw Rodnet said that customer had deleted some files from EMServers due to disk usage threshold problems. I have checked primary/secondary SM instances and they were not stable even the service ip was up and reachable.That is why, I have undeploy/deploy the both of primary and secondary SM instances . Then tried to access to gui but due to java problem could not launch it.Then, customer tried to launch the gui and they could launch without any issue.\n\r\nHowever, after we solved SM problems they reported that UCD calls are passing as well.\n\r\nThe system  is running on MCP_16.0.0.X so that Rca was not requested.\n\r\nSince the MCP can be launched and UCD calls are passing without any issue, we aggreed and I dropped the call.\n\r\nThanks','null'),(416,'Huseyin Tatay','AS-OAM','2015-04-30','150427-528197','Broadsmart (aka North American Telecom)','ER called A2-IMM pager(shadow) number, asking the username and password for the Websphere/IBM password on IMM server.\r\nCustomer couldn\'t find it while applying Transcription update onto IMM servers. The Websphere/IBM web page is needed to restart the IMM services, for updates become available.\r\nUfuk, Sevgi, Ahmet and I checked our notes, connected to SAM tool, and Genband Customer Site Access Information page, and shared several username-password combinations.\r\nThe Admin/iperia2013 combination worked, where username and password are noted for different nodes in SAM tool.\r\nKyle confirmed customer is able to login to Websphere, and we have ended pager session.','null'),(417,'Huseyin Tatay','AS-OAM','2015-04-30','150430-528654','Buckeye TeleSystem','IMM paged by ER, requesting support because IMM is not sending any email notifications. Joyce called shadow IMM pager number, asking for assistance.\r\nI have contacted Kyle by yahoo-IM.\r\nI have called Ufuk, to ask if he has connected to site before and if we should contact customer while attending IMM-pager in shadow mode.\r\nI have started yahoo-IM conference, inviting Kyle and Ufuk.\r\nI have checked the VMWare screen, but could not access to customer ste.\r\nI have tried Genband customer site access page and Site access tool to find any information, but there are no records saved for Buckeye, Ufuk has also confirmed he couldn\'t find any info.\r\nER Kyle called customer, but got no answer, he has left VM for calling back.\r\nSteve McCarthy contacted Ufuk, and he said the problem is solved, customer is ok now.\r\nWe have dropped from IM conference, pager session ended.','null'),(418,'Nuri INCE','AS-OAM','2015-04-29','150428-528456','Axtel','Customer Load ->MCP-17.0.7.14\n\r\nI was paged by Thomas Godwin from Emergency Recovery team for an E2 outage on Axtel Puebla Site.\n\r\nAlthough the MCP GUI was able to be launched it was not stable and healthy in terms of functionality. SM_0 was active, however SM_1 was failing to get Hot Standby and was getting stuck on Warm Standby.\n\r\nI have connected to the servers and checked the OSS logs. The oss logs were indicating a familiar problem known as SM Stability issue due to overloaded spool files. So I checked the number of spool files in the servers and verified that there were servers having more than even 1000 spool files to be processed.\n\r\nWhile investigating the issue, I was informed that the fiber line of the site was cut for more than 6 hours. Redundant line of the fiber was also cut so the customer experienced a 6 hours of full outage due to their network problem. The network issue they experienced was quite extraordinary and critical considering that both their primary and redundant lines were cut. \n\r\nI have explained the customer that the issue being observed on the SM instances are also -most likely- caused by the network outage they experienced. Due to instances in the system not being able to communicate with SM for more than 6 hours, they built/created enormous numbers of spool files which made it impossible to be processed by SM after network problem is resolved. So the SM lost its stability which caused an E2 situation.\n\r\nI have explained the customer that in the current situation, due to SM failure, they were not able to register with newly added lines and use MCP GUI functionally. \n\r\nI have explained the action plan to recover the SM instances to the customer which is composed of the steps below:\n\r\n1-) Kill all the NE\'s in the system at the same time\r\n2-) Clean the /var/mcp/spool directories of the servers\r\n3-) Start the instances starting with SM, SESM, BCP (one from each instances in the beginning)\n\r\nAt first, customer did not want to follow the action plan since they were saying that they had less than 40 monitored elements which should let SM handle the instance monitors and the server monitors on the site without issues. But after explaining them the root cause of the issue is expected to be spool build-up caused by the network outage -Not SM Capacity- and the only way to recover the SM is the action above, they agreed performing the action plan in the arranged maintenance window (2:00 AM CST).\n\r\nIn the maintenance window, the action plan is completed successfully and the site is recovered. Customer completed their tests on the site.\n\r\nWe agreed with customer and ER and ended the pager call.','null'),(419,'Cigdem Vural','AS-OAM','2015-04-14','150414-526209','UPC Nederland','GTS engineer informed that there is a BC and need to join to a conf call with customer.\r\nAs GPS we joined to the conf call, customer explained that they cannot do line provisioning via Ossgate and got the following error:\n\r\n> NEW $ 019446876 IBN RES01 0 0 0 SS 35 6 01 06 SIP_DATA SIP_PACKAGE at_ssw_res SIP_URI sb19446876@d8.upc.at + \r\n> SIP_CLIENT_TYPE sipline SIP_LOCATION cs2k SIP_PASSWD 5700VSzHNgvxN84AhjMa + \r\n> SIP_SUBDOMAIN d8.upc.at $ DPL Y 10 CWT DDN CFB P + \r\n> CFD P CWI CFU N 3WC ACRJ INACT WUCR CNDB LNR $ \n\r\nSystem:LineProv; EndPoint can not be added to GateWay. \r\nSystem:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP vmg8.sesm8:SS/035/6/0106 \r\nDetails: java.lang.Object.addSingleUserWithServiceSet(com.nortelnetworks.ws.common.DomainNaturalKeyDO, com.nortelnetworks.ws.data.core.ServiceSetNaturalKeyDO, com.nortelnetworks.ws.user.UserSecurityDO, com.nortelnetworks.ws.user.SSLUserDO) \r\n>\n\r\nGPS enabled verbose mode OPI and DAL logs for PROV and reproduce the scenario.\r\nAfter analyzing both PROV and CMT logs we saw that nothing is coming to the PROV side and on CMT side it is failing for JBOSS- axis fault:\n\r\n15.04.14 06:06:07.733 MAJ (MCSEMProxy) [####MCSEMProxy-request_1####] AxisFault\r\n faultCode: {http://xml.apache.org/axis/}HTTP\r\n faultSubcode: \r\n faultString: (404)Not Found\r\n faultActor: \r\n faultNode: \r\n faultDetail: \r\n	{}:return code:  404\r\n<html><head><title>Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:16px;} H3 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:14px;} BODY {font-family:Tahoma,Arial,sans-serif;color:black;background-color:white;} B {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;} P {font-family:Tahoma,Arial,sans-serif;background:white;color:black;font-size:12px;}A {color : black;}A.name {color : black;}HR {color : #525D76;}--></style> </head><body><h1>HTTP Status 404 - /prov/services/SIPLineServiceAdminService</h1><HR size=\"1\" noshade=\"noshade\"><p><b>type</b> Status report</p><p><b>message</b> <u>/prov/services/SIPLineServiceAdminService</u></p><p><b>description</b> <u>The requested resource (/prov/services/SIPLineServiceAdminService) is not available.</u></p><HR size=\"1\" noshade=\"noshade\"><h3></h3></body></html>\r\n	{http://xml.apache.org/axis/}HttpErrorCode:404\n\r\n(404)Not Found\r\n	at com.nortel.succession.ptm.utils.proxies.mcsemproxy.utils.httpsender.HTTPSender.readFromSocket(HTTPSender.java:755)\r\n	at com.nortel.succession.ptm.utils.proxies.mcsemproxy.utils.httpsender.HTTPSender.invoke(HTTPSender.java:145)\r\n	at org.apache.axis.strategies.InvocationStrategy.visit(InvocationStrategy.java:32)\n\r\nSo after we saw it is related with Jboss we restarted the PROV instances from MCP GUI. And re-try the failed command via Ossgate. It is succesfull after PROV restart. Also tested to add lines via PROV GUI before and after the problem without any failure.\n\r\nThen dropped off.','null'),(420,'Senem Gultekin','AS-OAM','2015-04-08','150408-525470','Shaw CableSystems','Problem Description:\n\r\nSWD contacted A2 OAM GPS for an upgrade failure during platform patch. On platform patch for secondary  HOST Servers, HOST4Server2 was failing.\n\r\nIt\'s a virtualized system. Server Type are Sandybrdige RMS.\n\r\nUpgrade path from 17.0.7.14 to 18.0.1.0. \n\n\r\nSolution:\r\n-	Accessed to the site and checked the problematic server, there was no ssh connection available for HOST4Server2.\r\n-	Requested from the site engineer to perform a hard reboot  button switch physically. This took around 20-30 min for the site engineer (customer) to reach to the server location.\r\n-	Since there were only MAS guest servers running on this host server, and MAS upgrade is not a part of the wizard, we have suggested the customer to proceed with the upgrade.\r\nAs a note: only MAS host servers are part of the wizard. MAS guest servers and application level are not included to the wizard upgrade.\r\n-	All the secondary HOST servers were already upgraded with the platform level 18.0.2. Customer agreed and SWD relaunched the wizard in debug mode to skip this screen. \r\n-	After physical reboot the server was still not coming up. Checked from the terminal server to see the server console, and it was giving the file system check error.\r\n/dev/mapper/vg01-var: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY\n\r\nAn error occured during the file system check\n\r\n-	To fix the file system Ive ran the following command from the server console.\r\nfsck y /dev/mapped/vg01-var\n\r\n-	After the above command server turned with the FILE SYSTEM WAS MODIFIED output. So basically the file system was corrupted and file system check repaired it.\r\n-	Rebooted the server again and the server came up properly. However when I checked the platform release it was showing in 17.0 platform. Which means that wizard was not able to start the platform patch to the server at all, that is probably server was unreachable at that point.\r\n-	SWD applied manually platform patch to this Host4Server2 as patchPlatform.pl script. With the patch it was upgraded 18.0.2 platform level same as the other HOST servers.\r\n-	While we were fixing this HOST server issue, wizard completed the full upgrade.\r\n-	Site was upgraded to 18.0.1.0 properly, and customer was happy with the result.','null'),(421,'Burak Biyik','AS-OAM','2015-04-09','150409-525600','Geisinger Health System','Kyle Mawst paged to me report that customer is not able to open MCP GUI and they were getting error \"no connection to database\".Beforehand, they saw so many alarm related to db failures. \n\r\nCustomer release is 17.0.22.3.\n\r\nWhile I was trying to access VM server to check problem, ER told me that GTS (Garret Yates) was involved and restart oracle database;\n\r\n-> /etc/init.d/dbora stop\r\n-> /etc/init.d/dbora start\n\r\nIt is written in the case that there were some IP conflicts that are appearing in MCP and affecting SESM. GTS had restarted PROV and double swact SESM after oracle restart.\n\r\nSince the issue was already resolved, I didn\'t need to be involved and dropped the call.','null'),(422,'Senem Gultekin','AS-OAM','2015-04-03','150403-524976','Geisinger Medical Center','Problem Description\n\r\nSWD paged OAM GPS for an upgrade failure during primary oracle upgrade  screen on wizard. Also, SWD was not able to launch the wizard anymore.\n\r\nUpgrade Path: 14.1.10.3 to 17.0.22.3\r\nOracle : 10g to 11g\n\r\nError:\r\nDetermining Which Operation To Perform... \r\n-Oracle Migration Operation In Progress- \r\n-Steps: Backup DB->Uninstall Old Oracle->Install New Oracle->Restore DB- \r\nTaking backup of the database in server 10.20.6.25 ... \r\nBackup database of server 10.20.6.25 is completed. \r\nUninstalling oracle from the server.. \r\nOracle uninstalled from 10.20.6.25 \r\nInstalling new Oracle to the server. This operation can take some time.Please wait... \r\nError executing Oracle install commands. \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/oraclePatch_logs/ut_oraclePatch.EMS1_db7a03cc-2ace-1b21-9642-001517ceb410_ut_oraclePatch.pl_PATCH_PRI_ORACLES_0.20150402_223023.log \n\r\nOperation Error\n\r\nInvestigation:\r\n	Accessed to SWD PC over teamviewer and tried the launch wizard, it gave the generic error;\r\nFollowing error occurred while starting wizard.\r\nA generic failure occurred.\r\n	Tried to launch MCP GUI, however the PC that SWD was using was having trouble with the Java security. MCP GUI for this customer was never launched from that PC before. Worked on the MCP GUI launching issue, and added the MPC GUI ip address to the java truested nodes.\r\nNote: Before every upgrade, MCP GUI should be launched and tested to avoid time loss during pager calls.\r\n	After MCP GUI was up tried SM double swact. Later on I have investigated wizard logs and realized that the wizard was not able launch due to no db connection.\n\r\n2015-04-02 23:53:59,157 DEBUG OmiErr - omi service failed: com.nortelnetworks.mcp.ne.sm.base.svc.event.RequestException: Cannot get DB connection\n\r\n	During Primary Oracle upgrade, there is a migration from 10g to 11g. This is performed by doing an Uninstallation and Installation for oracle in background. Since the failed screen was on Oracle patch/migration, after SWD performed save&exit, wizard was not able to talk with db anymore. \r\nHere are the major steps that oracle migration does;\n\r\nTake the current db backup -> perform oracle Uninstallation -> perform oracle Installation -> restore the taken backup. \n\r\nAll these are done in one screen. And the failed section was oracle installation. That means there is no data inside in the primary db at that point and the wizard is failing to launch due to db connection.\n\r\nSolution:\r\n	Performed manual oracle uninstall and install on primary DB server however it was failing again. At this point realized that /opt directory was getting full %100. Therefore oracle installation was failing exactly the same place where wizard was failing.\r\n	There were some unused files under /opt directory which was keeping space. Removed those files. Some pcap and some investigation logs were left.\r\n	Performed manual oracle uninstallation to clear out the primary db.\r\noracleUinstall primary\r\n	Performed manual oracle installation to bring the 11g to the primary db. And it completed successfully.\r\noracleInstallation primary\r\n	Restored the backup file to the primary db, which was taken right before the actual upgrade steps, on screen Prepare DB.\r\n	After the restore I was able to launch the wizard, of course it came with the Prepare DB screen instead of Primary Oracle Patch screen. This was expected because the wizard state in the db backup was from Prepare DB screen.\r\n	Passed the already completed screens, primary platform patch, primary oracle patch. And continued with the DB and SM Upgrade\r\n	Later on Upgrade has been completed. However customer reported an conf call issue which was not reported during the half upgrade test calls. Because they didnt test it during the half upgrade tests.\r\n	Conf Call issue has been fixed by CALLP GPS Bahar Sarioguz. Case ID: 150403-524996\n\r\nEnhancements: \r\n-	We will open a jira for the logging about disk space usage issues seen during oracle upgrade/migration.\r\n-	We will open a jira for the wizard re-logging issue on oracle patch primary screen.','null'),(423,'Seren Batmaz','AS-OAM','2015-04-01','150401-524616 ','Axtel','Problem Description:\n\r\nER Paged me due to many alarms occured in MCP GUI of the customer and informed me that most of alarms were related with Database. These alarms occured after the customer did some operations related with EMServer1 power supply. ER told me that, the power issue had been resolved, but the DB alarms in MCP GUI needed to be cleared.\n\r\nCustomer Load:\n\r\nMCP_17.0.7.14\n\r\nSolution:\n\r\nI connected to the site and checked the system. There were alarms on almost all of the Network Elements, stating that there was no connection to DB Instance 0. On primary DB, there was the communication alarm.\n\r\nFirst, I checked if it is possible to write to DB.\r\n-From MCP GUI, I tried to add an admin user and it failed. So primary DB was not working properly.\n\r\nThen, I restarted Oracle in Primary DB:\r\n-./etc/init.d/dbora stop\r\n-./etc/init.d/dbora start\r\nAfter this action is completed, the alarm on primary DB was gone and DB became writable. So, DB started working properly.\n\r\nHowever, the alarms on other Network Elements(NEs) still persisted. So, to clear the alarms, I restarted almost all of the NEs one by one. Alarms got cleared by that way.\r\nSecondary DB also has an alarm related with replication errors with Primary DB. So, to resynchronize two DBs, I took the action:\r\n- cd /var/mcp/run///bin/util\r\n- ./resync.pl\n\r\nAfter resync was completed, customer agreed that the issue with the DB was resolved. So, I left the call.','null'),(424,'Yunus Ozturk','AS-OAM','2015-03-18','150317-522433','Verizon Communications','Problem Description:\r\n------------------------\n\r\n- Primary DB fails to start on Primary DB/EM Server. There were 22 Active DB Alarms on the system since Sunday. \r\n- We have been informed that /dev/sdb disk on the Primary EM Server is dead and /dev/sda became read-only. The site sat like at this situation for 3 days but provisioning was somehow still (sort-of) working. \r\n- After rebooting the server, it came back up on one hard disk, and read-write enabled this time. Customer is aware that they have faulty disk.\r\n- GTS tried to resync the DB from Secondary to Primary but then noticed that Oracle was not running on Primary DB/EM Server. \r\n- After doing some deeper investigations on this, they found out corruptions in a dbf file on Primary DB. So, they recommended the customer to reinstall the Oracle on Primary DB/EM Server. \n\r\nSolution:\r\n------------\n\r\n- When GPS was paged out, GTS has already initiated a new resync process from Secondary DB to Primary DB as they were somehow able to start the Oracle on the Primary DB/EM Server (/etc/init.d/dbora start). So, we have waited this process to be completed for some time. However, it failed again on the same dbf file (/var/mcp/db/data/mcpdb/undotbs01.dbf)\r\n- Before we start the Oracle Re-installation, we have performed some pre-checks on the system noticed that wrong Oracle Installer file has been provided to the customer (installer-mcp-oracle-EE-10.2.0.4-22.LINUX32.tar).\r\n- We noticed that customer\'s current Oracle version was 21, not 22 on the Primary and Secondary EM Server (showversion.pl). So, we decided to provide the correct Oracle Installer file to the customer (installer-mcp-oracle-EE-10.2.0.4-21.LINUX32.tar)\r\n- Since the connection was slow on the customer site, it took several hours (around 4-5 hours) to upload the correct Oracle Installer file to the server (/var/mcp/media/ directory).\r\n- When the Oracle Installer file was uploaded, the MTC window was already over but customer requested us to continue with the installation as this installation would not impact the Callp. Just provisioning would be impacted.\r\n- We have applied the following steps on the Primary EM/DB Server to complete this activity\r\n  * Stopped the Oracle on Primary DB (/etc/init.d/ ./dbora stop)\r\n  * Manually edited the installprops.txt file (vi /var/mcp/install/installprops.txt) and set the db.type parameter to \"Single\" (db.type=Single)\r\n  * Uninstalled the Oracle (/var/mcp/install/ ./oracleUninstall.pl -primary)\r\n  * Re-installed the Oracle (/var/mcp/install/ ./oracleInstall.pl -primary)\r\n  * Manually re-edited the installprops.txt file (vi /var/mcp/install/installprops.txt) and set the db.type parameter to \"Replicated\" (db.type=Replicated)\r\n  * Installed the DB files (/var/mcp/install/ ./dbInstall.pl -fo)\r\n- We have applied the following steps on the Secondary EM/DB Server to complete this activity\r\n  * Took a DB backup (/var/mcp/run/MCP_14.1/ssdvdb_1/bin/util/ ./dbBackup.pl )\r\n  * Initiated the Resync process from Secondary DB to Primary DB (/var/mcp/run/MCP_14.1/ssdvdb_1/bin/util/ ./resync.pl)\r\n- When the resynch process is completed, customer informed us that they had \"Data Out of Sync\" and \"Data distribution\" alarms related to PROV instances on the MCP GUI.\r\n- Asked the customer to restart both PROV Instances on MCP GUI and the alarms cleared\r\n- Customer also informed us that they got the \"null\" output when they ran the qsip query for the lines that they had provisioned between Sunday and Today. \r\n- Asked the customer to perform double swact the SESM Instances on the MCP GUI as we thought that there might have been a problem with the SESM IMDB and it could not get the data from the DB for some reason. Swacting the SESM units fixed the \"null\" qsip data problem.\r\n- Customer performed additional provisioning tests on the system and they were all successful.','null'),(425,'Yunus Ozturk','AS-OAM','2015-03-17','150317-522077','Unitymedia','Problem Description:\r\n----------------------\n\r\nPost upgrade, having initially seen all secondary servers successfully brought into Hot Standby mode, it was noted that SESM5_1 and SESM1_1 were at unavailable status.\n\r\nSolution:\r\n-----------\n\r\nInitial diagnosis pointed to /var/mcp/run directory being 100% full due to old MCP14.0 core logs generated over the course of the Primary server update part of the upgrade.\n\r\n[root@sslsesm41 root]# df -h\r\nFilesystem Size Used Avail Use% Mounted on\r\n/dev/md4 2.0G 703M 1.2G 37% /\r\n/dev/md1 99M 15M 79M 16% /boot\r\n/dev/md5 6.0G 276M 5.4G 5% /opt\r\nnone 2.0G 0 2.0G 0% /dev/shm\r\n/dev/md6 5.0G 64M 4.7G 2% /var\r\n/dev/md7 51G 48G 0 100% /var/mcp\r\n/dev/md0 99M 4.1M 90M 5% /admin\n\r\n[root@sslsesm41 work]# du -h /var/mcp/run/MCP_14.0/SESM5_1/work\r\n4.0K /var/mcp/run/MCP_14.0/SESM5_1/work/journal/SESM51/events\r\n8.0K /var/mcp/run/MCP_14.0/SESM5_1/work/journal/SESM51\r\n12K /var/mcp/run/MCP_14.0/SESM5_1/work/journal\r\n47G /var/mcp/run/MCP_14.0/SESM5_1/work\n\r\n- After many checks, it was confirmed that these older MCP_14.0 core logs generated over the course of the Primary server update part of the upgrade could be deleted.\r\n- Once space was cleared, a successful redeploy and start of the servers was possible.','null'),(426,'Yunus Ozturk','AS-OAM','2015-03-17','150317-522077','Unitymedia','Problem Description:\r\n---------------------\n\r\nPerforming A2 Core upgrade, SESM3_1 fails patching of OS Linux at 14.1.7 with the following error: \n\r\n[*P-Info*] Unzipped \"sftpPullDaemon.zip\". \r\n[*P-Info*] Running \"sftpPullDaemon\" installer. \r\n[*P-Info*] Executing command: \"cd /var/mcp/os/install/workdirs/temp/sftpPullD \r\naemon/.;./sftpPullDaemonInstall.pl >/tmp/sftpPullDaemon-install.out 2>&1\" \r\n[*P-Error*] [*P-Error*] Retcode is >>0x0100<< \r\n[*P-Error*] Signal part is >>0x00<< \r\n[*P-Error*] Main part is >>0x01<< \r\n[*P-Error*] System return strings: \r\n[*P-Error*] None. \r\n[*P-Error*] Failed at installing \"sftpPullDaemon\". \r\n[*P-Info*] --------------------------------------------------------------------- \r\n----------- \r\n[*P-Error*] \r\n[*P-Error*] Patch part #1 exited with an error (2). \r\n[*P-Error*] \r\n[*P-Error*] ******************************************************************** \r\n** \r\n[*P-Error*] * \n\n\r\nSolution:\r\n------------\n\r\n- Performed reboot of server and re-initiated the patching session manually, same failure point was reached. \n\r\n- A rollback to the 14.0.26 OS level was performed and patching initiated once again manually. This time patching hung at 14.1.2 (part 1 of 20. \n\r\n- After 1 hour, the process was killed (Ctrl-C from command line). Various attempts were made to resume patching, all failed.\n\r\n- Finally original issue was seen to be that a base level user is not present on the server. We thought that someone has removed the OS base level users manually or by mistake..\n\r\nFailure of patch 14.1.7 as follows: \n\r\n[root@sslsesm21 tmp]# cat sftpPullDaemon-install.out\r\nexecuted system command: \"/bin/rm -rf /opt/mcp/sftpPull\"\r\nexecuted system command: \"/bin/rm -rf /var/mcp/run/sftpPull\"\r\nexecuted system command: \"/bin/mkdir -p /opt/mcp/sftpPull\"\r\nexecuted system command: \"/bin/mkdir -p /var/mcp/run/sftpPull\"\r\nexecuted system command: \"/bin/cp -fR * /opt/mcp/sftpPull/\"\r\nexecuted system command: \"/bin/chown -R root:ntossgrp /opt/mcp/sftpPull && /bin/chmod -R 754 /opt/mcp/sftpPull\"\r\nexecuted system command: \"/bin/chown -R root:ntossgrp /var/mcp/run/sftpPull && /bin/chmod -R 774 /var/mcp/run/sftpPull\"\r\n/bin/chown: `ntossadm:ntossgrp\': invalid user\n\r\nFailed at executing system command \"/bin/c \r\nFailed at executing system command \"/bin/chown -R ntossadm:ntossgrp /home/ntossadm/sftpPullFlags && /bin/chmod -R 770 /home/ntossadm/sftpPullFlags\", rc = 256! \n\r\n- At this point, advised that re-installation of this server is required as a core user has been removed at some point in the past.\r\n- Skipped the patching process of this server with Force Next on the wizard and completed the upgrade except for the problematic SESM server. \r\n- Customer will proceed with the re-installation of the problematic server separately.','null'),(427,'Cigdem Vural','AS-OAM','2015-03-17','150317-522072','Princeton University','Upgrade path: 14.1.10.3 to 17.0.22.1\n\r\nKeith first called CallP team for sip outbound calls failing. They have investigated amd make their tests.\n\r\nThen they checked and try to launch GUI but saw that GUI cannot be launched and paged OAM team. \n\r\nThe SM was not running so make the primary ( upgraded one) active. \r\nThe upgrade wizard was stuck at Upgrading primary instances screen.\n\r\nSM and DB were upgraded to the new load. PROV and SESM instances could not be up with the new load. They failed with rvstack error, at PROV and SESM work logs following error was written:\n\r\nat URL: https://{server.name}:{server.port}/uniformcalldist/messagebroker/amf\n\r\n A fatal error has been detected by the Java Runtime Environment:\n\r\n  SIGSEGV (0xb) at pc=0x644ecea7, pid=1937, tid=1862220720\n\r\n JRE version: 6.0_26-b03\r\n Java VM: Java HotSpot(TM) Server VM (20.1-b02 mixed mode linux-x86 )\r\n Problematic frame:\r\n C  [libRVStack-Linux-noepoll.so+0x172ea7]  void&+0xef\n\r\n An error report file with more information is saved as:\r\n /var/mcp/run/MCP_17.0/PROV2_0/work/hs_err_pid1937.log\n\r\n If you would like to submit a bug report, please visit:\r\n   http://java.sun.com/webapps/bugreport/crash.jsp\r\nThe crash happened outside the Java Virtual Machine in native code.\r\nSee problematic frame for where to report the bug.\n\n\n\r\nSWD engineer told mtc window has finished so requested rollback. And we asked for Oracle 10g load but they do not have it. At 10.4 upgrade documents it is written in so many parts that Oracle 10g load should be available for in case of any rollback but at that site there was not any. \n\r\nSo SWD started to look for the 10g load and we continued investigation while they are trying to find the 10g load.\n\r\nAfter an hour SWD found the 10g load and we started rollback.\r\nRollback completed and all test calls passed.\n\r\nSite is at 14.1.10.3 load right now.','null'),(428,'Nuri INCE','AS-OAM','2015-03-11','150311-521228','Vodafone New Zealand','Upgrade Path:14.1.8.0 -> 14.1.10.3\n\r\nI was paged for a problem observed on Upgrade Wizard on pre-Upgrade steps -Load Distribution Screen-.\n\r\nAlthough pre-upgrade wizard steps are not covered with the pager support, since I was paged in the office hours, I decided to help fixing the problem with a personal initiative.\n\r\nOn the path upgrading from 14.1.8.0 to 14.1.10.3, there is no ple1 platform upgrade and eventually, wizard is skipping the load extraction for A2I. Yet still 14.1.12 ple1 platform patch .iso file was being tried to distributed to other ple1 servers by wizard on Load Distribution screen.\n\r\nHowever since there is nothing to be distributed, wizard was failing with missing platform patch file errors.\n\r\nWith the design teams approval, we have verified that this was a software bug. After analysing the root cause of the problem, and since the problem was not going to affect the further steps in the upgrade wizard, we have launched the wizard in debug mode and force next\'ed that particular screen to let customer proceed with the upgrade.\n\r\nAfter passing the screen, we agreed with the caller and ended the pager call.','null'),(429,'Cigdem Vural','AS-OAM','2015-03-04','150304-520083,150304-520134','Bell Alliant','Bell Load: 17.0.12.16\r\nThey have 7 jar files at SESM\n\r\nBell Alliant reported that they have null data for qsip output for the newly provisioned lines.\n\r\n qsip 5066571196 \r\n------------------------------------------------------------------------------- \r\nSIP USER DATA: \r\n============= \r\nSIP URI: null \r\nACCOUNT STATUS: null \r\nREGISTERED: N \r\nNUMBER OF CONTACTS: 0 \r\nCONTACTS: \r\nSERVICE PACKAGE: \r\nSERVICES: \n\r\nSIP LINE DATA: \r\n============= \r\nENDPT ID: null \r\nVMG: null \r\nZONE ID: \r\nCLIENT TYPE: null \r\nSTATIC CLIENT: N \n\r\nSIP CALL DATA: \r\n============= \r\nNUMBER OF ACTIVE SESSIONS IN SESSION SERVER: UNKNOWN \r\nEND POINT DATA: \r\nSIP_CLIENT_TYPE: ont \r\nSIP_EP_NAME: SSL3/000/6/0118 \r\nSIP_VMG_NAME: VMGSSLI3-0 \r\nSIP_PMC_PASSWD: SET \r\nSIP_FIRST_NAME: SIPLineUser \r\nSIP_LAST_NAME: SIPLineUser \r\nSIP_DN: 5066571196 \r\nSIP_TIMEZONE: AST \r\nSIP_LOCATION: nb \r\nSIP_URI: 5066571196a@fibreop506.ca \r\nSIP_SUBDOMAIN: res506.fibreop506.ca \r\nSIP_OPTIONS: voicemail SP=voicemail_profile_1;clicktocall;allowedclients SP=allowedclients_profile_1;pmcdata pmcPassword=********;sipline vmg=VMGSSLI3-0 dn=5066571196 endPointId=SSL3/000/6/0118 sipProfiles=ont; \r\n------------------------------------------------------------------------------- \n\n\r\nWhen we checked for the line, it is existing at A2 DB but not existing at IMDB. \r\nWe did audit for subscriber table at IMDB and it did not help\r\nTry to do some nill changes at PROV GUI if we will see the user at IMDB but that did not help\n\r\nWe had a debug jar for this problem and the logs from oss as below:\n\r\nSESM3_0 DEBUG 87 INFO MAR03 19:55:06:924 MCP_17.0.12.16\r\n  ***BELLDEBUGJAR*** - IMSubscriberTable - addUsersToDomain - Line 772 \n\r\nSESM3_0 DEBUG 87 INFO MAR03 19:55:06:924 MCP_17.0.12.16\r\n  ***BELLDEBUGJAR*** - IMSubscriberTable - receiveUpdate - Line 589 \n\r\nSESM3_0 SWERR 799 ALERT MAR03 19:55:06:924 MCP_17.0.12.16\r\n  Error processing table dependency CacheEvent for IMSubscriberTable.\r\n    Reason : AddrLookup Users Added\r\n    OldData : null\r\n    NewData : null\n\r\n  java.lang.NullPointerException\n\n\r\nSESM3_0 DEBUG 87 INFO MAR03 19:55:12:585 MCP_17.0.12.16\r\n  ***BELLDEBUGJAR*** - IMSubscriberTable - addUsersToDomain - Line 772\n\n\r\nCustomer tried to restart Hot Standby instance and it stuck at Synchronzing state while doing checkpointing. \n\r\nWe performed stop both SESM instances and start them one by one.\r\nThen we could see the newly provisioned data at IMDB.\n\r\nThere are some other issues also experienced during restart and newly provisioned line that was seen by CallP team.\n\r\nAfter covering the problem, CallP team asked to collect all necessary data for the RCA.\n\r\nThere are 3 issues here:\n\r\n1- SESM stuck at syncrozing because of checkpointing and exceptions\r\n2-Newly provisioned lines are not seen at IMDB\r\n3- A rsrc_id shown at SESM3 while it is expected at SESM4','null'),(430,'Oktay Esgul','AS-OAM','2015-03-04',' 150304-520092  ','Hong Kong Broadband','Thomas Godwin from ER paged me in order report Accounting issue on HKBN system.\n\r\nThe system  level :\n\r\nSun Platform-MCS5200\n\r\nApplication: MCS 3.0\r\nOS: SunOS 5.8\n\r\nCustomer reported that they can not get any billing record for last 2 days. I connected servers and checked the server status, one of the server was up almost for 9 years (3150 days), however we could not run any command\r\non active accounting server, we were getting I/O errors.\n\r\nSince this system is quite old, we had troubles with launching the MCP GUI, we tried to open MCP Gui more than 1 hour,\r\nbut could not succeed (There is a strange procedure to launch the gui, firstly we need to change computer date 2005 year, then tried again, it did not help\r\n,then customer provide a java file to run gui directly , it did not help either. ) \n\r\nWhile we were trying, customer reported that they  swacted sm instances and they could launch the gui now.\r\nAfter that they launced the gui, they reported that there are disk errorr on active accounting manager server which seems possible outage reason.\n\r\nThen, I connected to other accounting server via nortel user. As far as I know, in this old system instance are working active/cold-standby instead of active/hot-standby.\n\r\nIn order to activate the cold-standby instance , I planned to run failover script on cold standby instance since this script stop the active one and start the cold-standby instance.\n\r\nI jumped to /IMS/acctmgr/bin directory which Failover.pl script located.\n\r\nTried to run script via nortel user, it failed with below error.\n\r\n[@AcctSvr0]/IMS/acctmgr/bin:=> Failover.pl start acctmgr   \r\nCould not open file to write => /tmp/Failover_acctmgr.log\n\r\nAbove error seems we have user privilege issue because I checked the tmp/ directory and Failover_acctmgr.log file was located under tmp directory, so that, I asked the customer to provide root user password , if they  know.\n\r\nI jumped to root user and ran the failover script again, it worked and cold-standby instance became active one.\n\r\n[@AcctSvr0]/IMS/acctmgr/bin:=> su\r\nPassword: \r\nYour password will expire in 2658 days.\r\n# cd /IMS/acctmgr/bin\n\r\n# ./Failover.pl start acctmgr\r\n> .\r\n> killed\r\nCreated new logical interface bge0:3\r\nLogs are written to /IMS/acctmgr/SetActiveFlag.log \r\n> .\r\n> .\r\n> .\r\n> started\r\n Output from ifconfig -a command is:\n\r\n lo0: flags=1000849 mtu 8232 index 1\r\n        inet 127.0.0.1 netmask ff000000 \r\n bge0: flags=9040843 mtu 1500 index 2\r\n        inet 10.15.192.8 netmask ffffff80 broadcast 10.15.192.127\r\n        groupname imspub\r\n        ether 0:3:ba:9b:b7:1 \r\n bge0:1: flags=1000843 mtu 1500 index 2\r\n        inet 10.15.192.10 netmask ffffff80 broadcast 10.15.192.127\r\n bge0:2: flags=1000843 mtu 1500 index 2\r\n        inet 10.15.192.7 netmask ffffff80 broadcast 10.15.192.127\r\n bge0:3: flags=1000843 mtu 1500 index 2\r\n        inet 10.15.192.11 netmask ffffff80 broadcast 10.15.192.127\r\n bge1: flags=69040843 mtu 1500 index 3\r\n        inet 10.15.192.9 netmask ffffff80 broadcast 10.15.192.127\r\n        groupname imspub\r\n        ether 0:3:ba:9b:b7:2 \n\r\n Logs are written to /tmp/Failover_acctmgr.log \n\r\nAfterward, we completed swact operation, new billing records started to be generated again,end of outage.\n\r\nCustomer asked  if there is any chance to recover last 2 days billing records, but as the server has disk issues and no chance to run any command in that server, I said them to open a case for follow up to find out if there is a way for recovery. They will open another case for disk replacement  for the problematic server as well.\n\r\nSince the main problem resolved,we aggreed to drop the call.\n\r\nThanks','null'),(431,'Oktay Esgul','AS-OAM','2015-02-20','150220-518428 ','County of Los Angeles','Kyle Mawst from ER paged me in order to report that UW stuck at SM/DB Upgrade screen.\n\r\nUpgrade Path : \n\r\nFrom:MCP_17.0.12.20\r\nTo: MCP_17.0.18.4\n\n\r\nEven though wizards had completed SM/DB upgrade screen successfully, there was not NEXT button to skip next screen.I connected site, checked the logs and MCP GUI and verified all SM/DB upgrade steps completed successfully.Then,focused on the wizards issue.\n\r\nWe tried Save&Exit , but it did not help.\n\r\nThen, I cleanded up the java cache of the laptop that wizards running ,then tried again , It did not help.\n\r\nAs a next step, I recommended to run wizards from another laptop, but since the SWD engineer running wizards on customer laptop , they said they would not be able to change laptop.\n\r\nSo then, I decided to run wizards in debug mode just to skip that screen. I run wizards in debug mode, skip that SM/DB upgrade screen.After that wizards continued the upgrade with next step without any issue.\n\r\nAs the wizards had been continued upgrade smoothly, we agreed with ER and upgrade engineer than I dropped the call.','null'),(432,'Burak Biyik','AS-OAM','2015-02-11','150211-516929','Razorline','Eric Duke from SWD team paged me to report an issue with Gencom Client after S3 Upgrade. Customer\'s upgrade path is as follows: SBC 8.1.2.8 -> 8.3.7.0 -> 8.3.7.4\n\r\nFollowing the upgrade, customer faced some issues with DTMF and S3 team wanted to involve Gencom GPS since they suspected that the problem was about Gencom client.\n\r\nIt was reported that customer would not allow SWD to complete whole upgrade steps until this issue was resolved.\n\r\nSo, I reported this issue to Gencom GPS and forward existing bridge information to join further discussion.\n\r\nAgree with SWD that this must be handled by Gencom GPS and dropped the call.','null'),(433,'Senem Gultekin','AS-OAM','2015-02-09','150208-516546','Bell Aliant','Problem Description: \r\nMark Zattiero (ER) reported that there were call issues still seen at St. Johns NB. Release: 17.0.12.16\r\nInvestigated the issue with Erdem Ekin (CALLP GPS) to understand if this is really related to the original issue. \n\r\nRCA: \r\nFound out that customer was using nb location for 4 different routability group. Where we were only focusing on none routability group. \r\nWhen they lost the locations on Sunday morning, they have corrected the outage by assigning the location of the routability group for none, JDI, AliantICN and delta2801. Eventough , it is not logical to assign multiple routability groups for a single domain location tuple, the provisioning allows such configuration and SESM choses to use the first assigned routability group for a given location. Since fiberop506.ca domain and nb location is associated with firstly with AliantICN and then none routability group, SESM processes these calls over AliantICN. \n\n\r\nSolution: \r\nER unchecked all nb locations on routability groups and left only nb location on none routability group. \n\r\nNext Action: \r\n1. Will open an enhancement jira to change the logic of the multiple routability groups(with the same location) for a single domain. Action: GPS \r\n2. Prepare a bulletin about manual correction of existing duplicate location. Action GPS','null'),(434,'Senem Gultekin','AS-OAM','2015-02-09','150208-516546','Bell Aliant','Problem Description:\n\r\nER paged OAM GPS for failures seen on SESM logs. Customer didn\'t report anything, but ER was monitoring the site for the location removal on routability group issue. This issue was seen in the morning.\r\nHe wanted to double check with GPS if the configuration is correct on Prov.\n\r\nRelease is 17.0.12.16. St John NB site.\n\r\nAction:\n\r\nAssisted how to check the locations on Prov. \r\nMedia Portal Routability Group -> none -> locations -> fiberop506.ca -> \'nb\' location was checked.\n\r\nER talked with Mike(Customer) and he verified that only \'nb\' needs to be checked. Therefore, we have agreed that the configuration was correct and there is lost with the locations.\n\r\nIf there were further call related issues suggested ER to contact CALLP GPS.\n\r\nAgreed and ended the call.','null'),(435,'Senem Gultekin','AS-OAM','2015-02-08','150208-516546','Bell Aliant','Problem Description:\n\r\nER paged OAM GPS for the location lost (uncheck on Prov) at St Johns NB site of Bell Aliant. Customer realized that there was a call outage. They logged into the PROV checked the locations for the reputability groups back for the problematic domains. \r\nThey requested to deploy the same trigger(which is already in Halifax) to block provisioning for locations. \r\nCustomer is running on 17.0.12.16 Release.\n\r\nSolution:\r\nWorked with Mert Zeybekler (OAM Architect). \r\nModified the existing trigger to not allow uncheck the locations. \n\r\n**Create Trigger** \n\r\n#! /bin/bash \r\nsu - ntdbadm -c \"echo \\\"CREATE OR REPLACE TRIGGER GPS_NO_DELETE_0 \r\nBEFORE DELETE \r\nON mproutegroup_locationmap \r\nFOR EACH ROW \r\nBEGIN \r\nraise_application_error (-20100, \'150205-516145 - GPS says you cannot untick route locations\'); \r\nEND; \r\n/ \r\n\\\"|sqlplus ssdvdb/ssdvdb\" \n\n\n\r\n**Drop Trigger** \n\r\n#! /bin/bash \r\nsu - ntdbadm -c \"echo \\\"DROP TRIGGER GPS_NO_DELETE_0;\\\"|sqlplus ssdvdb/ssdvdb\" \n\n\r\nApplied the above create trigger to both St Johns NB and St Johns NF sites.  Customer was fine with the applied triggers.\n\r\nAs a note: St Johns NB and St Johns NF sites do not have our jar(17.0.12.16_AAK-36038_Nov_7.jar) which was implemented for location lost issue. \n\r\nNext Action: \r\n1- Apply 17.0.12.16_AAK-36038_Nov_7.jar to St Johns NB and St Johns NF sites. Action: GPS \r\n2- We have collected the needed logs and will analyze if the issue is related to a CMT audit like we saw months ago at Halifax. Will give an update on this. Action GPS/DS.','null'),(436,'Senem Gultekin','AS-OAM','2015-02-07','150206-516517','BT Spain','Problem Description:\n\r\nSWD paged me for an upgrade step failure on wizard screen \"Prepare DB\". This screen is one of the actual upgrade steps (not pre upgrade steps)\n\r\nCustomer: BT Spain live site\r\nUpgrade Path: MCP_14.0.9.12_2012-10-28-1235 to MCP_14.0.16.3_2013-09-12-0840\n\n\r\nSolution:\r\n-	Accessed to the site over Edwins PC.\r\n-	Checked the logs under /var/mcp/upgrade_tools/logs/cleanupReplication. Error was as following;\n\r\n---------------------------------------------------------------\r\nError occurred executing NE commands (see below):\n\r\nNE command exited with the value: 1\r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20150206_222618_purA\r\nNot unlinking file\n\r\n> config ok\r\nNE command exited with the value: 1\r\n> exited\n\n\r\nFailed to cleanup replication on the server 62.7.41.69\n\r\n---------------------------------------------------------------\n\r\n-	Prepare DB screen drops the replication between 2 databases, and takes the database backup before the upgrade. However it seemed like drop replication failed due to communication between 2 databases which is done via NED during cleanupreplication script.\r\n-	Restarted both NED on both databases as following\r\nneinit restart\r\n-	After the NED restart SWD retried the Prepare DB screen, and it passed successful. \r\n-	SWD continued with the upgrade and completed fully to 14.0.16.3.','null'),(437,'Yunus Ozturk','AS-OAM','2015-01-30','140723-483633','BT MSL','Problem Description:\n\r\nGTS paged out GPS for a planned ATCA Blade RTM Card replacement activity on customer site.\n\r\nActions Taken:\n\r\n- NIS failed to boot the new RTM/Disc from CD even after plugging the DVD-Rom to NDM\r\n- Asked them to plug it directly to the blade itself but it failed again\r\n- Asked them to check the BIOS settings to see if the Boot sequence is correct not. \r\n- NIS checked the BIOS settings and reseated the front blade and then they were able to boot from CD \r\n- NIS could not connect to SESM blade from terminal server. They told us that there was no KVM on site but the script asked for KVM or COM port\r\n- NIS figured out the COM port connection \r\n- After restoring the users as a part of the provided the procedure, installation halted as follows; \n\r\nInstallation of platform on ATCA-i7 hardware should be NFS based.\n\r\nThis Installation has been halted. \r\nFor error details, see /tmp/PreInstall.log. Use -F2 to access the \r\nshell and view the file. Use -F8 to return to this screen \n\r\nPress the ENTER key from this screen when ready to reboot \n\r\n- Asked them to check if the CD is still in DVD-ROM or not as after the failure it might have been ejected\r\n- NIS confirmed that it is not ejected. \r\n- Suggested to check the DVD-ROM with another box\r\n- Customer told that without approve, they can\'t touch any other working server \r\n- Checked the BIOS settings and noticed that network option is on the top of the boot sequence list\r\n- Set the DVD-ROM as the first option then rebooted the server again\r\n- Server booted up from DVD-ROM and installation wizard started \r\n- Installation stucked again and NIS had to re-seat the front blade to reboot the it again.\r\n- Selected \"install-KVM ATCA\" option to start the installation but failed again\r\n- Blade reseated again but this time it was stuck at shell level as follows;\n\r\nshell> exit \r\nStuck at boot level as trying to boot from network \n\r\n- Decided to put the old RTM to the blade.\r\n- After reseating the blade, noticed the following Boot options on the BIOS settings;\n\r\n-> EFI SHELL \r\n-> Network \n\r\n- Server was not able to read any of the disks \r\n- Did reset the blade from NDM but did not work\r\n- NIS changed the cable from terminal to SESM and reseated the blade but did not work again.\r\n- Latest status is SESM1Server1 is up and running but SESM1Server2 is down\r\n- Since the MW is over, another MW is set up for today\n\r\n- GPS prepared a new step by step procedure and online support will be provided to the customer and site team.','null'),(438,'Senem Gultekin','AS-OAM','2015-01-30','150130-515221','Windstream','Problem Description:\n\r\nER paged OAM pager for a MOCK upgrade failure at Windstream. Even tough MOCK upgrade failures are not handled over pager support I have worked with ER during work hours.\r\nUpgrade path:  14.0.16.3 to 14.1.10.0\n\r\nError: db server 10.34.48.22 does not have enough disk space to perform db dry run upgrade!  \n\r\nSolution:\n\r\n	Accessed to the site over ERs PC.\r\n	Since MOCK upgrade is being ran on secondary database, Ive checked the EMServer2 disk space. It was %67.\r\n	Transferred oracle installer from /var/mcp/media directory (from both EM Servers) to another server. This was done only to decrease the disk usage.\r\n	After cleaning up some space on the secondary EM Server, the mock upgrade passed the problematic part. However it failed with another error;\r\nError: Fails to create duplicated schema from currently deployed schema.\r\n	This is a known issue at its only seen at older releases. We have a bulletin how to fix this failure. ID: 00008741-01\r\n	ER applied this bulletin and after that MOCK upgrade passed successfully.','null'),(439,'Yunus Ozturk','AS-OAM','2015-01-30','150130-515206','Singtel Optus Pty Ltd','Problem Description:\n\r\nGPS was paged out due to the following issue;\n\r\nUpgrade Path : From 17.0.18.1 to 17.0.18.4\n\r\nWhile upgrading Optus Blacktown IMS A2 TAS from 17.0.18.1 17.0.18.4, the Upgrade Wizard was unable to upgrade PROV1_0 (and later PROV2_0). PROV1_0 went to \"Online Down Unavailable\" and Wizard just stopped at that point waiting for PROV1_0 to come back up on the \"Upgrading Primary Network Element Instances / Upgrading Secondary Network Element Instances\" screens.\n\r\nTried to Save and Exit in Wizard, but that brought it to the same point.\n\r\nSolution:\n\r\nBased on the logs, we noticed that Prov instances could not come up due to the following error;\n\r\n2015-01-30 11:51:00,545 DEBUG PanelController - Upgrading NE : PROV1_0 with load MCP_17.0.18.4_2014-12-25-1926\r\n2015-01-30 11:51:00,826 ERROR NEIOperationsManagerImpl - Error occurred during operation : PROV1.0 is not in a stable state: Unavailable\r\n2015-01-30 11:51:00,826 ERROR PanelController - Upgrading PROV1_0 is not completed : NEI_UPGRADE_FAILURE\r\n2015-01-30 11:51:00,826  INFO PanelController - PROV1_0:Upgrade operation failed on Network Element Instance: PROV1.\r\n2015-01-30 11:51:00,826 DEBUG PanelController - Operation completed for : PROV1_0\r\n2015-01-30 11:51:01,670 DEBUG PanelController - PROV1_0\'s state hasn\'t been stable yet.\r\n2015-01-30 11:51:02,795 DEBUG PanelController - PROV1_0\'s state hasn\'t been stable yet.\r\n2015-01-30 11:51:03,904 DEBUG PanelController - PROV1_0\'s state hasn\'t been stable yet.\n\r\nWe have also seen many exceptions on the work logs of the PROV instances.\n\r\n- We have performed \"neinit restart\" to restart the NED on both EM Servers.. Did not work\r\n- Started the PROV1_0 with the \"./neStart.pl\" manually and it came up with the old load (17.0.18.1)\r\n- Did Save&Exit the wizard and re-launch it. The screen passed and the instance was upgraded to the new load (17.0.18.4)\r\n- Same thing happened for PROV2_0 instance\r\n- Killed the PROV2_0 instance on the MCP GUI manually and started back again.\r\n- It came up with the old load again and after re-trying the wizard screen, it passed and the instance was upgraded to the new load.\n\r\nRequested the logs for the root cause analysis and follow-up case was opened up.','null'),(440,'Yunus Ozturk','AS-OAM','2015-01-28','150128-514287','GTD','Problem Description:\n\r\nGTS paged out GPS and reported that they had many SIP Lines in NULL state on OSSGATE. They also had the lines eventually go unregistered.\n\r\nTo recover the problem, customer restarted both SESM Instances on A2 site and restarted some services on CMT side. \n\r\nGTS requested data collection assistance to be able to find the reason of this specific problem. \n\r\nSince data collection requests cannot be handled and supported on the pager call, asked the customer to raise a case and send it to the correct team for further analysis and root cause.','null'),(441,'Yunus Ozturk','AS-OAM','2015-01-29','150129-514551','Bragg Communications (Eastlink)','Problem Description:\n\r\nSWD paged out the GPS and reported the following issue;\n\r\nUpgrade Path : From 17.0.18.0 to 17.0.18.3\n\r\nWhile performing an A2 Patch upgrade from 17.0.18.0 to 17.0.18.3 load, the SM_0 instance took abnormal amount of time to startup on SM/DB Upgrade screen and the upgrade wizard screen stayed at stuck status. \n\r\nSolution : \n\r\nWe have accessed the site and checked the existing status of the SM instances. We noticed that even if the SM_0 was at Active State and SM_1 was at Hot Standby state on the MCP GUI, the upgrade wizard was stuck on the SM/DB upgrade screen where it is Starting up the SM_0. \n\r\nOn the corresponding SM/DB upgrade screen is, SM_0 (Active) is switched to Hot Standby mode and SM_1 (Hot Standby) is switched to Active mode. Then, Hot Standby unit (SM_0) is upgraded to the new load and it is switched back to the Active mode again. Since the SM_0 was at Active Mode and upgraded when we have opened up the MCP GUI, we are thinking that someone has performed a manual swact and switched the SM_0 to Active mode since it took so long time to start the instance on the wizard screen. As the instance was manually switched back, wizard stayed at stuck status. \n\r\nWhen we have performed the Save&Exit and Re-Launch the wizard, it has performed an internal validation check and identified that the status of the SM instances are correct which is SM_0 (Active / Upgraded) and SM_1 (Hot Standby / Not Upgraded) \n\r\nSince the issue was resolved and the reason of the failure was explained to the SWD Engineer, we agreed to drop the call.','null'),(442,'Yunus Ozturk','AS-OAM','2015-01-28','150128-514235','Windstream','Problem Description:\n\r\nSWD paged out the GPS and reported the following problem while doing an upgrade from 12.0.12.7 to 14.0.16.3 \n\r\nerror on Upgrade database and system manager \r\nExecuting DB & SM upgrade \r\nLogs are written to /var/mcp/upgrade_tools/logs/monitored_scripts/monitored.ut_mcpUpgrade.EMS1_999f1b80-2ac0-1b21-9cff-001b2110d0c3_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20150128_000153.log \n\r\nValidating the scripts and files required for upgrade. \r\nInvalid Load name given. Must have be of the form \".zip\" \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/mcpUpgrade/ut_mcpUpgrade.EMS1_999f1b80-2ac0-1b21-9cff-001b2110d0c3_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20150128_000158.log \n\r\nSolution:\n\r\nWe have accessed the site and checked the logs for this issue. It seems that the zip format of the patch load file does not exist under /var/mcp/loads/ directory of the Primary EM Server. \n\r\nTo figure out this issue, we had to create the zip format of the patch load file manually under /var/mcp/loads/ directory as it is required. It seems that someone has removed the zip format by mistake in the past. \n\r\nThe following steps have been applied; \n\r\n- zip -9 -r MCP_14.0.16.3_2013-09-12-0840.zip MCP_14.0.16.3_2013-09-12-0840 \r\n- chmod 664 MCP_14.0.16.3_2013-09-12-0840.zip \r\n- chown ntappsw:ntappgrp MCP_14.0.16.3_2013-09-12-0840.zip \n\r\nAfter creating this zip format manually, the upgrade wizard screen passed.','null'),(443,'Yunus Ozturk','AS-OAM','2015-01-27','150127-514098','Larry McLawhorn','Problem Description:\n\r\nSWD has paged out the GPS and reported that the upgrade wizard failed at the screen \"Upgrading Secondary Network Element Instances\" (35/46) \n\r\nUpgrade Path : From 14.0.16.3 to 14.1.10.0 \n\r\nSolution:\n\r\nWe have accessed the site and checked the status of the system. It seems that the Primary Network instances have been successfully upgraded to 14.1.10.0 Load. However, while upgrading the Secondary Network instances, a problem occurred and wizard has logged out somehow. \n\r\nSince the Remote Desktop PC where the wizard is being launched (Genband Access) does not allow us to do anything on it, we could not check the upgrade wizard logs where they are located on the local PC \n\r\nWhen we try to launch the MCP GUI or the Upgrade Wizard, we did encounter with Java issues. We have tried the following actions but they did not help us to figure out the Java issues. \n\r\n- Stopped/Started both SM units (smStop.pl / smStart.pl) \r\n- Executed the \"mcpInstallFirstLoad.pl\" script to get the new script files for the new MCP Load (14.1.10.0)\n\r\nBased on the actions above, we have asked the customer to check the Java issues on the Remote Desktop PC and try to launch the MCP GUI and Upgrade Wizard. \n\r\nSWD informed us that customer\'s IT Team will check the Java issues.','null'),(444,'Yunus Ozturk','AS-OAM','2015-01-27','150127-514069','Windstream','Problem Description:\n\r\nOn DB Mock upgrade screen, upgrade wizard failed with the following error;\n\r\nChecking db server 10.34.48.22 disk space to perform dry run upgrade ... \r\nDB server 10.34.48.22 total tablespace size is 15939453 KB \r\nError: db server 10.34.48.22 does not have enough disk space to perform db dry run upgrade! \n\r\nUpgrade Path : From 12.0.12.7 to 14.0.16.3 \n\r\nSolution:\n\r\nBased on the logs, the issue was related with the insufficient disk space on both DB Servers. Accessed the site and checked the disk space and usages. The existing DB data size of the customer under /var/mcp/db/data/ directory of the server was around 18GB but the server had 15 GB available disk space which was causing this problem. \n\r\nWe have moved the previous DB Backup files (around 4 GB) under /var/mcp/upgrade_bkups/pre/ directory to another local server (SESM Server) in order to free up the disk spaces on both DB Servers.  \n\r\nThe backup files under /var/mcp/upgrade_bkups/pre/ directory have been moved to the SESM Server (under /var/mcp/DB_backup directory) to free up the disk space on the EM Servers. \n\r\nEven after moving the DB backup files, the DB Servers had around 22 GB disk space and the DB Mock screen passed after retrying it.','null'),(445,'Yunus Ozturk','AS-OAM','2015-01-26','150126-513892','Ihub UK Limited','Problem Description:\n\r\nER has paged out OAM GPS to report all SIP calls and registrations failing in the Egham, UK CS2Kc. The SESM units were going into overload.\n\r\nCustomer Load : MCP_14.0.16.3\n\r\nSolution:\n\r\nSince, this issue was related to Callp GPS Team, we have asked the ER Team to page out the Callp GPS. \n\r\nCustomer had removed the SESM jar files and the SO file on the SESM Server by mistake and even after they re-apply the jar files, the issues (SESM instances bounced periodically) started to happen. \n\r\nThe SESM units were manually failed over to resolve the issue. Additionally, customer has not applied the SO file which handles NOTIFY keep-alive messages on RV level instead of Application layer and the reason of the traffic overload on SESM Server.','null'),(446,'Nuri INCE','AS-OAM','2015-01-22','TBD','Windstream','Upgrade Path, 12.0.12.7 -> 14.0.16.3\n\r\nSWD Antti paged me after receiving errors while trying to start Upgrade Wizard in Patching Primary Oracle screen.\n\r\nHe reported that after the platforms of primary servers are upgraded, one of the sesm servers failed to boot after application of platform patch.\n\r\nWe have tried to access wizard logs from the local PC to analyse the actual problem, however Antti was not able to access Wizard logs due to the permission restrictions on the PC he used for the upgrade.\n\r\nSo we tried to ping problematic SESM server but we could not. So we asked customer to perform hard reboot on the server. After the hard reboot, server again failed to come up.\n\r\nThen customer was asked to swap the disks on the server and retry hard rebooting it. After swapping the disks, server came up successfully.\n\r\nUpgrade Wizard became able to be launched after performing the activities above. However Antti said that we had only 15 minutes remaining in the maintenance window.\n\r\nSo we decided not to proceed with the upgrade. After explaining the customer that they will be able to perform provisioning and establish calls at the current state of the system, and agreeing with the SWD, we ended the pager call.\n\r\nThe day after the pager support, it is reported that upgrade was completed successfully starting from the state that we left the system during pager support.\n\r\nAntti reported that he was not able to raise cases through SFDC and he committed that he is going to raise a case and dispatch it to PS A2 OAM Queue as soon as he can.','null'),(447,'Nuri INCE','AS-OAM','2015-01-25','140723-483633','British Telecommunication','I was paged by ER for an assistance request received from BT for their RTM replacement activity.\n\r\nI have checked the case \"140723-483633\" opened for the problem and noticed that it was older than 4 months and documented server re-installations are not under pager support coverage unless exceptional situations/requests.\n\r\nI have asked ER to explain the customer that, the support they requested is not under pager support coverage and they need to arrange maintenance window with case owners to perform installation/restoration activities.\n\r\nCustomer also accepted and agreed, then we ended the pager call.','null'),(448,'Nuri INCE','AS-OAM','2015-01-22','150121-513335','Axtel','Customer load on Puebla Site: 17.0.7.12\n\r\nI was paged by Emergency Recovery for faulty hardware alarms on SESM2Server2, SESM3Server1 and SESM3Server2. I have connected to the site and launched MCP GUI.\n\r\nAfter checking the alarms, i have noticed that the alarms on servers, which belong to SESM3 pair, were HWERR706 and HWERR704. The server types were HtLangley. \n\r\nThere is a known issue causing Htlangley servers to generate fake HWERR alarms. After checking the alarm description, i have verified that the issue was showing same symptoms as the known HWERR problem. The problem is fixed with 17.0.18.x MR.\n\r\nFor the secondary EMServer and SESM2Server2, alarms were different than alarms on SESM3 pair. For these servers, there were power supply redundancy alarms which were actually correct.\n\r\nBefore finalizing the pager call, customer is informed about the fix for fake HWERR alarms in 17.0.18.x release. Additionally i have recommended the customer to place healthy secondary power units to secondary EMServer and SESM2Server2 to provide power supply redundancy.\n\r\nAfter giving the action plan to the customer and informing them about their systems situation, we agreed on ending the pager call.','null'),(449,'Nuri INCE','AS-OAM','2015-01-22','150121-513335','Axtel','Customer load on Puebla Site: 17.0.7.12\n\r\nI was paged by Emergency Recovery for faulty hardware alarms on SESM2Server2, SESM3Server1 and SESM3Server2. I have connected to the site and launched MCP GUI.\n\r\nAfter checking the alarms, i have noticed that the alarms on servers, which belong to SESM3 pair, were HWERR706 and HWERR704. The server types were HtLangley. \n\r\nThere is a known issue causing Htlangley servers to generate fake HWERR alarms. After checking the alarm description, i have verified that the issue was showing same symptoms as the known HWERR problem. The problem is fixed with 17.0.18.x MR.\n\r\nFor the secondary EMServer and SESM2Server2, alarms were different than alarms on SESM3 pair. For these servers, there were power supply redundancy alarms which were actually correct.\n\r\nBefore finalizing the pager call, customer is informed about the fix for fake HWERR alarms in 17.0.18.x release. Additionally i have recommended the customer to place healthy secondary power units to secondary EMServer and SESM2Server2 to provide power supply redundancy.\n\r\nAfter giving the action plan to the customer and informing them about their systems situation, we agreed on ending the pager call.','null'),(450,'Seren Batmaz','AS-OAM','2015-01-19','150119-512791','Ihub UK','Problem Description:\n\r\nER paged me for the problem occured during SM swact which had been run as a periodical activity. SM_1 was not able to become Hot Standby and the system was running on MCP_14.0.16.3 load.\n\r\nDescription:\n\r\nI connected to the site and launched MCP GUI. State of SM_1 seemed to be Configured Offline Unavailable. So, I tried to deploy and start SM_1 instance from MCP GUI. However, deploy operation did not work. \r\nSo, I wanted to ssh to EMServer2, on which SM_1 had been deployed. However, the customer didnt know the server passwords for this server. So, I reset ntsysadm and root passwords and then I was able to ssh to the server.\r\nAfter that, I ran the following command to be able to see the running instances on that server:\n\r\n-> neinit -p\r\nThe output was:\n\r\nRelease         Name             Pid\r\n-------         ----             ---\r\nMCP_14.0        SM_1             21170\r\nMCP_14.0        PROV1_0          27395\n\r\nSo, it was seen that, although the instance was running on the server, it was not seen to be working on the MCP GUI. So, I killed the SM instance from the server:\r\n-> kill -9 \n\r\nThen, I deployed and started SM_1 from MCP GUI and it was able to become Online Hotstandby state. As soon as customer agrees that the problem was resolved, we ended the call.','null'),(451,'Seren Batmaz','AS-OAM','2015-01-18','150117-512752 ','Maxcom','Problem Description:\n\r\nER paged me for the problem occurred after backup and restore ATP tests for Genius. I was told that this site will be live on Monday, 19th Jan, and it was commited to give pager support during the weekend, for this preproduction site.\r\nThe problem was two critical alarms appeared on PROV2_0 and PA1_0. Both alarms was stating the following description:\n\r\nAlarmName: Media Application Server Unreachable \r\nTimeStamp: Tue Dec 16 07:33:19 PST 2014 \r\nFaultNumber: 103 \r\nShortFamilyName: MASM \r\nLongFamilyName: MEDAPSVR \r\nSeverity: CRITICAL \r\nProbableCause: underlying resource unavailable \r\n...........\n\r\nER told me that, they have already tried MAS application restart and it did not work.\n\r\nThe release of the site was 18.0.1.0\n\r\nSolution:\n\r\n- As the first action, I suggested to restart MAS monitor from MCP GUI for the problematic MAS.(Network Elements->Media Application Server-> ->Monitor). After this action was completed, the alarms persisted.\r\n- Restarted problematic PROV and PA instance from MCP GUI. However, alarms did not get cleared.\r\n- As the last action, I rebooted problematic MAS server and this cleared the critical alarms on PROV2_0 and PA1_0\n\r\nSince we agreed that the problem had been resolved, we ended the call.','null'),(452,'Seren Batmaz','AS-OAM','2015-01-13','150113-511772','BSkyb','Problem Description:\n\r\nER paged me for the problem with launching MCP GUI, which occurred after applying new CA signed certificate to SM. The error message \"Path does not chain with any of the trust anchors\" was being recieved when MCP GUI was tried to be launched.\n\r\nRelease of the customer system was 14.0.9.14.\n\r\nSolution:\n\r\nI connected to the site and check the state of both SMs. It seemed that both SM instances were running properly. Then, I reviewed the procedure that the customer applied and it seemed to be correct according to the documentation about certificate management.\r\nAfter spending sometime on documentation to see anything problematic, I could not see anything that may cause the problem since we were not able to launch MCP GUI and see the details of the new certificate. On the other hand, SMs were running properly, it was just not possible launch MCP GUI. So, I asked ER if we can continue investigation in business hours since the service was not impacted. I also told that I was going to be able to involve design in business hours if needed. Customer and ER agreed on this and we continued the investigation with design after 3 hours and managed to resolve the issue by applying the steps below:\n\r\nTo be able to launch MCP GUI; \r\n- We changed the certificate of SM from new certificate to default certificate from DB manually. We updated \"HTTPS_KEYSTORE_CERT\" value in SM table to 1, which represents \"default\" certificate.\r\n- Restarted both SMs by running \"smStop.pl\" and \"smStart.pl\" on both EMServers.\r\nAfter these steps completed, we were able to launch MCP GUI and check Truststore and Keystore in MCP GUI. Then, we realized that the new certificate was signed by Symantec instead of VeriSign and there were no intermediate certificate added to Truststore to define Symantec as \"Trusted\". So, we added the intermediate certificate to TrustStore from MCP GUI and restarted SMs. After that we were able to launch MCP GUI. As soon as customer agreed that the problem was resolved, we ended the call.','null'),(453,'Senem Gultekin','AS-OAM','2015-01-03','150102-510510','Axtel','Problem Description:\n\r\nER paged me for an SM problem at Axtel Puente site (TLPNXMAECA0). Release is 17.0.7.14.\n\r\nHistory;\n\r\nSM1 -> Active\r\nSM0 -> HotStandby\n\r\nCustomer started to the power supply errors on SM1 server.\n\r\nSince there was power supply problem, customer shutdown the SM1 server manually. However, after the shutdown SM0 did not took activity, and still stayed on HotStandby instead of Active state.\n\r\nAlso customer was reporting that some of the calls were failing, Erdem Ekin from CALLP GPS was investigating that in parallel.\n\r\nSolution:\r\n-	Accessed to the site over ERs PC,  SM0 Server was responding very slow.\r\n-	Performed the following actions on SM0;\r\n./neStop.pl\r\n./neStart.pl\r\nkill  -9 \r\nreboot\r\n-	None of them change the SM0 instance state, it was still on HotStandby, and couldnt change it to Active.\r\n-	Requested from the customer to perform power cycle on SM0 Server.\r\n-	When customer was performing power cycle, they have pulled out one out of 2 power supply, at that time server response became very fast, and SM instance was trying to come up.\r\n-	However customer continued with the full power cycle, after the power cycle they didnt insert one of the power supply. Server came up properly, and SM0 instance was up as Active.\r\n-	Customer tested MCP GUI access and it was success.\r\n-	Since power supply was problematic, customer reported that new power supply will come to the site in 2 hours.\r\n-	Agreed with ER and customer to end the pager call, and if they need any help later on they can call back.\n\r\nAlso the call issues were fixed, but we do not know what fixed that yet.','null'),(454,'Senem Gultekin','AS-OAM','2014-12-31','141231-510419','AAPT','Problem Description:\n\r\nER paged me on New Years Eve for a CALLP outage at AAPT customer. They are running on 14.1.10 Release and when they paged me both SESMs were down.\n\n\r\nSolution:\r\n-	Accessed to the site over ERs PC, tried to restart the SESMs from the command  line as ./neStop.pl and ./neStart.pl\r\n-	However both SESMs were acting the same, once they were Active they were shutting down.\r\n-	Normally, in order to bring SESM up here are the steps the A2 application does;\r\nInitializing -> Activating -> Active \n\r\n-	Ive investigated the A2 related logs.\r\n-	Both SESM instances are losing service IP (210.87.44.134), and then try to ping Call Agent (10.0.2.16) in order to identify its isolated completely from the network. Since, there is no response, SESM instances put themselves to isolated state, and shutdown. This ping problem shows us that there is a routing issue.\n\r\nSESM1_0 SYSTEM 107 CRITICAL JAN01 02:25:44:636 MCP_14.1.10.0\r\nCommunication failures cause this network element instance to be isolated from the rest of the system.\n\r\nSESM1_0 GEO 101 CRITICAL JAN01 02:25:44:636 MCP_14.1.10.0\r\n  IP Address 10.0.2.16 of Active CS 2000 Call Agent failed to respond to 6 probes.\n\r\nSESM1_0 SYSTEM 807 INFO JAN01 02:25:44:636 MCP_14.1.10.0\r\n  State Transition: ACTIVE ---> DEACTIVATING.\n\r\n-	Since it seemed like a routing issue, ERS GPS joined the pager call.\r\n-	While ERS GPS was investigating the issue on ERS8600, somehow customer reported that the SESM service IP is pingable, and both SESMS were up as Active and HotStanby.\r\n-	After the 210.87.44.134 started to be pingable, issue was resolved. So we cannot say that this is directly an A2 issue, it\'s mostly routing issue.\r\n-	We have monitored the SESMs for a while and then ended the call.\r\n-	We had an agreement if there is an issue again ER will page us back.','null'),(455,'Nuri INCE','AS-OAM','2014-12-23','141223-509911','TELUS','Chriss Morris from Emergency Recovery paged me for the E1 outage on TELUS site. Customer Load: 17.0.12.20\n\r\nUsers were not able to register since 22nd of December, however the customer called ER a day after they realised the problem(23rd of December). According to the timeline, ER Chriss tried to access MCP GUI, yet MCP gui was not able to be launched. He tried to ssh to the secondary (active) EMServer but he couldn\'t, then he tried to connect to the server via console KVM, yet again there was not screen output.\n\r\nThen ER analyzed that the disk leds on the server were solid and in case of a disk failure, he hard rebooted the server. After the hard reboot, server came up successfully and the lines were able to register through the primary SM.\n\r\nAfter the site is recovered from outage, ER called OAM GPS pager for a system health check.\n\r\nFollowing actions are performed on the site to verify health status:\n\r\n1- Alarms are checked and customer is told to swact SM to clear jmxm 701 alarm\n\r\n2- Partition utilization has been checked via df -k and verified the server is ok\n\r\n3- CPU and memory usage of the server has been verified to be okay\n\r\n4- /var/log/messages, oss logs and work logs have been controlled and nothing abnormal was observed after reboot.\n\r\n5- disk and raid status have been checked via mcpSwRaid.pl -status and verified to be okay\n\r\nAfter verifying the system is healthy, collected the necessary logs for RCA investiation with ER and ended the pager call after agreeing with ER','null'),(456,'Oktay Esgul','AS-OAM','2014-12-19',' 141219-509550','Banner Health System','Brad Hettzel from ER paged me in order to report that  Banner Health System has oracle patch issue during their live site upgrade.\n\r\nCustomer was performing the upgrade on their own.\n\r\nUpgrade path :\n\r\nFrom: 14.1.7.2 \r\nTo:   14.1.10.3\n\r\nI connected site and checked /var/mcp/upgrade_tools/logs/oraclePatch_logs/\n\r\n---------------------------------------------------------------------\r\nPatchDB.pl was failing at below steps based oracle patch logs.\r\n---------------------------------------------------------------------\n\r\n/var/mcp/upgrade_tools/logs/oraclePatch_logs/\n\r\n> run output Installing database initial parameter file...\r\n> run output Database parameter file was installed.\r\n> run output \r\n> run output \r\n> run output Error: when execute above command => 1\r\n> run output \r\n> run output  ERROR: patchDB.pl Terminated at  =>  Thu Dec 18 19:43:29 2014\r\n> run output \r\n> run output      Reference the following log file for additional details:\r\n> run output       /var/mcp/db/install/patches/../../logs/patchDB.log\r\n> run output \r\n> run ok 1\r\n> run ok 0\r\n> run ok 0\r\n> exited\n\r\nError occurred executing NE commands (see below):\n\r\n Error: when execute above command => 1\r\nNE command exited with the value: 1\n\r\nError executing Oracle patch commands.\n\r\n 19:43:33 Cleaning ned session files\r\n 19:43:33 /opt/mcp/ned/bin/nedclient 10.65.227.177 4890 2>&1 << EOF\r\nconfig 3 dummyRel.oraclePatch_delete ROGUE_NE 0 root:root\r\nrun /bin/rm true root:root 400 -rf /var/mcp/run/dummyRel.oraclePatch /var/mcp/run/dummyRel.oraclePatch_state /var/mcp/run/dummyRel.oraclePatch_delete\r\nexit\r\nEOF\r\n 19:43:35 Running ned cmd: /bin/rm -rf /var/mcp/run/dummyRel.oraclePatch\n\r\nTerminated at  =>  Thu Dec 18 19:43:36 2014\n\r\n---------------------------------------------------------------------\r\nThen, I checked /var/mcp/db/install/patches/../../logs/patchDB.log to clarify what the root cause of this failure. Patching was failing due to below error\r\n---------------------------------------------------------------------\n\r\njre/1.4.2/.systemPrefs/\r\njre/1.4.2/.systemPrefs/.system.lock\r\njre/1.4.2/.systemPrefs/.systemRootModFile\r\njre/1.4.2/CHANGES\r\nInstalling database initial parameter file...\r\nDatabase parameter file was installed.\r\nError! There are more than one cpp rpm in the directory.\r\nPlease keep the desired version of the cpp rpm in the directory only.\n\n\r\nError: when execute above command => 1 at /usr/lib/perl5/site_perl/mcsBase/SysUtl.pm line 285\r\n        mcsBase::SysUtl::sysCmd(\'mcsBase::SysUtl=HASH(0x14a0ef80)\', \'/opt/mcp/.support_pkgs/build/buildEnv.pl load\') called at /usr/lib/perl5/site_perl/mcsBase/BaseOperation.pm line 695\r\n        mcsBase::BaseOperation::loadBuildEnv(\'PatchDB=HASH(0x150b9b50)\') called at /var/mcp/db/install/patches/patchDB.pl line 171\r\n        PatchDB::start(\'PatchDB=HASH(0x150b9b50)\') called at /usr/lib/perl5/site_perl/mcsBase/BaseOperation.pm line 561\r\n        mcsBase::BaseOperation::main(\'PatchDB=HASH(0x150b9b50)\') called at /var/mcp/db/install/patches/patchDB.pl line 132\n\r\n-----------------------------------------------------------------------\r\n-----------------------------------------------------------------------\r\nScript was failing due to duplicate oracle rpm package under /opt/mcp/.support_pkgs/build directory. Somehow, old version of these rpm were not removed when the new version are uploaded during upgrade and patchDB.pl was failing since it can not decide which version of these rpm has to be used.\n\r\nAs a solution, I have checked our lab and clarified which version should stay to be used for upgrade,then moved old version to /tmp/ directory.\n\r\nApplied this solution both of EMServer those primary and secondary dbs running.\n\r\nCustomer retried the failed screen and it worked, problem solved.\n\r\nOn the other hand, even patchDB progress seems completed at /var/mcp/upgrade_tools/logs/oraclePatch_logs/ wizards became hung status somehow, then to solve this issue and continue upgrade, I recommend customer to relogin the wizards. They did and wizards continued upgrade and completed without any further issue.','null'),(457,'Oktay Esgul','AS-OAM','2014-12-16','141216-508920','Telus','Tony Pittman from SWD paged me in order  to report he has network failure error during MR upgrade from 17.0.12.20 to 17.0.18 .\n\n\r\nFirstly, I asked Tony if this is lab upgrade or live site while trying to connect site via vpn info provided.\n\r\nAfter awhile Tony confirmed that this is lab of Telus, then I recommended him to open a  case and dispatch to gps queue for further investigation as we do not give pager support to lab upgrades.\n\r\nThanks','null'),(458,'Oktay Esgul','AS-OAM','2014-12-16','141216-508766','TELENET','Sabri KARAKAS from CallP gps who assists on site migration team of TELENET GENT site paged me to check hostname issue while restoring new virtual sesm server from remote backup of old legacy 3310 servers.\n\r\nI have connected site and checked backups, they were OK.But as seen the script capture,when they tried to use this backup, hostname was somehow coming with em1 hostname .\n\r\nThen,I removed old backups and get fresh backups of old server.\n\r\nOn site team, continued migration but they encountered hostname issue again.When they selected the remote backup file name of new virtual sesm servers , even though ip adresses are correct, hostname is wrong somehow.\n\r\nThen , we decided to continue restoration with existing backup then reconfigured hostname manually. We did it and reconfigured hostnames of new virtual sesm servers after platform installation completed.\n\r\nAfterward hardware migration completed, sesm instance had been started successfully on new virtual sesm server without any issue.\n\r\nI dropped the call, Sabri continued working with on site team as there are several SESM jar to be applied after migration completed.\n\r\nThanks','null'),(459,'Yunus Ozturk','AS-OAM','2014-12-04','141203-507064','TELUS','Problem Description:\n\r\nDuring the Telus Migration activity (CC3310 to HT Langley), the site engineer faced an issue. \n\r\n- They ran the script \"/var/mcp/install/dbInstall.pl -fo\" \r\n- They received the error \"put err Error! Timer expired while transferrring /var/mcp/run/MCP_17.0/ssdvdb_1/jar/mcp.jar.nedReceiving\"\n\r\nActions Taken:\n\r\n- This was a problem while transferring the mcp.jar file where it should locate under /var/mcp/run/MCP_17.0/ssdvdb_1/ directory. \r\n- This file should be sent from the Primary EM/DB to Secondary EM/DB without an issue if there is a healthy connection/transfer rate between the Primary EM/DB Server and Secondary EM/DB Server\r\n- Customer was able to ping the Secondary EM/DB Server from the Primary EM/DB Server. So, we were not sure if there was a real transfer rate issue between the servers. \r\n- Checked and verified that the both servers have the same date/time. \r\n- Ran the script \"neinit restart\" on both servers to restart the NED daemon. Did not work.\r\n- Rebooted the both servers. Did not work.\r\n- Compared the size and md5sum of the mcp.jar file on both Primary and Secondary Server and they were same. However, we could not understand why it was transferred to Secondary as \"mcp.jar.nedReceiving\" instead of \"mcp.jar\".\r\n- Since they had the same size and md5sum on both Primary and Secondary, we decided to rename the file from \"mcp.jar.nedReceiving\" to \"mcp.jar\" and changed the permissions/ownerships like on the Primary Server.\n\r\nAfter renaming the file name from \"mcp.jar.nedReceiving\" to \"mcp.jar\" and changing the permissions/ownerships, we were able to deploy and start the SM_1 instance that locates on the Secondary EM/DB Server.','null'),(460,'Yunus Ozturk','AS-OAM','2014-12-04','141203-507079','Lime','Problem Description:\n\r\nLime reported an issue about the Calix Cards. Since they had to check the alarms, they did need to access the MCP GUI. However, MCP GUI was not working properly.\n\r\nThe customer was running on a very old release : MCS 9.1\n\r\nActions Taken:\n\r\n- Tried to launch the MCP on different Genband ER VMs. Did not work.\r\n- Tried with different java versions. Did not work\r\n- Accessed the Primary EM Server to execute some scripts. However, customer did not know the root and nortel passwords.\r\n- Did have to reset the passwords with NED client method\r\n- Checked the scripts on the Primary EM Server but most of the scripts do not exist on this older load even they do on the new releases. \r\n- Executed the smStop.pl/smStart.pl. Did not work.\n\r\nSince this is a very old load, asked the customer to open a techincal support case to investigate this issue seperately as MCP GUI access does not have Callp impact and we could provide best effort support this old release.','null'),(461,'Yunus Ozturk','AS-OAM','2014-12-04','141204-507140','Axtel','Problem Description:\n\r\nAxtel reported a SM stability problem on the MCP GUI (ongoing long term issue, waiting FPM project solution implementation). All the network instances on the MCP GUI was gray out.\n\r\nActions Taken:\n\r\n- Since this is a known issue on the Axtel system, we have recommended the following workaround solution to fix this issue.\n\r\n- Kill all network element instances on all servers on the system. (kill -9 )\r\n- Remove the files in /var/mcp/spool. (rm -rf /var/mcp/spool/*)\r\n- Start SM_0 (smStart.pl)\r\n- Launch MCP GUI\r\n- Kill and start all other instances from MCP GUI\n\r\nSince these steps above will cause an temporarily outage on the system, customer did want to apply the workaround on their MTC window time.\n\r\n- Explained the steps of this workaround solution to ER Team and they have applied the solution on the MTC window. \r\n- When the steps are completed, MCP GUI started to work fine but DB Servers were reporting \"Oracle Replication Link Errors\"\r\n- To clear these alarms, setupDBReplication.pl script has been executed on Primary SM_0.\n\r\nThe alarms were cleared and the test calls were successfull.','null'),(462,'Burak Biyik','AS-OAM','2014-11-29','141129-506407','Axtel','Kyle Mawst from ER Team has paged me to report that MCP GUI is no longer working in Axtel site. The first reason that we came up with was unsupported number of network elements which caused several E2 situation before. Here is the case id : 141012-498213, that I had been paged a month ago.\n\r\nI connected to the site and verified that MCP GUI is not reachable since both SM instances were UNAVAILABLE. Customer is running MCP 17.0.7.13 load and has 33 Network Elements including System Manager. \n\r\nER Team told me that they didn\'t perform any operation after customer reports this inability. I firstly UNDEPLOY each SM instance and then removed EMS1_0 (which is not in use) from /var/mcp/run/MCP_17.0/ directory\n\r\n[root@MTYSSLM00 MCP_17.0]# ls\r\nEMS1_0  loads_0  PROV2_0  SM_0  ssdvdb_0\n\r\nI deployed and started SM instances respectively. SM_0 was ACTIVE while SM_1 was HOT STANDBY as expected. We had some Java related issues on VmWare servers but I successfully launched MCP GUI on my local PC via VPN connection as well as customer. I realized that all server monitors were off as requested and I wanted customer to keep them off until FPM server is shipped to site.\n\r\nI quickly reviewed OSS logs and saw that there were similar symptoms as before (Both SM instances were trying to be ACTIVE at the same time). Since we were able to recover the system only with these operations, there is a possibility that problem might be different. All OSS logs were attached to opened RCA case (141129-506407) for further investigation.\n\r\nWe were agreed to drop the call.','null'),(463,'Seren Batmaz','AS-OAM','2014-11-20','141119-504687','Axtel','Problem Description:\n\r\nER paged me to clarify the workaround procedure which was going to be applied to be able to make the SMs up and running.\n\r\nSolution:\n\r\nWe applied the workaround procedure with ER:\n\r\n- Killed all network element instances on all servers on the system. (kill -9 )\r\n- Removed the files in /var/mcp/spool. (rm -rf /var/mcp/spool/*)\r\n- Started SM_0 (smStart.pl)\r\n- Launched MCP GUI\r\n- Killed and started all other instances from MCP GUI.\n\r\nAs soon as these operations were completed, wh finished the call.','null'),(464,'Seren Batmaz','AS-OAM','2014-11-20','N/A','BT Spain','Problem Description:\n\r\nGTS paged me for the problem occurred during disk replacement of EMServer2. After replacing the disk in the server second time, during the synchronization with the existing disk, the script was complaining about the size of new disk.\n\r\nDisk /dev/sdb: 1073 MB, 1073741312 bytes\r\n64 heads, 32 sectors/track, 1023 cylinders\r\nUnits = cylinders of 2048 * 512 = 1048576 by\n\r\nSolution:\n\r\nWe have rebooted the server to let the script to recognize exact size of disk. After the reboot, the size of new disk was recognized successfully and the script \"mcpSWRaid.pl -add sdb\" successfully completed.','null'),(465,'Seren Batmaz','AS-OAM','2014-11-18','141118-504298','Claro Codatel','Problem Description:\n\r\nGTS paged me for the problem occurring while editing SIP PBX entry from MCP GUI-Call Server 2000 Integration- SIP PBX- CS2K SIP PBX. The error recieved was \"FK_MT_SIPTRUNK_TINFO_OBJ\"\n\r\nSolution:\n\r\nI have connected to the site and checked the system. I have found out that SIP PBX Link Maintenance was not working neither. When I told that to GTS and he reported that to the customer, they responded that they would like to get this problem resolved immediately.\r\nI asked GTS and Account Manager of this customer to understand the impact of that problem, because it seemed not to be E1/E2 causing situation. They told they were going to verify with the customer if there was E1/E2 situation. It took nearly 4 hours to understand that. Finally, they told me that calls were failing and the customer wanted to monitor SIP PBX Link maintenance immediately. When I said this problem was not service impacting, they told me that call processing outage will not be able to recovered if they cannot monitor SIP PBX Link. So, I accepted to continue giving pager support to them.\n\r\nTo correct this problem, the actions that I took was as below:\r\n- SM swact\r\n- SESM double swact\n\r\nAfter this problem is resolved, about the impact of this problem, I was able to learn that there had been no callp outage on the site. There were just problems related with AYT.','null'),(466,'Seren Batmaz','AS-OAM','2014-11-19','141119-504687','Axtel','Problem Description:\n\r\nER paged me for SM_1 unstability problem which was occurred 8 times before. SM_1 state was continuously changing as Activating->Unavailable->Initializing->Hot Standby, just like previous occurances.\n\r\nSolution:\n\r\nI informed ER about previous experiences about this problem and I also told him that I wanted to involve a design support engineer and an achitect to let them investigate it and see if there was less impacting solution to get out this situation.\n\r\nDesign continued investigation over 10 hours and could not find any other workaround solution yet. The only work arund we had is stopping all Network Element Instances and starting them one by one, beginning with SMs.\r\nDuring the investigation, customer reported that new provisioning was not applied to the system. I informed the customer that the issue is because of SMs had lost their functionality to datasync A2 Provisioning Client and SESMs and this was an expected behaviour in a site with this problem. So, the customer decided to apply the work around in maintenance window.\n\r\nI have informed ER about the steps of the operation to be run in MW and told to page me in case of any problem during the procedure. As soon as we agree, I left the call.','null'),(467,'Seren Batmaz','AS-OAM','2014-11-18','141118-504225','BT','Problem Description:\n\r\nSWD paged me for the problem occured in Prepare DB screen during the upgrade from MCP 14.0.16.2 to MCP 17.0.7.14. In first time of the failure, the error message was as below:\n\r\n\"Script hasn\'t completed in predefined time interval. \n\r\nPossible reasons are : \r\n- Network speed is slower than expected. \r\n- There is not enough available network bandwidth for the system. \r\n- Script is stuck due to an unexpected scenario. \n\r\nUse retry button to resolve the problem.\"\n\r\nWhen SWD clicked on retry button, that time it started to fail with the error below:\n\r\n\"Error: Running multiple instances of this script (at the same time) is NOT allowed.\"\n\r\nSWD tried to retry several times. However, the problem persisted. Since 3 hours of maintenance window had been passed, SWD and customer wanted to stop the upgrade.\n\r\nSolution:\n\r\nI connected to the site and checked the wizard and the system. The screen was failing due to there was an already running ut_dbBackup.pl script on the server. So, I checked process of this script and killed that process with the command below:\n\r\n> ps -ef | grep ut_dbBackup.pl\r\n> kill -9 \n\r\nWhen I retried the wizard, it failed again due to another problem. However, we did not continue investigating this problem since SWD and the customer decided to stop the upgrade already. So, we worked to make the DBs replicated since the only operation completed was cleanupReplication.\n\r\nTo be able to set up the replication, I ran the following operations on EMServer1:\r\n> cd /var/mcp/install\r\n> ./setupDBReplication.pl\n\r\nAs soon as this script completed successfully, we started DB monitors from MCP GUI, truncated wizard state and ran mcpInstallFirstLoad.pl. So, the system became to the state like before the upgrade attempt. So, we agreed to finish the call.','null'),(468,'Oktay Esgul','AS-OAM','2014-11-16','141116-503639','BT Telecommunications','Tom Draper from ER paged me due to secondary DB server Unreachable alerts on MCP Gui. Prior the pager call, Tom and customer had already checked the secondary EMServer hosting secondarydb.It was ok network perspective of view, but one of harddrive of the server was dead.\n\r\nEven though, harddrive is dead  Tom had paged me since customer insisted to setup DBReplication between primary and secondarr (there was a db replication broken alert on secondary db )\n\r\nWe tried to setupreplication several times but script goes to hung status since secondary EM was not acting properly. \n\r\nWhile trying to setup replication, primary db became hung status as well ,so I have run dbora stop/start to recover the primary db.\n\r\nI have made several provisioning test to verify primary db is acting properly, everything was fine. \n\r\nSince we will need to work on replication after replacement completed,We discussed with customer again and make them feel safe with current status since primary db is acting properly and all provisioning tests were successfull.\n\r\nCustomer aggreed to leave system as current till they replace secondary EMServer harddrivers.\n\r\nWhen the customer complete replacement, we will assist them to setup replication.','null'),(469,'Oktay Esgul','AS-OAM','2014-11-12','141112-503095','HKBN','Erdem Ekin from CallP GPS paged in order to check DBServer status of the HKBN since all registrations were failing .\n\r\nSystem Level : MCS 3.0\r\nPlatform : Solaris\r\nDB: Oracle 9.0\n\r\nWhen outage started ER had paged Erdem CallP pager prime firstly. Erden had tried to collect traces but since the system is quite old, it had taken long hours to collect traces and investigate the issue.\n\r\nBased on initial investigation, he had suspected that the trigger point of problem was communication issues between DBServers and AppServer,so he paged me to discuss  DBServer restart.\n\r\nI have connected site and checked dbserver status from gui that is quite different than current MCP GUI. There was only snmp related alarms on primary db while the secondary one had \"Replication Broken Alarm \". \n\r\nThen, ssh to dbservers. They were running on solaris platform and hardware type was not familiar for us. I ran uptime to check server uptime,servers were up over than 650 days which means there was not any quarente that servers would become up after reboot. On the other hand, ER notified us that some of disk on the servers were dead.So, we abondoned db restart due to high risk.\n\r\nKen Johnson involved the investigation since he is little bit familiar this oracle 9.0. He checked oracle status on the server and suspected that even db replication was broken, primary db was trying to replicate all data to secondary db , so those db transactions were causing high disk and resource usage which was causing the dbserver not to respond new request.Hence, we decided to drop replication to recover service outage.\n\r\nBefore drop replication, we tried to get DB backups since the last backup was taken at 2007 which is quite old.But, db backup progress failed as well due to not enough space on servers.So that, we decided to go with droping db replication without getting backups.\n\r\nBtw, customer reported that their test calls were passing somehow, so Erdem focused on investigation as CallP perspective of view. We found out that when AppServer3 was active call were success while AppServer1 and AppServer3 had issues and somehow Appserver3 was restarting themselves automatically because of trafic overload.This item changed the investigation  focus from DBServers to Appservers. \n\r\nWe waited stand by for awhile while Erdem was investigating AppServer behaviours.But, out of appservers problem, customer reported that hey have provisioning problem as well, so we turned back to db investigation again.When they attempt to provision new data, there were \"Data Acces Error\" which means DB is not responding.\n\r\nSo, we focused on droping db replication based on Ken`s theory . We tried to drop replicaton via below commands  , but it failed.(While we ere investigating db issue, call services were fine)\n\r\n-rw-r-----   1 oracle   oinstall 34359730176 Nov 13 09:05 repq_data.dbf\n\r\n-rw-r-----   1 oracle   oinstall 34,359,730,176 Nov 13 09:05 repq_data.dbf\n\n\r\nIn paralell to our investigation, ER and APAC team were discussing to customer to plan a new road map since their system was really old and hard to recover. Based on discussion, customer decided to migrate this antique system to A2 .\n\r\nCurrently, customer is migrating their system to A2 and 10k users migrated so far.\n\r\nThanks everyone who contributed this pager call.','null'),(470,'Burak Biyik','AS-OAM','2014-11-04','TBD','Philippines Long Distance TelCo (PLDT)','Tony Pittman from SWD was performing a major upgrade (16.0.2.8 ->17.0.7.15) and paged me to report a failure on wizard screen 16 (DB Mock Upgrade). Although I told him that pre-upgrade steps are not supported for pager calls, he told that upgrade was failed twice in different screens due to \"network connection timeout\" failure.\n\r\nI connected to his PC via TeamViewer and verified the error. After relaunching the wizard, it was able to continue the next screen.\n\r\nIf the network connection between SM and local PC (where upgrade wizard is running on) is slow enough, this might cause failures for some screens because upgrade wizard expects all scripts to be completed in predefined intervals.\n\r\nWe were agreed to drop the call with this information.','null'),(471,'Burak Biyik','AS-OAM','2014-11-03','141028-500706','ITian Corporation','Customer seemed to have some problems while trying to stop oracle on SM1 (/etc/init.d/dbora stop was hung) a week ago and had opened a case. Dbora script hang issue was cleared after rebooting SM1.\n\r\nIt was reported that there were high number of DB erros and deferred transaction count of ssdvdb_0 reached 1500000 as well as ssdvdb_1 was over 4500000 on MCP GUI. Customer has over 220000 user by the way. They wanted to resync two databases to clear these alarms and set a MW for this operation to be performed by GPS.\n\r\nCustomer told that there had been new lines provisioned while replication was broken so that we wanted to use primary DB backup to recover the system.\n\r\nCustomer was running EOL release (MCP 10.3.1.5) and following steps were applied to recover synchronization of database instances;\n\r\n1. Take backup of primary DB (var/mcp/run/MCP_10.3/ssdvdb_0/bin/util --> dbBackup.pl)\r\n2. Restore empty database on primary DB (export/home/oracle/bin --> resotre_empty_db)\n\r\nwe faced several issues while running this script. We could not figure out why oracle went into this state, but listener password seemed to be wrong.\n\r\n\"Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=x.x.x.x)(PORT=1521))) \r\nTNS-01169: The listener has not recognized the password\"\n\r\nSince we don\'t know previous password, we deleted encrypted listener password from listener.ora file (/opt/mcp/oracle/product/10.2.0/network/admin/) with following steps to proceed;\n\n\r\n     -> Issue $ps -ef |grep tns\n\r\n     -> $kill -9 {process_id}\n\r\n     -> Remove the line PASSWORDS_{listener_name} from the listener.ora file. \n\r\n     -> $lsnrctl start {listener_name}\n\r\n3. Restore primary db with backup (backups are under /var/mcp/backup/orabackup)\r\n4. Restore empty database on secondary DB \r\n5. Resync from primary to secondary\n\r\nAt the end of this procedure, SNMP Communication Failure alarm appeared on ssdvdb_0 instance. We run \"config_snmp\" (export/home/oracle/bin/config_snmp) to restart snmp connection of db instance and stopped/started DB monitor. Alarm cleared.\n\r\nFor a short period of time, there was \"oracle_replication_broken\" alarm on ssdvdb_1 and got cleared after synchronizing all links.\n\r\nCustomer made provisioning/qsip/qdn tests from OSS GATE and no issue was seen.\n\r\nAgreed to end MW activity.','null'),(472,'Ken Johnson','AS-OAM','2014-10-28','141028-500695','UPC','BCP7200 reboot following upgrade failed to restart.  Investigated this BCT Blade and it was determined that the hard disk was unable to boot.  Site to replace the blade.','null'),(473,'Senem Gultekin','AS-OAM','2014-10-26','TBD','NUVIA','Problem Description:\n\r\nI was paged for a server backup failure in the pre upgrade steps during wizard upgrade at NUVIA.\n\r\nUpgrade path: 17.0.12.20 to 17.0.18.0\n\r\nOnly one of the servers were failing which was located in HongKong.\n\n\r\nSolution:\r\n- Talked about the steps that Phil performed.\r\n- Phil already rebooted the server.\r\n- He also performed save&exit on wizard, and relaunched it.\r\n- He performed the relaunch again while we were on the call and the issue was resolved.\r\n- Phil continued with the upgrade.','null'),(474,'Senem Gultekin','AS-OAM','2014-10-23','141023-500050','Eastlink','Problem Description:\n\r\nSWD paged for an upgrade issue seen at Eastlink.\n\r\nDuring patching operating system of the primary HOST servers, 1  server (host1server1)  out of 3 didnt come up.\n\r\nUpgrade Path: 17.0.7.14 / 17.0.18.0 \n\r\nSolution:\n\r\n-	Accessed to the SWDs PC and investigated the logs. It seemed like platform patch was completed but after reboot host1server1 was not coming up.\r\n-	host1server1 was not pingable.\r\n-	Requested from the customer to perform hard reboot. Site engineer took action, and then the host1server1 was up back again.\r\n-	Continued with the upgrade. Primary side completed successfully. However customer didnt perform any test calls.\r\n-	During secondary side upgrade, patching operating system of secondary HOST servers screen failed. This time 2 out of 3 servers were not coming up. Host1Server2 and Host2Server2 were not reachable.\r\n-	Again we have requested from the customer to perform hard reboot for 2 of these servers\r\n-	Host1Server2 is a RMS server, and Host2Server2 is an ATCA server.\r\nHost1Server2 - hostname: HLFX0A2H12 IP address: 10.208.51.133 \r\nHost2Server2 - hostname: HLFX3MASH12 IP address: 10.145.5.120 \r\n-	Customer was not able to locate the servers. We have waited 3-4 hours experienced people to arrive to the site to locate the servers. Servers were labeled badly so it took hours for them to proceed.\r\n-	Since Host2Server2 was ATCA blade, customer reported that they have pulled the blade out of the chassis. \r\n-	At that point they asked that the status was showing just blank on the NDM status. And due to the Genius document(630-01241-01) it should be offline-uninstalled in order to reseat it. Since it was a warning in the document had engage Genius GPS to be sure it will not cause any impact to the blades if the customer just inserted back. \r\n-	Brent Thompson joined the bridge, and he have stated that inserting will be just fine. Customer reported that the document was confusing. Brent will take an action for the document update.\r\n-	After the reseat Brent checked the blade status from NDM as well and it seemed like it was all good.\r\n-	Also for the Host1Server2 had to be restarted from the button on the server, customer located that and performed hard reboot.\r\n-	After hours both problematic servers were up on running, all guest servers running on them came up and all instances were just in the proper state.\r\n-	Ive continued with the upgrade and no issues seen later on.\r\n-	Site was running fully on 17.0.18.0\r\n-	Agreed with ER, SWD and customer and left the bridge.\n\r\nCollected all the needed logs, in total 3 out of 6 HOST servers faced this problem. We will work with design for the RCA. \r\nRCA case: 141023-500157','null'),(475,'Senem Gultekin','AS-OAM','2014-10-24','141024-500374','UPC Nederland','Problem Description:\n\r\nSWD paged me for an Upgrade Wizard error seen during Prepare DB screen at UPC Upgrade.\n\r\nUpgrade Path: 14.0.16.3 to 17.0.7.13 \n\r\nError Output:\n\r\n2014-10-24 22:36:53,375 DEBUG DBOperationsManagerImpl - Stopping DB monitors.\r\n2014-10-24 22:36:54,084 ERROR DBOperationsManagerImpl - Error occurred during stopping DB monitor : ssdvdb_0 - Unexpected error! Command not processed..\r\n2014-10-24 22:36:54,085  INFO PanelController - Error occurred : An error occurred during stopping DB monitor of {}.\n\r\nSolution:\n\r\n-	Logged into the site and investigated ned logs, wizard logs, cleanupreplication logs, and  prepareDB logs.\r\n-	Saw that there is a time out error;\n\r\nCleaning up primary DB replication\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20141024_220820_QNhW | /opt/mcp/ned/bin/nedclient 212.142.27.228 4890\r\nCommand Output:\r\n> config ok\r\n> run err Error! Timer expired while executing: perl\r\n> exited\n\r\n-	Performed save&exit on wizard.\r\n-	Restarted ned on both EM server.\r\n           #neinit restart\r\n-	Relaunched the wizard and the screen completed successfully.\r\n-	Monitored SWD screen until the primary side was completed just to be sure everything was fine.\n\r\nNote: Later on, full upgrade completed with no issues.','null'),(476,'Mitat GOCMEN','AS-OAM','2014-10-19','141019-499165','Axtel','I was paged and asked to perform the BCP installation with ER for Axtel site. Unfortunately, customer didn\'t have any platform installation CDs. I found 12.0.7 platform installer in repository and customer had 14.0.19 platform patch, so I decided to install old version and patch it to the required. \n\r\nI connected and asked for the console access of the server. Customer was not able to provide console access at first stage. Site engineer connected a monitor and a keyboard to the server and translated what he saw on the screen to us via written conference and I was assisting him to start the installation. However, site engineer was not able to configure the server to boot from the installation CD. \n\r\nAfter a while, I connected to a PC that the customer provided to gain console access of the server. I connected to management module and observed that the problematic blade was not powered yet. When I power it, it started and I was able to boot from the CD. However, the installation failed at post installation screen with \"media check failure\". Normally, we have a workaround to pass this error, simply passing \"-noeject\" parameter to \"install-kvm\" script. However, the platform version didn\'t have an option to pass parameter to installation script. When I retried the installation with 12.0.7, it failed again with the same error.\n\r\nSince we didn\'t have platform installer 14.0.19, and installation with 12.0.7 failed, I decided to try installing what I found closest to the required platform level, 14.1.1. Platform installation completed successfully, but BCP was dropping while it\'s trying to get Active state. \n\r\nI provided action plan that we would need to find the exact level of platform installer (14.0.19) required and investigate why 12.0.7 installation fails in parallel to see whether there is a problem here. I handed over the installation issue to Senem Gultekin.','null'),(477,'Mitat GOCMEN','AS-OAM','2014-10-19','141021-499532','Axtel','Peter Maloney from SWD team was upgrading Axtel site from 12.0.12.7 to 14.0.16.3. I helped him on different issues encountered during the upgrade.\n\n\r\nIssue #1: \r\n=========\n\r\nI was paged that to report that wizard  was not able to patch the platform of a BCP server (CANBCP-PUEB1). The platform levels were being upgraded in the following path. \n\r\nfrom 12.0.7 to 14.0.19. \n\r\nI connected to the site and tried to patch the platform manually. The error was related to RPMs installed on the platform.\n\n\r\n***** Applying patch\n\r\n--------------------------------------------------------------------------------\r\nApplication part #35\r\nAppl Target: Update RPM Group\r\nCopying BZ2 archive \"14.0_fileset_1.tar.bz2\"\r\nCopying file \"../resources/14.0_fileset_1.tar.bz2\" to \"/var/mcp/os/install/workdirs/temp/resources/tar.bz2\"\r\nChanging mode of \"/var/mcp/os/install/workdirs/temp/resources/tar.bz2\" to \"600\".\r\nUncompressing archive; this may take a few minutes.\r\nUntar\'ing archive.\r\nUpdating RPM group [exim-4.63-3.el5.i386.rpm]\r\nError updating RPM group.\r\n--------------------------------------------------------------------------------\n\r\nPatch part #35 exited with an error (2).\n\r\n* PATCH INSTALLATION ERROR *\r\n* *\r\n* One or more error(s) have been detected during the installation of *\r\n* this patch. To avoid any further configuration errors on this *\r\n* system, please contact your next level of support to resolve this *\r\n* current installation before proceeding with additional patching. *\r\n* Once corrected, re-start the patching process, starting with this *\r\n* patch. *\r\n**********************************************************************\n\r\nError (2) executing patch application targets\n\n\r\nWe tried to reboot server, but didn\'t help us. I tried to uninstall the problematic exim RPM, but it was failing due to dependency to an other RPM, mdadm. I uninstalled both RPM packets and retried to patch platform, however it was still failing. I discussed this with Tugkan from DS team. According to his investigation, the exim packet should not be in this platform level, which was indicating that the platform was not standard or some RPMs might have been installed/uninstalled. As a solution, I recommended customer to reinstall platform either to 12.0.7 and try to patch again to 14.0.19 or install the platform directly to level 14.0.19 after the upgrade.\n\n\r\nIssue #2:\r\n=========\r\nThe upgrade wizard was failing in all the steps where an action is performed about the problematic BCP. Because it was already 24th wizard step out of 47, I didn\'t offer restarting the wizard from the scratch and excluding the problematic BCP from the upgrade process. Instead, I opened the wizard in debug mod and explained Peter how to use. If a screen fails due to the problematic BCP only, he passed the screen via \"force next\" option. \n\n\r\nIssue #3:\r\n=========\r\nThe upgrade wizard was stuck at screen \"Setup DB replication\" with progress bar showing at %47 and still processing for more than half an hour. I connected again to check the DB replication state and observed that the replication was already setup. I checked the logs to verify that the step was actually completed, then saved and exited the wizard. When I reopen the upgrade wizard, it showed the screen successful immediately with the following message. \n\r\nStarting resynchronization of the databases\r\nChecking plan table\r\nDisabling writes to the Regdest table until replication on the Primary Database has \n\r\nbeen dropped\r\nQuiescing the database\r\nSetting up master definition site\r\nEnabling writes to the Regdest table on the Primary Database\r\nTransferring backup files to the Secondary Database\r\nTransferring sys db user data to the Secondary Database\r\nSetting up remote master\r\nEnabling writes to the Regdest table on the Secondary Database\r\nBacking up individual user\r\nFinished resynchronization of the databases\r\nResynchronization of the databases is completed successfully\n\n\r\nIssue #4:\r\n=========\r\nOn the \"backup servers\" screen of the upgrade wizard, it was failing for EM server1 due to insufficient disk space. I connected to the server and removed old loads, oracle installers - patches and some old DB backups with approval. Then the wizard showed it passed for EM server1. \n\n\r\nIssue #5:\r\n=========\r\nAgain on the \"backup servers\" screen of the wizard, it was failing for SESM2 server 2 with the following error message.\n\r\nDaemon command timed out: config 3 MCP_14.0 SES2S2 0 root:root\n\r\nI saw that the SESM state was at \"killing - online - unavailable\" on MCP GUI. We were not able to SSH to the server, but it was pingable. I suggested power cycling the server. Customer sent an engineer to site and the server was hard rebooted. After reboot, the SESM state was corrected on MCP GUI and when I retried on the upgrade wizard, it passed the step successfully. \n\n\r\nIssue #6: \r\n=========\r\nWhen the upgrade was completed, customer reported that calls and provisioning were okay but there were 11 critical alarms on MCP GUI. I checked and verified that all of them was \"invalid cluster\" alarms on BCPs. I explained the customer that this was due to the excluded BCP from the related cluster and due to its current state, offline-unavailable. I provided the \"platform OS installation\" IM and ER paged for the installation. I explained that all the BCPs must be restarted to clear the alarms after the problematic BCP is recovered.\n\r\nAt the end, customer verified that upgrade was successful and all the NEs except for the excluded BCP were functioning properly. Then Peter and I dropped the call.','null'),(478,'Senem Gultekin','AS-OAM','2014-10-20','141019-499165','Axtel','Problem Decription:\n\r\nI was paged out for a BCP server problem seen during  MCP_12.0.12.7 to MCP_14.0.16.3 upgrade.\n\r\nMy collegue Mitat Gocmen investigated the issue and has gave an action to reinstall the problematic BCP server.\n\n\r\nAt this point all servers and instanced have been upgraded to 14.0.16.3 MR, only problematic BCP(CANCPPUEB1) was excluded.\n\r\nSo the BCP server was supposed to be installed from scratch. However there wasnt 14.0.19 platform installer around. And customer reported that they don\'t have 14.0.19 installer.\n\r\nSolution:\n\r\n-	Ive found the 14.0.19 platform installer for BCP (ple3) in our local servers.\r\n-	Uploaded it to netas ftp server so that customer can download and burn it.\r\n-	It took very long time (around 5 hours) to customer download it.\r\n-	Customer requested from me to transfer the iso file to their local FTP server.\r\n-	Once I logged into their local FTP server Ive realized that they already have the ordercode as ESD for BCP platform installer. A2EK0140.140.R.NCL.NAP.NPV.16.D.tar.\r\n-	Told the customer to use it. Again it took few hours to transfer to their PC and burn it.\r\n-	After they burned it to a CD, we were able to complete the BCP server installation.\r\n-	Once the installation was completed,  BCP instance was back up running properly on 14.0.16.3 Release.','null'),(479,'Senem Gultekin','AS-OAM','2014-10-20','141019-499168','Axtel','Problem Decription:\n\r\nI was paged out for a SESM unstable issue seen at Axtel site after MCP_12.0.12.7 to MCP_14.0.16.3 upgrade.\n\r\nHot Standby instances SESM1_0 and SESM2_1 were restarting themselves. \n\r\nMy collegue Mitat Gocmen handed over the issue to me.\n\r\nInvestigation:\n\r\nI\'ve accessed to the site and worked with ER. \n\r\n2 out of 6 SESM servers were becoming unreachable periodically.  This was causing an unstable state for SESMs that are running on these servers.\n\r\nProblematic Servers;\r\n	SESM1_0(189.210.92.4)\r\n	SESM2_1 (189.210.92.18)\n\r\nServers are cc3310 and platform levels are 14.0.19.\n\r\nSymptoms: cannot access to the servers via ssh, but they are pingable.\n\r\nSolution: We have a bulletin(2008009058) for cc3310 scsi bus rate. In order to resolve this issue, SCSI bus transfer rate should be reduced from U320 to U160 as a permanent fix. \n\r\nAction: I have changed SCSI bus rate from 320 to 160 for both problematic SESM servers.\n\r\nResult: Recovered SESM servers are stable since.\n\n\n\r\nSteps for changing SCSI rate:\n\r\n1.Login as root to the server and initiate reboot # reboot\r\n2. At boot pres  to enter Adaptec SCSISelect Utility\r\n3.Select the first SCSI controller by pressing the enter key\r\n4. Select Configure/View SCSI Controller Settings by pressing enter key\r\n5. Select SCSI Device Configuration under Additional Options press enter key.\r\n6. Navigate to all SCSI Device IDs by using arrow keys and select Sync Transfer rate by pressing enter key.\r\n7. Select 160 from the list and press enter key\r\n8. Press ESC key until you see Save changes Dialog box. Select Yes and enter.\r\n9. Press ESC again to see the first menu, there select the second SCSI Controller.\r\n10. Select Configuration/ View SCSI Controller Settings from the menu, press enter key\r\n11. Select SCSI Device Configuration under Additional options and press enter key\r\n12. Navigate to all SCSI Device IDs by using arrow keys and select Sync Transfer rate by pressing enter key\r\n13. Select 160 from the list and press enter key\r\n14. Press ESC key until you see Save changes Dialog box. Select Yes and enter\r\n15. Press ESC until you see exit screen, select yes and exit','null'),(480,'Mitat GOCMEN','AS-OAM','2014-10-18','141017-499092','Verizon Communications','Verizon had an E2 outage due to primary database server (primary EM server) hardware malfunction. Harddisks of the server was replaced and it had to be re-installed in order to put back in service. Customer load is 14.1.0.13.\n\r\nBrad paged me to report that he encountered an error while trying to install oracle via \"oracleInstall.pl -primary\". The error was stating that the permissions must be set as 432 instead of 777. ER had already tried to change oracle installer permissions back and forth in order to pass, but no help. I connected to site and changed permissions as 432 and then retried. Again failed with permission denied error. I changed the oracle installer owner from ntappsw to ntappadm. Then the script ran successfully and the oracle installation started. As a summary, the permission combination must be set as below. \n\r\nchmod 432 \r\nchown ntappadm:ntappgrp \n\r\nAfter oracle installation completed, he tried to run \"dbInstall.pl -fo\" on primary database server. Since there is no database on primary EM server yet, the script had failed and I was paged again. \n\r\nI re-connected to site and tried to run \"dbInstall.pl -fo\" on the secondary database server. Before proceeding with DB sync, I took a backup on secondary server to protect data loss.\n\r\nI ran \"resync.pl\" from the secondary DB server to synchronize primary database from secondary. When it is completed, asked customer to test if they have any DB related alarms on MCP GUI since we didn\'t have GUI connection due to Verizon firewall. In parallel, I connected to sqlplus on primary database and checked if the tables are correctly populated. The db instance table was showing the correct data. \n\r\nCustomer verified that they didn\'t have DB related alarms on MCP GUI and DB alarms on NEs are cleared. However, they reported that SM_0 and PROV_2 were offline-unavailable. I asked them to start from the MCP GUI, but SM was not able to communicate with NED on primary EM server. When I restarted the NED, they were able to start both SM and PROV. Customer also reported that PROV_1 had SYNC200 alarm. I asked them to restart the instance and explained that it was normal after a communication loss occurred with SM. \n\r\nIn the end, all the alarms were cleared and customer was satisfied with the solution and hence I dropped the bridge.','null'),(481,'Mitat GOCMEN','AS-OAM','2014-10-13','141012-498213','Axtel','Axtel had been experiencing E2 outage with unstable A2 System Manager states during the weekend. GPS had already been paged and design support had been engaged for investigation of the problem. ER paged me to verify whether design support and GPS collected the necessary data for analysis and whether they can continue with the action plan provided by GPS. \n\r\nI connected to the site and collected oss and work logs. Checked the system status and observed that one of the system manager instances was unavailable. When I tried to start it, I saw that it became \"Activating\" and then \"Deactivating\" instead of \"Hot stanby\". At the same time, I lost the MCP GUI connection. It was clear that both system manager instances were not able to communicate with each other, which is the known SM instability issue. \n\r\nER performed the action plan below to stabilize the system and preserve customer from provisioning outage. \n\r\n1. Stop all network elements from the platform\r\n2. Restart system managers\r\n3. Remove the spool content of each network element\r\n4. Start all network elements one by one\n\r\nAfter the system was recovered, there were hardware alarms on some of the servers. I suggested to reboot the relevant servers and open up case if the problem persists. I was told that the alarms were cleared after reboot, so we dropped the conference.','null'),(482,'Burak Biyik','AS-OAM','2014-10-12','141012-498213','AXTEL','Thomas Combs from ER team paged me and reported that MCP GUI was grayed out. I verified that MCP GUI was not able to show any data about managed elements.\n\r\nI accessed the site and check SM0 and SM1 status. No matter which one was trying to be Hot-Standby, it was stuck in ACTIVATING state. So they were always ACTIVE-ACTIVATING.. Sometimes ACTIVATING one became ACTIVE and took over bond IP, this time other one became OFFLINE. The system was unstable clearly.\n\r\nServer reboot, SM restart, undeploy/deploy operations were taken as initial actions but none of them worked. Network configurations were checked as well. There were no NEs remained in unexpected state (e.g ONLINE-DOWN-UNAVAILABLE).\n\r\nThen, it was understood that this issue was known for AXTEL Monterrey site and this was not the first time. Since there are high number of network elements, SM was not able to retrieve data from each NE at the same time and this was causing SM to be unstable. AXTEL was recommended to deploy an FPM to this site to prevent possible outages, but this action was not taken as I saw.\n\r\nBased on previous E2 conditions (solution is given in one of the previous cases: 140502-470710), there was only one solution to get rid of this situation. \n\r\n- Stopping all NEs\r\n- Clearing spool directories for each NE\r\n- Starting SMs and then the NE\'s one at a time\n\r\nThese actions should be taken in Maintenance Window. So, agreed to have customer set a MW and ERs performed workaround.\n\r\nI dropped the call.','null'),(483,'Burak Biyik','AS-OAM','2014-10-09','141009-497758','OPTUS','Ronald Mesina (APAC GTS) paged me to report inability to make provisioning from OSSGATE and customer was unable to provision new lines for a day so that it was reported as urgent issue.\n\r\nI connected to site, tried to add new line and got following error;\n\r\n------------------------------------------------------------------\n\r\n> new $ 0731610900 ibn O4MOBRIXXBRG 0 320 LS00 00 0 02 32 pci 1456 all $ y+ \r\nsci 2059 ain QLD_line_trig SIP_DATA SIP_SVCSET package-999 SIP_URI+ \r\n> 61731610900@sip13.yesphone.optus.com.au SIP_CLIENT_TYPE sipline+ \r\n> SIP_LOCATION qld4014 SIP_PASSWD 8eu4Fkzy SIP_SUBDOMAIN+ \r\n> QLD.SSL02.sip13.yesphone.optus.com.au SIP_TIMEZONE Australia QLD $ dpl y 3+ \r\n> dgt MTZ QLD 3WC CFS Y 3 3 3 Y 420 CFU N SUPPRESS PUBLIC N Y $ CNDB $ Y \n\r\nSystem:LineProv; EndPoint can not be added to GateWay. \r\nSystem:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP O4MO-SSL02-VMG00:LS00/000/0/0232 \r\nDetails: The root element is required in a well-formed document.  \n\r\n--------------------------------------------------------------------------\n\r\nAlso, no sip data returned after running qsip and qdn commands.\n\r\n------------------------------------------------------------------------------- \r\n> qsip 0733019965 \n\r\nLEN: LS00 01 1 10 19 \r\nEND POINT: O4MO-SSL02-VMG00 LS00/001/1/1019 \r\nTYPE: SINGLE PARTY LINE \r\nSNPA: 073 \r\nDIRECTORY NUMBER: 3019965 \r\nLINE CLASS CODE: IBN \r\nIBN TYPE: STATION \r\nCUSTGRP: O4MOBRIXXBRG SUBGRP: 0 NCOS: 320 \r\nSIGNALLING TYPE: DIGITONE \r\nCARDCODE: GWLPOT GND: N PADGRP: PKNIL BNV: NL MNO: N \r\nPM NODE NUMBER : 117 \r\nPM TERMINAL NUMBER : 1020 \r\nDNGRPS OPTIONS: \r\nNETNAME: PUBLIC \r\nSUPPRESS_DN \r\nSUPPRESS_NAME \r\nOPTIONS: \r\nCWT 3WC CCW CWI CNDB NOAMA SCWID DGT CND NOAMA AR NOAMA SUPPRESS PUBLIC Y Y \r\nCFU N $ I CFD P $ I CFB P $ I CBU CDU CFS Y 3 3 3 Y 420 CXR CTALL N STD MWT \r\nMWL N N MTZ QLD AIN QLD_LINE_TRIG PCI 1456 ALL Y SCI 2059 DPL Y 10 AGNTPCL \r\nSIP IETF \r\n------------------------------------------------------------------------------- \r\n>\n\r\nCustomer had already tried restarting NCAS link as well as SM start/stop/swact operations and they didn\'t work either. I was referenced to a case 141009-497758 which owned by OAM GPS Omur Ersan and was still under investigation. I suggested customer to keep on investigation with him and some folks from OSSGATE team in early morning. I dropped the call.\n\r\nDuring our investigation, we found out following exception from Provisioning Client (after enabling OPI/DAL logs) everytime we run qdn,qsip or add commands;\n\r\n\"PROV2_0 JBOSS 201 ALERT OCT10 18:46:11:132 MCP_14.1.0.12\r\n  2014-10-10 18:46:11,132 ERROR [org.apache.catalina.connector.CoyoteAdapter] An exception or error occurred in the container during the request processing\n\r\n  java.lang.NullPointerException\r\n        at org.apache.catalina.connector.CoyoteAdapter.parseSessionCookiesId(CoyoteAdapter.java:507)\r\n        at org.apache.catalina.connector.CoyoteAdapter.postParseRequest(CoyoteAdapter.java:449)\r\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:239)\r\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\r\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\r\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\r\n        at java.lang.Thread.run(Unknown Source)\"\n\r\nIt was clearly Provisioning Client internal exception due to JBOSS service and Prov Client instance needed to be restarted. After restart operation; we started to get authentication failure and learned that customer intended to change ctmadmin password.\n\r\nOSS GATE team involved and helped to set default password.\n\r\nThen customer was able to run \"qsip, qdn and add\" commands from OSSGATE.\n\r\nAgreed that issue was resolved.','null'),(484,'Ken Johnson','AS-OAM','2014-10-03','141003-496915','PowerTel','Continuing the A2 upgrade on the secondary servers the site experienced a software deployment failure which was resolved by restarting NED, and immediately thereafter encountered the same Database upgrade failures as the primary database (see previous pager report).  DB patch component manually executed and then skipped by next DB retry, missing file created to prevent copy operation from failing.  Wizard retry was successful and SWD continued with the upgrade...','null'),(485,'Ken Johnson','AS-OAM','2014-10-03','141003-496915','PowerTel','EXPERiUS AS (A2) upgrade wizard failed during patching leaving SM0 standby, swacted from SM1 to 0 allowing the wizard to continue...\n\r\nwizard patching of primary DB failed with patch 8350262, not a valid oracle OPatch package.  Patch manually executed and skipped on next retry to allow the wizard to continue...\n\r\npatching of primary Oracle DB failed as ....db/product/10.20/data/db could not be copied to logrotate.d, verified this had completed in a previous patch attempt, manually created a temporary file to enable to copy to succeed which enabled the wizard to continue...\n\r\nupgrade wizard failed to resume with patching primary Oracle DB and was forced to the next upgrade step after manually patching oracle. this allowed the wizard to continue...\n\r\nDB data changes, schema upgrades, SM SW upgrade, swact, etc subsequently completed without issue.  SWD continues with the upgrade process.........','null'),(486,'Seren Batmaz','AS-OAM','2014-09-30','140929-496296','Ziggo','Problem Description:\n\r\nSWD has paged me for a problem occured during an upgrade from MCP_14.0.16.3 to MCP_17.0.7.13. Prepare DB screen was failed and when it was tried to open MCP GUI/Upgrade wizard, it was failing with \"Database read error\".\n\r\nSolution:\n\r\nI connected to site and I tried to perfom Oracle restart operation on Primary DB. I ran the following command to stop Oracle first:\n\r\n-./etc/init.d/dbora stop\n\r\nStop operation did not completed successfully. So, I monitored running Oracle proccesses and killed them manually:\n\r\n- ps -ef | grep oracle\r\n- kill -9 \n\r\nAfter that, I tried to stop Oracle again and it did work tat time. After that I started Oracle successfully:\r\n-./etc/init.d/dbora start\n\r\nWe were able to launch MCP GUI and Upgrade wizard. However, upgrade wizard failed with the error \"An error occurred during stopping DB monitor of {}.\". I checked Upgrade Wizard logs and it was routing to cleanupReplication logs, under /var/mcp/upgrade_tools/logs/cleanupReplication/ on EMServer1. The error was as below:\n\r\nQuiescing replication groups\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20140930_002137_86s2 | /opt/mcp/ned/bin/nedclient 172.30.165.22 4890\r\nCommand Output:\r\n> config ok\r\n> run err Error! Timer expired while executing: perl\r\n> exited\n\r\nI restarted NED on both EMServers to see if it helps, but same issue persisted. So, I tried to cleanup replication manually, from EMServer1:\n\r\n- ./var/mcp/run/MCP_14.0/mcpdb_0/bin/util/cleanupReplication.pl\n\r\nManual attempt was successful and when I launched Upgrade Wizard, \"Prepare DB\" screen has been completed.\n\r\nSince the problem was resolved, we agreed to end the call.','null'),(487,'Senem Gultekin','AS-OAM','2014-09-16','140915-493753','Unitymedia','Problem Description:\n\r\nER paged me for a MCP GUI issue seen at UnityMedia. MCP GUI was grey out. Customer is running on 14.0.16.3 (8.0 BRC, EOL).\n\r\nER wanted to be sure if this is the known issue that SM Stability we have in older releases which are running on too many servers.\n\r\nSolution:\n\r\n-	Accessed to the site, and checked the MCP GUI, only 1 SM was up at that time and still MCP GUI was grey out.\r\n-	Couldnt perform any action from the MCP GUI, yet SM restarts didnt work.\r\n-	This is an issue we see when there are too many monitoring instances on the system and SM is trying to monitor them. In 10.3 (CVM 17) SM enhancement has been made to use FPM in the system for large systems.  This is a feature.\r\n-	Also there is bulletin named EXPERiUS AS 10.3 (CVM17) Introduction of FPM Server/Chassis Monitoring Capability to improve SM Stability - BULLETIN ID: 00010396 - 01\r\n-	To solve this issue we need to stop all the instances and clean their spool directories.\r\n-	ER performed stop and clear spool for the following elements.\r\n Both SMs \r\n Hot StandBy SESMs \r\n Both PROVs \r\n BCP instances\n\r\n-	After the restarts MCP GUI was working as expected. Issue solved.','null'),(488,'Senem Gultekin','AS-OAM','2014-09-15','140915-493577','Axtel','Problem Description:\n\r\nER paged me for a provisioning issue at Axtel site after rollback from MCP_17.0.12.16 to MCP_14.1.10.0.\n\r\nError is;\n\r\nNEW $ 3330709951 IBN GUADJA 0 0 SSL2 00 03 00 79 SIP_DATA SIP_PACKAGE default_sesm2 +\r\n^J> SIP_URI 3330709951ip@gdl1.wimax SIP_CLIENT_TYPE sip_line SIP_LOCATION GDL +\r\n^J> SIP_PASSWD 9951 SIP_SUBDOMAIN sesm2.gdl1.wimax $ DPL Y 3 DGT MTZ CTL $\r\n^J\r\nSystem:LineProv; EndPoint can not be added to GateWay.\r\n    System:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP VMGSSL2GWC15SL2/000/3/0079 \r\nDetails:    CMT security certificate validation failed. \r\n  Please contact your next level support!\n\n\r\nSolution:\n\r\n-	Accessed to the site, and checked the A2 Database, it was working properly.\r\n-	Logged into the Provisioning Manager and added a test user, which was able to write to the database, and it was success.\r\n-	The problem was core related, and after the rollback completed, the certificates needed to be applied to CMT again.\r\n-	Dilara Aydin from CMT GPS was engaged, and she applied the certificate on CMT.\r\n-	After the certificate was applied provisioning from ossgate started to work.\r\n-	Customer wanted to perform more tests, waited them to be complete.\r\n-	At the end, all provisioning issues were solved.','null'),(489,'Seren Batmaz','AS-OAM','2014-09-15','140915-493577','Axtel','Problem Description:\n\r\nGPS was asked to perform rollback the system of Axtel from MCP_17.0.12.16 to MCP_14.1.10.0 since the system was on E1 outage situation.\n\r\nSo, I started applying rollback procedure in NN10440-450 document. However, a problem occured while applying Step 17- Restore the secondary database from backup. The error log below was printed by Oracle:\n\r\n...........\r\nProcessing object type SCHEMA_EXPORT/USER\r\nORA-39126: Worker unexpected fatal error in KUPW$WORKER.PUT_DDLS [USER:\"MCPUSER\"]\r\nORA-01403: no data found\n\r\nORA-06512: at \"SYS.DBMS_SYS_ERROR\", line 95\r\nORA-06512: at \"SYS.KUPW$WORKER\", line 6345\r\n..........\n\r\nSolution:\n\r\nThe actions that we took are as below:\n\r\n1- Restart DB on secondary EMServer\r\n   - cd /etc/init.d\r\n   - ./dbora stop\r\n   - ./dbora start\r\n2- Resynchronizing secondary DB from Primary DB:\r\n   -From primary DB Server cd /var/mcp/run/MCP_14.0/mcpdb_0/bin/util/\r\n   -./resync.pl\r\n3- Rebooting Secondary EMServer\r\n4- Re-installing A2 DB on Secondary EMServer\r\nThese actions didnt resolve the problem. So, we decided to re-install Oracle on Secondary EM Server. For this action we needed Oracle installer transfered to Secondary EMServer and asked customer to transfer Oracle installer to the server.\r\nIt took about 5 hours for the customer to transfer the load. As soon as we have Oracle installer on Secondary EMServer, I was able to re-install Oracle. \r\nAfter the latest action, I was able to restore DB backup to Secondary DB and proceeded rollback procedure.\n\r\nAs soon as rollback procedure was completed, we terminated the call.','null'),(490,'Seren Batmaz','AS-OAM','2014-09-15','140915-493577','Axtel','Problem Description:\r\nI was paged due to registration problem occured after upgrading the system from MCP_14.1.10.0 to MCP_17.0.12.16. Call Processing GPS was already involved the call and after their investigation, they needed OAM assistance to run some OAM related operations.\n\r\nFollowing actions were applied:\r\n- Restart Oracle restart on primary DB:\r\n   - cd /etc/init.d\r\n   - ./dbora stop\r\n   - ./dbora start\r\n- Restart SESM1&2 from MCP GUI\r\n- Deploy SESM1_0 which was running on HP-CC3310 server to SESMServer5 which was IBM BCT\n\r\nThose actions did not resolve the Call Processing issue. So, the customer decided to roll the system back to MCP_14.1.10.0.','null'),(491,'Seren Batmaz','AS-OAM','2014-09-13','140913-493424','Axtel','Problem Description:\n\r\nER paged me reporting that the SM_1 of the system was Down Offline Unavailable State.\r\nMCP Load of System: MCP_17.0.7.13\n\r\nI connected to the site and started SM_1 from MCP GUI. SM successfully came up and running. However, when I tried to obtain alarms from alarm browser, SM_1 was going down.\r\nThe same issue exactly happened before and I was aware of that case(140712-481975). I checked states of all Network Elements and saw that 3 of them was Configured Down (2 BCPs and 1 PROV).\n\r\nSolution:\n\r\n- I have deployed and started problematic BCPs from MCP GUI.\r\n- I have restarted SM_1.\n\r\nAfter these two actions, MCP GUI started to work properly, it was able to obtain alarms and the state of SM_1 stopped bouncing. So, E2 outage terminated. Then, I started the PROV which was Down Unavailable from MCP GUI. \r\nAs soon as PROV became Online Up Active, we agreed to finish the call.','null'),(492,'Burak Biyik','AS-OAM','2014-09-04','140905-491811','UnityMedia','I got paged to assist the Major Release upgrade of Unitymedia (12.0.12.8 --> 14.0.16.3). The Upgrade Wizard was failing in screen 25 \"Patching Oracle database on the primary database server\".\n\r\n- I checked the latest oraclePatch logs from /var/mcp/upgrade_tools/logs/oraclePatch_logs (referred by monitored scripts on wizard)\n\r\n- It was clearly stated that upgrade was failing due to invalid ne.config property. See following line;\n\r\n\"... The ne.config property (CS2K-A2_Small_TIGH2U) failed validation \n\r\n Aborting, invalid ne.config property ...\"\n\r\n- I checked System Manager Engineering parm from MCP GUI (Network Elements -> System Manager -> Instance). It was \"CS2K-A2E_Medium_TIGH2U\"\n\r\n- I compared this parm with installprops.txt data in /var/mcp/install . ne.config was set as \"CS2K-A2_Small_TIGH2U\"\n\r\n- Oracle patch script uses this txt file to get config parms. Due to mismatch of these configs, UW was failing.\n\r\n- I edited installprops.txt with current data (what is displayed on MCP GUI)\n\r\n- We were able to pass the screen and proceed to upgrade.\n\r\n- I dropped the call','null'),(493,'Senem Gultekin','AS-OAM','2014-08-29','140829-490674','Windstream','Problem Description:\n\r\nSWD paged out for a wizard upgrade issue seen at Windstream live site.\n\r\nUpgrade Path:  12.0.12.4 to 12.0.12.7 \n\r\nWizard Screen: Switching to Secondary NEs\n\r\nEven though SM0 has restarted on MCP GUI, wizard is still seeing SM0 as unavailable. \n\r\nOn MCP GUI\n\r\nSM1: Online Active\r\nSM0: Online HotStandby\n\r\nSolution:\r\n-	Suggested SWD to perform save&exit on wizard and relaunch. However it didnt work.\r\n-	Accessed to the SWD PC and worked together. \r\n-	Restarted SM0 from MCP GUI, it was restarted properly on MCP GUI, but wizard was not getting the proper state of SM0.\r\n-	Performed SM swact from MCP GUI.\r\n-	Launched wizard when SM0 is active and SM1 is hostandby.\r\n-	Wizard was able to restart SM0 properly, and the problematic screen passed.\r\n-	SWD continued with the upgrade. Monitored it for a while.\r\n-	Since 12.0 (CVM13) is an EOL release, no RCA.','null'),(494,'Yunus Ozturk','AS-OAM','2014-08-29','140829-490659','Singtel Optus Pty Ltd','David Giomi paged GPS and reported the following issue that he has on the customer  lab.\n\r\nProblem Description:\r\n---------------------\n\r\nDuring the Wizard upgrade the wizard stopped because the admin state on SM0 was OFFLINE. On the MCP GUI it shows that Admin=offline link=up and oper=active. \n\r\nThey also have the issue where they can only access MCP GUI on the static IP .132 (floating=.134) and only if SM0 oper=active and sm1 oper=HS. \n\r\nActions Taken:\r\n---------------\n\r\n- They tried to stop, kill on MCP with error code \"cannot stop an NE instance that is offline or not deployed. \r\n- They tried cd /var/mcp/install ./smStop.pl./smStart.pl and this did not fix the issue. \r\n- They also changed the operational status in sql and this appeared to resolve it and from here they could progress with the wizard. After wizard upgrade step was completed they did a MCP GUI stop on SM0 and could not log into MCP anymore. \r\n- Action to restore was ./smStart on SM0 and kill -9 (pid) on SM1 then ./smStart.pl on SM1. From this point they could only login to MCP GUI from the static ip (.132) of SM0. This was where they currently were with this issue. The other impact was that wizard failed post/upgrade backups on SM0 and SM1. They want to fix this admin status offline issue so that they can complete wizard post upgrade.\n\r\nSince, this is a lab problem, we have informed the customer that we do not give pager support for the lab issues. So that, we have asked the customer to collect the required logs from the SM units and open a case to follow up this problem. The investigation will continue with a case.','null'),(495,'Yunus Ozturk','AS-OAM','2014-08-26','140826-490142','Shaw CableSystems','Problem Description:\n\r\nCustomer was experiencing memory exhaustion since the servers have been up 200+ days. They performed reboots on the SESMs and SMs. Afterwards,they were unable to launch MCP GUI.\n\r\nActions taken:\n\r\n- Accessed the site and checked the status of the EM Servers. They were both at Unavailable status. However,the EM Servers were both pingable. \r\n- Started the SM instance manually on the Primary EM Server with the following script;\r\n    * cd /var/mcp/install\r\n    * ./smStart.pl\r\n- After running this script, SM_0 unit was at Active status and we were able to access the MCP GUI\r\n- Then, started the SM_1 unit with the following script as well;\r\n    * cd /var/mcp/run/MCP_17.0/SM_1/bin\r\n    * ./neStart.pl\r\n- Furthermore, we had to kill/start the instances on MCP GUI as well. Since we have started them manually from the server itself, the Admin status of the instances were at Offline status. \r\n- After kill/start process on the MCP GUI, the instances were at Active/Online status.\r\n- Applied the same kill/start process for the SESM units as well.','null'),(496,'Oktay Esgul','AS-OAM','2014-08-24','140824-489954	','AXTEL','After A2 upgrade completed, even there are post upgrade steps has to be completed, customer had asked to SWD to make some ossgate provisiniong test, and all test had failed.\n\r\nUpgrade Path :\r\nFROM/TO LOAD: 14.0.16.3 / 14.1.10.0 \n\r\nChris paged me again then I connect and checked the error, provisiniong was failing due security certificate mismatched since major release upgrade performed.\n\n\r\nSystem:GWEMProxy; Provisioning is not supported on Gateways containing this version of load.; Reason: \r\nProblem encountered when attempting to connect to the remote MCSEM server. \r\nEnsure that the MCSEM connection data has been configured correctly using SESM\'s \'configure\' tool. \r\nFirst EM URL connection failure cause:  sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nSecond EM URL connection failure cause:  sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\r\nI recommended to complete all post upgrade steps, CMT certificate upgrade inclueded,then perform test. \n\r\nChris performed the certificate update procedure which is stated upgrade documents, then customer made another provisioning test. All tests passed after certificate update , then I dropped the call.','null'),(497,'Oktay Esgul','AS-OAM','2014-08-24','140824-489954','AXTEL','Chris Henwood from SWD paged me to report pathcPlatform failure on Axtel .\n\r\nUpgrade path:\n\r\nFROM/TO LOAD: 14.0.16.3 / 14.1.10.0 \n\r\nI connected site and check the wizards, all primary server was pathced with new platform except EMServer1.\n\r\nI connected EM1 server and try to patch it manuelly with pathcPlatform.pl ,it failed again. Then i checked /var/mcp/os/logs/patchlogs and found that the patching failing since below error .\n\r\n[*P-Info*] Processing element (file) \"/root/install.log\"\r\n[*P-Error*] Element \"/root/install.log\" does not exist. \n\r\nThen, I created install.logs  file manuelly and set the permissionS. After that I ran patchPlatform again and the server patched successfully. \n\r\nWizards screen passeed succesfully and then I stayed stand by during upgrade process just in case assistance and support needed.\n\r\nAfter db replicaton setup completed, the last step of upgrade process, I dropped the call.','null'),(498,'Oktay Esgul','AS-OAM','2014-08-22','TBD','HKBN','Tony Hype from SWD paged me to report server backup failure during pre-upgrade steps of HKBN upgrade.\n\r\nEven we do not give pager support for pre-upgrade steps, I just connected to site and had a quick look at the problem.\n\r\nWizards was failing with \"state file already exist \" error at  server backup screen. We discussed with Tony and he said that wizards had been dropping due to network issues.\n\r\nI recommended Tony to raise a case for detailed investigation since the system network and wizards do not seem stable, then dropped the call.','null'),(499,'Oktay Esgul','AS-OAM','2014-08-20','140820-489376	','OPTUS','David Giomi had called Harun to get direct support  for the problem he encountered during patch rollback and  Harun paged me to support David.\n\r\nSite:\r\nOptus Pre Production Site \n\r\nRollback Path: \n\r\nFrom : MCP_17.0.12.19\r\nTo : MCP_17.0.12.15\n\n\r\nDavid sent me the teamviewer access and I connected to site.\n\r\nHe was failing with below error while running dbInstall -fo for the primary db.\n\r\nManagement (SM) Node: 10.116.152.73 (local host)\r\nSelected Load: MCP_17.0.12.15_2014-05-15-1504\r\nDatabase Host: 10.116.152.68\r\nDatabase NE Name: mcpdb\r\nDatabase App User: mcsdbapp\r\nDatabase Type: REPLICATED\r\nSecondary Host: 10.116.152.71\r\nPerform Backups: N\r\nOperation to Perform: FILES_ONLY\n\r\nContinue with these settings?(Y/N)[N]: y\r\nUse of uninitialized value $engAlias in concatenation (.) or string at /var/mcp/install/mcpParseXML.pm line 299,  line 11.\r\nUse of uninitialized value $engAlias in concatenation (.) or string at /var/mcp/install/mcpParseXML.pm line 318,  line 11.\r\nCouldn\'t find .ORA file and tblsize for \n\n\r\nI checked the installprops file and it was empty since David did not say \"N\" when the mcpInstallFirstLoad.pl script ask to overwrite installprops,so that script was failing.\n\r\nI filled up installprops from scratch, then run the dbInstall -fo again and this time script completed successfully.\n\r\nAfter the dbInstall script completed, I have restored the db with preupgrade backup and then completed the rest of  rollback steps .\n\r\nSystem was rolbacked to 17.0.12.15 patch load successfully.\n\r\nWe should add a attention bullet to Patch Rollback section of Patch/MR document (630-01217) regarding installprops question while running mcpInstallFirstLoad.pl script.\n\r\nWe agreed with David to raise a new document update case and then I dropped the call.','null'),(500,'Seren Batmaz','AS-OAM','2014-08-16','140805-486103','MTS Allstream','Problem Description:\n\r\nMTS Allstream had a problem with the replication on their two databases. Database sync operation should have been done in maintenance window. Additionally, IPs of 3 servers(DB0, DB1, MgmtSvr0) were changing when they were rebooted. GPS needed to make the IP permanent on those servers.\n\r\nSolution:\n\r\n- Setting up Replication between two databases:\r\nGPS applied the following procedure to be able to replicate two databases:\n\r\nOn primary db \r\n1. login as oracle \r\nsqlplus imsdba/imsdba2001 \r\nalter user ims identified by temp; \r\n2. login as root \r\n/etc/init.d/dbora stop \r\n/etc/init.d/dbora start \r\nAll the service will switch to secondary db \n\r\nOn secondary \r\n1. login as oracle \r\nsqlplus imsdba/imsdba2001 \r\nalter user system identified by manager; \n\r\nOn primary db \r\n1 login as oracle \r\nremove_replication.sh \r\n2. sqlplus imsdba/imsdba2001 \r\nalter user ims identified by ims2001 \n\r\nOn secondary db \r\n1. login as oracle \r\nsqlplus imsdba/imsdba2001 \r\nalter user system identified by system2001; \r\n2. login as root \r\n/etc/init.d/dbora stop \r\n/etc/init.d/dbora start \r\nAll service will back to primary db \n\r\nOn primary db \r\n1. login as oracle \r\nremove_replication.sh\n\r\n2.  Deploy DB bundle to the secondary DB\n\r\nLogin as nortel to the mgmtsvr\r\ndbdeploy.pl\r\nWhen asked for replication, answer: N\r\nWhen asked for the type of deployment,\r\nchoose Install Files only.\r\nWhen asked for the Primary DB IP address, enter the Secondary DB IP\n\r\n3.Prepare to replicate the database\n\r\nlogin as oracle to the Primary DB\r\ncd /IMS/imssipdb/data/db_schema/logs\r\npwd  (ensure you are in the above directory)\r\nrm * \r\ncd /IMS/imssipdb/data/db_schema/util\r\n./prepare_replication.sh\r\nwhen prompted to restart the database, type Y\n\r\n4.Execute the add_secondary_db.sh script (which performs a backup of the Primary DB, which copies the data up to a given SCN, and starts the queuing of all changes made to the Primary DB).\n\r\nlogin as oracle to the Primary DB\r\ncd /IMS/imssipdb/data/db_schema/util\r\n./add_secondary_db.sh\n\r\n5.Set up replication\n\r\nWhen the secondary database is added as a master site, all the data in the master definition site (primary DB) will be automatically propagated to the secondary DB. \n\r\nlogin as oracle to the Secondary DB\r\ncd /IMS/imssipdb/data/db_schema/util\r\n./activate_secondary_db.sh \r\nWhen prompted for the oracle@ password, please enter the oracle password for the Primary Database\r\nhen prompted to restart the database, type Y\r\nlogin as oracle to the primary DB\r\ncd /export/home/oracle/bin/\r\n./archivelogctl off\r\nwhen prompted to restart the database, type Y\r\nPerform the following on both the primary and secondary Databases:\r\nlogin as oracle\r\ncd /IMS/imssipdb/data/db_schema/util\r\n./clean.sh\n\r\n6.Config the DBSNMP agent on secondary DB\n\r\nlogin as root to the secondary DB\r\ncd /export/home/oracle/bin/\r\n./config_snmp\n\r\n7.Undeploy the DB bundle from the Secondary DB\n\r\nlogin as nortel to the Secondary DB\r\ndsmundeploy load  -node \n\r\nDB Replication has been completed after this procedure was applied.\n\r\nTo make server IPs permanent, we change the \'loghost\' IP in /etc/hosts file, and rebooted the servers.\r\nAs soon as customer agreed that the system was working properly, we terminated the call.','null'),(501,'Seren Batmaz','AS-OAM','2014-08-17','140817-488520','Axtel','Problem Description:\n\r\nSWD paged me for the problem occured during the upgrade 12.0.12.7->14.0.16.3, on Cleanup DB Replication Screen. The error log was as below:\n\r\nCleaning up primary DB replication \r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20140817_003621_B57g | /opt/mcp/ned/bin/nedclient 189.205.62.180 4890 \r\nCommand Output: \r\n> config ok \r\n> run err Error! Timer expired while executing: perl \r\n> exited \n\r\nError occurred executing NE commands (see below): \n\r\n> run err Error! Timer expired while executing: perl \r\nUnlinked /var/mcp/upgrade_tools/work/cleanupReplication//init_ned_cmd_20140817_003621_B57g \r\nNot unlinking file \n\r\nSolution:\n\r\n-Since it failed to run cleanupReplication.pl through NED, my first action was restarting NED on primary DB server with the command below: \r\n>neinit restart \r\n-This action didnt help to overcome the problem. So, my second action was stopping and starting oracle on DB servers with the command below: \r\n>./etc/init.d/dbora stop \r\n>./etc/init.d/dbora start \r\n-However, on primary DB, it even failed to stop Oracle. When I checked the \"uptime\" on two DB servers, I saw that they had been running for more than 200 days. So, I requested to reboot two DB servers as the third action(First, primary and then the secondary). After this action, the screen was completed successfully.','null'),(502,'Seren Batmaz','AS-OAM','2014-08-11','140805-486103','MTS Allstream','Problem Desctiption:\n\r\nThe system of the customer was running on MCS 3.0. Replication between two databases was broken and they needed to be synchronized. After the previous maintenance operations on this site it had been decided to delete the rows in Network_Calllog table in A2 DB(It had approximately 7 million rows) and sysnchronize two databases.\n\r\nSolution:\n\r\nSince we were reported that network_calllog table had been purged, we started database replication:\n\r\n-./single_to_rep_db.sh\n\r\nAfter running this script, we waited for more than 7 hours. However, DB sync was not completed. It seemed that Network_callog table still had about 7 million rows which was preventing sync operation to complete.\r\nSince users started to have registration problems(DB was not writable during sync operation), we had to interrupt the sync. After that, the customer continued having registration problems. We checked if DB was on quisced mode which disables write authorization to DB and we saw that it was on quisced mode. We have changed the status to Normal by applying following procedure:\n\r\n>select distinct gname from ALL_REPOBJECT;\r\n>BEGIN\r\nDBMS_REPCAT.RESUME_MASTER_ACTIVITY(gname =>\'\',override =>true);\r\nEND;\r\n/\n\r\nWe also found out that the IP was the DB server was changed when related DB server reinstalled. Then, server IP was changed back to what it supposed to be. After those operations, DB became writable. So, registrations started to being successfull. After the problem resolved, I left the call.','null'),(503,'Seren Batmaz','AS-OAM','2014-08-13','140805-486103','MTS Allstream','Problem Description:\n\r\nER paged me for registration problem on customer site. Hard phones were not able to register to IPCMs. Release of  the system was MCS 3.0.\n\r\nSolution:\n\r\nTo be sure if the problem is related with IPCM or DB, I have tested whether provisioning was working properly first. Since I was able to add a user from Provisioning Client and verified that there was no issue with accessiblity of DB. So, we took the actions on IPCMs as listed below:\n\r\nAction#1: Restarted IPCM instances. This action didn\'t resolve the issue, evet things got worse.\r\nAction#2: Rebooted IPCM servers and deleted unnecessary log files from IPCM servers, which had filled up /var directory.\n\r\nSecond action resolved the issue and the customer reported that the hard phones were able to register then.','null'),(504,'Seren Batmaz','AS-OAM','2014-08-14','140814-487824','Axtel','Problem Description:\n\r\nSWD paged me for a problem occured during preupgrade steps of A2. He reported that, while running preupgrade steps of Upgrade Wizard, SM swacted and all NEs seemed to be at Configured state although all of them were running. Additionally, Upgrade Wizard was not able to be launched. The system was running on MCP_12.0.12.7.\n\r\nSolution:\n\r\nAs first action I suggested to swact SM instances, so that SM_0 were going to become active. For that, I followed the steps below:\n\r\n-ssh to EMServer2 with ntappadm user\r\n-cd /var/mcp/install\r\n-./smStop.pl\n\r\nAfter stopping SM_1, SM_0 became active successfully. We launched MCP GUI and checked the states of NEs. They were all Online Active/Hotstandby. They weren\'t at Configured state anymore. Also, we have tested if Upgrade Wizard was able to be launched and it was successfull as well.\r\nAfter all reported problems were agreed to be resolved, I left the call.','null'),(505,'Omur ERSAN','AS-OAM','2014-08-06','140805-486103','MTS Allstream','Jeffrey Brennan paged me about the outage which was going on since 2 days. \n\r\n- Primary DB Server was replaced upon GPS action plan previously. Solaris OS and Oracle DB installation was done on the replaced Primary DB server. The replaced server was another old server that customer had. Genband provided a new server but that was not used this time in order to stop outage as soon as possible. \n\r\n- After OS and Oracle installation, dbdeploy.pl script and oracle user did not work. So oracle was reinstalled. \n\r\n- After reinstalling operation, dbdeploy.pl script was run but it was asking for the load. So It was transferred from the Management Server1. After the load transfer, dbdeploy.pl script ran successfully. \n\r\n- MCP Gui was unable to be opened due to DB was set as Single but Management server was set as Replicated.\n\r\n- Before the replication, Logs were taking a huge part (%97) of DB, so Network Call Logs deleted from DB.\n\r\n- To set Management Server1 as single we needed to run mgmtdeploy.pl. It failed while stopping the stuck processes on it and could not reach the service IP. \n\r\n- We checked the uptime of the Management Server1. It was 2709 days. To run mgmtdeploy.pl script successfully, we needed to reboot the server. After reboot, server was not able to start up again.\n\r\n- On the secondary Management Server, failover.pl script was run to take the control on itself from the primary server.\n\r\n- After secondary server took control, MCP Gui was able to be launched. \n\r\n- SESM servers restarted from MCP Gui. Lastly, registrations and call tests were successful, the outage was over. \n\r\nCall was dropped after the agreement with ER and customer.','null'),(506,'Mitat GOCMEN','AS-OAM','2014-08-08','140808-486946','UPC Nederland','Guido from the SWD team called me to report a problem encountered during the upgrade that was being performed in UPC live site. The upgrade wizard was failing at step 19, \"Prepare DB\" with the following error message. \n\r\nrun err Error! Timer expired while executing: perl\n\r\nThe exact upgrade path is: 12.0.12.7 to 14.0.16.3\n\n\r\n>> We retried on the wizard, but it failed with the same error. \n\r\n>> I checked the logs under /var/mcp/upgrade_tools/logs. The logs were stating the same error about the time out on clean up replication. \n\r\n>> Checked the disk space on both em server, but it seemed ok.\n\r\n>> Restarted both databases via dbora stop (start)\n\r\n>> Realized that it took some time to restart the DBs meaning that it may take more time than normal to clean up replication and so the wizard shows failed since it expects operations be performed in a predefined time interval. \n\r\n>> Ran the cleanupReplication.pl on primary EM server and it finished successfully. \n\r\n>> Wizard was able to pass the step after retry. \n\r\n>> Then I dropped the call since the problem was resolved.','null'),(507,'Burak Biyik','AS-OAM','2014-08-06','140804-485994','Verizon Communications','Thomas Godwin from ER paged me for assisting installing Oracle and restoring DB on EM0. Primary DB was unavailable due to faulty server so that new installation was required for this site to remove E2 situation.\n\r\nI performed following steps;\n\r\n- Reinstalled linux platfrom (14.1.10 was installed instead of wrong release (14.1.12) which was done in another pager call)\n\r\n- Transferred all necessary files to EM0 (MCP core load, Oracle installer)\n\r\n- Run \"mcpInstallFirstLoad.pl\" for the patch load (14.1.0.13) to be used\n\r\n- Run \"populateInstallpropsFile.pl\" to fill necessary data in installprops.txt \n\r\n- Run \"./oracleInstall.pl primary\" (Oracle release 10.2.0.4.0)\n\r\n- Run \"dbInstall.pl -fo\" \n\r\n- Connected to secondary EMServer hosting secondary db instance and run \"resync.pl\" to sync data from secondary to primary\n\r\n- Applied new license key from MCP GUI (for new server), then started SM0 instance as HotStandby\n\r\n- There were some sync alarms on Prov instances, SMs were swacted and they were gone. They were caused by not being able to access primary db for a long time.\n\r\n- SM instances were up and DBs were synced.\n\r\n- Customer were able to provision new lines as well as the ones failing before redundancy.\n\r\n- Agreed that the system is working fine and dropped the call.','null'),(508,'Mitat GOCMEN','AS-OAM','2014-08-06','140806-486367','Genband US LLC','Phil from the GTS team paged me for a problem occurred on Genband Corp. The load running was MCP_10.0.\n\r\nThey were in the process of migrating around 580 users for the NETAS domain and during the migration they need to delete the users on the corporate EXPERiUS, before they can add them back into the NUViA system. (They use SLR). \n\r\nFinally upon trying to delete the users (From PROV via SOAP) they get a NullPointer exception thrown by PROV. \n\r\nThe had done over 3000 users successfully over the past few months and they reported that it was the first time they were seeing this issue. The exception thrown was below.\n\n\r\njava.lang.NullPointerException\r\n        at com.nortelnetworks.ims.base.prov.opi.server.rsrcmgmt.ResourceMgrDBImpl.getServiceDisplayString(ResourceMgrDBImpl.java:766)\r\n        at com.nortelnetworks.ims.base.prov.opi.server.rsrcmgmt.ResourceMgrDBImpl.decrementDomainResourceUsage(ResourceMgrDBImpl.java:601)\r\n        at com.nortelnetworks.ims.base.prov.opi.server.rsrcmgmt.ResourceMgrDBImpl.decrementDomainResourceUsage(ResourceMgrDBImpl.java:563)\n\n\r\nI involved Tugkan from DS team and we have realized that this issue was already fixed by DS team before. But the fix was submitted in 8.0 SP1 and Experius 10.2 and later supported releases.\n\r\nWe have found out the problem. It was the subscriber count on domain resources tab. The count was set to 0 and when Phil wanted to remove more users it was trying to decrease the number lower, but failing due to boundary checking.\n\r\nWe added a user in the same domain and verified that the subscriber counter was increased by one. When we tried to remove a user, it was successful this time and then the counter is set to 0 again. As the workaround we suggested to add 601 dummy users in bulk and try to remove the remaining 601 users. It worked successfully. Phil was able to remove all the users he wanted and he was also able to add them in the new Experius system. \n\r\nThen with the agreement, we dropped the call.','null'),(509,'Senem Gultekin','AS-OAM','2014-08-06','140806-486217','Liberty Global','Problem Description:\n\r\nSWD paged for a pre upgrade step on wizard. Even though we do not give pager support for pre upgrade steps, this site had an agreement for pre upgrade support.\n\r\nUpgrade Path:  12.0.12.7 -> 14.0.16.3\n\r\nRun Audit Screen was failing for PCC and MOC with the following error;\n\r\nCurrently running PCC and MOC versions => \r\nPCC Version : 7.0.2109 \r\nMOC Version : 7.0.2109 \n\r\nAnd wizard Run Audit step is complaining that => \n\r\nMOC Version \r\nSystem : 7.0.2109 \r\nRequired : 7.0.2157 \n\r\nPCC Version \r\nSystem : 7.0.2109 \r\nRequired : 7.0.2157 \n\r\nSolution:\r\n-	Checked the logs and loadlineup. Failure was caused due to PCC and MOC versions were not aligned with the MR load of the system. It seems like after the previous MR upgrade (which was manually) the PCC and MOC was not upgraded.\r\n-	The best solution for this kind of scenario is to bring the PCC and MOC version to the required.\r\n-	Since there is a tight schedule for this upgrade, decided to change LLU requirement only for PCC and MOC versions.\r\n-	PCC/MOC upgrade is done in the post upgrade steps, therefore LLU change will not impact the actual upgrade at all.\r\n-	Modified 14.0 LLU as following;\n\r\nFrom:\n7.0.2157\nPCC_url\n7.0.2157\nMOC_url\n7.0.2157\n\r\nTo:\n7.0.2109\nPCC_url\n7.0.2109\nMOC_url\n7.0.2109\n\r\n-	Later on SWD canceled the upgrade and started from screen 1, imported the modified LLU, and issue resolved.\r\n-	Agreed with SWD and ended the pager call.','null'),(510,'Mitat GOCMEN','AS-OAM','2014-08-06','TBD','Verizon Communications','Thomas from the ER team paged again to report that the customer Verizon have found a working hard disk on a working server. The customer had already been experiencing E2 outage for 2 days. He called to ask assistance to install A2 platform. \n\r\nThe customer load was MCP_14.1 and platform was 32 bit PLE1. \n\n\r\n>> I provided the IM document for manual platform installation to follow. \n\n\r\n>> helped in the below configurations for platform installation\n\r\n- install screen\r\n- check integrity of the install media\r\n- license agreement\r\n- date and time, NTP configuration\r\n- syslog server\r\n- preconfigured accounts\n\n\r\n>> platform installation had started successfully\n\n\r\n>> asked him to check platform release after installation via \"mcpInstall.pl\" script and dropped the call in agreement.','null'),(511,'Mitat GOCMEN','AS-OAM','2014-08-06','TBD','Verizon Communications','Thomas from the ER team paged to report that Verizon had a failure in primary DB server and the site had already been experiencing E2 outage for 2 days. \n\r\nThe customer load was MCP_14.1\n\n\r\n>> I checked the failure on the server. \n\r\n\"PXE-E61 media test failure check cable\"\r\n\"PXE-M0F exiting intel boot agent\"\r\n\"boot failure system halted\"\n\n\r\n>> I told them that the server is not able to recognize the disk drives inserted. Asked them to replace the server or the hard disks. But the below conditions were already tried by Thomas and Garrett. The error message was the same for the below conditions.\n\r\n1. Old disk on spare server\r\n2. Spare disk on old server\n\n\r\n>> I explained the action plan that should be adopted next. The problem was due to hardware failure(s), and the customer has to replace the whole server, install platform, oracle and involve GPS again to set DB replication and sync.\n\n\r\n>> After agreement we dropped the call.','null'),(512,'Mitat GOCMEN','AS-OAM','2014-08-05','140805-486103','MTS Allstream','Robert Redman from GTS team paged me to report that there was a problem in registration of more than 15000 users. I asked him to page callp pager, but he told me that primary DB was down. \n\r\nThe customer load, MCS5200 3.0, is very old and retired long ago. \n\n\r\n>> I connected to the site and tried to login MCP GUI to check alarms. But MCP GUI failed to load after password verification. \n\n\r\n>> Checked from the platform if the SM application was running via \"meinit -p\" command\n\r\nThe SM was running with the application name mgmtserver.\n\n\r\n>> Checked if the SM service IP (floating IP) was on SM server to see if the SM application was active via \"ifconfig -a\" command.\n\n\r\n>> Tried to connect DB servers via SSH, but it failed for DBserver0.\n\n\r\n>> Asked customer to hard reboot the server, they did, but didn\'t help\n\n\r\n>> Asked for serial console connection to this server. They connected both DB servers via serial console port. But DB server0 was not able to switch to console from ALOM port. I was asking for ALOM login credentials, but the customer didn\'t know the password. I found out that the default password is always set as the last 8 digits of the chassis\' serial number, but it didn\'t help to login. We were neither able to switch to console port from ALOM nor able to login to lom port. \n\n\r\n>> Asked Robert to involve callp team to decide if the registration problem was caused due to the DB server failure in fact. Sabri connected and investigated the SESM (named as Sip Application Server for MCS 3.0) traces and he decided that the problem was due to the failure in primary DB.\n\n\r\n>> We wanted the server be rebooted by customer again and investigated all the boot messages of the server, but it did neither print out any boot messages nor show us the BIOS options or boot options. \n\n\r\n>> We concluded that the server is dead and has to be replaced. Then we dropped the call in agreement with customer.','null'),(513,'Mitat GOCMEN','AS-OAM','2014-08-05','140805-486041','Columbia University Medical Center','Keith Marshall from SWD team paged to report that the upgrade wizard failed on step 38 (Patching Oracle database on the secondary database server). \n\r\nThe exact upgrade path is: MCP_12.0.12.7 --> MCP_14.0.16.3\n\n\r\n>> Connected to the site and checked the error message. \n\r\nError executing Oracle patch commands. \r\nSee log file for possible details: \r\n/var/mcp/upgrade_tools/logs/oraclePatch_logs/ut_oraclePatch.EMS1_397fe086-2a9d-1b21-9b61- \r\n001517e71de0_ut_oraclePatch.pl_PATCH_SEC_ORACLES_3.20140804_235637.log \n\r\nOperation Error\n\n\r\n>> Checked the oracle versions on both EM servers via \"showversion.pl\"\n\r\nPrimary EM oracle version   : 10.2.0.4-22\r\nSecondary EM oracle version : 10.2.0.4-16\n\n\r\n>> Checked the logs to identify the problem. However, the problem was not clearly defined in the logs. \n\n\r\n>> Rebooted the secondary EM server, but it did not help and wizard failed again with the same error. \n\n\r\n>> Checked the oracle patch files on both EM servers and realized that there were oracle patch 21 on primary EM, but not on secondary EM. \n\n\r\n>> Checked the release notes of MCP_12.0.12.0, and realized that the oracle patch version should have been 21 instead of 16. \n\n\r\n>> Patched oracle manually in the secondary server to 10.2.0.4-21 via \"oraclePatch.pl\". \n\n\r\n>> After verifying that the patch 21 has been applied successfully on secondary server, I retried on the wizard. \n\n\r\n>> Upgrade wizard was able to patch oracle to 10.2.0.4-22 this time. \n\n\r\n>> The problem was resolved and the call ended.','null'),(514,'Omur ERSAN','AS-OAM','2014-07-24','140724-483760','Bell Aliant','Problem Description:\n\r\nChris Henwood, from SWD team, paged me to report an error during the upgrade process. Upgrade wizard was stuck on db resync operation. Retry button and save&exit did not work. \n\r\nUpgrade path: 17.0.7.4 to 17.0.12.16\n\r\nSolution:\n\r\n- Running resync.pl manually on Primary SM server did not work due to DB was locked previously on upgrade process.\r\n- Tried to cleanup current replication by running cleanupReplication.pl but it was also stuck.\r\n- Stopped and started oracle DB on primary server but the result was the same.\r\n- Restarted both SM servers one by one.\r\n- Started upgrade wizard. This time resync was done successfully.','null'),(515,'Burak Biyik','AS-OAM','2014-07-18','140718-482919','WindStream','Chris Henwood from SWD paged me to report an inability to perform platform patch. patchPlatform.pl script failed for EMServer2 due to faulty raid devices while other servers\' platforms were patched successfully. Here is the upgrade path;\n\r\nMCP 10.3.1.20 --> MCP 10.3.2.13\n\r\nI used \"mcpraidadm.pl -summ\" to view status. All sda partitions were up while sdb partitions do not even exist in the table. I also verified that sdb disk can not be accessible by running \"fdisk /dev/sdb\".The output is;\n\r\n---- Unable to open /dev/sdb\n\r\nI also checked the same output for sda by \"fdisk /dev/sda\" to make sure that sdb disk is problematic. Here is the output;\n\r\n-----The number of cylinders for this disk is set to 8924.\n\r\nI suggested customer to reseat undetected sdb disk and reboot the server. It might re-sync partitions during boot. Otherwise, customer will have to replace this faulty disk with new one.\n\r\nCustomer will set another MW as soon as possible. We agreed to end the call with this information.','null'),(516,'Seren Batmaz','AS-OAM','2014-07-13','140713-482004','Axtel','Problem Description:\n\r\nSWD has paged me about problems occured during MCP_14.0.16.3 -> MCP_14.1.8.0. There were 3 problems:\n\r\n1)SWD told that upgrade wizard failed to upgrade PROV1_0, BCP2_0, BCP4_0.\r\n2)BCP1_0 was being upgraded manually and after platform patch has been completed on the server, MCP load level of this BCP couldnt be changed.\r\n3)BCP3_0 was one of the servers which was tried to upgrade manually. However, platform patch failed on this server.\n\r\nSolution:\n\r\n1)I connected to the site and launched upgrade wizard. That time, upgrade wizard didn\'t fail to upgrade PROV1_0, BCP2_0, BCP4_0. They were successfully upgraded.\r\n2)I tried to change the load of the instance BCP1_0 from MCP GUI. I was able to change the load, deploy and start the instance successfully.\r\n3)Platform patch on BCP3 server was unsuccessfull due to the error:\n\r\nApplication part #2\r\nAppl Target: Stop snmpd service \r\nCould not stop snmpd service.\n\r\nI manually stopped snmpd service from the server, using the command \'service snmpd stop\' and re-run \'patchPlatform.pl\'. This time platform patch was completed successfully. After that I deployed and started BCP3_0 instance with the new load. As soon as the instance became active, all of the problems were resolved.\n\r\nSWD checked the system and we agreed to terminate the call.','null'),(517,'Seren Batmaz','AS-OAM','2014-07-12','140712-481975','Axtel','Problem Description:\n\r\nER paged me and reported that the SM_1 of the system was Down Configured Unavailable State.\n\r\nMCP Load of System: MCP_17.0.7.13\n\n\r\nSolution:\n\r\n- First, I recommended to deploy SM_1 and start it. After ER applied my first suggestion, SM_1 became Online Up HotStandby state. \r\nHowever, after a few minutes, we recognized that SM_1 became Synchronizing State. While I was working on SM_1 to make it Hot Standby, I realized that SM_1 state was changing as Synchronizing when I try  to retrieve the alarms from MCP GUI.\n\r\n- To be able to decrease the load of SM, I suggested to stop monitors of BCP servers. There were many BCPs for which SM tries to process OMs and logs. When we stop server monitors, we saw that the issue still persists.\n\r\n- After that, I also suggested to stop server monitors of SESMs and when we applied that, it didnt help us to resolve the issue neither.\n\r\n- We have also tried to reboot EMServer1 and saw that the issue did not disappeared. EMServer2 had been rebooted just before paging A2 OAM pager.\n\r\n- While I was checking other components in MCP GUI, I saw that one of the BCPs was at Online Down Unavailable State. I also saw that MCP GUI was not able to monitor one of the MAS servers. I stoped the monitor of MAS server and make the problematic BCP Online Up Active.\r\nAfter that, when I started SM_1, it was able to become and stay at Hot Standby state. Also, I was able to retrieve the alarms of the system without any problem.\n\r\nFor RCA, I asked ER to collect OSS and Work logs for SM_0, SM_1 and BCP instance and we agreed to terminate the call.','null'),(518,'Mitat GOCMEN','AS-OAM','2014-06-26','140626-479696','Bell Aliant','Chris paged for an upgrade failure issue. \n\r\nThe upgrade path is: 17.0.7.4 to 17.0.12.16\n\r\nThe wizard failed on screen 19th, \"Prepare DB\" with the error \"Error occurred : An error occurred during stopping DB monitor of {}\".\n\r\n>> When I checked upgrade wizard logs, the failure was mentioned as below. \n\r\n2014-06-25 23:55:15,758 DEBUG DBOperationsManagerImpl - Stopping DB monitors.\r\n2014-06-25 23:55:17,008 ERROR DBOperationsManagerImpl - Error occurred during stopping DB monitor : ssdvdb_0 - Unexpected error! Command not processed..\r\n2014-06-25 23:55:17,008  INFO PanelController - Error occurred : An error occurred during stopping DB monitor of {}\n\r\n>> I connected to EM server 1 and checked the logs. The failure was mentioned as below\n\r\nclearRep command: /var/mcp/upgrade_tools/bin/ut_cleanupReplication.pl -nc -m EMS1_329380ae-2a96-1b21-9e65-000e0cb2583e_ut_prepareDB.pl_PREPARE_DB_0 -s 10.92.65.50\r\n*** Cleanup Replication FAILED ***\r\nSee Cleanup Replication log for details\n\r\n>> I checked the hardware type, it was hp3310 and checked memory and disk usages.\n\r\n>> It was /var directory that had 90% filled. Since the replication is broken and DB backup is taken in this step, this was obviously not enough for this step to complete successfully. \n\r\n>> Removed the DB backups from 2011\n\r\n>> Disk usage decreased to 80%.\n\r\n>> I ran the \"cleaupReplication.pl\" manually from the platform and it succeeded. \n\r\n>> We saved, exited and retried the step\n\r\n>> The step successfully passed and the upgrade continued without issues.','null'),(519,'Mitat GOCMEN','AS-OAM','2014-06-24','140624-479232','Cable and Wireless LIME','Robert paged to report an E2 outage faced in customer live site. The customer was running a retired load MCP_10.3. \n\r\nI connected to the site and checked the alarms on MCP GUI. The problems I observed were below\n\r\n1. SM_1 was in \"Stopping - online - hot standby\" state\r\n2. PROV_1 was in \"Killing - online - active\" state\r\n3. Both DBs had \"Oracle Replication Link Error\" alarms\r\n4. Both SMs had \"DBComm\" alarms related to both DBs\n\r\nThe customer wanted to power cycle secondary EM server to clear DB alarms and the outage started according to their declaration.\n\r\n>> I wanted to stop and kill SM_1 and PROV_1 on MCP GUI, but it failed with error \"already in maintenance operation\"\n\r\n>> Rebooted the secondary EM server for the states to come true, but it didn\'t help. Nothing got corrected. \n\r\n>> Restarted primary DB. DB was restarted but an error was generated about a log file that is not being able to be edited. \n\r\n>> I checked disk usage on primary EM server and observed that /opt got filled to 100%. \n\r\n>> Got a DB back up and kept here \"/var/mcp/backup/orabackup\"\n\r\n>> In order to overcome the outage as soon as possible, I wanted to clean up and set up replication in order to save time.\n\r\n>> The cleanupReplication script got stuck. \n\r\n>> Checked all the files to identify what is eating up the most disk space under /opt and found out that the below oracle log file had size in Gigabytes. Copied it to /var/mcp directory and removed from /opt. \n\r\n/opt/mcp/oracle/product/9.2.0/network/log/sqlnet.log\n\r\n>> Disk usage decreased to 78%.\n\r\n>> Tried to run cleanupReplication again, but it got stuck again at the same point as below. \n\r\nOracle9i Enterprise Edition Release 9.2.0.6.0 - Production \r\nWith the Partitioning, OLAP and Oracle Data Mining options \r\nJServer Release 9.2.0.6.0 - Production \n\r\nSQL> SQL> SQL> SQL> SQL> SQL> 2 3 4 5 6 7 \r\n8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \r\n24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 \r\n40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 \r\n56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 \r\n72 73 74 75 76 77 78 79 80 \n\n\r\n>> Rebooted the primary EM server\n\r\n>> When the server came to up and running, checked the alarms and status in the MCP GUI. \n\r\n>> SM_1 and PROV_1 states got corrected. DB alarms on SM were cleared. Only the secondary DB had replication link error alarm. \n\r\n>> Asked customer to test provisioning. \n\r\n>> Customer verified that the provisioning started working without problems. \n\r\n>> Ran resync from primary DB to secondary DB to clear replication alarm. \n\r\n>> Resync.pl cleaned up and set up the replication automatically\n\r\n>> Then Robert found out that the customer had already opened a case to GTS for replication link error issue (140620-478782)\n\r\n>> As far as I observed, the customer was trying to run qsip in bulk. We think that the qsip may have been blocked and the DB_0, the primary DB was stuck due to the deferred transactions.\n\r\n>> Since the outage ended and the customer satisfied, we dropped off the call.','null'),(520,'Cigdem Vural','AS-OAM','2014-06-20','140619-478554','Axtel','Customer is preparing to upgrade from 14.0.16.3 to 14.1.8.0\r\nAnd when they did pre-upgrade steps, they told upgrade wizard started with screen 1. And there are DB alarms on secondary DB.\n\r\nWhen I checked for history and secondary DB logs I saw that a resync has been run from secondary DB to primary DB which should not be done. Cause secondary db does not keep wizard state and when a resync from secondary to primary DB done, it just clears the wizard state. So they have to start from beginning.\n\r\nAnd the alarm on secondary DB is DBMN 727, related with conflicts.\r\nTo check the replication conflicts, run the below script and then deleted the conflicts. so alarm cleared.\n\r\nCustomer or SWD team will start the upgrade from beginning\n\r\n[ntdbadm@SSLM001-PTE util]$ ./viewRepConflicts.pl\n\r\nviewRepConflicts.pl Started at => Fri Jun 20 05:40:33 2014 by ntdbadm\r\nSummary of Database Replication conflicts found\r\n************************************************************************\r\nPrimary DB\r\n==========\r\n0 conflicts found\n\r\nSecondary DB\r\n============\r\n1 conflict found\r\n1 table was involved in the conflict(s).\n\r\nREGDEST\n\r\nActions to be performed\r\n=======================\r\nGiven all table(s) involved in the conflict(s) have conflict\r\nresolution implemented, the conflict(s) can be safely deleted\r\nby executing the deleteRepConflicts.pl script (in the same\r\ndirectory as this script).\n\r\nFor additional details regarding each conflict/error, please\r\nreference the following file\r\n/var/mcp/run/MCP_14.0/ssdvdb_1/work/conflict_details\r\n************************************************************************\n\n\r\nviewRepConflicts.pl Completed at => Fri Jun 20 05:40:37 2014\n\r\nReference the following log file for additional details:\r\n/var/mcp/run/MCP_14.0/ssdvdb_1/work/viewRepConflicts.log\n\n\r\n[ntdbadm@SSLM001-PTE util]$ ./deleteRepConflicts.pl\n\r\ndeleteRepConflicts.pl Started at => Fri Jun 20 05:43:05 2014 by ntdbadm\r\nSummary of conflict deletion conducted by ntdbadm at 20JUN2014 054306\r\n************************************************************************\r\nPrimary DB\r\n==========\r\nNo replication conflicts were found/deleted\n\r\nSecondary DB\r\n============\r\nConflict (trans_id=18.5.114843) has been deleted\r\n************************************************************************\n\r\ndeleteRepConflicts.pl Completed at => Fri Jun 20 05:43:07 2014\n\r\nReference the following log file for additional details:\r\n/var/mcp/run/MCP_14.0/ssdvdb_1/work/deleteRepConflicts.log\n\r\n[ntdbadm@SSLM001-PTE util]$ ./viewRepConflicts.pl\n\r\nviewRepConflicts.pl Started at => Fri Jun 20 05:43:22 2014 by ntdbadm\r\nSummary of Database Replication conflicts found\r\n************************************************************************\r\nPrimary DB\r\n==========\r\n0 conflicts found\n\r\nSecondary DB\r\n============\r\n0 conflicts found\r\n************************************************************************\n\n\r\nviewRepConflicts.pl Completed at => Fri Jun 20 05:43:23 2014\n\r\nReference the following log file for additional details:\r\n/var/mcp/run/MCP_14.0/ssdvdb_1/work/viewRepConflicts.log','null'),(521,'Cigdem Vural','AS-OAM','2014-06-15','140615-477821','Axtel','SWD was doing an upgrade and he told BCPs has Critical Alarms.\r\nAlarms were related about Cluster configuration.\n\r\nUpgrade path: 12.0.12.7 - 14.0.16.3\n\r\nPeter told 2 of the BCPs were not upgraded and he did patchPlatform.pl but could not go on from MCP GUI with patch/MR manager.\n\r\nI checked for mcpRelease.pl and platform was upgraded.\r\nThen I did undeploy old load and deploy new load for these 2 BCPs. BCP instance came active with 14.0 load but HAL instance did not.\r\nThen removed MCP_12.0 directory and restarted the BCPs and HAL instance became up with 14.0 too. \n\r\nTo clear the Cluster configuration alarms restart 4 BCPs(BCP1-2-3-4) which are in same cluster and alarms were cleared also.\n\r\nTold him that load is not one that we provide RCAs, but just in case attach all upgrade wizard logs and upgrade logs to the case.\n\r\nDropped off.','null'),(522,'Mitat GOCMEN','AS-OAM','2014-06-08','140608-476824','Axtel','Peter Maloney from SWD paged for assistance on two of upgrade wizard steps. The upgrade was being performed for Axtel site in the following path. \n\r\nMCP_14.1.8.0 to MCP_17.0.7.13\n\n\r\nIssue #1\r\n========\r\nThe following message was displayed on step 17, just at the beginning of the Upgrade portion of the upgrade wizard: \n\r\nPlease stop the primary instances of non-EXPERiUS Core componenets deployed on the following primary Host Server(s): MTYHS1 \n\r\nSolution #1\r\n===========\r\n- I connected to the site and logged on to the host server MTYHS1. \r\n- Ran the following command to list which guest servers reside on this host server\n\r\n/var/mcp/install_tools/bin/it_manageGuest.pl -list\n\r\n- There were MAS guest servers 1 and 3.\r\n- We stopped the two MAS applications and locked platforms from the EM GUIs.\r\n- Continued with the upgrade\r\n- This was due to MAS hosted on 2 of AS servers.\n\r\nIssue #2\r\n========\r\nThe upgrade wizard was not able to stop two of BCPs on the step 38, upgrading secondary network element instances. Peter asked if killing the BCPs would have an impact on upgrade.\n\r\nSolution #2\r\n===========\r\n- I approved that killing the BCPs would help to pass the step since I had already opened up a Jira issue, AAK-34103, for the wizard behavior to be corrected. Currently, the upgrade wizard fails to stop BCPs having call on. \n\n\r\nThen the upgrade continued without needing GPS assistance. So we dropped the call off.','null'),(523,'Mitat GOCMEN','AS-OAM','2014-06-05','140604-476239','Verizon Communications','Timothy from ER team paged and provided the phone number of Nhelson Buitron from GTS team to work together. I called Nhelson and we started working. \n\n\r\nProblem\r\n===========\r\nThe customer had 2 ssl servers having a different platform version from all the other servers. So they had platform version of the two servers re-installed to align with other servers in the following path.\n\r\nOld platform release: mcp_core_linux-14.1.12\r\nNew platform release: mcp_core_linux-14.1.10\n\r\nAfter platform installation following SNMP alarm was raised for the two servers. \n\r\nAlarmName: SNMP Agent Communication Failure\r\nTimeStamp: Tue Jun 03 06:34:11 EDT 2014\r\nFaultNumber: 101\r\nShortFamilyName: SRVR\r\nLongFamilyName: SERVER\r\nSeverity: MAJOR\r\nProbableCause: unspecified reason\r\nDescription: SNMP Request Timeout\n\r\nCustomer servers are cc3310 running ple1 and their MCP release is MCP_14.1.0.13.\n\r\nThe problematic servers had SESMs deployed.\n\n\r\nSolution\r\n========\r\n- I checked the NE status from the MCP GUI and saw that both SESMs were running properly with states \"Active-Hotstandby\".\n\r\n- Tried to ping two servers from EM server and verified that the communication channel was open. \n\r\n- Verified with the following command that snmpd service was running on the two servers \n\r\n\"service snmpd status\"\n\r\n- Restarted snmpd service with the following command, but didn\'t help.\n\r\n\"service snmpd restart\"\n\r\n- Rebooted the servers, but didn\'t help\n\r\n- Compared the SNMP configuration files located under \"/etc/snmp\" on the two problematic servers with the other servers running without alarms. We observed that SNMP profile was set as \"public\" on the two alarming servers and set as \"snmnpub1003\" on the other servers. \n\r\ncom2sec mynetwork 0.0.0.0/0      public\n\r\ncom2sec mynetwork 0.0.0.0/0 snmnpub1003\n\r\n- We checked SNMP profile configuration from the MCP GUI and verified that SNMP profile was set as \"snmnpub1003\" for the alarming servers. \n\r\n- So we edited the SNMP configuration file as to set profile to snmnpub1003 like configured in MCP GUI. \n\r\n- After restarting snmpd service, the alarms are cleared. \n\n\r\nWe all agreed that the problem was resolved, so we dropped the call.','null'),(524,'Mitat GOCMEN','AS-OAM','2014-06-02','140602-475679','Nuvia','Phil paged me for an issue occurred on Upgrade Wizard\'s \"Upgrading secondary network element instances\" screen. \n\r\nThe upgrade path is 17.0.12.13 -> 17.0.12.15.\n\r\nThe wizard failed due to failure in one of the BCPs, namely bcps2_0, when stopping. The wizard fails to stop BCPs that have active calls on. This behavior is being changed with the jira (AAK-34103) I opened for a similar issue. After the fix, the wizard will not try to kill BCPs having call, it will rather alert the operator to kill BCPs manually from the MCP GUI.\n\r\nThe log flow was as follows in the problematic scenario.  \n\n\r\n2014-06-02 01:32:23,862 DEBUG PanelController - bcps2_0 - bcps2_0: Maintenance State: Deploying -> None\r\n2014-06-02 01:29:53,774 DEBUG PanelController - bcps2_0\'s state hasn\'t been stable yet.\r\n2014-06-02 01:29:55,537 DEBUG PanelController - bcps2_0\'s state hasn\'t been stable yet.\r\n2014-06-02 01:29:57,300 DEBUG PanelController - bcps2_0\'s state hasn\'t been stable yet.\r\n2014-06-02 01:30:13,196 DEBUG PanelController - bcps2_0 - bcps2_0: Maintenance State: None -> Deploying\r\n2014-06-02 01:30:13,196 DEBUG PanelController - bcps2_0 is operating.\r\n2014-06-02 01:34:24,091 DEBUG PanelController - bcps2_0 validation has failed.\r\n2014-06-02 02:27:41,576 ERROR NEIOperationsManagerImpl - NEI is already in maintenance operation.\n\n\r\nPhil tried to reboot bcps2_0\'s server, but he retried the wizard before the server had come up to running. When I connected, I tried to deploy and start bcps2_0. After it became online-up-active, I retried the wizard and the screen passed successfully. The log flow for the rest is as below. \n\n\r\n2014-06-02 02:51:14,983 DEBUG PanelController - No NEI requires operation.\r\n2014-06-02 02:51:45,497 DEBUG PanelController - bcps2_0 validation has completed successfully.\r\n2014-06-02 02:51:46,090 DEBUG PanelController - All steps completed.\r\n2014-06-02 02:51:46,090  INFO PanelController - Screen completed successfully.\r\n2014-06-02 02:51:46,090  INFO MainController - Next button is enabled.\r\n2014-06-02 02:52:00,800  INFO MainController - Next is clicked.\n\n\r\nThen since the problem was resolved, we dropped the call.','null'),(525,'Senem Gultekin','AS-OAM','2014-05-30','140530-475444','Twin Lakes Telephone','Problem Description:\r\nSWD paged for a upgrade wizard launch issue during upgrade at Twin Lakes.\n\r\nUpgrade screen was at Patching Platform on Secondary Servers.\n\r\nUpgrade Path: 14.1.0.13 to 14.1.8.0 \n\n\r\nSolution: \r\n-	Accessed to SWD PC and worked on this environment. He had java related issues on his PC, he was not able to launch both MCP GUI and Upgrade Wizard.\r\n-	There were several wizard sessions open in the background which were opened in debug mode. SWD opened the wizard in debug mode because there was a failure on patching platforms of secondary BCP  However this was server related and the server was not reachable.\r\n-	The wizard launch error was ;\r\n________________________________________________________\r\nERROR TopologyManagerImpl - Error occurred while getting topology:\r\n________________________________________________________\n\r\n-	Java related issues are not caused by MCP GUI or Upgrade Wizard, and needs to be fixed in the PC itself. It seems like there were some java version additions and removals from that PC. Wizard and MCP GUI was trying to come up with Java 1.7 version even though there wasnt a 1.7 Java in the PC.\r\n-	Since we were in the middle of the upgrade had to go with a workaround by modifying jnlp file of wizard.  \r\n	Copy wizard link to the desktop.\r\n	Add 1.6.0_16+ and http://java.sun.com/products/autodl/j2se to the related line.\n\n\r\n-	Also killed wizard sessions in the background from windows task manager.\r\n-	After the modification wizard was able to come up but BCP4 failed again, because it was not reachable. Customer hard rebooted the server and after that platform patch completed successfully.\r\n-	SWD completed the upgrade.\r\n-	Left the bridge.','null'),(526,'Senem Gultekin','AS-OAM','2014-05-27','140527-474690','HKBN','Problem Description:\n\r\nSWD paged for a MOCK Upgrade failure at HKBN. \r\nUpgrade path is 17.0.4.3 to 17.0.7.13.\n\r\nFailure was as following;\r\n______________________________________________________________________\r\n> config ok\r\n> run err Error! Timer expired while executing: perl\r\n> exited\n\r\nError caused by problems from (ned, sftp, oracle etc.)\r\n______________________________________________________________________\n\r\nNote: Even though MOCK upgrade is not a part of the actual upgrade steps, I had to give support because the actual upgrade was planned right after the mock upgrade the same night. It shouldnt be scheduled like this. MOCK upgrade purpose is to check if the DB upgrade will have any issues and by seeing a MOCK upgrade we need time to investigate within business hours. SWD team should schedule the upgrade plan as MOCK Upgrade and Upgrade steps different days for every customer.\n\n\r\nSolution:\r\n-	Accessed to the site and tried to access to secondary DB via sqlplus but it was stuck in shutdown state.\r\n-	Tried to bring secondary DB up by running the following command ./ startDatabase.pl under /opt/mcp/db/bin  directory. However I had the following error and secondary DB was not opening;\r\n______________________________________________________________________\r\nERROR at line 1:\r\nORA-01157: cannot identify/lock data file 20 - see DBWR trace file\r\nORA-01110: data file 20: \'/var/mcp/db/data/mcpdb/dry_indx01.dbf\'\r\n______________________________________________________________________\n\r\n-	Accessed to secondary DB via sqlplus with sys as sysdba and ran the following statements;\n\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_indx01.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_cpl_data.dbf\' offline drop;      \r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_data.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_largest_indxs.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_largest_tables.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_large_indxs.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_large_tables.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_lob_data.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_mid_sized_indxs.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_mid_sized_tables.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_ntwkcalllog_in.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_ntwkcalllog_indxs.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_ntwkcalllog_out.dbf\' offline drop;\r\nalter database datafile \'/var/mcp/db/data/mcpdb/dry_picture_data.dbf\' offline drop;\n\r\nalter database open;\r\n-	After the above actions secondary db was up and running.\r\n-	Ran mock upgrade from wizard again. However it failed in a different point.\r\n-	Again investigated the logs and there was;\r\n______________________________________________________________________\r\nINT interuption caught, exiting ....\r\n______________________________________________________________________\r\n-	It seemed like there was an interruption to the script and was failing everytime in a different point.\r\n-	Investigated the logs and performed server reboot, db restarts, ned restarts however none of them worked.\r\n-	Maint window had limited time so couldnt continue with the upgrade. \r\n-	Left the bridge\n\r\n-	After detailed investigation we have realized the there is an interruption by ned on the server.\n\r\n-	Customer was running on 17.0.9 ple4 platform which belongs to 17.0.4.0 MR and this is not the latest release for 10.1. Ned timeout issue was fixed in 17.0.10 ple which is in 17.0.5.0 MR and this is the latest release for 10.1.\n\r\n-	Manually updated the ned for the 4 servers (2 SMs and 2 DBs). Updated ned versions from 17.0.5 to 17.0.6.\r\nTo install the higher ned version;\r\nTransfer the ned file ned-17.0.6-Linux-i686 from the new platform level\r\n Unzip the file on the server. Directory doesnt matter.\r\n After the unzip is complete the will be an install script in the new file created. \r\nJust run ./install and this will update the new ned in the server.\n\r\n-	After the NED update MOCK upgrade was success.\n\r\n-	Upgrade can be performed. Customer will continue with the upgrade next week.\n\n\r\nRCA:\r\n-	Basically customer was running on an old version. This was causing a timeout issue in ned(fixed already in 10.1 latest releases and higher versions) and this was affecting the upgrade scripts.\r\n-	The first issue again is related to NED timeout because it seems like during DB restart action in the MOCK upgrade step script was interrupted and DB was stuck in shutdown state.','null'),(527,'Oktay Esgul','AS-OAM','2014-05-17','140516-472818','Hong Kong Broadband','Roddney Neese paged me to support primary db recovery for HKBN.\n\r\nApplication Layer : MCP3\r\nPlatform : Solaris\r\nOracle Version : 9.2.0.4\n\r\nEven Er team and Ken performed rebooting servers and dbinstall -fo during last night, primary db can not be recovered due to oracle problems.We could not run any oracle related scripts since oracle is not responding.\n\r\nWe decided to perform fresh oracle install but since the customer does not have the installer file,we could not perform installation tonight.\n\r\nAfter customer get the oracle ready for installation,another MW will be arranged and  we will try to install oracle 9.2.0.4 version .\n\r\nAfter we aggreed,I dropped the call.','null'),(528,'Oktay Esgul','AS-OAM','2014-05-16','140516-472818','Hong Kong Broadband','Rodney Neese from ER paged me to report primary db is down at HKBN site for MCS3.0 .\n\r\n==>I connected and checked the system :\n\r\nApplication Layer :MCP_3.0\r\nPlatform: Solaris\n\r\n==>As the system is quite old ,it took allmost 30 minutes to find out how to run mcp gui,and connect servers.\n\r\n==>After we accessed to MCP gui, verified that all NEs have dbcommunication alarms.\n\r\n==>Connected to DBServer1 and verified ,db is not running\r\n==>Tried to stop/start oracle via dbora stop/start command under /etc/init.d/ directory.\n\r\n==>Dbora stop command failed .\r\n==>Tried to find similar cases from salesforce and realized that there are similar case for the same site .\n\r\n==>In the case attachment, there was a procedure to recover and it requiers restarting both db servers.\n\r\n==>Customer did not allow restarting the dbserver and a MW window is arrange at midnight on their timezone.\n\r\n==>I provided MOP document to ER and Er will perform the procedure at MW.\n\r\nAfter we aggreed to road map,I dropped the call.','null'),(529,'Oktay Esgul','AS-OAM','2014-05-13','140513-472111','K-OPT','We were providing online support to genband team to upgrade K-OPT live system  without site access .MW was scheduled just 4 hours and due to host platform patch screen failed as host can not recover after reboot and there was no one on site for hard reboot we spend over than 1 hour at MW.After servers were recovered,customer reguested to  rollback system since we just had 30 minutes left.We provided required details to genband team from yahoo for rollback,after platform rollbacked, we encountered DBComminication failure to primary DB.We suggest to check dbserver comminication and after they verify network is ok, we tried to resync primary db from secondary as it seems network is ok between primary and secondary db,it failed via following error.\n\r\n> run output \r\n> run output sqlplus  -S fails\r\n> run output sqlplus  -S fails at /usr/share/perl5/mcsBase/SysUtl.pm line 400.\r\n> run output \r\n> run output  ERROR: resync.pl Terminated at  =>  Tue May 13 07:01:15 2014\r\n> run output \n\n\r\nAll system were up but, primary db was unreachable .Since the MW was over and outage started genband team call ER to provide access over ER team officially.\n\r\nJeff Brennan from ER paged me and provide site access,afterward we started investigation via remote access.\n\r\nFirstly we check virtual primary DB servers network setting,try to access dbserver from other guest and it was succeeded, network was ok.\n\r\nThen check db is running and dbappuser and dbschema user can access db properly,it was ok.\n\r\nThen,continued investigation based on  below resync error logs and check oracle listener status,even it was running properly , communication problem was being observed.\n\r\nWith the Partitioning, OLAP, Data Mining and Real Application Testing options\r\nERROR:\r\nORA-12541: TNS:no listener\n\n\r\nERROR:\r\nORA-01017: invalid username/password; logon denied\n\r\nThen ,we ran following command to restart oracle.\n\r\n/etc/init.d/dbora stop /start then retry the resync, it failed again.\n\r\nThen, we tried to stop and start just oracle listener to ensure ,listener works properly via following command.\n\r\n/opt/mcp/db/bin/lsnradm.pl stop/start\n\r\nResync was starting without any problem, files we transfering properly but at last part of reync was failing via above error.\n\r\nWe put quice mode db manuelly and activated back it via following script,but db was not accessable again.\n\r\n/var/mcp/run/MCP_17.0/mcpdb_0/bin/util/quiesceRepDB.pl\n\r\n/var/mcp/run/MCP_17.0/mcpdb_0/bin/util/activateRepDB.pl\n\r\nThen,we checked listener.ora and tnsnames.ora under following directory to check network setups.\n\r\n/opt/mcp/db/product/10.2.0/network/admin \n\r\nWhen we compared this files with other working  system, we realized that in tnsnames.ora contains external OAM address while working system not.We modified the file manuelly.And,we checked listener.ora as well it was external OAM too,then modified it manuelly via removing second record for External OAM.\r\nAfter modificiations, we stop/start oracle again.\n\r\n/opt/mcp/db/bin/lsnradm.pl stop/start \n\r\nAfter above changes,DBComm alarms were cleared for MCP gui and primary DB became writable again.We ran resync from secondary db to primary and resync completed successfully.\n\r\nWe aggreed with ER and customer that everything is ok ,required all platform logs and upgrade logs for rca,then we dropped the call.','null'),(530,'Oktay Esgul','AS-OAM','2014-05-13','140509-471616','Telecominication Services Trinidad Tobago','Lary Mac from SWD paged me to report Wizards failure at SM/DB upgrade screen.\n\r\nWe connected site and checked the installprops file and observed the  engineering parameter is set wrong \n\r\nParameter was set as CS2K-A2_Medium_CC3310 while it should  be CS2K-A2E_Medium_CC3310 since the  this engineering parameter is avaiable at later release.\n\r\nModified engineering parameter and retry the wizards.Screen completed successfully and  We dropped the call.\n\n\r\nUpgrade Path:\r\nMcp 12.0 to ==>  MCP_14.0.9\n\r\nThansk','null'),(531,'Seren Batmaz','AS-OAM','2014-05-09','140509-471639','Cable&Wireless','Problem Description:\n\r\nER paged me for the problem about accessiblity of MCP GUI. The customer was not able to launch MCP GUI, both SMs were down.\n\r\nSolution:\n\r\nI connected to the site and checked if SM instances were running on EMServers:\n\r\n> neinit -p\n\r\nThere were no SM instance running on Servers. So, from primary EMServer, I started SM_0 manually:\n\r\n> smStart.pl\n\r\nSM_0 became OFFLINE ACTIVE after that action. So, I was able to launch MCP GUI and I started SM_1 from MCP GUI. It became ONLINE HotStandby.\r\nTo correct SM_0 Admin state from OFFLINE to ONLINE, I ran the following steps:\n\r\n-Stopped SM_0 from EMServer1 by running smStop.pl\r\n-Re-launched MCP GUI\r\n-Started SM_0 from MCP GUI.\n\r\nIn the end, SM_1 was ONLINE Active and SM_0 was ONLINE HotStandby state. Since the problem is resolved, I left the call.','null'),(532,'Seren Batmaz','AS-OAM','2014-05-09','140505-470917','Axtel','Problem Description:\n\r\nER paged me for problem about availability of SM_1 in customer site. SM_1 was down and that happened before a few days. GPS had been suggested to stop server monitors of BCPs until FPMs arrive to the site.\n\r\nSolution\n\r\nI connected to the site and started SM_1 from MCP GUI and it successfully became UP ONLINE HOTSTANDBY. So, the E2 problem was resolved. \r\nAdditionally, I suggested to stop server monitors of other servers as well to decrease the load of SM. The customer said that they will discuss that suggestion internally.\n\r\nAfter we agreed, I left the call.','null'),(533,'Cigdem Vural','AS-OAM','2014-05-04','140504-470778','Axtel','Upgrade path:MCP12.0.12.7  to MCP14.0\r\nSite name:Monterrey\n\r\nPeter paged and told that one of the server is not coming back after reboot on the screen 24 at upgrade wizard.\n\r\nHere below is the list of the issues we saw during the upgrade:\n\r\n1-He told they made a manual reboot but no help. Connected to site and checked upgrade wizard it stuck for SESMServer3 at %60.\r\nSSH was not possible to the SESM server. I asked customer to check if they have any CD left on the server, as Axtel always do.\r\nAnd they have a CD left there, take it and reboot completed. \n\r\n2-We retried on the wizard and this time it failed at screen 25 for oracle patching with following error:\n\r\nThe patch file given is not applicable to the Oracle version installed!\n\r\nChecked for /var/mcp/media directory and there were older patch_installer files there, again we see on Axtel upgrades..\r\nMoved the older patch_installer files to /tmp directory and retry the upgrade wizard. It passed that the step again.\n\r\n3-Wizard seems stuck at %80 of Oracle patch installation. Followed on log files and Oracle patch installation finished without any errors.\r\nWaited for another 5 minutes if wizard will pass the screen but it did not.\r\nSAve and Exit and relogin to upgrade wizard.\n\r\n4-Screen 28 all Servers seemed failed to be active on the new load. I just checked for the instances and they were becoming active but slow.\r\nSo asked Peter to wait for a while, after confirming all primary instances(except SESM primary instances) are active with new load we did Next\r\nand passed the screen\n\r\n5-At screen 35 while checking states of secondary instances it failed for all servers again since the instances are so slow to deploy/start.\r\nI advised to wait and check the states of the instances via MCP GUI. After confirming all instances are Online-Active at MCP GUI, we did a retry and passed the step\n\r\n6-At screen 42 post upgrade Backup Servers screen it failed for EMserver1 and EMserver2 since they do not have enough space for backup.\r\nChecked for them, at EMserver1 I deleted the old loads from /var/mcp/media. And at EMserver2 there were all BCP and SESM backup files under /var/mcp/upgrade_bkups/pre & post directories. I requested customer to copy them in local and delete from that directory.\r\nAfter they got them, deleted the files and retry the screen.\n\r\n7- After upgrade completed all BCPs have critical alarms, so Seren- A2 GW did restart to all of them and critical alarms gone.','null'),(534,'Mitat GOCMEN','AS-OAM','2014-04-25','140425-469669','Ventelo Bedrift AS','Gary Norwood from ER paged me on April 25, 2014. Ventelo had issues with connecting to MCP GUI via browser three times before. \n\r\nThere were three cases opened for similar issues before and the latest one was closed offering the logs that should be collected in the re-occurrence of the same issue. The cases opened for previous issues and the closure reasons are listed below. \n\r\n1. 140102-448119 / No RCA\r\n2. 140214-455545 / No Solution \r\n3. 140217-455839 / Not enough data.\n\r\nGary called me to make sure that the logs needed for investigation are collected correctly. I connected to the site and checked the logs, then offered to send them to GPS for investigation. The problem had already been resolved by ER with power cycle in unit 0.\n\r\nAfter listing the logs needed for investigation at the end, the call ended.','null'),(535,'Mitat GOCMEN','AS-OAM','2014-04-27','140427-469770','Oneconnect','Thomas Combs from ER paged me for a MAS issue. The customer restarted two MAS servers within a routine maintenance operation. One of the MAS GUI was not able to launch after the server restart. \n\r\nI connected to the site and wanted to check alarms about the related MAS from the MCP GUI firstly, but I was interrupted due to java issues. Then I tried to restart jboss service on problematic MAS server. When I retried to access the MAS from the browser, it was timed out again and connection failed. \n\r\nI wanted to access SM GUI for alarms, so I downloaded and installed java7u21 on customer PC. When I re-checked the MAS page, it worked and I could logged in to the MAS GUI successfully. My investigation will continue to identify if the problem was resolved with java update or jboss service restart. \n\r\nThe customer checked the GUI and agreed by the problem resolution. Then the call ended.','null'),(536,'Mitat GOCMEN','AS-OAM','2014-04-27','140427-469768','Axtel','Chris Henwood from SWD paged me for an issue on 31st Upgrade Wizard screen, which is Upgrade Database and System Manager. The upgrade is performed on the following path in Axtel\'s site. \n\r\nMCP_14.1.8.0 to MCP_17.0.7.13\n\n\r\nProblem\r\n=======\n\r\n* Upgrade wizard failed with the following error message on 31st screen. \n\r\nExecuting DB & SM upgrade \r\nInvalid Load name given. Must have be of the form \".zip\"\r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/mcpUpgrade/ut_mcpUpgrade.EMS1_0f163d1e-2a88-1b21-aca9-000e0cf5b2d0_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20140427_025614.log\n\r\nWrong Loadname\r\nDB & SM upgrade failed.\n\n\r\nSolution\r\n=========\n\r\n* I connected to the site and checked the logs mentioned in failure message. \n\r\n* Connected to EMServer1 via ssh and checked the MCP loads under /var/mcp/loads directory\n\r\n* Verified that there were base load and its zip under loads directory\n\r\n* Then realized that there was no zip file of the destination load, which is MCP_17.0.7.13_2014-03-17-1238.\n\r\n* Zipped the destination load and arranged its permissions as below using the following commands\n\r\n\"zip -9 -r MCP_17.0.7.13_2014-03-17-1238.zip MCP_17.0.7.13_2014-03-17-1238\"\r\n\"chmod 660 MCP_17.0.7.13_2014-03-17-1238.zip\"\r\n\"chown ntappsw:ntappgrp MCP_17.0.7.13_2014-03-17-1238.zip\"\n\r\n* When I clicked on retry button on wizard, it passed that screen successfully.\n\r\nThe call then ended and the upgrade operation continued.','null'),(537,'Mitat GOCMEN','AS-OAM','2014-04-27','140427-469765','Axtel','Chris Henwood from SWD paged me for an upgrade issue occurred on 26th Upgrade Wizard screen, Switching Service to Secondary Network Element Instances. The upgrade is performed on the following path in Axtel\'s site. \n\r\nMCP_14.1.8.0 to MCP_17.0.7.13\n\r\nUpgrade wizard was not able to make maintenance operations on two BCPs, namely TORB1 and TORB2. The status was stuck at \"Stopping-Deactivating\" and the screen showed as failed. \n\r\nChris reported that retry was tried before, but it failed again after 30 minutes. \n\r\nI connected to the site and tried to run ./neStop.pl on both BCPs. But I got the following error message many times repeatedly. \r\n\"A java operation in progress\"\n\r\nThen I killed the BCPs from the platform and restarted the ned application by \"neinit restart\" command on both BCP servers.\n\r\nThe wizard was then able to success that step and pass the screen. I listed the logs needed for RCA analysis and the call then ended.','null'),(538,'Mitat GOCMEN','AS-OAM','2014-04-25','140425-469590','MTS Allstream','Brad Hetzel from ER paged me for a MAS issue. The customer has been performing MAS upgrade on the site. They were not able to open up MAS00 GUI, which was one of the upgraded MAS elements. \n\r\n* I connected to the site and checked MCP release, MCP_17.0.7.13. \n\r\n* Checked the alarms from SM GUI and observed the below alarm on MAS00. \n\r\nAlarm Id: 305 \r\nSeverity: Critical \r\nComponent: MAS Resource Manager\r\nDate & Time: (CDT)2014-04-25 00:45:15\r\nDescription: Platform Locked\r\nProbable Cause: The MAS platform has been locked and will not accept new calls.\r\nCorrective Action: Unlock the MAS platform.\n\r\n* Tried to connect GUIs of other upgraded MASes and verified that the other GUIs were able to open. \n\r\n* Restarted jboss service on MAS00 platform using the following command.\n\r\n\"service jboss restart\"\n\r\n* MAS00 GUI was then able to open. I checked the alarms on MAS00 GUI and saw that the alarm was still there. \n\r\n* Unlocked the platform from the following path in MAS GUI. \n\r\n1. In the navigation pane, clicked System Status > Element Status.\r\n2. From the More Actions list, selected Unlock to unlock the MAS.\r\n3. Clicked Confirm to unlock the system.\n\r\n* Then I checked the alarms from both SM and MAS. All the alarms were cleared and MAS GUI access problem had resolved. So the call ended.','null'),(539,'Mitat GOCMEN','AS-OAM','2014-04-24','140424-469428','Midcontinent Communications','Eric Duke paged me for an issue observed after the MR upgrade performed on MidContinent from MCP_17.0.5.1 to MCP_17.0.7.13. \n\r\nProblem\r\n=======\r\nMAS2Server guest on one of host servers was not able to come up and runnning. The error message thrown after attempting to start guest server is below. \n\r\nroot@host12:/var/mcp[root@host12 mcp]# /var/mcp/install_tools/bin/it_manageGuest.pl -start MAS2Server \r\nGuest Operation is failed:\r\nerror: Failed to start domain MAS2Server\r\nerror: Unable to allow access for disk path /var/mcp/MAS_Applications_Installer_16.0.0.781_2013.03.28.bin: No such file or directory\n\n\r\nSolution\r\n========\r\n* I connected to the site and checked the logs and investigated especially on the file in the given path. \r\n/var/mcp/install_tools/logs/manageGuest/it_manageGuest.20140424_063303.log\n\r\n* Eric set up Xming on customer PC for me and I opened virt-manager on host server using putty.exe\n\r\n* Retried to start guest console from virt manager, but got the same error message. \n\r\n* When I was checking the guest configurations on virt-manager, I saw that there was a CD image inserted on guest server. The steps to follow for ejecting the CD image are below. \n\r\n1. select guest\r\n2. open\r\n3. Details\r\n4. IDE CD ROM\r\n5. Disconnect\n\r\n* The problem was then resolved. \n\r\n* MAS guest server and NE were both brought up. \n\n\r\nThe disk image left on MAS guest server was not inserted in the upgrade operation, but in the installation or it was just inserted in fault. The problem could be exposed by anytime before upgrade if the guest server had been tried to be started.','null'),(540,'Mitat GOCMEN','AS-OAM','2014-04-24','140424-469424','Midcontinent Communications','Jarrick Jones from SWD paged me for an upgrade issue on customer Mid-Continent.\r\nThey were performing upgrade in the following path. \n\r\nMCP_17.0.5.1 to MCP_17.0.7.13\n\r\nProblem\r\n=======\r\nPatching platform failed in secondary host server with the following error. \n\n\r\nApplication part #1\r\nAppl Target: Update Igb Modules\r\nCopying BZ2 archive \"17.0_fileset_13.tar.bz2\"\r\nCopying file \"../resources/17.0_fileset_13.tar.bz2\" to \"/var/mcp/os/install/workdirs/temp/resources/tar.bz2\"\r\nChanging mode of \"/var/mcp/os/install/workdirs/temp/resources/tar.bz2\" to \"600\".\r\nUncompressing archive; this may take a few minutes.\r\nUntar\'ing archive.\r\nUpdating new RPMs: igb\r\nError installing RPM: igb-5.0.5-1.x86_64.rpm\n\r\nPatch part #1 exited with an error (2).\n\r\nError (2) executing patch application targets\r\n[*PD-Error*]    Command: \"cd /var/mcp/os/\n\n\r\nSolution\r\n========\r\n* I connected to the site and checked the logs. \n\r\n* tried to patch the platform manually using \"patchPlatform.pl\" and got the same error message. \n\r\n* tried to list igb RPMs with \"rpm -qa\" command and got the following errors about disk space. \n\r\n[root@host11 ~]# rpm -qa\r\nrpmdb: /var/lib/rpm/__db.002: No space left on device\r\nrpmdb: PANIC: No space left on device\r\nrpmdb: unable to join the environment\r\nSegmentation fault\n\n\r\n* When I run \"df -k\", I saw that the disk usage under /var directory was about 21%. However, the problematic disk space usage was caused by inodes of operating system. \n\r\n* Observed that the inode usage level was 100% on /var directory by running the below command. \n\r\n[root@host11 clientmqueue]# df -i\r\nFilesystem            Inodes   IUsed   IFree IUse% Mounted on\r\n/dev/mapper/vg01-root\r\n                      327680   55808  271872   18% /\r\ntmpfs                8260557       1 8260556    1% /dev/shm\r\n/dev/md0               23808      81   23727    1% /admin\r\n/dev/md1               64000      37   63963    1% /boot\r\n/dev/mapper/vg01-home\r\n                       32768      74   32694    1% /home\r\n/dev/mapper/vg01-opt  393216    1229  391987    1% /opt\r\n/dev/mapper/vg01-tmp   32768      58   32710    1% /tmp\r\n/dev/mapper/vg01-var  131072   89124   41948    100% /var\r\n/dev/mapper/vg01-var_log\r\n                      196608     206  196402    1% /var/log\r\n/dev/mapper/vg01-var_mcp\r\n                     2621440  189659 2431781    8% /var/mcp\n\r\n* Compared the file sizes under /var folder with the server that platform patching succeeded, and observed that /var/spool/clientmqueue directory sizes were quite different from each other. \n\r\n* Running the below command to free up inode space worked and inode usage level decreased to 68% on /var. \n\r\nfind . -type f -print0 | xargs -0 rm\n\n\r\nThe problem was then resolved and upgrade wizard was able to pass that screen after clicking retry button.','null'),(541,'Mitat GOCMEN','AS-OAM','2014-04-22','140422-468972','UT Austin','Eric Duke paged me for an upgrade issue occurred on screen 19, which is \"Upgrade Database and System Manager\". \n\r\nThe upgrade is performed in the following path:\n\r\nMCP_16.0.2.2 to MCP_16.0.2.9\n\r\nThe error message on upgrade wizard was below:\n\r\nConnection Loss occurred. \r\nScript hasn\'t give a response till a predefined time interval.\n\r\nWe investigated SM, DB and MCP upgrade logs to find a solution. Both DB instances were upgraded and were running with the new load MCP_16.0.2.9 while SM had the old load MCP_16.0.2.2. \n\r\nWe tried to upgrade SM from the platform with smUpgrade.pl script. SM was upgraded and deployed with the new load, however it was not able to come up an running state. We observed the following exception in SM work logs: \n\r\nTask system setup failed: com.nortelnetworks.mcp.ne.base.parm.ParmNotFoundException: com.nortelnetworks.mcp.base.collections.NotFoundException: key does not exist:\r\nTaskFW.P1RunnerCountParm Name: TaskFW.P1RunnerCount\r\ncom.nortelnetworks.mcp.ne.base.parm.ParmNotFoundException: com.nortelnetworks.mcp.base.collections.NotFoundException: key does not exist:\r\nTaskFW.P1RunnerCountParm Name: TaskFW.P1RunnerCount\n\r\nSince the same customer faced a similar case before and the system was restored to DB backups a couple of times, we suspected from a DB corruption and wanted to test the DB image in our labs. Therefore, the system was restored to its previous version MCP_16.0.2.2 in agreement with the SWD and customer. The SM and other NEs were brought up. Customer performed test calls and provisioning tests successfully. \n\r\nFor further investigation, we collected logs and DB backups to restore for troubleshooting the problem in our labs. Then we dropped the call.','null'),(542,'Mitat GOCMEN','AS-OAM','2014-04-21','140421-468885','KOPT','Tom Draper from ER paged me for an FTP issue on Accounting Managers. The customer load was MCP_17.0.5.x.\n\r\nAccounting manager raised alarms complaining about log in error to FTP push destination. None of accounting files were being transferred to the FTP destination. Accounting files are processed by customer within 24 hours at their FTP push destination server, so it was important for them to receive the files to FTP server. \n\r\nI logged in to the site and restarted the FTP session from the following window in SM GUI and observed that the alarms are cleared. \n\r\nNetwork Elements --> Accounting Managers --> AM1 --> RU Processing --> Accounting Rules Maintenance --> FTP Push tab. \n\r\nWe requested customer to check if the problem was resolved. After monitoring the system for a while, the customer approved that the problem was resolved. I requested the necessary log files for RCA investigation and dropped the call.','null'),(543,'Cigdem Vural','AS-OAM','2014-04-15','140415-468130','Bragg Communications(EastLink)','Upgrade path ==> 14.1.0.2/14.1.8.0\n\r\nDuring upgrade Antthi told it stuck at Upgrading DB & SM state ( screen 25/46)\r\nand he did manually start SM secondary instance without waiting any success or error message at wizard screen.\n\r\nSo after that wizard throwed an error as saying \" An unexpected error occurred. Please contact with next level of support\". \n\r\nLogged in to the system and make Primary SM Hot standby then launch Upgrade wizard at debug mode and get one step back after that do a Next (not a force next) from that screen (24/46) and all upgrade finished without any problem.\n\r\n** If upgrade wizard is used during an upgrade, an error or success message should be waited to make any manual action.','null'),(544,'Oktay Esgul','AS-OAM','2014-04-14','140413-467794	','Ziggo','Thomas Godwin paged me to report that the MPRoutabilityGroup Location problem is observed again .I logined the system checked the prov and DB and verified all configuration is lost and MPROUTEGROUP_LOCATIONMAP table is empty.Then i applied the existing workaraund procedure after enabling DAL logs for both PROV and start monitoring with hope that will reoccur .However, as there is very low trafic in the system , I manuelly deleted the location info to check  comminication details between DB and PROV while DAL logs enabled that we may need this logs during investigation.\n\r\nI monitored system over than 1 hour and the problem did not reoccur,then I disabled dal logs and get latest oss logs from system.We aggred with Thomas that he will continue monitoring the system and if the problem reoccurs he will apply the procedure,then i dropped the call.\n\r\nInvestigation will continue during day to find out the RCA asap.\n\r\nThanks','null'),(545,'Oktay Esgul','AS-OAM','2014-04-13','140413-467794	','Ziggo','Tom Draper paged me to report that the second outage occured at Ziggo after the first one that Seren Batmaz and Serdar Onderoglu worked together and fix via assigning location to MPRouteabilityGroup since this data is deleted somehow from  MPROUTEGROUP_LOCATIONMAP table at DB.\n\r\nI logined the system,then check PROV and DB and verified that data is lost in DB again.ER applied the procedure that Serdar provided before to end up the outage.After that I check oss logs and enable DAL logs for both Prov Manager via following command after telnet connection  \"debuglevel set DAL verbose\" to see  comminication logs between PROV and DB to catch the problem if it was reoccured again ,and started monitoring the system.\n\r\nI had monitored system for over 3 hours and investigate the existing oss logs,but the problem did not observed again during monitoring.I collected all oss and ned logs and disable dal logs to avoid  filling server capacity.We discussed with ER and decided to keep continue with the workaraund if the problem observed again till we finish investigation with desing team and find our RCA,then I dropped the call.','null'),(546,'Seren Batmaz','AS-OAM','2014-04-13','140407-466356','Ziggo','Problem Description:\n\r\nSerdar from Call Processing GPS team paged OAM pager due to there was no Location assigned to Media Portal Routable group although it was assigned previously. That situation was resulting in an outage and calls were failing. That problem had occured on 8th April and this was the second occurance.\n\r\nSolution:\n\r\nTo indicate if the problem was related with SESM IMDB or A2 DB, we have connected to A2 DB and checked table MPROUTEGROUP_LOCATIONMAP:\n\r\n- select * from MPROUTEGROUP_LOCATIONMAP;\n\r\nWe saw that there was no entry in that table. Some how, location information had been deleted from DB. So, I asked ER to collect oss logs of both PROVs to work on RCA of this issue.\r\nSerdar has selected the locations to resolve the outage and asked the customer to test the calls. Customer reported that the problem had been resolved.\r\nWe told ER that we will investigate the RCA from Prov logs and if it is needed, we will discuss the issue with Prov architects in bussiness hours.\n\r\nAfter we agreed, we droped the call.','null'),(547,'Oktay Esgul','AS-OAM','2014-04-11','140410-467401','Verizon','Bradley Hetzel paged me to report site engineer can  not deploy 3rd Sesm pair for Verizon after getting new proper licence key and adding 3rd SESM pair at MCP properly.\r\nI connected to site and check the load level of the newly added sesm pair,it was different than SM load,then changed the load level and redeploy the instance.After that Sesm instances could be deployed but even instance become active they automaticaly become down and RV exception observed at sesm work logs.\n\r\nThen , I checked the engineering parameters of the instances and realized engineering parameters for instance were default value as  A2_AS_Large_4GB2CORE.I undeployed the instance and change the engineering parameters to  CS2K_A2_Medium_CC3310 as newly added servers are CC3310,then deploy the instance again and initialize them .Instances became active/standby properly and I dropped the call.','null'),(548,'Oktay Esgul','AS-OAM','2014-04-10','140410-467401','Verizon','Kyle Mawst paged me to report site engineer can not add 3rd SESM pair to Verizon lab even they have a new licence key because of following error message \"Add operation failed\" Runtime validation failed: Not enough licenses for this network element sessmgr, check license key.\"\n\r\nHe sent to me the licence key, I decoded the licence and verified their licence limitation is  just 2 for AppComboKey,so they are facing error while adding 3rd one.\n\r\nI sugested them to apply for a new licence and then try to add new SESM pair again,then dropped the call.','null'),(549,'Oktay Esgul','AS-OAM','2014-04-08','140407-466420','Ziggo','Mark Zattiero paged me to check MediaPortalRouteableGroup table from DB to see that DB is sync with Prov or not since they can not see routeableGroup from IMDB properyly.\n\r\nI connected to site and check DBs that they are sync or not. I have modified some configuration for MediaPortalRouteableGroup to see DB synchronize with PROV asa modification done.Test was successfull ,but we checked IMDB it did not synced with DB,then callP GPS ran audit to sync IMDB with DB and problem is resolved.','null'),(550,'Mitat GOCMEN','AS-OAM','2014-04-05','140405-466255','TeleCable','Gary Norwood from ER paged me on Saturday 5th for an issue caused callp outage twice a day. The customer had just upgraded their system to MCP_17.0.12.12 on the day before the outages were faced. The problem had been resolved by ER by double swacting. The issue was caused due to the CPU usage that hit to 100% on SESM servers. I connected to the site and collected necessary logs needed for OAM investigation. In parallel, I wanted ER to make a call to callp pager and Ozgur has joined in investigation. Ozgur also collected necessary logs for callp investigation. Since the problem was resolved for that moment we dropped the call for detailed investigation. \n\r\nTom Draper from the ER paged me on Sunday 6th for the same problem that had re-occured. After some more investigation, we agreed to rollback the SESM load back to MCP_17.0.12.8 that was used just before their upgrade. A jar file that the customer had used before the upgrade, which is AAK-24882_AAK-30738_MCP-17.0.12.8.jar, is applied to SESMs. We had customer make test calls after both SESMs came to up and running state. We wanted the customer to leave SESM1_1 active since the incidents were seen when SESM1_0 was active and we dropped the call to continue with detailed investigation.','null'),(551,'Senem Gultekin','AS-OAM','2014-03-18','140319-460726','BT Ireland','Problem Description:\n\r\nCustomer was applying a patch upgrade from 17.0.7.2 to 17.0.7.11. They have got stuck on upgrading primary element instances screen on wizard. SESM1_1 was running on Validating state on wizard and it was hung.\n\r\nSolution:\r\n-	Accessed to the site over ERs machine.\r\n-	Checked the SESM1_1 on wizard and launched the MCP GUI. SESM1_1 was on Online-Active state. However wizard was expecting to see it in Online-Hot Standby state. That\'s why it was trying to validate.\r\n-	Restarted SESM1_1 from MCP GUI.\r\n-	SESM1_0 became Online-Active, and SESM1_1 became Online-Hot Standby. At this point it came to a state which wizard is expecting to see.\r\n-	Wizard screen became success. \r\n-	Customer continued with the upgrade and followed them until the end of the upgrade, it was success, all instance were running on 17.0.7.11.\r\n-	Ended the call.','null'),(552,'Seren Batmaz','AS-OAM','2014-03-16','140310-459048','Axtel','Problem Description:\n\r\nSWD paged me for problem occured during upgrade from 14.0.16.3 to 14.1.8.0. Some of the Network Elements(including 1 SESM and 5 BCP instances) had not been upgraded. The customer recognized that situation, after the upgrade completed.\r\nSWD has informed me that the issue occured previously, during the upgrade to 14.0.16.3, and it was resolved by GPS.\n\r\nSolution:\n\r\nI connected to servers of not upgraded NEs. I have checked the Platform level on those servers and seen that platform patch were not applied on two of them. Platform of remaining ones had been upgraded successfully.\r\nHence, I needed to upgrade platform on 2 BCP servers using the command below:\n\r\n>patchPlatform.pl\n\r\nAt the same time, I upgraded other NEs from MCP GUI. Upgrade of NEs includes the operations below:\r\n- Undeploy the NE\r\n- Change release level to the new release from instance in MCP GUI.\r\n- Deploy the NE\r\n- Start\n\r\nAs soon as platform patch and upgrades of NEs had been completed, we agreed to end the call.','null'),(553,'Mitat GOCMEN','AS-OAM','2014-03-09','140309-459007','Axtel','ER paged me for critical alarms raised on BCPs at Axtel site. \n\r\nThe site was just upgraded to 14.0.16.3 load.\n\r\nI logged in to the site and observed following alarms on BCPs. \n\r\n-- DB communication failure to DB instance 1 and 2. \n\r\n-- BCPs are in invalid cluster configuration. The blades should be in either a 3+1 or 2+1 configuration and they are in 3+0 and 2+0 configurations.\n\r\nI suggested manually upgrading the two BCPs that upgrade wizard neglected. Because the active calls are not effected from this situation, ER arranged a maintenance window to take action. Then the call ended.','null'),(554,'Mitat GOCMEN','AS-OAM','2014-03-10','140310-459048','Axtel','ER paged me for critical alarms raised on BCPs at Axtel site. \n\r\nThe site was just upgraded to 14.0.16.3 load.\n\n\r\nProblem # 1 : \r\nThere were DBComm alarms on two BCPs.\n\r\nSolution # 1: \r\nI realized that two BCPs were not upgraded by the upgrade wizard. I patched both BCPs\' platform manually and deployed them with the new load from the SM GUI. DB communication alarms are cleared.\n\n\r\nProblem # 2 : \r\nThere was the following alarm on all the BCPs. \n\r\nBCPs are in invalid cluster configuration. The blades should be in either a 3+1 or 2+1 configuration and they are in 3+0 and 2+0 configurations.\r\nInvalid cluster configuration alarms were not cleared after upgrading all the BCPs. \n\r\nSolution # 2: \r\nI stopped and started all the BCPs in maintenance window. All the alarms were cleared. I wanted customer to perform test calls. Test calls were also successful.','null'),(555,'Mitat GOCMEN','AS-OAM','2014-03-09','140309-458998','Axtel','SWD paged me for an upgrade issue on Axtel. \n\n\r\nUpgrade path: \r\n12.0.12.7 --> 14.0.16.3\n\n\r\nProblem # 1 : \r\nDuring A2 upgrade, the following Oracle patching error halted the upgrade: \n\r\nThe patch file given is not applicable to the Oracle version installed! \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/oraclePatch_logs/ut_oraclePatch.EMS1_b2f9ed8a-2a7f-1b21-8207-001b2110cc8d_ut_oraclePatch.pl_PATCH_PRI_ORACLES_1.20140309_012321.log \n\r\nSolution # 1: \r\nI realized that there was old oracle version on /var/mcp/media directory. Wizard was not able to handle which version of oracle used. I removed the old oracle installer and retried on the wizard. \n\n\r\nProblem # 2: \r\nWizard did not fail with the previous error. However, I followed oracle patch logs and observed that the operation was successful, but the wizard application got stuck. \n\r\nSolution # 2: \r\nI clicked on Save and Exit and when I logged back in to wizard, it was able to pass that step successfully.','null'),(556,'Cigdem Vural','AS-OAM','2014-02-27','140227-457494','University of Texas-Austin','Rodney called and told that customer cannot make any calls.\r\nSESM unit1 is down unavailable.\n\r\nWe had performed a rollback on same site today, and after rollback all instances were OK and test calls were passing without any issues.\n\r\nLogged into the site and check for the status of both SESM units.\r\nAt MCP GUI Unit seems as unavailable and SESM0 seems as active.\n\r\nTried to start SESM1 but it did not let me to do. So I run neStop.pl and then kill -9  at ssh session. After that at MCP GUI i did undeploy/deploy SESM1 and it come up as HotStandby. And same time SESM0 went to unavailable state and SESM1 got the activity.\n\r\nI killed the SESM0 at MCP GUI and deploy it from GUI then started and it came up as HotStandby.\n\r\nAfter that Kim applied the jar files to both SESM units. For a second both SESM units showed as active then SESM0 restarted itself.\n\r\nThen I did a swact between SESM units to check it will came up as expected ( active and  standby) and it is OK.\n\r\nCustomer could perform test calls without any issue. Monitored around an hour and dropped off.\n\r\nER and Kim will monitor during day, I told them if any help needed they can page me again.','null'),(557,'Cigdem Vural','AS-OAM','2014-02-27','140227-457344      ','University of Texas-Austin','SWD team was performing a patch upgrade from 16.0.2.2  to 16.0.2.8.\nCustomer had 16.0.2.2_UT_Austin_Part2.jar file for UCD groups and they told UCD is not working by upgrading to 16.0.2.8 and they want to perform rollback.\n\nCallP team involved to the issue at first. They removed the jar and UCD worked but not stable. They told still there are issues and rollback needed.\n\nSo we performed rollback(NN10437-500 ) and Eric put all jar files back.\nCustomer did the testings and told everything is OK.\n\nThen dropped off','null'),(558,'Senem Gultekin','AS-OAM','2014-02-21','140221-456532 ','Bell Aliant','Problem Description:\n\r\nER paged me for an wizard upgrade failure on DB&SM Upgrade screen.\r\nMCP 14.1.8.1 to MCP 17.0.7.4.\n\r\nError:\r\nUnlinked /var/mcp/upgrade_tools/work/smUpgrade//init_ned_cmd_20140220_231925_TziI\r\nNot unlinking file\r\nwaitingTime/MaxTime = 303  300\r\n	Unable to start SM\r\n	INITIALIZING\n\r\nSM upgrade unsuccessful. Unable to start SM	INITIALIZING\n\n\r\nIn total we have face 4 different issues, here are the solutions;\n\r\nSolution:\r\nIssue #1\r\n-	Accessed to the site and checked  the detailed logs under /var/mcp/upgrade_tools/logs. From the logs it is seen that DB was upgrade, SM was upgraded but when starting the primary SM with new load it was getting stuck on INITILAZING state and wizard was failing.\r\n-	Accessed to the DB and checked the current release for DB. It was with the new load which is 17.0.7.4. \r\n-	Accessed to the MCP GUI and the primary SM was in online hotstandby status running on 17.0.7.4 release.\r\n-	We have performed Save&Exit on wizard and restarted, DB&SM Upgrade screen failed again due to invalid SM status.\r\n-	Had to restart the primary SM, but realized the primary SM was coming up in around 15 minutes which is really long, servers are HP CC3310(old and slow servers) and wizard had failed due to timeout in the first place.\r\n-	We have passed this screen on debug mode in order to keep up with the wizard timeout problem. I will open a jira to increase timeout for DB&SM Upgrade screen.\n\r\nIssue #2\r\n-	The next screen was importing the new license key and upgrading the primary element instances. However the license key was failing.\r\n-	After investigating the logs realized the license key was failing due to status of primary SM. It was OFFLINE and ACTIVE.  This was because we had to perform manually smStop.pl and smStart.pl scripts via SSH for issue #1.\r\n-	Accessed to the primary db and changed the admin state as ONLINE for primary SM. After that I was able to add license key from the MCP GUI successfully. \r\n-	Wizard passed the failure and continued with the primary element instances upgrade.\n\r\nIssue #3\r\n-	While wizard was upgrading primary element instances, all instances were upgraded to 17.0.7.4 except 2 BCPs, they were failing.\r\n-	Checked the BCP1 and BCP3, I was not access them via ssh.\r\n-	Tried to ping BCP1 and BCP3 from primary SM and they were not pingable.\r\n-	Asked the customer to check the BCPs physically. They reported that they had a problem with ESM before.\r\n-	They restarted ESM and the BCPs became reachable.\r\n-	After retry wizard screen was success. BCP1 and BCP3 were upgraded as well.\n\r\nAt this point upgrade came to the pause point where half upgrade was completed. However maint window was over.\r\nSince the primary side was up and running on 17.0.7.4 release, customer decided to go with the upgrade.\n\r\nIssue #4\r\n-	After 2 hours customer reported that, during setting up the replication between 2 DBs, it was stuck at Setting up master definition site. They had wait around 1,5 hour\r\nStarting resynchronization of the databases\r\nChecking plan table\r\nDisabling writes to the Regdest table until replication on the Primary Database has been dropped\r\nQuiescing the database \r\nSetting up master definition site -> Was not continuing here.\r\n-	Accessed to both primary and secondary DB servers. Restart the neds and restarted the db itself by the following commands.\r\n#neinit restart\r\n#/etc/init.d/dbora stop\r\n#/etc/init.d/dbora start\r\n-	After the restarts hit retry button on wizard and the screen was successful after around 30 min. It seems like the script was stuck in the background, I investigated the logs however couldnt find the reason. We see slow responses on HP CC3310 servers like this often. Will again check the logs to see if I can catch anything more.\n\r\nCustomer continued with the post upgrade steps, and they were fully upgraded to 17.0.7.4 release after all.\r\nEnded the work with ERs and customer.','null'),(559,'Seren Batmaz','AS-OAM','2014-02-14','140214-455362','Codatel','Problem Description:\n\r\nSWD paged me for a problem occured during half rollback operation from MCP_14.0.16.3 to MCP_12.0.12.7. NTP Document, 630-01494-01 01.12 was being used during that operation.\r\nSWD stated that SM_1 was not able to start due to the error saying that there was no suitable database instance. The reason for that error was SM_1 wasnt trying to communicate with secondary DB instance which was running on MCP_12.0.12.7.    \n\r\nSolution:\n\r\n- I checked /var/mcp/install/installprops.txt file. In that file, DB type was single.  \r\n- I checked neprops.txt on SM_1 and there was no information about db.secHost which had been the reason for not communicating with secondary DB Instance.\n\r\n- The first operation that I ran, was changing DB type in installprops as \'Replicated\', deploying and starting SM_1:\r\n   - vi installprops.txt\r\n   - smDeploy.pl\r\n   - smStart.pl\r\n  The result was not successfull.\n\r\n- I ran \'dbInstall -fo\' command, deployed and started SM_1. The problem persisted.\n\r\n- I added the db.secHost information to the file neprops.txt on SM_0 and started it.\r\n   - vi neprops.txt\r\n   - added \'db.secHost=\r\n   - smStart.pl\n\r\nAfter that, SM_0 was able to become active successfully. I also started SM_1 from MCP GUI. \r\nSince there weren\'t any other problem, we agreed to end the call.','null'),(560,'Seren Batmaz','AS-OAM','2014-02-13','140211-454716','Axtel','Problem Description:\n\r\nSerdar, from A2 Call Processing team, paged me about a problem related with SESM server. He told me that CPU occupancy reached %100 and the server went down continuously. \n\r\nSolution:\n\r\nI asked him to stop SESM instance running on that server to be able to see if the CPU problem was server related or instance related and we found out that it was server related. So, I continued the investigation.\n\r\nWhen I checked raid status of the server, I saw that disks were not mirrored. So, first, I tried to mirror the disks. I used the following commands for that:\n\r\n- mcpSwRaid.pl -remove sdb\r\n- mcpSwRaid.pl -add sdb\n\r\nThe mirroring operation could not be completed due to the server continued going down. \r\nAfter that, I asked the customer if he could change sdb disk with a new disk. After changing the disk, I could successfully mirror the disks.\r\nAfter monitoring the server for an hour, we agreed with the customer to terminate the call.','null'),(561,'Seren Batmaz','AS-OAM','2014-02-12','140212-454904','Optus','Problem Description:\n\r\nER paged me due to the problem occured while starting SESM2_0 instance from MCP GUI. There were the exception below:\n\r\n\"Unable to initialize NetworkInterfaceManager. \r\ncom.nortelnetworks.mcp.ne.base.subsystem.SubsystemException: \r\nNetworkInterfaceManager creation failed. \r\n.............................\r\nCaused by: java.io.IOException: Error running command. \r\nCommand: /opt/mcp/nif/bin/getIfInfo.pl --redirect \r\nExit code: 255 \r\nCommand output \r\n[\"api\" is not defined in %NifUtils::Linux::EXPORT_TAGS at (eval 18) \r\nline 0 \r\nUndefined subroutine &NifUtils::getAllIfInfoImpl called a\r\n.............................\"\n\r\nER informed me that there were a disk replacement operation which took place on that server last week. GPS had handled case 140114-449945 which was for a Raid Sync problem. \n\r\nSolution:\n\r\nWhen we talked with the GPS engineer who handled the case 140114-449945, I learnt the backround of it. Problem in this case had been a known issue which was fixed in later platform releases. Since the customer had not wanted to upgrade the platform, GPS had applied a workaround to be able to sync raid. The workaround includes changing and adding some platform scripts on that server.\n\r\nAfter our investigation, we found out that getIfInfo.pl script which was the cause of the exception, had dependencies with changed scripts. So, we changed the scripts back. Then, when we started SESM Instance from MCP GUI, it successfully became Hot Standby.\r\nSince the problem resolved, I left the call after we agreed with ER.','null'),(562,'Cigdem Vural','AS-OAM','2014-02-04','140204-453482','ThunderBay','During upgrade it failed at DB Mock Upgrade screen with following error:\n\r\nThe upgrade scripts in load MCP_14.0.16.3_2013-09-12-0840 \r\ndid not get tested against the current database because \r\nthe dry run upgrade scripts failure, the reason for the failure: \r\nError caused by problems from (ned, sftp, oracle etc.) \n\r\nPlease contact next level of support to solve the problem.\n\r\nBefore calling to GPS, SWD tried ned restart and ./dbora stop start but did not help.\r\nWe did secondary server reboot but the server did not come up about half an hour time and we did powercycle.. After a while it could come up but server was so slow. (CC3310)\n\r\nThen retried the same step and DB Mock upgrade screen passed.','null'),(563,'Selen BAYRAKTAROGLU','AS-OAM','2014-02-02','140202-453266','Axtel','ER paged me for an issue on Axtel site. \r\nCustomer Load:12.0.6.13(7.0 SP1) \n\r\nProblem Description : \n\r\nBoth SMs were in activating state and not able to get active state.\n\r\nConnected to site and checked BCPs and SESMs to see if there was an outage or not. All the SESMs and BCPs were up&running.\n\r\nSolution Summary:\n\r\nConnected to SMs and checked spool directory under SM. There were lots of files under PROVs. Killed both SMs and PROVS and cleared spool directory under both SMs. After that, started secondary SM. SM was able to get active state. Customer launched the MCPGUI without any issue. Tried to start primary SM but it could not become up. \n\r\nThere were SESM related exceptions. I requested to restart SESMs and start primary SM again in maintenance window. Maintenance window was scheduled for 0200CST to perform the recommended actions. Customer tested provisioning and ended the call.','null'),(564,'Cigdem Vural','AS-OAM','2014-02-03','140202-453266','Axtel','That is an ongoing issue from other night. And there was an action plan to restart SESM instances. And SM0 was in unavailable state.\n\r\nWe restarted the SESM instances. After that we have started to work on the issue about SM instances. Both SM instances could not be up at same time ( active and Hot Standby) \r\nWhen one is active we did start the other instance and expect to see the second to be up as hot standby but it is not. The second one also trying to be active and current active just gone to unavailable state. The same behavior for both SMs\n\r\nThere is no spool problem on SMs. We tried stop/start & deploy/undeploy SMs and reboot to the server but no help. \r\nIn every try it is same and only one SM can be active and the other is unavailable.\n\r\nSince mtc window finished, customer does not want to take any other action and ask to make SM 0 active since they will have HP for the fan problem on SM1.\n\r\nCurrent state: SM0 is active \r\n               SM 1 is unavailable \n\r\n               All other instances are OK. \n\r\n               GUI is stable and working\n\n\r\nAction plan for the next mtc window: \n\r\n1- Stop monitoring for the instances to eliminate any issue with SNMP since we have similar with Eastlink on PAs and try to make both SMs up( one active other hot standby)\r\n2- In parallel, design will prepare a debug jar to be able to see the communication between both SMs( The issueseems related the communication between SM instances. SM which is unavailable tried to be start and it cannot recognize that there is already on Active SM on the system and tries to be Active too. And that causes the current SM to be unavailable)\n\r\nNext mtc will be @0230am MST (GMT-7)- @11:30am (GMT+2) Istanbul','null'),(565,'Selen BAYRAKTAROGLU','AS-OAM','2014-01-30','140130-452777','CABOVISAO SA','SWD paged me for an upgrade issue on CABOVISAO site.\n\r\nUpgrade path:14.0.12.9->14.0.16.3 \n\r\nPROBLEM SUMMARY  \r\nDB&SM upgrade failed with an error,\n\r\nExecuting DB & SM upgrade \r\nLogs are written to /var/mcp/upgrade_tools/logs/monitored_scripts/monitored.ut_mcpUpgrade.EMS1_a0a4f956-2a78-1b21-8522-000e0caff715_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20140129_235523.log \r\nValidating the scripts and files required for upgrade. \n\r\nInvalid Load name given. Must have be of the form \".zip\" \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/mcpUpgrade/ut_mcpUpgrade.EMS1_a0a4f956-2a78-1b21-8522-000e0caff715_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20140129_235528.log \n\r\nWrong Loadname \r\nDB & SM upgrade failed. \n\r\nSolution:\n\r\nI checked /var/mcp/loads directory but could not find .zip file. Connected to server and applied manually,\n\r\n#zip -r .zip \n\r\nAfter that ne.load failed. Checked installprops.txt file and saw that ne.load was incorrect(ne.load=am).I corrected ne.load with the right load and issue resolved.','null'),(566,'Selen BAYRAKTAROGLU','AS-OAM','2014-01-28','140127-452103','Bragg Communications (Eastlink)','Customer load: MCP_14.1.0.13 (A2 8.0 SP1)\n\r\nER paged me for an issue on Eastlink site and and stated that both SMs were going ACTIVE state, hence they were unable to view alarms and perform actions on the network elements.\n\r\nEastlink replaced firewall on 01/24 between 01:00 and 05:30 site time and  they noticed later on that their GUI would not stay up. ER stated that the logs related to the servers bouncing started @~01:32 on 01/24. The customer was telling that the FW they replaced had nothing to do with the A2, the 8600s were checked and the SMs were on the same subnet/vlan.\n\r\nI tried to kill both SMs, then start one by one. Once SM0 was active, then the other one did not stay at HOT STANDBY, instead became ACTIVE (did not see the peer ACTIVE and initialized itself to be ACTIVE), so it did not work. At that moment, SM0 was kept as ONLINE. I asked the site to verify that all ports between 12100 and 12150 for communication between the SMs were open on the firewall, and customer stated that there were no issues on these ports. \n\r\nIn parallel, ERS GPS team was paged out to investigate if there was anything abnormal with 8600s.\n\r\nAt that time, I noticed that PA instances were not ONLINE and started them. It fixed the problem after starting SM1 again, it became HOT STANDBY. Issue was resolved and ended the call.','null'),(567,'Seren Batmaz','AS-OAM','2014-01-14','140114-449858','Princeton University','Nate paged me due to they cannot launch MCP Management Console. He said that the error message was \"Error:Null\".\n\r\nI said that it was related with Java version of the PC that they were trying to launch the GUI and the problem was not a result of any service effecting issue. So, I asked Nate to send the case to PS A2 OAM queue and daid that A2 OAM GPS was going to investigate the case in business hours.\n\r\nSince we agree, I left the call.','null'),(568,'Seren Batmaz','AS-OAM','2014-01-16','140116-450250','Telecom Liechtenstein (TLI)','Problem Description:\n\r\nCamila paged me for the problem which occured during DB Mock Upgrade while running 14.0.9.11->14.0.16.2 MR upgrade. The error message was as below:\n\r\n\"Error caused by problems from (ned, sftp, oracle etc.)\"\n\r\nSolution:\n\r\nI connected to the site, checked the logs. First, I restarted ned on EMServer2 which Secondary DB was deployed on:\r\n>neinit restart\r\nWhen I saw that the problem had not been resolved yet, I restarted Oracle on EMServer2:\r\n>./etc/init.d/dbora stop\r\n>./etc/init.d/dbora start\r\nRestarting Oracle did not help neither. So, I suggested rebooting EMServer2 and with approval of the customer, we rebooted EMServer2.\r\nAfter the server came up, we retried running DBMock Upgrade screen again. At that time it worked and it was successfully completed, so, we ended the call.','null'),(569,'Selen BAYRAKTAROGLU','AS-OAM','2014-01-12','140111-449327','Axtel','ER paged me for an issue on Axtel site and reported that all instances (SESM, BCP, PROV,SM) were in CONFIGURED state and new siplines could be provisioned but was unable to register.Customer arranged a maintenance window to apply kill/restart all the NEs.\n\r\nCustomer load: MCP_12.0.6.13 \n\r\nI tried to kill and deploy HOTSTANDBY SM and got invalid license key failure.Customer replaced SM_1 server last week and did not apply the new license key with the new MAC addresses. I applied new license key to database and swacted SMs. At that point, all NEs were able to get online/active state after SM swact. Customer tested new siplines /provisioning and issue resolved.','null'),(570,'Selen BAYRAKTAROGLU','AS-OAM','2014-01-09','140109-448969','Verizon Communications','SWD paged me for an upgrade issue on Verizon site.\n\r\nUpgrade path: 14.0.9.12 to 14.1.0.13\n\r\nWhile taking database backup on prepare DB screen, it was failed with an error;\n\r\nError: Database backup is failed.\r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/dbBackup/ut_dbBackup.EMS1_a1913fae-2a74-1b21-b5f2-000e0cede34a_ut_prepareDB.pl_PREPARE_DB_4.20140109_012350.log\r\nPrepare Database FAILED\r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/prepareDB/ut_prepareDB.EMS1_a1913fae-2a74-1b21-b5f2-000e0cede34a_ut_prepareDB.pl_PREPARE_DB_4.20140109_012251.log\n\r\nOperation Error\n\r\nSolution:\r\nChecked permissions and tried manual backup but it failed again. Connected to database and executed below command. \n\r\nSQL> COLUMN object_name FORMAT A30\r\nSELECT owner,\r\n       object_type,\r\n       object_name,\r\n       status\r\nFROM   dba_objects\r\nWHERE  status = \'INVALID\'\r\nORDER BY owner, object_type, object_name;SQL>   2    3    4    5    6    7  \n\r\nOWNER                          OBJECT_TYPE\r\n------------------------------ -------------------\r\nOBJECT_NAME                    STATUS\r\n------------------------------ -------\r\nSYS                            PACKAGE BODY\r\nKUPW$WORKER                    INVALID\n\r\nSYS                            VIEW\r\nKU_NOEXP_VIEW                  INVALID\n\r\nThere were invalid packages and tried rebuild it.\n\r\nSQL> @$ORACLE_HOME/rdbms/admin/catdpb.sql\n\r\nIssue was resolved and ended call.','null'),(571,'Selen BAYRAKTAROGLU','AS-OAM','2014-01-08','140108-448752','Verizon Communications','Customer load:12.0.12.x\n\r\nProblem Description:\n\r\nSWD paged me for an Oracle Migration (from 9i to 10g) issue on Verizon site.\r\nOracle migration failed due to time out failure on primary EMServer.\n\r\nError occurred executing NE commands (see below):\n\r\n> run err Error! Timer expired while executing: perl\r\nError executing Oracle migration commands.\n\r\nSee log file for possible details: /var/mcp/run/install/logs/oracleMigration.log.20140107_234051\n\r\nSolution:\n\r\nRestarted the NED and database on primary EMServer.\n\r\n#neinit restart\r\n#./dbora stop\r\n#./dbora start \n\r\nIssue was resolved and ended the call.','null'),(572,'Selen BAYRAKTAROGLU','AS-OAM','2014-01-06','140106-448491','Axtel','Customer load: MCP_12.0.6.13 \n\r\nER paged me for an issue on Axtel site. Customer reported that all SIP traffic on the Monterrey Experius Application Server was failing.\n\r\nI noticed that all SESMs (5 pair) were down and tried to recover SESMs by killing and starting them manually. At that point, primary SM (SM_0) appeared to be ACTIVE however, SM GUI was unresponsive. Even though SESMs were ACTIVE after manually starting, they became OFFLINE again a few minutes later.\n\r\nI suggested customers network engineers to check their network according to logs.\n\r\nThe customer powered down/up Secondary SM (hotstandby) and after this action was completed, SESMs became ACTIVE and SMGUI got responsive again.There was a communication problem between primary SM (SM_0) & SESM NEs and Secondary SM (SM_1) that prevented SM_0 to provide service. All lines started to register again and provisioning was successfull. Issue was resolved and ended the call.','null'),(573,'Selen BAYRAKTAROGLU','AS-OAM','2014-01-06','140106-448386','Axtel','Customer load: MCP_12.0.6.13 \n\r\nER paged me for an issue on Axtel site and stated that newly provisioned subscribers were unable to register or make/receive calls.\n\r\nI accessed the site and investigated that all BCP NEs (*19) were down/unstable and both SMs were not able to get ACTIVE state. The customer asked to fix BCP NEs first and killed and started all BCP NEs manually on the server\n\r\n#ssh to server as a root user\r\n#neinit -autorestart=off\r\n#neinit -p\r\n#kill -9 \r\n#./neStart.pl\r\n#neinit -autorestart=on\n\r\nand resolved BCP issue. I requested to remove spool directory of SMs to address SM failure. Customer requested to arrange a maintenance window during the night.\n\r\nBCPs were able to get active state and ended the call.','null'),(574,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2014-01-04','TBD','Axtel','Customer Load: 12.0.12.6\n\r\nProblem Description: MCP GUI was not launchable and System Managers were unfunctional.\n\r\nSolution: \n\r\n-Checked the OSS logs on EMServers and files/folders under the directory /var/mcp/spool\r\n-Checked the states of System Managers\r\n-Analyzed that there was spool overload problem which was exhausting System Managers and preventing them from getting activated.\r\n-After making sure that the problem being experienced in Axtel\'s system was spool issue, explained following action plan to the customer and determined a maintenance window to perform the activities.\n\r\n   1.Kill all the NEs in the System\r\n   2.Remove all files/folders under the directory /var/mcp/spool\r\n   3.Start and recover System Managers\r\n   4.Start all other NEs starting with BCPs and SESMs\n\r\nAfter explaining the action plan to the customer and deciding on a maintenance window, agreed with the customer and ended the pager call.','null'),(575,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2014-01-03','140102-448119','Ventelo','Customer Load: 17.0.7.9\n\r\nProblem Description: SM\'s were not functional and were not able to get activated.\n\r\nSolution: \n\r\n-I connected to the site and checked if i was able to SSH EMServers and analyzed that i was not able to establish a SSH connection to Primary EMServer.\r\n-I suggested the customer to reboot Primary EMServer.\r\n-After the reboot, i was able to SSH Primary EMServer.\r\n-Checking the oss logs on EMServers and the states of EMServers, i suspected on spool overload problem which was causing the SMs to be unfunctional.\r\n-We have set a maintenance window with customer to perform activities explained below to recover System Managers\r\n   1. Kill all the NE\'s in system\r\n   2. Remove all files/folders under the directory /var/mcp/spool\r\n   3. Start and recover System Managers\r\n   4. Start all other NEs starting with BCPs and SESMs\n\r\nAfter explaining the activities for maintenance window and setting the time interval, agreed with customer and ended the pager call.','null'),(576,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2014-01-02','131231-448023','Mobistar','Customer Load: 14.1.0.8\n\r\nProblem Description: SESM1_0 was not able to communicate with other network elements.\n\r\nSolution:\n\r\n-Checked SESM1_0 if it was able to ping its gateway and found that it was not.\r\n-Controlled /etc/sysconfig/network file, /admin/userinfo.txt file and analyzed that the gateway ip in these files were wrong.\r\n-Asked the customer if i can execute the script reIp.pl to reconfigure gateway ip address for SESM1_0\r\n-Executed the script successfully and problem solved.\n\r\nAgreed with the customer and ended the pager call.','null'),(577,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-12-19','TBD','Axtel','ER paged me for an issue on Axtel site. SMs were not able to get active state.\n\r\nChecked logs/configurations of SMs and tried to kill and start SMs,\n\r\n#ssh to server as a root user\r\n#neinit -p\r\n#kill -9 \r\n#cd /var/mcp/install\r\n#su ntappadm\r\n#./smStart.pl\n\r\nIt did not resolve the issue and checked /var/mcp/spool directory. The issue was related with files under /var/mcp/spool directory.\n\r\nSolution:\n\r\n#killed all the NEs to stop sending files to SMs\r\n  ssh to server as a root\r\n  neinit -p\r\n  kill -9 \r\n#removed all the files under /var/mcp/spool directory\r\n#started SM and it was able to come active state,\r\n  cd /var/mcp/install\r\n  su ntappadm\r\n  ./smStart.pl\r\n#started other NEs via MCPGUI\n\r\nIssue was resolved and ended te call.','null'),(578,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-12-18','131218-447001','Emirates','Customer load:17.0.7.x\r\nER paged me for an issue on Emirates site. Customer could not launch the MCPGUI via the address,\n\r\nhttps://10.0.66.27:12121/mgmtconsole-windows.jnlp\n\r\nI asked customer to verify if provisioning was working or not. Customer stated that provisioning was working and the only issue was while launching MCPGUI.\r\nI could not connect to site and check MCPGUI because of site restrictions. Customer tried to launch the MCPGUI over proxy and it worked,\n\r\nhttps://iems0dhcc.du.ae:9091/servlets/com.nortel.iems.server.servlets.LaunchProxyMCS?jnlp=mgmtconsole.jnlp&host=10.0.66.27&remotePort=12120&remotePortSec=12121\n\r\nI suggested customer to check internal network configurations with a network engineer.The issue was resolved after the modification done by their IP Backbone Team and ended the call.','null'),(579,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-12-17','131217-446914','Ziggo','ER paged me for an issue on Ziggo site. Customer stated that he could not launch the MCPGUI and provisioning was not working.\n\r\nCustomer load:14.0.9.7\n\r\nWhile I was trying to start the SMs, I got the below failure,\r\njava.lang.OutOfMemoryError\r\n        at java.io.FileOutputStream.writeBytes(Native Method)\r\n        at java.io.FileOutputStream.write(Unknown Source)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.MemFile.grow(MemFile.java:80)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.MemFile.(MemFile.java:64)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.getOrCreateMemFile(OMServer.java:140)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.setMemFileSlot(OMServer.java:212)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.CSVParser.parseStream(CSVParser.java:55)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.processDir(OMServer.java:185)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.collect(OMServer.java:163)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.server.OMServer.processDirectories(OMServer.java:52)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.runtime.OMFileTask.handleStartEvent(OMFileTask.java:72)\r\n        at com.nortelnetworks.mcp.ne.share.omserver.runtime.StartEvent.handle(StartEvent.java:24)\r\n        at com.nortelnetworks.mcp.base.task.SimpleTask.handle(SimpleTask.java:26)\r\n        at com.nortelnetworks.mcp.base.task.Task.processEvents(Task.java:707)\r\n        at com.nortelnetworks.mcp.base.task.Task.run(Task.java:541)\r\n        at com.nortelnetworks.mcp.base.task.TaskRunner.run(TaskRunner.java:84)\r\n        at java.lang.Thread.run(Unknown Source)\n\r\nChecked logs and tried to kill, undeploy and deploy SMs. \r\n#neinit -p\r\n#kill -9  \r\n#cd /var/mcp/install\r\n#./smUndeploy\r\n#./smDeploy\r\n#./smStart\n\r\nIt did not resolve the issue and rebooted the servers. After the reboot, SM\'s came back and customer could access the MCPGUI. \r\nCustomer confirmed that they could do QSIP, but he was not able to verify provisioning until the morning. Agreed to continue tomorrow morning and ended the call.','null'),(580,'Senem Gultekin (NETAS External)','AS-OAM','2013-12-13','131213-446450','Verizon','Problem Description:\n\r\nSWD paged me for DB MOCK failure at Verizon. Upgrade path is; \r\nCurrent load = MCP_12.0.12.7_2012-12-11-2138\r\n     New load = MCP_14.0.9.12_2012-12-19-1845\n\r\nWizard was failing at MOCK Upgrade screen.\r\nError caused by problems from (ned, sftp, oracle etc.)\n\r\nSolution:\r\n1-	Checked the logs but the error was very generic.\r\n2-	Restarted the ned and db but the MOCK upgrade was still failing.\r\n3-	Rebooted the both EM servers.\r\n4-	After the reboot MOCK upgrade completed successfully.\r\n5-	SWD continued with the upgrade and completed the whole upgrade successfully.','null'),(581,'Senem Gultekin (NETAS External)','AS-OAM','2013-12-11','131211-446034','Verizon','Problem Description:\r\nSWD paged me for an upgrade failure at Verizon. Upgrade path; \r\nCurrent Load => MCP_10.3.2.12_2010-12-20-1912\r\nUpgrade Load => MCP_12.0.12.0_2011-10-26-0612\n\r\nSince this an old release, the upgrade is performed manually. \n\r\n./mcpUpgradeFrom10To12.pl  failed with the following error;\r\n___________________________________________________________________\r\nError occurred executing NE commands (see below):\n\r\n Error: when execute above command => 1\r\nNE command exited with the value: 1\r\nError executing start commands.\n\n\r\nSee log file for possible details: /var/mcp/run/install/logs/dbInstall.log.20131211_000628\r\n___________________________________________________________________\n\n\r\nSolution:\r\n1-	Checked the logs and went to the referenced logs. The following was in the repairNotNulls.log\r\n___________________________________________________________________\r\nChecking Status of user objects .....\r\nDECLARE\r\n*\r\nERROR at line 1:\r\nORA-20980: Invalid object found: object type =TRIGGER object name =DELETE_LOCSVCMGR status=INVALID\r\nORA-06512: at line 8\r\n___________________________________________________________________\r\n2-	There were stuck triggers in the DB. \r\n3-	Accessed to the site, checked primary DB for the triggers and dropped the triggers as following;\n\r\nTo see the triggers;\n\r\nCOLUMN object_name FORMAT A30\r\nSELECT owner,\r\n       object_type,\r\n       object_name,\r\n       status\r\nFROM   dba_objects\r\nWHERE  status = \'INVALID\'\r\nORDER BY owner, object_type, object_name;\n\r\nWith the output we have seen 3 different triggers and dropped them;\n\r\ndrop trigger UPDATE_LOCSVCMGR;\r\ndrop trigger INSERT_LOCSVCMGR;\r\ndrop trigger DELETE_LOCSVCMGR;\n\r\n4-	After the above steps, SWD performed ./mcpUpgradeFrom10To12.pl script again and it completed successfully. \r\n5-	SWD continued and completed his upgrade.','null'),(582,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-12-07','131207-445578','OneConnect','Problem Description:\n\r\nI was paged by Joyce related with the problem with Unified Communication Service observed in OneConnect after they upgrade their system from 14.0.16.2 to 17.0.7.10. \n\r\nI have been told that the account status of some of the customers were appearing as \"NONE\" on prov and there were no configuration errors on MAS side, so i connected started investigating PROV side of customer site.\n\r\nI investigated customer\'s provisioning client yet could not find anything which may cause the problem. I consulted to Yunus Ozturk(Gateways GPS) and Utku Ozbek(Provisioning Design Architect) about the problem.\n\r\nOur co-operative investigation showed that there were not any issues related with Provisioning Client. The problem was, the problematic users were missing some of their .xml files on MAS Media Management.\n\r\nJoyce mentioned that they had to apply a workaround for backup/restoration process on MAS as the customer had huge size of backup data(11GB) which is not supported by MAS Element Manager. She explained that she manually zipped the storage root folder while taking backup and restored those zip\'s when the upgrade on .36 is completed(which was the only way to perform backup/restoration process at that point).\n\r\nWe analyzed that some of the .xml files was not matching the user records in MAS database.\n\r\nIt was analyzed that adding .xml files to media management manually was solving the issue. After confirming the cause of the problem, we ended the pager call to  team up early in the morning with MAS designers to find a way to fix this mismatch between Storage Root and MAS Database','null'),(583,'Senem Gultekin (NETAS External)','AS-OAM','2013-12-10','131210-445823','Ventelo','Problem Description:\n\r\nER paged me for an IEMS  A2 communication problem. After CMT/IEMS was upgraded to CVM15, it was not able to retrieve the alarms from A2 and A2 was greyed out on IEMS.\r\nA2 is running on 14.0.16.3.\n\r\nSolution:\n\r\n	Primary SM was down and they performed power cycle. \r\n	They have performed SM swact, but it didnt work.\r\n	Asked them if it was working just before the upgrade. The answer was yes. IEMS was showing A2 alarms, but right after the upgrade it was not there.\r\n	Since  issue wasn\'t there prior to CMT/IEMS upgrade, suggested them to contact IEMS GPS and let me know if anything needed from A2 side as pager support.\r\n	Ended the call.','null'),(584,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-12-04','TBD','Paltel','Rollback Path: 17.0.7.7 to 17.0.4.3 for only SESM Instances.\n\r\nProblem Description:\n\r\nCustomer decided to rollback their system to 17.0.4.3. However the latest database backup was taken on November 11th and the customer mentioned that they provisioned more than 10.000 users from that date. So they did not want to restore their database with the backup taken on November 11th.\n\r\nCustomer asked if it was possible to perform a rollback just for SESM Instances. We informed the customer that what they ask for is unsupported, yet still they wanted to try rolling back SESM Instances. We discussed the situation internally and decided to try deploying 17.0.4.3 on SESM Instances.\n\r\nFollowing activities has been performed on the system:\n\r\n- Database backup is taken\r\n- Using MCP GUI, Sesm instances are rolled back to 17.0.4.3 one by one to keep at least one instance active during the maintenance activites to prevent a full outage.\r\n- Before starting each SESM Instance, related jar files are applied on SESM instances after deploying 17.0.4.3 loads on them.\n\r\nAfter the activities above were completed, customer performed call tests and confirmed that their system is up and running functionally.\n\r\nAgreed with GTS and Customer on call and ended the pager.','null'),(585,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-12-03','131202-444724','Princeton University','Upgrade Path: CVM13 to CVM15\n\r\nProblem Description:\n\r\nPatching the platform of EMServer1 was failed with an error message \"General\" on Wizard Panel.\n\r\nSolution:\n\r\n- I connected to the site and checked the error message appeared on Wizard.\r\n- Established SSH to EMServer1\r\n- Restarted NED on EMServer1\r\n- Manually executed the script \"patchPlatform.pl\" on EMServer1\r\n- After automatic reboot after completing platform patch on EMServer1, SM_0 instance could not get in Hot Standby mode which prevented wizard to proceed.\r\n- I manually killed SM_0 instance and started it again.\r\n- SM_0 instance got in hotstanby mode successfully.\r\n- Wizard screen passed successfully.\n\r\nAfter performing the activities above, agreed with SWD and ended the pager call','null'),(586,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-11-22','131122-443267','BT Spain','Customer Load: 14.0.9.12\n\r\nProblem Description:\n\r\nJuan paged me to ask if the customer is able to provision with the DBMON 727 alarm or not.\n\r\nSolution:\n\r\nI explained Juan that the customers primary database is active and functional. So their provisioning activities will not be affected by the alarm and the only effect of the alarm on provisioning would be the newly added users is not going to be replicated to secondary database.\n\r\nAgreed with Juan and ended the pager call.','null'),(587,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-11-22','131122-443267','BT Spain','Customer Load: 14.0.9.12\n\r\nProblem Description:\n\r\nI was paged by Juan Martin. He reported that DBMON 727 alarm did not clear after the execution of resync.pl script.\n\r\nSolution:\n\r\n1-) I connected to the site and checked the history of the server to check if the script is run or not\r\n2-) After analyzing the script is run, i checked the work logs of database to see if it is executed successfully.\r\n3-) I analyzed that script failed to execute successfully.\r\n4-) I was informed that the maintenance window of customer was going to end in around 45 minutes.\r\n5-) Since the reason of the problem was not clarified at that time and we were running out of time in maintenance window, i suggested the customer to raise a case and investigate the issue with high priority.\n\r\nAgreed with Juan and ended the pager call.\n\r\nAfter investigating the logs and related traces, we analyzed that a deadlock occured when ER attempted to run resync.pl script. We discussed the issue with designers and architects and suggested a procedure to run the script again in next maintenance window.','null'),(588,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-11-22','131122-443267','BT Spain','Customer Load: 14.0.9.12\n\r\nProblem Description: \n\r\nAfter recovery of SM activity during the maintenance window. Customer has asked for the assistance with DBMON 727(Database Replication) alarm, which was also present before the activities performed in the maintenance window.\n\r\nER was supporting the customer and Brent asked me the procedure for running resync.pl script from primary database to secondary database.\n\r\nFollowing action plan is explained in details to ER:\n\r\n1-) SSH to primary database as ntdbadm user.\r\n2-) Navigate to /var/mcp/run/MCP_14.0/mcpdb_0/bin/util directory\r\n3-) Take a database backup using \"./dbBackup.pl\" script in case an error occurs related with database during resync.\r\n4-) After database backup is taken successfully, run the script \"./resync.pl\" on primary database to clear the alarm.\r\n5-) After running the resync script check the information the script asks approval for and make sure resync is being performed from primary database to secondary database.\n\r\nAfter explaining the procedure and action plan in detail, we agreed with ER and ended the pager call.','null'),(589,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-11-21','130930-431862','Verizon Communications','Customer Load: 12.0.12.x\n\r\nProblem Description: \n\r\nI was paged by Kyle Mawst from ER related with an issue reported from Verizon. One of their SESM servers was not able to boot up after a disk replacement.\n\r\nCustomer was trying to boot the server up with a single Hard Drive. I checked the printout of booting screen and confirmed with designers that the platform installed on the Hard Drive was not the platform of A2.\n\r\nFollowing Action Plan has been provided to the customer:\n\r\n1-) Swap the disk with a spare disk or a brand new one.\r\n2-) Install A2 Platform on inserted Hard Drive and i provided the procedure for platform installation on HP CC3310 Servers.\r\n3-) Deploy the A2 load to the SESM using MCP GUI.\r\n4-) Start the SESM NE.\n\r\nSince the customer did not have the Platform Installation CD available on the site or a Hard Drive having A2 Platform Installed on it, they ordered an Installation CD.\n\r\nHaving explained the action plan to the customer, we agreed with ER and pager call ended.','null'),(590,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-11-21','131122-443266','BT Spain','Customer Load: 14.0.9.12\n\r\nProblem Description:\n\r\nAlthough SM_0 was active, MCP GUI was not functional and SM_1 was constantly failing to get activated when i was paged by ER John Kisner.\n\r\nSolution:\n\r\n- I accessed to the site and checked the state of MCP GUI and SM\'s\r\n- Realized that although SM_0 was active, it was not functional as we were not able to perform any activities using MCP GUI\r\n- SM_1 failed to start a few times.\r\n- We analyzed that SM\'s were not able to handle both getting activated and processing the other tasks with other NE\'s on the system(like monitoring/log processing).\n\r\nTo solve the issue, following action plan has been suggested to apply in maintenance window: \n\r\n1-) Kill all the NE\'s at the same time\r\n2-) Remove all files folders under the directory /var/mcp/spool\r\n3-) Start SM_0 and SM_1\r\n4-) Start other instances starting with 1 BCP from each cluster and 1 SESM instance\n\r\nThe customer did not approve the application of the procedure in the first place as the procedure would cause a full outage for around 30 minutes.\n\r\nWe worked with designers during the day to suggest another procedure to avoid the outage.\n\r\nAfter discussing the issue co-operatively with designers following two procedures have been suggested to the customer as action plan.\n\r\nProcedure #1  No Outage  Approximately 30-40 minutes(which took around 7 minutes to complete in maintenance window.)\n\r\n- Stop one of the secondary SM instance in order to prevent SM swact. Make sure only one of the SM instances are up&running.\r\n- Make sure NED doesnt restart the stopped SM automatically by issuing the following command in the Secondary EM server: neinit -autorestart=off\r\n- Change the admin status of NEs to OFFLINE by manually intervening to Database (Table: NEI_ADMIN_STATE Column: ADMIN_STATE )\r\n- Change the admin status of Server Monitors to F by manually intervening to Database (Table: SERVER_MONITOR_STATE Column: MONITOR_ENABLED )\r\n- Clear the files under /var/mcp/spool in each server that those NE instances reside. \r\n- Set CS2K_CALL_AGENT_ADMIN_STATE Table, ADMIN STATE to OFFLINE.\n\r\nIf Procedure #1 doesnt work, we can continue with Procedure #2.\n\r\nProcedure #2  Outage  Actual workaround - Approximately 30-40 minutes (which took around 20 minutes in maintenance window)\n\r\n- Start killing all the NEs except one BCP blade(SMs, PROVs, PAs, SESMs, BCPs) (activity estimation is 15-20 minutes)*Outage starts*\r\n- Remove all the files / folders under the directory /var/mcp/spool on each server. (estimation for the activity is 10 minutes)\r\n- Start SMs and wait until they are Active/HotStandby (estimation for the activity is 3 minutes)\r\n- Start all other network elements starting with one SESM NE and one BCP from each cluster from MCP GUI.(activity estimation is 2-3 minutes) *Outage Ends*\r\n- Wait until all the network elements are up and functional.\n\r\nSince the action plan is approved by the customer, ended the pager call and provided support during the maintenance window successfully with using Procedure #2 as Procedure #1 did not work out.','null'),(591,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-11-20','131120-442428','TELENET N.V.','Problem Description:\n\r\nSM_1 was unavailable when i was paged by Canan Yigit from GTS.\n\r\nSuggested Solution #1:\n\r\n- To kill the SM_1 as its state was online-unavailable\r\n- Start SM_1 from MCP GUI\n\r\nSM_1 failed to start so i suggested Canan to try establishing ssh to secondary EMServer. However the Secondary EMServer was not reachable.\n\r\nSuggested Solution #2:\n\r\n- Hard Reboot the secondary EMServer.\r\n- Check the network configurations/connections of the server.\n\r\nAfter rebooting the server, EMServer became reachable and SM_1 successfully got activated.\n\r\nAgreed with GTS and ended the pager call.','null'),(592,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-11-15','131115-441788','Verizon Communications','Donnell Williamson paged me for an upgrade issue which was seen in Verizon Communications.\r\nUpgrade path:10.3.2.12-12.0.12.6\n\r\nWhile patching the secondary EM server, server could not become up again.\n\r\nConnected to site and customer tried to reboot server manually.There was not any difference. Checked boot screen and hard drive was the first option on that list. Changed CD-ROM as first option and tried reboot the server manually but it failed again.\n\r\nCustomer replaced the faulty disk drive and applied power cycle.Server became up and SWD completed upgrade.','null'),(593,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-11-14','131114-441553','CIBC','Claude paged me for an upgrade issue which was seen in Canadian Imperial Bank of Commerce.\n\r\nWhile patching operating system of the primary servers, script was failed with an Unable to connect to daemon error.Script was failed because server was down.\n\r\nConnected to site and tried to reboot the server via virt-manager. While trying to reboot server, it failed with an unable to open disk path error. There was an CD on the host server. Checked the IDE CDROM section in details tab and disconnected it. Then clicked forceoff and run button. Server became up and screen passed succesfully.','null'),(594,'Senem Gultekin (NETAS External)','AS-OAM','2013-11-08','131017-436315','Hawaiian Telcom','Problem Description:\n\r\nER paged me for a Prov instance issue seen at Hawaiian Telecom.\n\r\nPROV1 was stuck in configured state. Site is running on 14.0.9.4 Release.\n\n\r\nSolution:\r\n-	Accessed to the site and checked the system. PROV1 is running on EMServer2 and the CPU was really high, it was around %99.\r\n-	Checked the logs.\r\n-	Performed, kill, undeploy, deploy, start, stop action few times but even the state was Active the status was Offline. Also the server was slow than the EMServer1. Both of them running on CC3310. \r\n-       Critical CPU threshold alarm was coming and going on EMServer2.\r\n-	EMServer2 was up for 281 days, performed reboot on server.\r\n-	After the reboot CPU turned to the normal level, but PROV1 was still OFFLINE and ACTIVE.\r\n-	Performed double swact for SMs, after the swact started PROV, it was ONLINE and ACTIVE.\r\n-	Customer was satisfied, they will keep monitoring it for over the weekend and report us if there are any issues.','null'),(595,'Senem Gultekin (NETAS External)','AS-OAM','2013-11-05','131105-439551','Verizon','Problem Description:\n\r\nSWD paged me for a DB MOCK failure at Verizon. Upgrade path is 14.0.9.12 to 14.1.0.13\n\r\nError:\r\nGetting DB Upgrade Dry-run result ... \r\nFails to create duplicated schema from currently deployed schema. \n\r\nThis is a known issue and we have a bulletin(00008741-02) for this. Even though SWD applied the bulletin MOCK Upgrade was still failing.\n\r\nHe was also getting the following error during applying the bulletin;\r\nSP2-0310: unable to open file \"/opt/mcp/db/product/10.2.0/rdbms/admin/catdbp.sql\"\n\r\nSolution:\r\n-	Accessed to the site over SWD PC. Checked the related directories and scripts under /opt/mcp/db/product/10.2.0/rdbms/admin. It was fine for both primary and secondary DBs. \r\n-	While SWD was paging me he had restarted secondary DB.\r\n-	Ive applied the bulletin again which are the steps;\n\n\r\n*****************************************************************************\r\nFrom Secondary DB\r\n1. Login Oracle 10g Database as ntdbadm user \r\n[ntdbadm@vcarsSMLt0 ntdbadm]$ sqlplus \r\nSQL*Plus: Release 10.2.0.4.0 - Production on Wed Aug 18 13:40:30 2010 \r\nCopyright (c) 1982, 2007, Oracle. All Rights Reserved. \r\nEnter user-name: sys as sysdba \r\nEnter password: MCP_RAPTOR_1901 \r\nConnected to: \r\nOracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production \r\nWith the Partitioning, OLAP, Data Mining and Real Application Testing options \r\nSQL> \r\n2. SET ESCAPE OFF on SQL*Plus \r\nSQL> set escape off \r\n3. Catdph.sql will Re-Install DataPump types and views \r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdph.sql \r\n4. prvtdtde.plb will Re-Install tde_library packages \r\nSQL >@ $ORACLE_HOME/rdbms/admin/prvtdtde.plb \r\n5. Catdpb.sql will Re-Install DataPump packages \r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdpb.sql \r\n6.Dbmspump.sql will Re-Install DBMS DataPump objects \r\nSQL >@ $ORACLE_HOME/rdbms/admin/dbmspump.sql \r\n7. To recompile invalid objects, if any \r\nSQL >@ $ORACLE_HOME/rdbms/admin/utlrp.sql\n\r\n*****************************************************************************\n\n\r\n-	I didnt run into any issues during applying the bulletin this is probably because secondary DB was restarted.\r\n-	SWD hit retry for the DB MOCK Upgrade on wizard and waited around 20-30 min, it was success.','null'),(596,'Senem Gultekin (NETAS External)','AS-OAM','2013-11-05','131105-439539','Cable&Wireless','Problem Description:\n\r\nSWD paged me for a SESM bouncing issue during half upgrade at Cable&Wireless. Upgrade path is 12.0.12.7 to 14.0.16.3.\n\r\nThe following 4 jar files were applied for upgraded SESM instance;\n\r\n14.0.16.3_AAK-25964_Sept20_500ms.jar \r\nAAK-31171-14.0.16.3.jar \r\nAAK-31198_14.0.16.3.jar \r\nVDF_14.0.16.3_Restriction.jar -> checkpointing jar\n\r\nSolution:\n\r\n-	Retrieved the details about the upgrade path and the current situation. \r\n-	Asked SWD that if he is sure that he have applied the jars for 14.0 Release. The answer was yes.\r\n-	Suggested him to contact CALLP GPS since all 4 jar are SESM related.\r\n-	CALLP GPS was involved later on and they have investigated the issue. At this point I was out of the picture.\r\n-	However maint window was not enough and half rollback was performed later on.  System is fully back in 12.0.12.7 now.','null'),(597,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-24','131023-437267','Vodafone','ER paged me reporting that the customer was not able to establish calls. I connected to the site and logined to MCP GUI, however there were not any implications of an outage as both of the SESM NE\'s were redundantly active. Additionally there were not any alarms related with OAM side on MCP GUI.\n\r\nSo i redirected the ER to call Call Processing GPS Pager if the outage persists and ended the pager call. \n\r\nYet after a while, customer realized that they were running test calls with PBX users and as the IEMS problems has not been resolved by that time, they were not able to establish calls with PBX users. So there were actually no outage on A2 side.','null'),(598,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-24','131023-437267','Vodafone','ER paged me for the application of the action plan provided in previous pager call which is as following: \n\r\n1-) Kill all the Network Elements included active SESM\r\n2-) Delete the files/folders under /var/mcp/spool directory in all servers.\r\n3-) Start SM and try to connect MCP GUI.\r\n4-) Start all the Network Elements using MCP GUI.\n\r\nAction plan has been applied with the approval of the customer and the issue has been resolved, since the MCP GUI was reachable and all the Network Elements were online/redundantly active. Ended the pager call','null'),(599,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-24','131023-437267','Vodafone','ER paged me for an outage issue being experienced in Vodafone NZ. ER was called first for an IEMS issue yet they realized that there were also problems with A2 side. MCP GUI was not reachable and SM\'s were not functional. ER stated that customer was able to open MCP GUI a while ago yet all the tables and NE instances in MCP GUI was empty.\n\r\nThe issue is suspected to be caused by both SM\'s being offline for a while and when trying to activate one of them, SM becomes overloaded because of the spool files being received from all the NE\'s. So SM fails to start.\n\r\nTo solve the issue, following operations has been tried with the approval of customer:\n\r\n1-) Kill all the Network Elements except for active SESM\r\n2-) Delete the files/folders under /var/mcp/spool directory in all servers.\r\n3-) Start SM and try to connect MCP GUI.\n\r\nAction plan did not work as expected because all of the Network Elements should be killed in order to apply the workaround. \n\r\nCustomer was informed about the situation and they confirmed that we can kill all the network elements in their next maintanance window. Ended the pager call.','null'),(600,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-22','131019-436565','Axtel','ER paged me for an outage issue being experienced in Axtel. \"Admin\" states of all the Network Elements were stuck at \"Configured\". Customer was reporting they were able to establish calls, yet they were not able to perform provisioning.\n\r\nFollowing action plan have been applied on the system with the approval of the customer: \n\r\n1-) SSH all the servers and kill the NE instances with making sure one of the redundant pairs is always online.\r\n2-) Deploy NE Instances to get rid of the \"Configured\" state.\r\n3-) Start Network Elements after deploying them.\n\r\nAll of the servers have been rescued from \"Configured\" state by performing the actions stated above.\n\r\nDeploying and starting BCP blades caused cluster configuration alarms. So we connected to BCP blades via ssh and after running mptool -l script on all servers, we realized that for each cluster one of the blades was giving \"bad script request\" response for the script. After discussing the issue with my team we decided that we had to reboot the related BCP blades(which did not effect active calls as there were no active calls on those blades.). After rebooting related BCP blades alarms are cleared and pager call ended.','null'),(601,'Senem Gultekin (NETAS External)','AS-OAM','2013-10-07','131005-433186','Paltel','Problem Description:\n\r\nER paged me for Eric Renkoffs MCP GUI access issue, he was having trouble with his own java, not A2 related. Customer is running on 17.0.4.3 Release.\n\r\nSolution:\n\r\n-	Started to talk with Eric from yahoo. He needed to launch MCP GUI for the disk server/replacement activity which was planned before. \r\n-	Gave him some actions for this such as cleaning his java cache, using different java version and restarting his browser. Is these do not work use another PC which others can launch MCP GUI.\r\n-	Given workarounds worked and he was able to launch MCP GUI.\r\n-	But, they didnt want me to go and they wanted me to stay online until the planned activity was ended. Planned activity was replacing SESM2_1 server to increase RAM size of it.\r\n-	Server replacement was made and they have used the old disks, it was successful. The server was up and running with 4 GB RAM.\r\n-	Explained him how to apply the SESM related jar to the replaced SESM2_1 instance.\r\n-	Once SESM2_1 was up they requested CALLP GPS to be involved just for monitoring the site. Explained that its 2 am and monitoring memory usage and checking SESM logs is not something that should be done over pager support. If they have any kind of call failure we can page them. Tried to push back them to page CALLP GPS. But Yaacov insisted on calling CALLP GPS pager.\r\n-	ER tried the page CALLP GPS but they had some connection problem. I believe it was only for long distance calls. Ive paged CALLP GPS for customer and ER.\r\n-	Ozan Altinbas from CALLP GPS accessed to the site and monitored SESM2_1 instance for a while, there were no issues seen, everybody was happy at the end and pager was ended.','null'),(602,'Senem Gultekin (NETAS External)','AS-OAM','2013-10-06','131005-433186','Paltel','Problem Description:\n\r\nEric and Yaacov paged me for a question. They had a problematic SESM2_1 instance which was causing outages, and it was determined that the RAM size of the server was lower than the other SESM servers. Decision was made as server replacement. But they didnt have the document for installation and had few more questions about their installation process.\n\r\nCustomer site was upgraded to 17.0.4.3 release recently. All SESM servers are HP CC3310.\n\r\nSolution:\n\r\nProvided them the following 2 options for server replacement.\n\n\r\nOption #1:\r\n1-	Shutdown the server.\r\n2-	Remove 2 disks from the server.\r\n3-	Place the new server and insert the previous 2 disks.\r\n4-	Boot up the server. If SESM instance does not come up, you may have to stop/deploy and start the SESM instance, depending on the status on MCP GUI.\n\r\nOption #2:\r\n1-	Copy the current userinfo.txt file from the server to your local PC. You may need this info during installation.\r\n#cat /admin/userinfo.txt\r\n2-	Start the installation to your new server (which includes new disks) by following the attached installation document. Section SAM-XTS (CC3310) OS & Base Installation.\r\n3-	Once you complete the installation, go to the MCP GUI and deploy  and start the SESM instance.\n\n\r\nThey have planned to do Option#1 first and if that doesnt work they will go with Option#2 for the next maint window.','null'),(603,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-09','131007-433798','Nuvia','I was paged for a provisioning outage observed in Nuvia. Customer reported that  Adding/deleting/modifying users from both Nuvia Custom Provisioning Portal or A2 default provisioning client can sometimes does not work and cause provisioning to stuck.\n\r\nWe tried adding/modifying a test user on different domains, at first we were able to perform these activities successfully. However provisioning client stuck when we try to add a user in synaps3.net domain.\n\r\nRestarting Prov server solved the issue temporarily. So we told the customer that until we provide a permanent solution, they should restart their related prov server if the issue happens again.\n\r\nAfter activating debuglevel port and collecting logs, we realized that provisioning client was getting stuck not because of by the actions of ours. The service was getting hang by the actions of the user \"bulkprovtool\". We also analyzed that Null Pointer Exceptions are observed with the actions of that user in Prov logs.\n\r\nThe necessary logs have been collected for further investigation and ended the call.','null'),(604,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-07','TBD','KOPT','I was paged for an SNMP trap alarm issue reported from KOPT\'s site. By the time I was able to connect to the site and investigate the issue, I received a mail including the screenshots of the alarm from Hiroki Baba.\n\r\nThe alarm was actually an LDAP alarm instead of SNMP trap alarm. The issue was related with IMM side and been resolved before in case id 130808-419830 with restarting LDAP.\n\r\nCustomer was redirected to ER and IMM GPS teams to solve the problem.\n\r\nEnded the call.','null'),(605,'Senem Gultekin (NETAS External)','AS-OAM','2013-10-02','130930-431995','Cable&Wireless','Problem Description:\r\nER paged me for a provisioning issue seen from ossgate. \r\nWhen trying to view, add , delete sip line in ossgate the commands hangs.\r\nMert Cokluk (SESM GPS) investigated the issue and he stated that A2 was not responding. \r\nCustomer is running on a really old release which is MCP 10.3.1. Its been EOL for years.\n\r\nSolution:\r\n  - Accessed to the site and checked the primary SM if it was up and running properly. It was up and active. But I was not able launch the MCP GUI, learned from customer that they had firewall and to launch the MCP GUI they had to remote to PC, but I was not able to remote to that PC. \r\n-	Gave directions to ER and he shared with customer. \r\n-	From MCP GUI Prov1 was unavailable and Prov2 was active. We were not able to start Prov1.\r\n-	Checked Prov1 the server was not reachable. It was pingable but I was not able to login via ssh.\r\n-	Requested from customer to perform hard reboot.\r\n-	In old releases ossgate point to only one physical Prov server, for this site it was Prov1 which was unavailable. Suggested to change it to Prov2 which was active. Mert made that change and customer was able to do provisioning.\r\n-	During that time Prov1 server came up but prov1 was still not able to start.\r\n-	Since the customer was able to do provisioning with Prov2, Ive requested a case for Prov1 startup issue to be investigated during work hours. Case id is 131002-432298\r\n-	 Agreed and ended the call.','null'),(606,'Senem Gultekin (NETAS External)','AS-OAM','2013-10-02','130109-378857','Telenet','Problem Description:\n\r\nI was paged for a wizard upgrade problem. Wizard was failing at Switching Service to Secondary NEs screen because SM0 was in Hot Standby-Offline state. \n\r\nSabri was giving online support for mostly CALLP related jars. He was working with the person who was doing the upgrade.\n\r\nUpgrade path: 14.0.9.11 to 14.0.16.3\n\r\nSolution:\r\n  - SM will stay in OFFLINE state if it was started from command line. Damla (who is performing the upgrade) started the SM0 from command line (via ssh) and it was staying in OFFLINE state.\r\n-	At this point, SM1 was Active-Online, SM0 was Hot Standby-Offline.\r\n-	To bring Offline state to Online, suggested them kill current SM0 state and start it again from MCP GUI.\r\n-	After that, SM0 was Hot Standby and Online.\r\n-	Wizard was success.\n\r\nNote: The case id is for CALLP issues','null'),(607,'Senem Gultekin (NETAS External)','AS-OAM','2013-10-02',' 130109-378857','Telenet','Problem Description:\n\r\nI was paged for a wizard upgrade stuck issue at Telenet.  Extract Upgrade Tools screen was stuck.\n\r\nSabri was giving online support for mostly CALLP related jars. He was working with the person who was doing the upgrade.\n\r\nSolution:\r\n-	Suggested them to perform Save&Exit and launch the wizard again.\r\n-	After re-launch wizard screen was success.\r\n-	Ended the call.\n\r\nNote: The case id is for CALLP issues.','null'),(608,'Senem Gultekin (NETAS External)','AS-OAM','2013-09-30','131002-432311','Paltel','Problem Description:\n\r\nWe have agreed to give pager support to Paltel during their upgrade for the upgrade steps.\r\nUpgrade path: 14.0.16.2 to 17.0.4.3\r\nUpgrade Wizard failed at Patching Primary Servers Platform only for one server which is emserver4, and SESM2_1 instance is running on it.\n\r\nSolution:\n\r\n1-	Accessed to the site and checked tried to run the platform patch manually, but it failed as following;\r\nThe following raid-1 devices are faulty:\r\n\"md0\"\r\n\"md4\"\r\n\"md5\"\r\n\"md6\"\r\n\"md7\"\r\nPlease correct faulty software raid-1 device(s) and re-execute the patching tool\n\r\n2-	Ive tried to check the raid status but it failed as well;\r\n[root@RMLASSL11 root]# mcpSwRaid.pl -status\r\nError; Not all drives are accessible. All attached drives must be accessible\r\nfor installation to proceed\n\r\n3-	Requested from the site engineer to swap the disks for few times pyhsically, but that didnt work as well.\n\r\n4-	Asked the customer if they have a spare disk, they did not.\n\r\n5-	Customer told that they can remove one of their disks from SST system. Agreed and they have inserted to emserver4.\r\nRan mcpSwRaid.pl status and it worked.\r\nAll partitions for sdb were up and sda were unavailable. \n\r\n6-	Ran add command for sda\r\n#mcpSwRaid.pl add sda\r\nSyncing the disks took around 2 hours.\n\r\n7-	After the RAID was complete successfully, platform patch was successful as well.\n\r\n8-	Continued with the wizard upgrade steps and gave online support until the end of the upgrade.','null'),(609,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-09-30','130927-431575','K-OPTI','Problem Description:\r\nPenny had paged me about customer does not change resource value on the PROV client and they are getting error related with IMM Combokey.\n\r\nMCP Load:\r\n17.0.5.1\n\r\nSolution of Problem:\r\nPenny counted their reasons why we need to work on this issue but I respond her as; this is not system affecting issue, these types of LK issue can be founded easily and resolved within hours. She understood my situation and said me that \"I will let local team to know this and if they do not accept, will page you again.\" Then, I guaranteed her that I will resolve the issue within 4 hours. She informed the team as it is. Then, left the call.\n\r\nPS: Case was resolved Monday morning immediately within given promise hours and applied our procedure when customer gave the permission.','null'),(610,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-09-30','130927-431575','K-OPTI.COM','Problem Description:\r\nTianjun had paged me about customer does not change resource value on the PROV client and they are getting error related with IMM Combokey.\n\r\nMCP Load:\r\n17.0.5.1\n\r\nSolution of Problem:\r\nTianjun again called me about same problem and he asked for me working on it. I asked him why do you want me to work on an issue which does not need pager support. He again mentioned about KOPT customer\'s importance. But I said him that I will be at office after 1.5 hours and I am going to work on it with first priority and he left the call.','null'),(611,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-09-30','130927-431575','K-OPTI.COM','Problem Description:\r\nTianjun had paged me about customer does not change resource value on the PROV client and they are getting error related with IMM Combokey.\n\r\nMCP Load:\r\n17.0.5.1\n\r\nSolution of Problem:\r\nI asked about details of the problem. He explained as when they enter PROV gui and want to change resource value, they cannot able to change due to LK is failing from IMMComboBox. I had asked for the urgency of the issue since it is not affecting system. Tianjun said me about importance of the KOPT and wanted me to connect and work on it. But I told him that I will be office within hours and if it is possible to postpone it for 2 hours, he agreed and hang up the call.','null'),(612,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-02','131001-432071','CODATEL','SWD paged me for an E2 outage issue during upgrade from 12.0.7 to 14.0.16. Upgrade Wizard failed patching oracle version of Primary DB as the customers oracle patch level was 16(although in release notes it is stated that it should be 21) and wizard was trying to patch oracle to 22. Yet there is no patching path from 16 to 22.\n\r\nAction plan was provided to the customer yesterday (10/1/2013). I started working on customers site and followed the action plan below.\n\r\n1-) Uninstall oracle in primary DB\r\n2-) Install oracle (patch 22) in primary DB\r\n3-) Restore database from backup\r\n4-) Proceed with upgrade until patching oracle of secondary DB\n\r\nAfter applying the action plan upgrade continued and completed for primary side.\n\r\nDuring the patchPlatform of secondary sesm server customer pressed save&exit button before script is completed. We had to reboot Sesm2 server and after reboot wizard completed the script and we proceed until patching oracle of secondary database. In that screen following procedure has been applied:\n\r\n1-) Uninstall oracle in secondary DB\r\n2-) Install oracle (patch 22) in secondary DB\r\n3-) Proceed with upgrade and complete upgrade process\n\r\nAfter completing the screen, SWD continued and completed upgrade until post-upgrade steps.\n\r\nEnded the call.','null'),(613,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-10-01','131001-432071','CODATEL','SWD paged me for an E2 outage issue during upgrade from 12.0.7 to 14.0.16. Upgrade Wizard failed patching oracle version of Primary DB as the customers oracle patch level was 16(although in release notes it is stated that it should be 21) and wizard was trying to patch oracle to 22. Yet there is no patching path from 16 to 22.\n\r\nWe asked the customer if we could install oracle with patch version 22 manually, yet they decided to freeze provisioning for a day and apply the installation of oracle in next maintanance window.\n\r\nFollowing action plan has been provided to the customer:\n\r\nIn next maintanance window:\n\r\n1-) Uninstall oracle in primary DB\r\n2-) Install oracle (patch 22) in primary DB\r\n3-) Restore database from backup\r\n4-) Proceed with upgrade until patching oracle of secondary DB\r\n5-) Uninstall oracle in secondary DB\r\n6-) Install oracle (patch 22) in secondary DB\r\n7-) Proceed with upgrade and complete upgrade process\n\r\nAfter explaining the action plan to the customer in details, ended the call.','null'),(614,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-09-30','TBD','Paltel','Problem Description:\r\nEric had paged me about Upgrade Wizard problem on the Paltel site.\n\r\nMCP Load:\r\nFrom 14.0.16.2 To 17.0.4.3\n\r\nSolution of Problem:\r\nHe said that he had a problem in the prep steps of the Upgrade Wizard. Then, I warned him as we do not give pager support in Prep and Post steps of Upgrade Wizard, but he mentioned about the importance of the customer and project timeline. Then, we asked him if this can not be done in the next business hour? He checked the schedule and persuaded to wait until our business hours. Then, left the call.\n\r\nPS: The Upgrade was finished as of yesterday. (01/10/2013)','null'),(615,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-09-24','130912-428408','Paltel','Problem Description:\r\nMatvey had paged me about provisioning was down after he applied platform patch manually. \n\r\nMCP Load:\r\n8.0 BRC\n\r\nSolution of Problem:\r\nI have talked with him about what he did until that time and he said that he just applied platform (14.1.18) patch on inactive PROV and he saw an alarm in the MCP GUI. First of all, I have asked him why he patched the system manually because Upgrade wizard already does platform patch automatically. He answered as due to customer\'s system was at wrong platform patch it has been suggested by GPS to upgrade their system to correct level. Then, I persuaded to look at their system about provisioning system. Then, I noticed that after patch has been applied he did not restarted PROV instance, that\'s why he saw such a alarm in the MCP GUI. Then, I started it and alarm was cleared. Then, left the call.','null'),(616,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-09-17','130916-429436','Emirates Telecommunications','Mark Zattiero has paged me for an issue with provisioning of A2 subscribers in Emirates Telecommunications.\r\nCustomer is in progress of migrating 1/2 of their Cs2K to another physical location and powering down al the unit-0s.\n\r\nCustomer asked that if there is a supportable way to force SM_1 (DB_1) to become primary and allow provisioning.Informed that there is no way to make SM_1 as primary.Secondary db is readonly and they should put provisioning on hold until SM_0 is up again.\n\r\nCustomer stopped the SM_1 (SM_0 is powered down), and they were unable to access MCP_GUI.Restarted SM_1 via CLI,\n\r\ncd /var/mcp/run/MCP_14.0/SM_1/bin/\r\n./neStart.pl\n\r\nThey have accessed the MCPGUI and dropped the call.','null'),(617,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-09-16','130916-429609','TELEFONICA MOVILES','Camila Guzzo from SWDEL has paged me for an issue which was seen at Telefonica upgrade.\n\r\nUpgrade path:14.0.16.2 to 17.0.7.4\n\r\nLoad extraction step failed with an error for A2P load => Wrong CD version: 16.0.1 instead of 17.0.8.\n\r\nAccording to loadlineup A2P should be;\nA2 10.2 (17.0.8) Software@A2 APM Applications instead of A2P00160.\n\r\nThe new load A2P00170 has been moved to V status and issue was resolved.','null'),(618,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-09-12','TBD','Midcontinent Communications','Eric Duke from SWDEL has paged me in order to report DB Mock upgrade has been failed with a timer expiration problem in the Midco upgrade.\n\r\nStarting NED and oracle did not resolve the issue.\n\r\nThis error is related to the old version platform level on 10.0 Release.\n\r\n> run err Error! Timer expired while executing: perl \n\r\nIt has been fixed with new platform patches 10.1 and 10.2. When a script was running, randomly it was failing with the timeout error, this is only for older version platforms.\n\r\nWe have installed the new version ned manually to both SMs and both DBs. Older NED version was ned-17.0.5 and now its ned-17.0.6.\n\r\nIssue was resolved and completed upgrade.','null'),(619,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-09-09','130910-427795','Midcontinent Communications','Tony Pittman has paged me in order to report SESM1_0 was unable to get started in the Midco upgrade.\n\r\nUpgrade path:17.0.4.2 to 17.0.5.1\n\r\nConnected to site and checked logs.\n\r\ncom.nortelnetworks.mcp.ne.base.parm.ParmNotFoundException: com.nortelnetworks.mcp.base.collections.NotFoundException: key does not exist:\r\nRegistrar.DatabaseFailedExpirationTimerParm Name: Registrar.DatabaseFailedExpirationTimer\n\r\nThis issue was related with one of the SESM configuration parameter(DatabaseFailedExpirationTimer), which is deployed with SM upgrade, was missing on that load.The reason for that failure was related to the incorrect upgrade attempt which was done before.Performed rollback on the site and SWDEL started upgrade again.\n\r\nIssue was resolved.','null'),(620,'Nuri Tugkan INCE (NETAS External)','AS-OAM','2013-09-17','130916-429436','Emirates Integrated Telcommunications Company','ER paged me for an outage issue. They had failure with running MCP GUI and Provisioning. By the time i was able to connect to site and investigate, GTS provided support with following action plan:\n\r\n------------\n\r\nSolution:\r\n1) Perform a swact on SM\'s\r\n2) Restart Prov1\n\r\n------------\n\r\nMCP GUI and Provisioning client was able to be launched after the customer applied the action plan provided by GTS. However the customer were still reporting service impacting slowness on provisioning. ER was not able to contact with the customer to learn which PROV server they were using.\n\r\nHowever, using Prov2_0(Deployed on Primary EM Server) adding a user was taking approximately 15 seconds, yet on the other hand Prov1_0(Deployed in Secondary EMServer) was not accessible. We performed health check on both EMServers and the results were fine.\n\r\nThen we learned that the customer were using https://:8443 which was redirecting them Prov1_0 because of the swact action performed before. After checking the uptime of secondary EMServer it is analysed that secondary EMServer was up for 329 days. So the following action plan has been suggested to the customer to solve the issue:\n\r\n---------------\n\r\nIn maintanance window,\n\r\n1)Reboot secondary EMServer\r\n2)Restart Prov1_0\n\n\r\n---------------  \n\r\nEnded the call.','null'),(621,'Ken Johnson','AS-OAM','2013-09-10','130909-427663','Cypres','AS5200 IPCM down and not bootable.  Disk0 had a fault on its platter that prevented data from being retrieved from several blocks.  Disk1 had a bad motor and would not spin up.  Site is working to replace these Sun 72G disks (EOL).','null'),(622,'Ken Johnson','AS-OAM','2013-08-26',' 130827-423637 ','NWU','A2 upgrade failed when SIP Profile changes introduces profiles with duplicate names.  SIP Profile names must be unique and this raised a constraint error, halting the upgrade.  Multiple config changes were made to address three such name duplications after which the upgrade continued.\r\n  Both from and to loads are MD, verified issue present in the next GA (14.1) load and will pursue a fix there.','null'),(623,'Senem Gultekin (NETAS External)','AS-OAM','2013-08-23','130823-423337','Shaw Cable Systems','Problem Description:\n\r\nSWD paged me for SESM alarms seen at customer site after the upgrade (17.0.5.1 to 17.0.7.2). \n\r\nAlarm;\r\nAlarmName: Peer presumed failed \r\nTimeStamp: Fri Aug 23 11:26:34 EDT 2013 \r\nFaultNumber: 101 \r\nShortFamilyName: SYS \r\nLongFamilyName: SYSTEM \r\nSeverity: MAJOR \r\nProbableCause: software program abnormally terminated \r\nDescription: Peer network element instance presumed failed. \r\nPeer control transport address: 10.191.149.155:21050 \r\nCorrective Action: Normal condition if peer instance manually stopped. Starting peer or, if \r\npeer stopped in order to discontinue use, removing it from network \r\nconfiguration will clear alarm. If peer instance running, check network \r\nconnectivity between local and peer network elements. If alarm \r\nrepeatedly raised/cleared, fault tolerant timer values may need to be \r\nincreased to account for higher than engineered message transit time \r\nbetween network elements. \n\n\r\nSolution:\r\n-	This was a customer lab and normally pager support is given for lives sites, not for labs.\r\n-	Since it was Shaw and the action pretty quick Ive worked with Tony on it.\r\n-	Suggested him to perform double swact on both SESM units.\r\n-	After the swact, alarms were cleared.\r\n-	Everything was good and system was on 17.0.7.2.','null'),(624,'Senem Gultekin (NETAS External)','AS-OAM','2013-08-23','130823-423258','UPC Nederland','Problem Description:\n\r\nER paged me for a SESM3_0 down problem at UPC. Customer is running on 14.1.0.12 Release with ATCA blades.\n\r\nSolution:\r\n-	Customer was not able to ssh to the SESM3_0.\r\n-	They have tried to ping it as well, but it was not pingable. \r\n-	There was no output form the remote console.\r\n-	Requested them to reseat the problematic blade which was slot slot 3.\r\n-	After the blade reseat, SESM3_0 server came up, and after a while SESM3_0 became UP and Hot Standby.\r\n-	They have opened a follow up case(130823-423339)for RCA.\r\n-	Ended the call with agreement.','null'),(625,'Senem Gultekin (NETAS External)','AS-OAM','2013-08-23','130823-423178','GET - Oslo','Problem Description:\n\r\nUpgrade Path: 14.1.0.13 to 14.1.7.1\n\r\nSWD paged me for an pre upgrade failure at Run Audit Screen.\n\r\nSolution:\r\n-	All BCPs (7 BCPs) were failing due to firmware check from Management Module.\r\nError is following;\r\n**************************************************************\r\n  ut_queryFirmware.pl started at  =>  Fri Aug 23 01:38:12 2013\n\n\r\n........Querying Blade Server Firmware......\n\n\r\nSpawning /usr/bin/ssh  USERID@172.22.130.36...\r\nStarting expect session on spawned command...\r\nsshConnect:: %%%%TIME OUT WAITING ON password prompt 172.22.130.36%%%%%\r\nsshConnect:: No response from server%%%%%\r\nsshConnect:: Check IP address%%%%%\n\r\nERROR: Expect Session timeout!\n\r\nsshConnect received error(s)!\r\n.\n\r\nqueryBCTFirmware received error(s)...\r\n*************************************************************\n\r\n-	Explained to SWD that this is a pre upgrade step, and it can be handled during work hours.\r\n-	Agreed with her and planned to be worked on the morning specifically for this failure.\r\n-	Case has been created(130823-423178) and investigation details will be submitted.','null'),(626,'Senem Gultekin (NETAS External)','AS-OAM','2013-08-22','130822-422984','Verizon','Problem Description:\n\r\nUpgrade Path: 10.3.2.x to 12.0.12.6\n\r\nUpgrade was performed few weeks ago. Today they were performing oracle migration and paged me for an 9 to 10 oracle migration time out problem.\n\r\nAfter he ran ./oracleMigration.pl primary, he waited for 1,5 hour and receive the following error.\n\r\nError occurred executing NE commands (see below): \n\r\n> run err Error! Timer expired while executing: perl \r\nError executing Oracle migration commands. \n\r\nSolution:\n\r\n-	Investigated the migration logs. They were basically showing time out.\r\n-	We have ran migration script again and waited for around 30 min, it was success.\r\n-	SWD continued with the next steps.','null'),(627,'Senem Gultekin (NETAS External)','AS-OAM','2013-08-21','130821-422718','Get - Oslo','Problem Description:\r\nCustomer is Get  Oslo\r\nUpgrade path 14.1.0.13 to 14.1.7.0\n\r\nSWD paged for an unclear situation in the upgrade wizard.\r\nHe was supposed to start upgrade wizard from screen 1. However when he launched wizard it was at Screen 17 swacting SM. This is not an wizard issue, someone else had come until this screen and SWD was aware of this situation.\r\nHe ran truncateWizard script from the server to clear Wizard state from the database.\r\nAnd he was having concerns about current alarms in the system, was it a stable system or not.\n\r\nSolution:\r\n-	It seems like previously came until screen 17 and performed save&exit and left the wizard at the situation. Dave was not aware of this and he was expecting screen 1 so he can perform the pre upgrade steps.\r\n-	Asked SWD to check SM instances for their releases. They were running on 14.1.0.13, which means there was no upgrade performed yet.\r\n-	Requested customer to do some test calls. They were all success.\r\n-	Requested customer to perform provisioning, no issues on that.\r\n-	Dave reported there are 30 major  RTA101 alarms on IEMS. I wanted him to double check it from MCP GUI, the only alarm was Upgrade/Patch is in progress which is normal and does not have an impact to the system.\r\n-	The planned 14.1.0.13 to 14.1.7.0 is on 23rd (Thursday night  Friday morning). Requested all access info and wizard logs from Dave so I can do a health check on the system. \r\n-	Agreed to be done during work hours.\r\n-	Ended the call.\n\r\nAdded my findings about the health check in the case (130821-422718) notes. They are good to go with the upgrade after a SM swact.','null'),(628,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-08-16','130816-422031','UPC Nederland','Problem Description:\n\r\nRobert Starling paged me about a provisioning problem that was occurred on one of UPCs site. The customer was not able to add a new line into A2 from OSSGate. Whenever they tried to do that, they received the error  message below;\n\r\nSystem:LineProv; EndPoint can not be added to GateWay. \r\nSystem:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP vmg9.sesm8:SS/033/2/0200 \r\nDetails: Adding Predefined Time block Group information failed: The field(s): TimeBlock Group must be unique\n\r\nSolution:\r\nThe problem was related to sequence objects in database. The provisioning server application was trying to add new lines with the primary ids that are already allocated in the DB.Sequences were messed up. We have connected to Oracle database as sysdba user (Oracle Admin) and corrected the problematic sequence objects. Also, we set up the necessary permissions in order to allow A2 applications to use those objects.\n\r\nWe have restarted Provisioning Server instances via SM GUI (System Management Console) in order to let them fetched newly generated IDs from database after correcting the objects in the DB. Finally, the customer has tested line provisioning from OSSGate and Core successfully. Then, we left the call.','null');
INSERT INTO `generalinfo` VALUES (629,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-08-14','130809-420058','Twin Lakes Telephone Co-op','Problem Description:\n\r\nThomas paged me for an issue after SESM1_0 server was replaced and when the server was powered up, it failed to become hotstandby. It had no status at all.Also, they were unable to launch MCPGUI.\n\r\nSolution:\n\r\nFound that /var/mcp was 100% full. Deleted all the core logs under /var/mcp/run/MCP_14.1/SESM1_0/work logs which were 2G each.After that /var/mcp was 5%.\r\nLaunched MCPGUI and found that \"engineering\" was not set to \"CS2K-A2_Medium\". Once that was corrected, SESM1_0 went hotstandby.\n\r\nIssue was resolved.','null'),(630,'Ege Varhan (NETAS External)','AS-OAM','2013-07-15','130715-415071','Telefonica','One of Telefonica\'s OPE applications was down. Connected to the site and tried to start it via SM GUI. It didn\'t work. Undeploy /deploy also didn\'t work. Changed some configuration files of OPE\'s application and it started to work. However the NE has still some cache problems.\n\n\r\nA2 OAM\r\nEge Varhan','null'),(631,'Ege Varhan (NETAS External)','AS-OAM','2013-07-16','130618-406723','Unity Media','Impact: No redundancy between primary and secondary databases.\n\r\nResync process was failing  between primary and secondary databases. \n\r\nThere was a problem in one of Oracle replication tables. We have corrected it and performed testReplicaton.pl. It was successful. The job hasn\'t been broken since then(Normally it fails within 5 mins.). \n\r\nThanks,\r\nEge Varhan\r\nOAM GPS','null'),(632,'Selen BAYRAKTAROGLU (NETAS External)','AS-OAM','2013-07-18','N/A','Emirates','Problem Description:\n\r\nKyle paged me for an issue while making provision on SIP lines.The error message was as below:\n\r\nDetails: Cannot perform operation. Invalid User. User 044328060test@du.ae not \r\nfound. \n\r\nSolution:\n\r\nI connected to the site and restarted Prov2 to restore provisioning. A restart on Prov1 was also performed to clear table corruption.\n\r\nIssue was resolved.','null'),(633,'Seren Batmaz','AS-OAM','2013-07-31','N/A','Verizon','Problem Description:\n\r\nJosh paged me out for a problem while running the script patchPlatform.init on EMServer2. The script was getting stucked when stoping Oracle.\n\r\nSolution:\n\r\nI connected to site and ran mcpRelease.pl to see whether platform patch had been started. It hadn\'t started yet. \n\r\n->I restarted secondary database:\n\r\n- ./etc/init.d/dbora stop\r\n- ./etc/init.d/dbora start\n\r\nHowever, that did not work. The same failure was recieved.\r\nAfter that, I rebooted EMServer2 to make Oracle up and running correctly. As soon as the server came up, we tried to run the script and it could successfully stop Oracle and end platform patch.','null'),(634,'Seren Batmaz','AS-OAM','2013-07-30','130730-418281','Axtel','Problem Description:\n\r\nER paged me for the problem related with Admin State of Network Elements. All the NEs were Configured and Active/Hot Standby.\n\r\nSolution:\n\r\nI connected to site and check the NEs. All of them were in \"Configured\" state. Hence, I asked ER to ask for permission to the customer to Kill-Deploy-Start operations. While waiting for the permission, I saw that the customer were running that operations.\n\r\nAfter the customer ran those operations, all NEs became ONLINE UP ACTIVE/HOTSTANDBY.\n\r\nSo, we agreed to leave the call.','null'),(635,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-07-27','130727-417908','Verizon','Problem Description:\n\r\nEric had paged me about resync was failed in the Verizon upgrade.\n\r\nUpgrade Path:\n\r\nFrom 12.012.7 to 14.0.9.5\n\r\nSolution:\n\r\nThere was no site connection access neither over SWD computer nor directly. I have requested Eric to provide the Upgrade Wizard logs. There was obviously error message in the Upgrade Wizard logs like below;\n\r\n    Performing wizard specific initialization tasks: Fri Jul 26 23:06:42 2013\r\n    ERROR:\r\n    ORA-01017: invalid username/password; logon denied\n\r\nI had joined this site\'s pager before and I noticed that the schema user\'s password was changed since they did not remember the password. Resync was failing because of the different passwords on the primary and secondary database. We have found the schema password on the secondary database by decrypting it. Then, we had changed it on the primary database and then tried resync and it succeeded.','null'),(636,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-07-26','130717-415407','Verizon','Problem Description:\n\r\nJosh had paged me about upgrade was failing because of the schema mismatch problem.\n\r\nUpgrade Path:\n\r\nFrom 12.012.7 to 14.0.9.5\n\r\nSolution:\n\r\nI have connected to site over Josh\'s computer and checked upgrade wizard logs firstly. There was a procedure given by Selen (OAM GPS) in order to resolve this schema mismatch but I noticed that this procedure had been applied properly and they got following error;\n\r\n    SQL> drop procedure WRITE_LICENCEKEY_TO_TXT_FILE;\r\n    drop procedure WRITE_LICENCEKEY_TO_TXT_FILE\r\n    *\r\n    ERROR at line 1:\r\n    ORA-04043: object WRITE_LICENCEKEY_TO_TXT_FILE does not exist\n\r\nI requested schema user of their database in order to run above sql but they had not known. So, I have changed the database schema user\'s password by running following commands;\n\r\n     alter profile SYSTEM_PROFILE limit PASSWORD_VERIFY_FUNCTION null;\r\n     alter user MCPUSER identified by MCPUSER1234_MCPUSER1234_;\r\n     commit;\n\r\nThen, we applied the procedure and it succeed this time. Then, we noticed that database upgrade had been finished successfully but we had not able to open the Upgrade Wizard. Then, we manually ran \"smUpgrade.pl\" script in order to make the SM upgraded to latest load. Then, upgrade wizard was opened and we went on our upgrade until half point. Then, we requested Verizon to test their Provisioning is working well on the primary site. They informed us that our provisioning was not working fine on OSSGate site. We suspected that their SESM server on OSSGate side might know the secondary PROV (on the old load) as its primary server. Then, we checked it and fixed is as selecting primary server as new loaded PROV. Then, provisioning was worked properly and we left the site.','null'),(637,'Seren Batmaz','AS-OAM','2013-07-11','130618-406723','Unity Media','Problem Description:\n\r\nJohn paged me out for an error on \"./oracleInstall.pl -secondary\" step of following procedure:\n\r\n-> SSH to EMServer1 with the user, ntappadm. \r\n-> cd /var/mcp/install \r\n-> ./oracleUninstall.pl -secondary \r\n-> Transfer the oracle installer (installer-mcp-oracle-EE-10.2.0.4-22.LINUX64.tar) using SFTP (Oracle installers exist in A2EC0120 or A2ECM120 DVD). \r\n-> ./oracleInstall.pl -secondary \r\n-> ./setupDBReplication.pl \n\r\nSolution:\n\r\nAfter checking the servers, I recognized that EMServer1 and EMServer2 timezone were configured different. \r\n->I changed secondary server\'s timezone and made them same.\r\n->./oracleUninstall.pl -secondary\r\n->./oracleInstall.pl -secondary\n\r\nAgain, oracleInstall.pl failed with the same error:\n\r\n\"Oracle installation has failed.  Please resolve the underlying issue that\r\n  caused the failure.  Once the issue is resolved\r\n       ./oracleUninstall.pl -secondary   (in the /var/mcp/install/ directory)\r\n  Then re-execute this script again to attempt the install again.\"\n\r\nIn /var/mcp/db/logs/dbInstall.log, the cause of error could be seen:\n\r\nProcessing object type SCHEMA_EXPORT/USER\r\nORA-39083: Object type USER failed to create with error:\r\nORA-00959: tablespace \'MCP_DATA\' does not exist\n\r\n->Talked with db design architect, Yalcin Tosun, and decided that the problem is related with OS of the server.\r\n->Hence, we suggested the customer to reinstall the server and install the oracale on secondary DB.','null'),(638,'Senem Gultekin (NETAS External)','AS-OAM','2013-07-04','130624-410786','Optus','Problem Description:\n\r\nER paged me for a server replacement activity at Optus. Server is HT Langley and customer is running on 14.1 Release. \r\nSESM4-1 Server was problematic and customer was replacing the server with a spare one.\n\r\nSolution:\n\r\n-	Customer reported that after replacing the server and using the old disks, server was booting up but it was rebooting itself every 8 minutes.\r\n-	Checked the messages logs and they were not normal.\r\n-	Requested from the site engineer to swap the disks. But the result was the same.\r\n-	Requested from site engineer to use one of the old disks and one of the new disks. This time server was rebooting itself every 11 minutes.\r\n-	New server was on its way and it was going to take one day more to arrive to the site.\r\n-	Suggested Optus to re-install the new server with the new disks. \r\n-	Explained the steps and provided the documents.\r\n-	Left the conf call with agreement.','null'),(639,'Ege Varhan (NETAS External)','AS-OAM','2013-06-19','130613-405919','UPC Nederland','I was paged by ER and told that UPC is not able to access to MCP GUI. I have connected to the site and checked the spool directories of some NEs (Network Elements such as SESM, PROV, PA) in the system. Some NEs have more than 4,000 files stored in their spool directory. I have told that we need to clean all of the files under spool directory and before we do that we need to stop those NEs. Since, that caused an outage, UPC didn\'t accept that action plan. A new action plan has been provided and performed on one of UPC\'s site.\n\r\nRegards,\r\nEge Varhan\r\nOAM GPS','null'),(640,'Ege Varhan (NETAS External)','AS-OAM','2013-06-21','130621-409901','Shaw CableSystems','I was paged by Martha Foster from ER. She told me that Shaw was not able to use the mobility service successfully. Because, Provisioning and LDAP Servers were not synced somehow. She told me that it may be a software bug. The feature was working on 10.0 release, but it was broken with 10.1 upgrade.\n\r\nI have quickly replicated the problem at both customer site and Genband labs. When i checked the provisioning server\'s code, i have realized that there is a problem about the ldap operation. Then, i have opened a new CR  to get it fixed and dispatched it to the design team\'s queue.\n\r\nThe jar file has already been created and provided to the customer. The problem is fixed.\n\r\nRegards,\r\nEge Varhan\r\nOAM GPS','null'),(641,'Ege Varhan (NETAS External)','AS-OAM','2013-06-20','130621-410145','Claro Codetel (Dominican Republic)','I was paged by Chris from SWD about a remote backup problem.  He indicated that he couldn\'t transfer the backups from primary EM Server to secondary EM Server. The error message is below;\n\r\n0;root@SSLMMEL0:/admin[root@SSLMMEL0 admin]# bkupSvr.pl -remote\r\n  19:42:04 Locked .bkupImpl.pl \r\n  19:42:04 Parsing BkRstr config parms\r\n  19:42:04 Dirs to backup: /admin /var/mcp/install /var/mcp/loads /var/mcp/media /var/mcp/app/loads /var/mcp/db/backup \r\n  19:42:04 Backup to remote file on host 190.167.240.21.\n\r\n****** WARNING *****\r\nSSH Keys must be regenerated.\r\nSSA must run configSvrBkup for the svr job to regenerate SSH Keys.\n\r\n****** WARNING *****\r\n  19:42:06 SSH Operations not configured.\n\n\r\nI have cleaned up and re-generated the SSH keys again via  \"configSvrBkup.pl\" as \"root\" user again.\n\r\nThanks,\r\nEge Varhan\r\nOAM GPS','null'),(642,'Ege Varhan (NETAS External)','AS-OAM','2013-06-20','130618-406723','Unitymedia','I was paged by ER about a database replication problem of Unitymedia. The primary DB server of the customer has been replaced with a new one, as a result all the software has been installed again including platform, database, A2 etc. After the installation, the customer has started to see the error messages below;\n\r\nAlarmName: Oracle Broken Job\r\nTimeStamp: Wed Jun 19 20:32:35 EDT 2013\r\nFaultNumber: 728\r\nShortFamilyName: DBMN\r\nLongFamilyName: DBMON\r\nSeverity: MINOR\r\nProbableCause: unspecified reason\r\nDescription: The job [declare rc binary_integer; begin rc := sys.dbms_defer_sys.push(destination=>\'MCPDB.SECONDARY\', execution_seconds=>25, parallelism=>4); end;] is broken.\r\nCorrective Action: Contact next level of support if the problem persists. \n\n\r\nI have connected to the site and ran \"resync.pl\" script to correct the replication problem, however it didn\'t work. We also stopped/started oracle software with no success. The investigation still continues. We will have an another M.W. on June 26, 2013.\n\r\nRegards,\r\nEge Varhan\r\nOAM GPS','null'),(643,'Ege Varhan (NETAS External)','AS-OAM','2013-06-20','130618-407106','Verizon Communications','I was called by Timothy Tron from ER. He was working on an activity to restart / reboot SESM1_0 server. Even though SESM1_0 server was rebooted, the status of it\'s instance on MCP GUI was \"stopping\".\n\r\nI have suggested him to swact System Managers (SM) to correct the status. The problem was solved after the swact of SMs.\n\r\nThanks,\r\nEge Varhan\r\nOAM GPS','null'),(644,'Ken Johnson','AS-OAM','2013-06-11',' 130611-405548','UT Austin','Users were unable to login to the A2\'s prov which prevented the customer from supporting their existing, or provisioning new lines. Determined that the login was actually successful, but the subsequent attempt to update the user\'s last_login time was hanging. This prevented the user from accessing the prov application. There were no current locks or blockers on this table, but earlier this morning an oracle deadlock has occurred on this same table.\r\n  With no identifiable sessions locking/blocking the table no further precise action could be taken and so the db instance was restarted. This allowed updates to the effect table and users to again login to prov.\r\n  Followup case 130612-405665 will track the root cause investigation and pursue any necessary corrective actions to prevent future lock outs.','null'),(645,'Ken Johnson','AS-OAM','2013-06-11',' 130611-405625','Canadian Imperial Bank of Commerce (CIBC)','Paged when mirrored sites (not Geo, full redundant sites) could not restore database, leaving the SMs and other servers down on the second site.  second site failing to start SM as its config data (IP,Key,etc) were that of the primary site.  found that the config data backup on the secondary site contained Primary site details.  located an Aug 29 backup which contained secondary site details in the secondary site config data backup and restored the corresponding primary site full db backup and config data backup from Aug 29.  this enabled the apps to come into service and will enable the site\'s next secondary site config data backup to contain secondary site details.','null'),(646,'Ege Varhan (NETAS External)','AS-OAM','2013-06-05','130604-404076','Verizon Communications','Problem Description\r\n--------------------------\r\nWhen the customer was upgrading the platform level of the secondary servers via \"patchPlatform.pl\" script, the script stalled at the step below;\n\r\n[*P-Info*] Precondition #51\r\n[*P-Info*] Precond Target: checkSshdUsers\r\n[*P-Info*] Shutting down oracle (if running).\n\r\nThe oracle failed to stop.\n\r\nSolution Description\r\n---------------------------\r\nGPS tried to stop Oracle manually via \"/etc/init.d/dbora stop\" command, but it didn\'t work. GPS had to kill oracle processes manually via \"kill -9  \" command. After killing all Oracle processes, \"patchPlatform.pl\" script was completed successfully.\n\r\nEge Varhan\r\nOAM GPS','null'),(647,'Ege Varhan (NETAS External)','AS-OAM','2013-06-04','130604-404076','Verizon Communications','Problem Description\r\n-------------------------------------------\r\nI was paged by ER (Emergency Recovery) and told that Verizon was not able to deploy and start SM_1 and PROV_1 instances on EMServer2.\n\r\nSolution Description\r\n--------------------------------------------\r\nThe previous night, Verizon has upgraded the primary side (SM_0, PROV2 on EMServer2) to A2 12.0.12.6 load, and left the secondary side on MCP 10.3.12.x load. SM_0 was active on A2 12.0.12.6 load. They were trying to deploy and start MCP 10.3 NEs (SM_1 and PROV1_0) via SM GUI of A2 12.0.12.6. Since, platform users have been changed via upgrade from \"nortel\" to \"ntappadm\", NED commands failed because of unknown user. We have suggested Verizon to complete the platform upgrade of the secondary side in order to deploy and start SM_1 and PROV1_0 in the new load which is A2 12.0.12.6.\n\r\nThanks,\r\nEge Varhan\r\nOAM GPS','null'),(648,'Ege Varhan (NETAS External)','AS-OAM','2013-06-04','130604-404076','Verizon Communications','Problem Description\r\n-------------------------------------------\r\nI was paged by ER (Emergency Recovery) and told that \"mcpPatch.pl\" script got stucked during DB (Database) upgrade from A2 12.0.12.0 to 12.0.12.6. Verizon has upgraded it\'s site from MCP 10.3 to A2 7.0 SP1 (12.0.12.0) load first and then they tried to patch it to A2 12.0.12.6 patch load. But Sheila from Verizon reported that she waited about an hour until 02:14 AM then restarted the upgrade and the script got stucked again.\n\r\nSolution Description\r\n--------------------------------------------\r\nGPS has logged into the site and checked the patch logs. The patch logs didn\'t have any information about the problem. Then GPS has checked Oracle software\'s internal logs. We have seen some internal oracle errors in the logs. It seemed that the database was not stable. We have restarted the DB and requested Sheila to run the script again. Sheila was able to patch the primary side of A2 to 12.0.12.6 load successfully. I have left the call since half upgrade was completed and pause point was reached.\n\r\nThanks,\r\nEge Varhan\r\nOAM GPS','null'),(649,'Seren Batmaz','AS-OAM','2013-05-20','130520-401958','Verizon','Problem Description:\n\r\nER paged me for a database mismatch error while running DB&SM Upgrade. The upgrade path was 12.0.12.7 to 14.0.9.12.\n\r\nSolution:\n\r\nWhen we investigated the logs, we saw that a procedure called \"WRITE_LICENCEKEY_TO_TXT_FILE\" was added to Primary DB by customer. That procedure caused the mismatch while running DB&SM Upgrade.\n\r\n->Connected to Primary DB\r\n->Droped the procedure.\n\r\nThe customer retried the DB&SM Upgrade Screen, saw that the problem had been resolved and passed the screen successfully.','null'),(650,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-05-27','+444111222333','SingTel (OPTUS)','Problem Description:\r\nMark has paged me about SM gray out problem on the customer.\n\r\nMCP Load:\r\nMCP 14.1.0.0\n\r\nSolution of the Problem:\r\nSince the customer was ATO, the site access was not being allowed. Tom (handed over from Mark) set up a call with Mick (customer) and we let to customer to apply below procedure on their live site;\n\r\n1) Stop secondary SM instance in order to prevent SM swact. Make sure only your primary SM instances are up&running.\r\n2) Make sure NED doesn\'t restart the stopped SM automatically by issuing the following command in the Secondary EM server:  \"neinit -autorestart=off\"\r\n3) Clear the all files under /var/mcp/spool in each server with following command; \r\n\"rm -rf /var/mcp/spool\"\r\n4) Kill all instances except active SESM in order to get rid of CALLP failure.\r\n5) Then, start the primary SM.\r\n6) Restart all other elements again.\n\r\nThen, SM came up and everything was working properly.\n\r\nThen, we agreed to left the call.\n\r\nMehmet','null'),(651,'Ege Varhan (NETAS External)','AS-OAM','2013-05-08','130328-393482','Verizon Communications','Product: MCP_10.3.x.x, A2 6.0SP1, RETIRED, CVM12\r\nCustomer: Verizon Communications\n\r\nProblem:\r\n-------------------------\r\nThe customer was observing  \"DBMN727 - Oracle Replication Link\" errors on SM GUI and wanted to clear it.\n\r\nSolution:\r\n--------------------------\r\nI have suggested ER to perform re-sync from primary to secondary database. The re-sync process was completed successfully. The replication between primary and secondary databases were also tested via utility scripts.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(652,'Ege Varhan (NETAS External)','AS-OAM','2013-05-10','130510-400451','Cincinnati Bell','Product: MCP_14.0.9.x, A2 8.0BRC, Retired, CVM14\r\nCustomer: Cincinnati Bell\n\r\nProblem\r\n-----------------------\r\n1- Customer can not perform provisioning\r\n2- Constant CPU spikes are observed on both EMServers\n\r\nSolution\r\n-----------------------\r\nThomas from ER paged and told us that the customer was observing CPU spikes on the active SM (System Manager) constantly. We performed the actions below in order to solve the problem;\n\r\n1- Stopped / Started active SM and let the inactive one to become active.\r\n2- We have started to see the CPU spikes on the secondary SM after activating it.\r\n3- Stopped / Started PROV instances on EMServer1 and EMServer2 with no luck.\r\n4- GPS has realized that some oracle processes were consuming the most of the available CPU in the system. We have suggested the customer to reboot both EMServers one by one. Both servers were turned to normal after the reboot.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(653,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-05-14','TBD','','Problem Description:\r\nMaloney had written instant messege over yahoo about patching problem he had. He was unable to patch the PROV and other instances to 12.0.12.4 load in the MCP GUI.\n\r\nMCP Load:\r\nUpgrade from 12.0.12.0 to 12.0.12.4\n\r\nSolution of Problem:\r\nI have connected to site and checked the DB and SM instances are upgraded. Then, I undeploy and redeploy the PROV instances. In the meantime, I have checked following commands\' output on the server; df -k, df -i, top and uptime. Then, I noticed that server is up for 433 days and I requested for the reboot operation. He gave the permission and I rebooted the EMserver2 server. By the way, I have patched the SESM and BCP instances. After server came up, I have make the PROV instances active.\n\r\nAfter all instances are working properly in the new load, I have left from the call.','null'),(654,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-05-14','TBD','','Problem Description:\r\nCamilla had paged me about upgrade wizard problem. She was not able to go on upgrade because of the failure.\n\r\nMCP Load:\r\n12.0.12.4\n\r\nSolution of Problem:\r\nShe mentioned that in the DB mock step, she took some failure messages and then she said she closed the screen with \"X\" at the top of the screen. She also stated that she had started to this upgrade before 20 days ago.\n\r\nThen, I said her that this problem is related with upgrade wizard timeout which is written in the document that if someone does upgrade via upgrade wizard, it should not exceed 7 days.  Because of this, she saw below  exception in the upgrade logs;\n\r\n2013-05-13 21:57:39,188 ERROR UpgradePanelController - Exception in init\r\njava.lang.NullPointerException: null\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.control.ServerOperationController.initialize(ServerOperationController.java:807) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.control.DBMockUpgradeController.initialize(DBMockUpgradeController.java:45) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.base.UpgradePanelController.exec(UpgradePanelController.java:69) ~[wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.control.UpgradeMainController.displayPanel(UpgradeMainController.java:528) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.ui.control.UpgradeMainController.start(UpgradeMainController.java:670) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.wizard.UpgradeWizard.openWizard(UpgradeWizard.java:203) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.wizard.UpgradeWizard.start(UpgradeWizard.java:89) [wizardws.jar:na]\r\n	at com.nortelnetworks.mcp.client.upgrade.wizard.UpgradeWizardMain.main(UpgradeWizardMain.java:25) [wizardws.jar:na]\r\n	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_26]\r\n	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[na:1.6.0_26]\r\n	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:1.6.0_26]\r\n	at java.lang.reflect.Method.invoke(Unknown Source) ~[na:1.6.0_26]\r\n	at com.sun.javaws.Launcher.executeApplication(Unknown Source) [na:na]\r\n	at com.sun.javaws.Launcher.executeMainClass(Unknown Source) [na:na]\r\n	at com.sun.javaws.Launcher.doLaunchApp(Unknown Source) [na:na]\r\n	at com.sun.javaws.Launcher.run(Unknown Source) [na:na]\r\n	at java.lang.Thread.run(Unknown Source) [na:1.6.0_26]\r\n2013-05-13 21:57:39,190 ERROR UpgradeMainController - Error occured during initializing panel\n\r\nThen, we agreed to do the same pre-check in the upgrade wizard from scratch and left the call.','null'),(655,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-05-13','130412-395606','Windstream','Problem Description:\r\nTony had paged me about patching problem he had. He was unable to patch the SM instances in the MCP GUI.\n\r\nMCP Load:\r\nUpgrade from 12.0.12.0 to 12.0.12.4\n\r\nSolution of Problem:\r\nI have connected to site over Tony\'s computer and checked that SM instances were on the old load but the database had been patched successfully. I have undeploy and redeploy with the new load for SM_1 via MCP GUI and I have changed the SM_0\'s load from over scripts in the ssh connection.\n\r\nI have deployed and started all instances with new load (12.0.12.4) and left the site.','null'),(656,'Senem Gultekin (NETAS External)','AS-OAM','2013-04-24','130424-397600','Optus','Problem Description:\n\r\nRonald paged OAM pager for a rollback request at Optus. They were having one-way speech problem after from 14.1.0.10 to 14.1.0.12 patch upgrade.\n\r\nSolution:\n\r\n-	Joined to the call with customer.\r\n-	Explained them that since this is only patch upgrade they can first try to change the SESMs load back to 14.1.0.10 from the MCP GUI. If that doesnt work we can go with the full rollback by restoring the 14.1.0.10 database to the system.\r\n-	Also explained that they need to engage CALLP GPS to take traces for investigation.\r\n-	Customer changed the SESM release back to 14.1.0.10 patch and the issue was gone.\r\n-	Kimberly Jones (CALLP GPS) was involved for taking traces. Investigation will continue on CALLP GPS.\r\n-	Left the call.','null'),(657,'Seren Batmaz','AS-OAM','2013-04-20','130419-397091','Axtel','Problem Description:\n\r\nER paged me for provisioning problem after EMServer1 became up after server reinstallation.\r\nThey were trying to provision  through OSSGate.\r\nPROV1 which had been deployed to EMServer2 was CONFIGURED DOWN UNAVAILABLE state and cannot be deployed. \n\r\nSolution:\n\r\nFirst, I recommended to try to use PROV2, which had been deployed to EMServer1,while provisioning via OSSGate. It was ONLINE AVTIVE. So that, we could make sure that the issue is not related with the DB after the customer had just installed the Primary DB.\n\r\nER told that the customer cannot make provisioning using PROV2 also. So, I connected to site and checked the alarms on NEs, especially in DB. There was no alarm on both databases. \n\r\nI tried to add a new user using A2 Prov Gui(using PROV2) and I succeeded. Hence, I thought that the customer could not test the provisioning using PROV2 correctly. The problem was related with PROV1 was down.\n\r\nI ssh to EMServer2 and saw that PROV1 was ACTIVE on the server. So, I applied the following operations:\n\r\n->Killed PROV1 Pid on the server.\r\n->Deployed PROV1 from MCP GUI.\r\n->Started PROV1 from MCP GUI.\n\r\nAfter these operations, the customer started to provision via OSSGate without any problem. So, I left the call.','null'),(658,'Seren Batmaz','AS-OAM','2013-04-20','130419-397091','Axtel','Problem Description:\n\r\nER paged me for the problem that EMServer1 was not boot up.\r\nHe said that in a spare server, they seated the old disks and still the server did not become up.\n\r\nSolution:\n\r\nI connected to the site and rebooted server to see the process while booting up.\r\nThe problem seemed to be related with the disks. So, I recommended to replace the old disks with the new ones and I provided the procedure below for the reinstallation of server after changing the disks:\n\r\n-> Backup secondary database:\r\n  -> login to EMServer2 User with ntdbadm user.\r\n  -> cd /var/mcp/run///bin/util\r\n     ./dbBackup.pl \r\n-> Transfer the backup file, which is under /var/mcp/db/backup, to your work space.\r\n->In the document Installation Method  35-1391.\r\n     -> Configure bios\r\n     -> 4.3 Red Hat Linux Software Installation -> Install Linux on the server. (You need linux installer of the same A2 release with A2 systems other servers)\r\n      -> 4.7 Installation of the Oracle Database (You need A2 Core Bundle. Oracle is available in core bundle. Extract all contents to server from core bundle)\r\n-> Install DB-> Run the script /var/mcp/install/dbInstall.pl\r\n-> After this operation is done, resync two databases from secondary database: \r\n          -> login to EMServer2 with ntdbadm user.\r\n          -> cd /var/mcp/run///bin/util\r\n          -> ./resync.pl\n\r\nAfter providing the procedure, I agreed to leave the call with ER and told him to page me if any problem occurs.','null'),(659,'Seren Batmaz','AS-OAM','2013-04-16','130415-395951','Verizon','Problem Description:\n\r\nThomas paged me for cleanupReplication script ran for 2 hours and it failed. After that, when resync.pl was tried to run, it failed again.\n\r\nSite was running on MCP_10.3 Release.\n\r\nSolution:\n\r\n->Asked ER why the customer wanted to cleanup replication. He said that to clear some alarms, GTS recommended to run cleanupReplication.ps and Resync.pl.\n\r\n->Tried to run cleanupReplication.ps and got the error:\n\r\nERROR: To restrict multiple DB scripts from being executed simultaneously, a Lock is required for the execution of this script.  With that, this script cannot be executed at this time because the Lock has already been acquired by\r\n=>  Resync.pl\n\r\n->Resync.pl was stucked. So, I stopped and started oracle:\r\n./dbora stop\r\n./dbora start\n\r\n->Ran Resync.pl. However it failed, since, cleanupReplication.ps had not been completed successfully.\n\r\n->Ran cleanupReplication.ps successfully.\n\r\n->Ran Resync.pl and got the error:\n\r\nERROR at line 1:\r\nORA-04021: timeout occurred while waiting to lock object SYS.DBMS_LOCK\n\r\n->Checked uptime and saw that the server was up for 439 days.\n\r\n->I wanted to check the server alarms on EMServer1 from MCP GUI and reboot server. However, ER had limited access to the site and could not launch MCP GUI.\n\r\n->So, I did not want to reboot server, just in case it would not come up.\n\r\n->Tried Resync.pl again and it was completed successfully.\n\r\n->Recommended customer to check alarms on MCP Gui and reboot the server.\n\r\n->The customer wanted us to reboot the server and we rebooted the server. It came up without any problem.\n\r\nAsked customer to check the system and if everything was okey, they can start provisioning.\n\n\r\nThe customer has not reported any problems. So, I left the call.','null'),(660,'Seren Batmaz','AS-OAM','2013-04-17','130417-396178','BSkyB','Problem Description:\n\r\nGuido paged me for SESM1_0 NE was bouncing after upgraded to new load.\n\r\nThe upgrade path was 12.0.12.4 -> 14.0.9.14\r\nPrimary NEs had been upgraded to 14.0.9.14 and should had been ONLINE HOT STANDBY state.\r\nHowever SESM1_0 state was bouncing as below:\r\nInitializing-> Syncronizing -> Unavailable\n\n\r\nSolution:\n\r\nI connected to site and killed problematic SESM NE and started it again.\n\r\nSESM state continued bouncing. So, I told Guido to call Call Processing Pager and left the call.','null'),(661,'Seren Batmaz','AS-OAM','2013-04-16','130416-395987','Cable Onda','Problem Description:\n\r\nPeter paged me for Schema Duplication Error during DB Mock upgrade. He said he had   retried and could not success that screen.\n\r\nSolution:\n\r\nSince, DB Mock Upgrade is a preupgrade step, I told Peter and his collegue, Kevin Jones, that we did not give pager support for preupgrade screens. I recommended to raise an SFDC case to solve the problem. Also, I told them that we will give first priority to this issue in bussiness hours.\n\r\nPeter and Kevin agreed to drop the call and and raised an SFDC case, 130416-395987.','null'),(662,'Senem Gultekin (NETAS External)','AS-OAM','2013-03-29','130328-393482','Verizon','Problem Description:\n\r\nER paged me for a DB related alarms and SM1 inactive situation. Customer is Verizon and they are running on 10.3.x (EOL)\n\r\nSolution:\n\r\n- Accessed to the site over ERs PC. I was able to access to SM1 via ssh. At that point customer reported that SM1 was active again. \r\n- ER told me that there was a site engineer at site at they were switching some ports (do not know why). And after they turned it back the ports they were able to see SM1 as active.\r\n- Later on Verizon told us that they were having alarms on the GUI;\n\r\nOn SESM instance - SM 0 Major - no connection to DB instance 1\r\nOn primary SM - MCP DB major - DBMN101 snmp request timeout.\n\r\nER ran snmp restart on both SMs... \n\r\n[root@DRPKNYDPSEM1 bin]# cd /home/oracle \r\n[root@DRPKNYDPSEM1 oracle]# cd bin \r\n[root@DRPKNYDPSEM1 bin]# ./config_snmp\n\r\n- After the restart DBMN101 alarm was cleared but still had the DB instance connection failure on SESM instance. Suggested them to restart the SESM instance, since they were in traffic hours they didnt want to do it and postponed it to a maint window.\r\n- Dropped the call.','null'),(663,'Senem Gultekin (NETAS External)','AS-OAM','2013-03-28','130327-393255','BT SPAIN','Problem Description:\n\r\nER paged me for a disk issue seen on primary EM server at BT Spain. Customer is running on 12.0.12 MR with CC3310 servers.\n\r\nThey have replaced sda and after the replacement they were not able start mirroring between sda and sdb.\r\nError:\r\nmcpSwRaid.pl -add sda \r\nOpening log file \"/var/mcp/os/logs/sw_raid/mcpSwRaid.log\". \r\nDevice \"/dev/sda\" either not defined or inaccessible\n\r\nSolution:\r\nAccessed to the ERs PC and started to work on the issue.\r\nSuggested them to swap the sda and sdb physically, boot the server again,  and run mcpSwRaid.pl -add sda command.\r\nER rebooted the server without a swap first, after the reboot they were able to run mcpSwRaid.pl -add sda command successfully.','null'),(664,'Seren Batmaz','AS-OAM','2013-03-12','130312-391157','Frontier','Problem Description:\n\r\nER has paged me for SM_0 on ATCA Server is unavailable. So, SM_1 was in service with no peer.\n\r\nSolution:\n\r\nConnected to site and saw that SM_0 was CONFIGURED DOWN UNAVAILABLE state and couldn\'t be deployed.\n\r\nConnected NDM and run the following scripts:\n\r\n-cli>ha ap sh all (to monitor the status of slots)\r\n-cli>ha ap reset 0 0 8 0 (to reset slot 8 where SM_0 was located)\n\r\nAfter the reset was done, SM_0 state changed as CONFIGURED HOT STANDBY. \r\n-> Killed SM_0 instance.\r\n-> Deployed and started SM_0\n\r\nAfter these steps, SM_0 became ONLINE UP HOTSTANDBY. \n\r\nSM_0 was working with no problem. So, I droped the call.','null'),(665,'Seren Batmaz','AS-OAM','2013-03-15','130315-391624','Sunrise','Problem Description:\n\r\nDave from SWD Team has paged me for, while running Manual Upgrade steps, SESM1_0 stucked in SYNCRONIZING state after the platform patch. \r\nThe upgrade path:\r\n14.0.9 to 14.0.14 (Platform MR upgrade)\r\n14.0.9.4 to 14.0.9.12 (MCP patch upgrade)\n\r\nThere are two SESM Pairs in the customer site. Server types and patched platforms are as below:\n\r\nSystem Manager = CC3310 (to 14.0.18)\r\nSESM1 Pair  CC3310 (to 14.0.18)\r\nSESM2 Pair = HT Langley Intel-TIGH2U (to 14.0.25)\n\r\nSolution:\n\r\nSince the customer does not enable split tunnelling, I had no connection to site nor Dave\'s PC.\r\n- I suggested to run the command mcpRelease.pl on both SESM1 servers. We saw that platform patches were applied successfully.\r\n- > df -k (to monitor disk space, on both SESM1 servers)\r\n- > free -m ( to monitor CPU usage on both SESM1 servers)\n\r\nThe disk space and CPU usage seemed normal. Also, there was no server alarm on the problematic servers.\n\r\nWhile we were checking all those information, the SESM1_0 instance state changed as HOT STANDBY after staying SYNCRONIZING state for about 90 munites.\n\r\nI asked if it take much time to take response of any command on problematic servers. However, Dave said no.\n\r\nSo, I thought that there is no server related issue and told Dave to raise a case for Call Processing queue.\n\r\nAfter we agreed, I dropped the call.','null'),(666,'Seren Batmaz','AS-OAM','2013-03-11','130212-385678','Pres.ColumbiaUni.Med.Center','Problem Description:\n\r\nVikram has paged me for a possible data corruption of the customer\'s database.\n\r\nCustomer and Genband Professional Services were working on deleting the DNs and domains on the A2 via OSSGate. While deleting the DNs; one DN is corrupted.\n\r\nSolution:\n\r\nI asked Vikram if this site is in service or not and learnt that it is not in service yet. Also, I saw the related case was opened 27 days ago. So that, I told that we cannot give pager support for this case and asked Vikram to send the case to PS A2 OAM queue and dropped the call.','null'),(667,'Senem Gultekin (NETAS External)','AS-OAM','2013-02-08','130208-385021','Verizon','Problem Description:\n\r\nER paged me for an upgrade failure at Verizon Live site.  Upgrading Database and SM failed with the following error;\n\r\nERROR UpgradePanelController - Error during omi relogin: fail:WebServiceFailed:null\n\r\nSheila (who is performing the upgrade) hit save&exit, and after relogin she was not able to launch wizard, she was receiving the following error;\n\r\nError Dialog : Login to Wizard Error, Admin doesn\'t have enough permission to run wizard.\n\r\nUpgrade Path;\r\n14.0.9.12 to 14.1.0.13\n\n\n\r\nSolution:\r\n-	Accessed to the site over ERs PC and checked all the related logs.  It was showing that DB and SM were upgraded successfully in the background. But once the upgrade is done for that screen SM is doing a relogin in the background and that was failing due to web service failure.\r\n-	ER was not able to access to the wizard and MCP GUI. Had to perform some actions but since we were not able to login, contacted Sheila and assisted her to perform some actions like SM stop start from the GUI.\r\n-	Had to perform SM stop start for few times for both SM instances, at last performed SM start from server. \r\n-	Sheila was still not able to launch the wizard. Ive sent her the steps how to open wizard in debug mode.\r\n-	She opened wizard in debug mode. Continued with next.\r\n-	Next Screen is Upgrade Primary NEIs. This failed due to license key. Because SM was in OFFLINE and ACTIVE state. The reason for this is that once you start SM from the server it stays in offline state.\r\n-	Sheila tried to deploy primary instances from the server but got error due to SM offline state.\r\n-	At that point  there was really short time for the end of the maint window. Decided to continue the upgrade in debug mode for few screen more.\r\n-	Next screen was patching the operating system for the secondary NEIs. Once the servers are patched  they reboot themselves, and that point secondary servers were not reachable for 2-3 minutes. Which caused a very short time registration issue.\r\n-	During this time we have accessed to the database and changed the SM0 state as ONLINE and ACTIVE manually.\r\n-	Sheila applied license key from the MCP GUI.\r\n-	Deployed and started primary NEIs. \r\n-	At that point Sheila reopen wizard in normal mode.\r\n-	Sheila continued with the upgrade and completed successfully.','null'),(668,'Senem Gultekin (NETAS External)','AS-OAM','2013-02-07','130207-384807','Verizon','Problem Description:\n\r\nER paged me for a DB MOCK failure at Verizon live site.\r\nUpgrade path is; \r\n14.0.9.12 to 14.1.0.13\r\nError is;\n\r\nDescription:\'Checking db server 101.40.42.18 disk space to perform dry run upgrade ... DB server 101.40.42.18 total tablespace size is 6895616 KB Deploying DB run time scripts necessary for the Dry-run DB upgrade ... Performing DB Upgrade Dry-run, please be patient. Getting DB Upgrade Dry-run result ... Fails to create duplicated schema from currently deployed schema. See log file for possible details: /var/mcp/upgrade_tools/logs/dryRunDBUpgrade/ut_dryRunDBUpgrade.EMS1_0147617e-2a31-1b21-8c8d-000e0ce4f0ec_ut_dryRunDBUpgrade.pl_DB_MOCK_UPG_3.20130206_235943.log \'\r\n	Error:\'Schema Duplication Error\'\n\r\nSolution:\r\n-	Checked the logs and accessed to the site.\r\n-	This same error was seen only once in another customer where it was suspected its only specific to the site database. This upgrade path has been performed in customers and in our labs several times but didnt see any issue.\r\n-	Performed the actions below from secondary database;\n\r\n1.	Login Oracle 10g Database as ntdbadm user \r\n[ntdbadm@vcarsSMLt0 ntdbadm]$ sqlplus \r\nSQL*Plus: Release 10.2.0.4.0 - Production on Wed Aug 18 13:40:30 2010 \r\nCopyright (c) 1982, 2007, Oracle. All Rights Reserved. \r\nEnter user-name: sys as sysdba \r\nEnter password: MCP_RAPTOR_1901 \r\nConnected to: \r\nOracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production \r\nWith the Partitioning, OLAP, Data Mining and Real Application Testing options \r\nSQL> \r\n2. SET ESCAPE OFF on SQL*Plus \r\nSQL> set escape off \r\n3. Catdph.sql will Re-Install DataPump types and views \r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdph.sql \r\n4. prvtdtde.plb will Re-Install tde_library packages \r\nSQL >@ $ORACLE_HOME/rdbms/admin/prvtdtde.plb \r\n5. Catdpb.sql will Re-Install DataPump packages \r\nSQL >@ $ORACLE_HOME/rdbms/admin/catdpb.sql \r\n6.Dbmspump.sql will Re-Install DBMS DataPump objects \r\nSQL >@ $ORACLE_HOME/rdbms/admin/dbmspump.sql \r\n7. To recompile invalid objects, if any \r\nSQL >@ $ORACLE_HOME/rdbms/admin/utlrp.sql\n\r\n-	Once the above steps were performed, Sheila hit retry for DB Mock Upgrade screen of Wizard and it was success.\r\n-	Left the call with agreement.','null'),(669,'Senem Gultekin (NETAS External)','AS-OAM','2013-02-06','130206-384681','Videotron','Problem Description:\n\r\nER paged me for a MCP GUI access problem at Videotron site. System is running on 14.0.9.7 Release.\n\r\nOnce they are logging into the MCP GUI all fields were greyed out and MPC GUI connection was failing after a while.\n\r\nSolution:\r\n-	Accessed to the site over Vernons PC, performed several actions which took time, such as;\r\nStop Start both SMs.\r\nUndeploy Deploy both SMs.\r\nReboot both SM servers.\r\nChecked all SM related logs.\n\r\nBut coulnd\'t find any specific thing that can cause the issue.\n\r\n-	Asked customer were there any config changes on the system. They told us that firewall and routing changes were done in other boxes which were not related to A2.\r\n-	Requested from the customer to try the MCP GUI from local laptop or pc that is connected directly to the servers where there is no firewall or routing. \r\n-	Customer told that they can send someone on site later.\r\n-	Agreed with ER and customer to finalize the test we requested and will let us know the result later.\r\n-	Dropped the call.','null'),(670,'Senem Gultekin (NETAS External)','AS-OAM','2013-02-07','130205-384353','Wide Open West','Problem Description:\n\r\nSWD paged me for a Wizard Upgrade access problem. The day before he hit save & exit on wizard at Upgrading database and system manager screen. And the next day he was not able to open wizard. \n\r\nError Dialog : Login to Wizard Error, Admin doesn\'t have enough permission to run wizard.\n\r\nUpgrade path is from 14.0.9.12 to 14.1.0.10.\n\r\nSolution:\r\n-	Accessed to the site and checked system.\r\nDatabase: MCP_14.1.0.10_2012-09-20-1829\r\nSystem Manager: MCP_14.0.9.12_2012-10-28-1235\r\n-	The day before SWD had a SM upgrade failure due to the unfound zip file under /var/mcp/loads directory on SM upgrade. Ive explained that the needed zip file was copied to another directory in my previous pager report.  And this caused the SM upgrade failure.\r\n-	In this situation Database was in a higher level and SM was in a lower level, which was causing wizard relogin issue due to the admin role changes in 14.0 to 14.1 release.\r\n-	Ive ran SM Upgrade  script from the server manually.\r\n-	Once the SM was upgraded and running on 14.1.0.10 same as the database, we opened wizard in the debug mode and continued few screens in debug mode. \r\n-	Once the primary side was upgraded, SWD continued with his upgrade in normal mode.\r\n-	Tracked SWD until the end of the upgrade. Completed successfully.','null'),(671,'Senem Gultekin (NETAS External)','AS-OAM','2013-02-05','130205-384353','Wide Open West','Problem Description:\n\r\nSWD paged me for a Wizard Upgrade failure at screen DB and SM upgrade. \n\r\nWrong Loadname\r\nDB & SM upgrade failed.\r\nExecuting DB & SM upgrade\n\r\nUpgrade path is from 14.0.9.12 to 14.1.0.10.\n\r\nSolution:\r\n-	Accessed to the site and checked primary EM for the loads under /var/mcp/loads directory.\r\n-	There was no zip file for MCP_14.1.0.10_2012-09-20-1829 patch.\r\n-	The reason for this that in the previous MOCK upgrade failure issue (130205-384329), the disk space was not enough. SWD copied the zip files of the loads to another directory to create some space. But actually zip files should have been stayed there.\r\n-	Created zip files back again and Tim hit retry. But this time it failed with the following error;\n\r\nDescription:\'Validating the scripts and files required for upgrade. Upgrading the Database Lock acquired by ut_dbInstall.pl Error: Running multiple instances of DB related scripts (at the same time) is NOT allowed See log file for possible details: /var/mcp/upgrade_tools/logs/dbInstall/ut_dbInstall.EMS1_8d3d2b4c-2a30-1b21-a094-000e0ce4f334_ut_mcpUpgrade.pl_UPGRADE_DB_SM_8.20130205_060419.log \'\r\n	Error:\'Cannot get lock\'\n\r\n-	This was because multiple scripts were running at the same time, with our previous attempt. \r\n-	Had to restart the NED, but SWD told that the maint window is ending and he will continue next day.\r\n-	Ive restarted NED on primary EM server.\r\n-	Agreed with SWD to continue on the upgrade next day, ended the call.','null'),(672,'Senem Gultekin (NETAS External)','AS-OAM','2013-02-05','130205-384329','Wide Open West','Problem Description:\n\r\nSWD paged me for a DB MOCK failure at Wide Open. DB MOCK was failing due to disk space error on the primary EM server with the following error;\n\r\ndb MOCK upgrade fails with lack of disk space:\n\r\nWizard Upgrade path is from 14.0.9.12 to 14.1.0.10.\n\r\nSolution:\r\n-	Accessed to the site and checked primary EM server for disk space, /var/mcp  directory was over %75 .\r\n-	Removed zip files under /var/mcp/remote_backup directory which was taking to many space. Also removed some unnecessary files from /opt directory.\r\n-	Once the unnecessary files were removed the disk space was reduced to %61. \r\n-	Tim retried the Mock Upgrade screen and it was success.\r\n-	Dropped the call.','null'),(673,'Senem Gultekin (NETAS External)','AS-OAM','2013-02-03','130130-383600','Puerto Rico Telephone','Problem Description:\n\r\nER paged me for an SM ssh connection problem at Puerto Rice Telecom. Previously they had some call failures and was handled with the case  130130-383600. For this issue ER was trying to collect logs by accessing to primary SM server over CMT, but he was getting connection failure.\n\r\nSolution:\r\n-	Accessed to the site and tried to access primary SM server over CMT. But I received connection failure as well. Direct ssh was not possible as well.\r\n-	Pinged the server it was alive, but connection was not possible.\r\n-	Requested from ER to try ssh access to secondary SM and SESM servers. They were all the same, all pingable  but not ssh connection.\r\n-	Asked customer that when was the last time they have accessed to the servers. Customer answer was; we are not able to access any of the servers since last year.\r\n-	At that point we have agreed with ER that this is something that customer needs to fix in their network, and defiantly not something that needs to be worked over a pager.\r\n-	Ended the call.','null'),(674,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-02-07','130206-384681','Videotron Ltee','Problem Description:\r\nGary had paged me about MCP gui access unstable in Videotron customer.\n\r\nMCP Load:\r\nMCP_14.0\n\r\nSolution of Problem:\r\nI have connected to site and we were unable to connect MCP GUI but ssh connection to the servers was working properly. I have checked the oss logs and spool files if they are bigger than normal. Although I applied some steps it did not respond and still we had an MCP issue on the customer. We decided to set MW for this problem and agreed to apply below procedure with customer.\n\r\nI have talked with Mario(customer) with help of Brad and connected his computer.\n\r\nI have applied the below procedure and it took 5 min outage being active of SESM servers. \n\r\n1) Undeploy both SM instances. \r\n2) Stopping all NEs (EM,PA,SESM servers) \r\n3) Deploying SM_0 instance. \r\n4) Starting SM_0 instance. \r\n5) Opening MCP GUI. \r\n6) Come up all other NEs. \n\r\nAfter the procedure had been applied, the site was been stable, I had said to customer, they can do their provisioning tests. They would start in the next morning in their time. Then, I dropped the call.','null'),(675,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-02-08','130208-385107','Verizon Communications','Problem Description:\r\nRodney had paged me about Certificate failure after A2 upgrade from 14.0.9.12 to 14.1.0.13 in Verizon.\n\r\nMCP Load:\r\n14.1.0.13\n\r\nSolution of Problem:\r\nI have connected to site and checked the problematic logs;\n\r\nSystem:GWEMProxy; Provisioning is not supported on Gateways containing this version of load.; Reason:\n\r\nWe have suspected from certificate issue and since the query was coming from CMT site, I have involved the Mert Cokluk from CMT side. He was the pager prime of that week.\n\r\nHe has connected to site and renewed the certificate on the system and it resolved. Then, we drop the call and follow-up case was opened for deeply investigation.','null'),(676,'Tugrul Timorci (NETAS External)','AS-OAM','2013-01-25','130124-382115','Wide Open West','Chris Henwood (SWD)CAlled me at dryrun upgrade step.\n\r\nDryrun failed with schema mismatch error. I checked the mismatches and these mismatches were on primary DB. I advised to execute resync (primary to secondary) and after continue from  stucked step\n\r\nAfter agreement I dropped the call','null'),(677,'Tugrul Timorci (NETAS External)','AS-OAM','2013-01-24','130125-382365','Wide Open West','Chris Henwood called me for platform patch failure14.0.13 to 14.0.18. While patform patch running NED installation failed. Server was running since 490 days\n\r\nI manually restart NED and retried to execute platform patch bu it failed again. I tried to stop and start NED  manually but it failed. \n\r\nI restart the server and tried again this time passed successfully. After agreement I dropped the call','null'),(678,'Tugrul Timorci (NETAS External)','AS-OAM','2013-01-23','TBD','Videotron','Timothy Tron (SWD)called me for asking some question which are related Videotron rollback steps. \n\r\nVideotron wants to rollback their system 14.1.0.8 to 14.1.0.5. \n\r\nVideotron, does not want their MCP patched current, but rather, want it only up to 14.1.0.5 not the current 14.1.0.8\n\r\nI provided the rollback information. After agreement I dropped the call','null'),(679,'Tugrul Timorci (NETAS External)','AS-OAM','2013-01-22','130122-381458 ','GET OSLO','Dave Barlett (SWD)called me for preupgrade steps failure. He got below message from upgade wizard. \n\r\nUpgrade path is MR 14.1.3->14.1.4 \n\r\nCannot determine blade hardware type for blade 4.Use of uninitialized value in concatenation (.) or string at \r\n/var/mcp/upgrade_tools/bin/../lib/ut_Firmware.pm line 812 (#1) \r\n(W uninitialized) An undefined value was used as if it were already \r\ndefined. It was interpreted as a \"\" or a 0, but maybe it was a mistake. \r\nTo suppress this warning assign a defined value to your variables. \n\r\nTo help you figure out what was undefined, perl tells you what operation \r\nyou used the undefined value in. Note, however, that perl optimizes your \r\nprogram and the operation displayed in the warning may not necessarily \r\nappear literally in your program. For example, \"that $foo\" is \r\nusually optimized into \"that \" . $foo, and the warning will refer to \r\nthe concatenation (.) operator, even though there is no . in your \r\nprogram. \n\r\nUse of uninitialized value in pattern match (m//) at \r\n/var/mcp/upgrade_tools/bin/../lib/ut_Firmware.pm line 813 (#1) \n\r\nprintBladeFirmware received error(s)! \n\n\r\nqueryBCTFirmware received error(s)... \r\nLoad Lineup Validation : ............................................ [FAILED] \r\n- The Firmware Levels in the server could not be determined. \r\n- Following error(s) occured while querying the Firmware: \n\n\r\nPlease wait while reading the inventory from the BCT MM... \n\r\n---------------------------------------------------------------- \r\n-----------------Firmware Query for Blade Center---------------- \r\n---------------------------------------------------------------- \n\r\nBlade 4 \r\nHardware Type: (--) \r\nBIOS \r\nBuild ID: BWE132AUS \r\nVersion: 1.13 \r\nDIAG \r\nBuild ID: BWYT29AUS \r\nVersion: 1.12 \r\nBMC \r\nBuild ID: BWBT41A \r\nVersion: 1.34 \r\nNIC \r\nCard/Firmware: eth1/5704s-v3.38 \r\nCard/Firmware: eth0/5704s-v3.38 \r\n----------------------------------------------------------------- \n\n\r\nI logged in to the site and checked the issue. ut_Firmware script can not get the hardware type. I manually add this value to script and successfully passed this screen.','null'),(680,'Ken Johnson','AS-OAM','2013-01-18','130119-380869','ATO','Site attempted to apply A2 patch 14.0.9.12 which appeared to have locked up during application.  The site had aborted patch application and swacted the SMs in an attempt to clear Database Communication alarms; then contacted ER.\r\n  GPS reviewed oracle trace files to determine in a fault caused the primary database to go down, restarted the primary database, resolved replication issues, and provided additional data collection procedures in the event of another occurrence while applying 14.0.9.12.  GPS and ER monitored the site for a while and the patch did successfully complete application after 2.5 hours.','null'),(681,'Ken Johnson','AS-OAM','2013-01-14',' 130110-379062','Columbia University Medical Center','Site was unable to login to an A2 System Manager.  While executing a password recovery procedure the system reported both hard drives were inaccessible. The site was booting from a MCP_9.1 (SN09.1) CD, but the old 9.1 CD was unable to access the HT Langley SAS controller and hard drives.\r\n  Using a MCP_12 (CVM13) CD the site was able to use KVM to recover the root and ntsysadm user passwords.  GPS audited the system for change RCA, security, and corruption.','null'),(682,'Seren Batmaz','AS-OAM','2013-01-10','130110-379012','Emirates Integrated Telcommunications','Problem Description:\n\r\nEarl paged me and told me that SWD has run the rollback steps from 14.0.9.12 to 14.0.9.7. However, they still couldn\'t make provisioning with the old SIP profiles. \n\r\nSolution:\n\r\nI connected to site and launch the MCP GUI and checked the Database version. The database was still 14.0.9.12.\r\nHence, I told Earl that we need to restore preupgrade backup which belonged to the load 14.0.9.7. After confirmation of the customer, I restored the DB with that backup. \r\nAs soon as I finished restoring the backup, Earl told me that the customer changed  his mind and wanted to stay in 14.0.9.12 load.\r\nSo, we needed to restore the DB with the backup of 14.0.9.12. First, we cleaned up replication, restored the DB with 14.0.9.12 backup and resync two databases.\r\nAfter that, we needed to launch MCP GUI, we stoped SM, undeployed and deployed with the load 14.0.9.12 and started the SM. We repeated that steps for all the NEs. So, the system started to work with the load 14.0.9.12 and all the NEs were ONLINE UP ACTIVE/HOTSTANDBY. \r\nSo, I told GTS (Hatcki) to tell the customer that operation is done and they can run some basic call tests and if they have any problem, they should call the A2 OAM pager.\r\nAfter that,I dropped the call.','null'),(683,'Seren Batmaz','AS-OAM','2013-01-10','130110-379012','Emirates Integrated Telcommunications','Problem Description:\n\r\nKyle paged OAM GPS for the problem customer cannot make provisioning and they asserted that this problem occured after they have upgraded their system from 14.0.9.7 to 14.0.9.12. \n\r\nSolution:\n\r\nI saw that they were trying to provision with old SIP Profile name sip_line. However, in the bulletin 20121214126 it is asserted that  invalid characters removed from A2\'s SIP Profiles and Templates. I recommand Kyle to let the customer know about it and I told customer should provision with \"sipline\" instead of the old SIP Profile name \"sip_line\".\r\nAfter ER told the customer about that, customer wanted to rollback to 14.0.9.7. ER  asked me if I can do that for them. However, I told ER that Software Delivery should run the rollback steps since they are the one who have applied the patch. After that, I dropped the call.','null'),(684,'Ege Varhan (NETAS External)','AS-OAM','2013-01-12','130110-379012','Emirates Integrated Telcommunications','Problem Summary\r\n-----------------------------------------------\r\nEmirates Integrated Telecommunications A2 system was upgraded from MCP-14.0.9.7 to MCP-14.0.9.12 by Genband on January 10, 2013. The customer wanted to rollback after the upgrade was done. While the latest backup of MCP-14.0.9.7 was being restored into the system, the customer decided to stay on MCP-14.0.9.12 again. Then, GPS has cancelled the rollback and restored the latest backup of MCP-14.0.9.12 again. The latest backup file of MCP-14.0.9.12 was taken on January 10, 2013 at 3:33 AM in the morning. The backup was restored around January 10, 2013 at 7PM server time. During the day, the customer has continued to perform provisioning even though they were told not to do that. As a result, many mismatches occurred between A2 and Core. There are 16 hours difference between those two products(A2 and Core). \n\r\nFor instance;\r\nIf the customer has added a new line between 3AM and 7PM on January 10, 2013, A2 currently doesn\'t have that line\'s information since it was restored to 3AM safe point, however core still has that line info as being assigned and used.\n\r\nSolution Description (Still being investigated)\r\n---------------------\r\nGPS has asked to the customer to find out what kind of provisioning they did between 3AM and 7PM on January 10, 201 and amount of it. The customer told us that they can not find the exact data since there are multiple provisioning teams at the company. GPS has decided to run line audit on SESM to find out the possible mismatches. If it doesn\'t work, we will go through the logs and try to find out what was provisioned during that time period. A2 GPS is working with SESM/CMT GPS (Mert Cokluk) to find out all possible mismatches and correct them. This may tike sometime. I have told the caller (Noureddine Hachki) to contact with SESM/CMT GPS to get the line audit results and possible solution in terms of SESM/CMT/Core perspective.\n\r\nWe haven\'t been able to fully solve the problem yet. \n\r\nGPS\'s Concerns\r\n------------------------\r\nThe customer blames Genband about this mess currently and wants us to take the whole responsibility about the problem since the upgrade, downgrade and database restore actions were done by Genband engineers. However they continued to perform provisioning even though they were told not to, while GPS, GTS and SWD were working at the site between 3AM and 7PM on January 10, 2013. That is really concerning. Also the customer doesn\'t tell what they did in terms of the provisioning between 3AM and 7PM on January 10, 2013 and they want us to solve the problem as quickly as possible without giving any useful information and help. In order to solve this problem, manual action is a must. We need to clean all data mismatch problems manually from the core site. After that, if there are any other problems observed in A2, SESM or Core, we can continue to handle them case by case.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(685,'Mehmet Salim Demir (NETAS External)','AS-OAM','2013-01-10','130110-379006','Chunghwa Telecom (CHT) - Taiwan','Problem Description:\r\n	John Kishner had paged Ege Varhan (A2 OAM GPS) about SM GUI was not coming up properly. It seems grayed out and any NEs does not seem connected and alarms did not appear.\n\r\nMCP Load:\r\nMCP_16.0.2.0\n\r\nSolution of Problem:\r\nSM GUI was not coming up properly when you connect to SM service IP and both SM_0 and SM_1 was trying to activate itself in the same time. Ege tried some methods to activate SM GUI, it did not work. Then, Ege handed it over to me since he was working from home, I connected to site and checked the situation. I  undeployed / redeployed SM GUI so that I can activate it but it did not work as well.  Although SM_1 seems active, when you connect SM GUI everything seems grayed out and you could not do anything. I checked the spool logs on the SM and other servers. Then, I saw that there were many logs written on the spool directory (/var/mcp/spool) there were some cases similar to this and we were applied below actions on that cases; \n\r\n1-	Stop all NEs except active SESM. Otherwise, we the calls will be lost.\r\n2-	Clean all spool data under those NEs with following command; rm -rf log/* om/* tmom/*  (This script will not remove acct data so that we cannot loose IPDR data)\r\n3-	Start SM_0 and make sure it becomes Active\r\n4-	Then, open MCP GUI and start all other NEs.\r\n5-	Be sure that all NEs has Online Up Active state. If there is something different you can kill and start it again. Be careful about SESM instances.\n\r\nAfter this procedure site was cleaned and everything was worked properly. Then, we checked the IPDR data and alarms seen on SM GUI. Then, I left the conference.','null'),(686,'Ege Varhan (NETAS External)','AS-OAM','2013-01-09','130108-378780','Verizon Communications','Customer: Verizon\r\nUpgrade to MCP_14.1.0.13 from MCP_14.0.9.x\n\r\nI have been paged by Bill Doty about the LAB upgrade problem of Verizon. Bill has told me that SM upgrade was failed with the error below;\n\r\nWizard Logs\r\n-----------------------------------\r\n--- Upgrading the SysMgr (SM)  ---\r\nsmUpgrade command: /var/mcp/upgrade_tools/bin//ut_smUpgrade.pl -p /var/mcp/install/installprops.txt -vof -rboff -primarySM -nc -m EMS1_f7e3e766-2a2a-1b21-ad79-000e0c9f5c70_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0 -s 106.40.227.22 -start\r\n  Issuing ned command \"put 106.40.227.21:2100 /var/mcp/upgrade_tools/work/EMS1_f7e3e766-2a2a-1b21-ad79-000e0c9f5c70_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.state ../../../../../var/mcp/upgrade_tools/work/EMS1_f7e3e766-2a2a-1b21-ad79-000e0c9f5c70_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.state ntappsw:dummyPassword bd6c2938d0e5534b21dbaae3d088f76e 3600\"  \r\n*** SM Upgrade FAILED ***\n\r\nDetailed Logs\r\n-------------------------\r\nUnlinked /var/mcp/upgrade_tools/work/smUpgrade//init_ned_cmd_20130108_154935_F4UR\r\nNot unlinking file\r\nPolling ned to see if NE has stopped.\r\nExecuting: /bin/cat /var/mcp/upgrade_tools/work/smUpgrade//init_ned_cmd_20130108_154939_SKVq | /opt/mcp/ned/bin/nedclient 106.40.227.21 4890\r\nCommand Output:\r\n> query ok { }\r\n> exited\n\r\nUnlinked /var/mcp/upgrade_tools/work/smUpgrade//init_ned_cmd_20130108_154939_SKVq\r\nNot unlinking file\r\nSM instance (MCP_14.0, SM_0) stopped.\r\nMultiple SM_0 directories were found under the /var/mcp/run/ directory\n\r\nMultiple SM_0 directories were found under the /var/mcp/run/ directory\n\n\r\nI have connected to the site through Bill\'s computer and checked SM GUI. I have realized that SM_0 admin state was \"Online\" even though it\'s link and operation were \"Down\" and \"Unavailable\". During the SM upgrade, the SM in the previous release is stopped and then the upgrade takes place. However, at Verizon lab, SM_0 couldn\'t be stopped successfully. I have killed SM_0 manually via SM GUI and told Bill to tell the customer to try the failed step on Upgrade Wizard again. Since, the customer had already left the office, we couldn\'t complete that action and i have left the call.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(687,'Senem Gultekin (NETAS External)','AS-OAM','2013-01-03','130103-378051','Shaw Cable Systems','Problem Description:\n\r\nER paged me for a SESM1_0 instance not coming up issue after a restart at Shaw Cable Systems.\r\nSystem is running on 14.1.0.3 Release.\r\nThey were seeing few alarms on SESM1_0 related asSNMP Agent Communication Failures, Loss of Comms, power and and CPU Occupancy Threshold issues.\n\r\nSolution:\r\n-	Accessed to the site and checked the MCP GUI, SESM1_0 was in Configured state and Unavailable, SESM1_1 was on Online and Active state. \r\n-	Since the SESM1_0 was in configured state hit deploy, but still was in configured state. Had to switch the SM instances to bring SESM1_0 to offline state.\r\n-	Once SESM1_0 was offline state Ive hit start button but it was really slow, and was stuck on initializing.\r\n-	Accessed to the SESM1_0 server to check configuration but it was really slow processing.\r\n-	Rebooted the server, after the reboot it was much faster and the alarms were cleared.\r\n-	Started SESM1_0 and it was online and hotstanby mode.\r\n-	Customer was happy. \r\n-	Left the conference.','null'),(688,'Senem Gultekin (NETAS External)','AS-OAM','2013-01-05','130105-378410 ','Hong Kong Broadband','Problem Description:\n\r\nER paged me for a RTP Portal not coming up issue at Hong Kong Broadband customer. \r\nThey are running on 3.0 (3.0.55_build832), which is a really old release. It been EOL almost 6 years ago. \n\r\nCustomer was not able to get any traffic on Media Portal 1. ER rebooted the host for RTP 1 Portal but it didnt come up.\n\r\nSolution:\n\r\n-	ER explained the history to me and the last action he did was to reseat all blades on RTP 1 Portal.\r\n-	After the reseat blade came up until some point but it returned with the file system check error.\r\n*** An error occurred during the file system check.\r\n-	John performed fsck command on the blade and rebooted the blade again;\r\nIntel machine check reporting enabled on CPU#0.\n\r\n                   CPU: Intel Pentium III (Coppermine) stepping 0a\n\r\n                   Checking 386/387 coupling... OK, FPU using exception 16 error reporting.\n\r\n                   Checking \'hlt\' instruction... OK.\r\n-	Requested from customer to find their 3.0 platform installation CD, if we need to do a fresh install the portal 1. But they couldnt find the CDs.\r\n-	John handed over his shift to Rondey. RTP Portal 1 was still showing unknown state on MCP GUI.\r\n-	Customer had a spare CPU card, requested them to replace it.\r\n-	After the CPU card replacement, Rodney performed sfck and reboot for the blade and it came up.\r\n-	Checked the MCP GUI the Portal 1 was showing unlocked enabled and customer was happy.\r\n-	Agreed with Rodney and customer and ended the call.','null'),(689,'Senem Gultekin (NETAS External)','AS-OAM','2012-12-14','121214-376098','VTR Global','Problem Description:\n\r\nER paged me for an no dialtone on mulitple lines at VTR.\r\nSite is running on 10.3 Release (EOL).\r\nAndy Putzer from CALLP GPS was already working on the issue. They were suspecting that it can be related to DB issue, and they pager OAM pager.\n\r\nSolution:\r\n-	Accessed to the site and checked the DB servers, they were up and running.\r\n-	Accessed to the DB with sqlplus and ran few queries to verify DB is running.\r\n-	Checked the DB logs but was not able to see any DB related error.\r\n-	Andy continued to work on the issue. He investigated all the traces.\r\n-	Ege Varhan from OAM GPS took over and worked with Andy as well, where he worked on this site for a week, resync issue(121211-375605)\r\n-	Ege ran cleanupreplication and SESM restart was performed. Issue was gone.\r\n-	This issue may be related to OAM case which Ege owns 121211-375605. Since theres network freeze due to Christmas holiday, the investigation will continue later.','null'),(690,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-12-21','121221-377258','VENTELO SVERIGE AB','Problem Description:\r\n   Burak had paged me about the problem which was power failure of the MCS-5200 and it was down. He said that we succeeded to restart SESM servers and we got rid of call outage but PROV servers were still seem down.\n\r\nMCP Load:\r\n   MCS 9.0\n\r\nSolution of Problem:\r\n   I have connected to site and connected to MCS GUI. I checked the PROV server\'s with connecting it via ssh. But I could not able to connect it due it was down and the server was not pingable. I have warned the customer and said them please power cycle the all down servers for PROV instance. I also noticed that their one of the SESM server was also down and I also said them to start it. Sometime later, the customer said that power cycle operation was not succeeded and servers could not able to activate themselves normally. Due to 9.0 version of the MCS and Sun Netra 240 was end of life already, I said them I have not experienced on that servers to much and I suggest you to reinstall your platform on problematic servers. Since these servers were one of the SESM and PROV servers, the call outage was not existed and they accepted the suggestion and I dropped the call.','null'),(691,'Senem Gultekin (NETAS External)','AS-OAM','2012-12-14','121208-375082','UPC','Problem Description:\n\r\nER paged me for an workaround that needs to be applied at UPC site during maint. window\r\nUpgrade path is from 14.1.0.5  to 14.1.0.12\n\r\nSolution:\r\n-	Accessed to the site and applied the following steps, these steps are already known by GPS and customer;\r\n-	Took a backup of the database\r\n-	Login to the primary database and checked the sip profile of a sip line as following queries\r\nSQL> select sm.oid, sm.profile_name, sp.description from sip_profiles sp, sip_media sm where sp.oid = sm.oid and sm.profile_name = \'sipline\';\n\r\n       OID PROFILE_NAME\r\n---------- --------------------------------\r\nDESCRIPTION\r\n--------------------------------------------------------------------------------\r\n     17161 sipline\r\nsip_line\n\n\r\nSQL> update sip_media set profile_name = \'sip_line\' where oid = 17161;\n\r\n1 row updated.\n\r\nSQL> select sm.oid, sm.profile_name, sp.description from sip_profiles sp, sip_media sm where sp.oid = sm.oid and sm.profile_name = \'sipline\';\n\r\nno rows selected\n\r\nSQL> select sm.oid, sm.profile_name, sp.description from sip_profiles sp, sip_media sm where sp.oid = sm.oid and sm.profile_name = \'sip_line\';\n\r\n       OID PROFILE_NAME\r\n---------- --------------------------------\r\nDESCRIPTION\r\n--------------------------------------------------------------------------------\r\n     17161 sip_line\r\nsip_line\n\n\r\nSQL> commit;\n\r\nCommit complete.\n\r\n-	Customer applied their test calls and they were all success.\r\n-	Agreed with them and ended the call.','null'),(692,'Senem Gultekin (NETAS External)','AS-OAM','2012-12-20','121213-375933','Twin Lakes','Problem Description:\n\r\nSWD paged for and jar request in the middle of an upgrade. \r\nAAK-27643_14.0.9.9_Oct_10.jar\n\r\nUpgrade path is from 12.0.12.4 to 14.0.9.12\n\r\nSolution:\r\n-	Checked our FTP server and the requested jar file was not there. Our FTP server content is deleted automatically at every end of month. This jar file was uploaded 2 months ago.\r\n-	Requested from patch team to upload to the FTP server again.\r\n/home/ftp-docs/one-month-dropbox/120905-358433/14.0\r\n-	SWD was able to download the jar fail, applied the jar and continued with the upgrade.','null'),(693,'Senem Gultekin (NETAS External)','AS-OAM','2012-12-12','121212-375746','LGU','Problem Description:\n\r\nKim paged me for an replication issue seen at LGU. They are running 10.3.1.15 (EOL long time ago) and due to link errors they have planned to run resync, but it was stuck.\n\r\nSolution:\n\r\n-	Accessed to the site and checked the logs, there was time out error first, restarted the neds on both database servers. \r\n-	Kim ran resync script again and it passed the stuck part failed at adding REGDEST table as following;\r\n____________________________________________________________________________________\r\n> run output   ERROR at line 1:\r\n> run output   ORA-20100: ORA-20100: Can not add repobject REGDEST since the table does not\r\n> run output   have primary key\r\n> run output   ORA-06512: at \"SSDVDB.MCSDB_REPGROUP\", line 218\r\n> run output   ORA-06512: at line 1\r\n____________________________________________________________________________________\r\n- Ran cleanupreplication script to drop the replication.\r\n- Ive took the current database backup.\r\n- Ran restore_empty_db command for primary db and cleared all the data inside of it.\r\n- Restored the previous database which was taken one day earlier. But during restore I saw the REGDEST primary key issue again.\r\n- Cleared the secondary database by running restore_empty_db\r\n- Checked the database for the REGDEST table and primary key was not there. Needed some time for investigation.\r\n- Customer was running out of maint window, they wanted to perform resync in the next maint window, and until that time we will work on the issue to fix it from the database.\r\n- Agreed with Kim, and ended the call. \r\n- Later on issue has been fixed by adding primary key to the database manually. Ill put more detail in the case.','null'),(694,'Senem Gultekin (NETAS External)','AS-OAM','2012-12-11','121214-376164','BT','Problem Description:\n\r\nHenry paged me for a wizard upgrade issue seen at BT live site.\n\r\nUpgrade path is from 14.0.9.7 to 14.0.9.12.\n\r\nIt was failing at the EXTRACT UPGRADE TOOLS screen.\n\n\r\nSolution:\r\n-	Normally GPS does not give pager support for a pre upgrade step failure, since this was BT and had issue previously applied an exception for this case.\r\n-	Accessed to the site and checked the upgrade tools logs from the server, the following error was occurring.\n\r\nRunning command: \"/bin/rm -rf /var/mcp/loads/MCP_14.0.9.12_2012-10-28-1235\"\r\nCommand failed => /bin/rm -rf /var/mcp/loads/MCP_14.0.9.12_2012-10-28-1235\r\n Exit code: 256\n\r\n Could not remove /var/mcp/loads/MCP_14.0.9.12_2012-10-28-1235\r\nMoving \"/var/mcp/loads/extractTmp_ut_mcpPrepareLoad_20121211_205917/MCP_14.0.9.12_2012-10-28-1235\" to \"/var/mcp/loads/MCP_14.0.9.12_2012-10-28-1235\"\r\nRunning command: \"/bin/mv /var/mcp/loads/extractTmp_ut_mcpPrepareLoad_20121211_205917/MCP_14.0.9.12_2012-10-28-1235 /var/mcp/loads/MCP_14.0.9.12_2012-10-28-1235\"\r\nCommand failed => /bin/mv /var/mcp/loads/extractTmp_ut_mcpPrepareLoad_20121211_205917/MCP_14.0.9.12_2012-10-28-1235 /var/mcp/loads/MCP_14.0.9.12_2012-10-28-1235\r\n Exit code: 256\r\n-	Previously BT attempted manual patch upgrade from 14.0.9.7 to 14.0.9.12 release and it failed due to database issue, we fixed it on 121208-375077. Since official upgrade procedure is wizard, requested from BT to apply wizard upgrade in the next attempt. \r\n-	Since during manual upgrade the patch load(MCP_14.0.9.12_2012-10-28-1235) was extracted manually under /var/mcp/loads wizard was failing to run over it.\r\n-	Normally wizard unzips the patch zip file, if its unzipped manually before wizard can fail.\r\n-	I removed the previous unzipped MCP_14.0.9.12_2012-10-28-1235 load from /var/mcp/loads directory.\r\n-	Henry hit retry and wizard unzipped the patch file automatically, screen passed successfully.\r\n-	Waited them to continue in their next screens. It failed again on Selecting Primary & Secondary NEIs screen. BT has 40 BCPs and they prefer to exclude them from the upgrade.\n\r\nError:\n\r\nThere should ne be any NEIs added on to servers which are excluded from upgrade.\n\n\r\nThe BCP exclude fix came in 14.0.9.7 patch with the issue ID of AK-25261.\r\nIn 14.0.9.12 a new fix which is N-3 Upgrade Feature (AAK-28020) has broke this previous fix, and customer is not able to exclude any BCP servers form the wizard upgrade.\r\n-	Discussed with Henry and decided to continue with manual upgrade. For the BCP exclude issue Ive reported to design and opened an issue(AAK-28429) so it will be fixed asap.\r\n-	BT completed their upgrade manually to 14.0.9.12 patch successfully.','null'),(695,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-11-23','121211-375526','WIND Telecom','Problem Description:\r\n   Chris has paged me due to similar problem like below issue; \r\nAAK-25613 - A2E-SSL - SIP Profile configuration ignored - DB Fix (121123-372495)  \r\nand said me that the customer wants to remove the patch that he had done. The problem of the customer was the same with that issue.\n\r\nMCP Load: \r\n   From:14.0.9.0 to 14.0.9.11/12\n\r\nSolution of Problem:\r\n   First of all, I have warned Chris about why are you paging me about this problem because this is not E1, E2 or BC situation for customer. Fix already has been merged with the 14.0.9 MRs last patch and anymore the SIP Profiles can only have alpha numeric characters. But Chris said me this is important because customer wants to take the patch back and they want to return 14.0.9.0 MR because of the this problem. After that, we had searched for database backup for restoring it back but Chris said me that I did not use upgrade wizard. Then, I had warned him about you should use upgrade wizard and latest documentations. Because of the upgrade wizard was not used, database backup did not be taken automatically and also manual patch operation did not include taking database backup, we have not any database backup on our hand. So, I said him there is nothing to do for restoring backup back since no backup and communicate this situation with customer and inform them. Then, we dropped the conference.','null'),(696,'Senem Gultekin (NETAS External)','AS-OAM','2012-12-11','121208-375077','BT','Problem Description:\n\r\nHenry paged for an BT activity to be done on site.\r\nPreviously they have upgraded from 14.0.9.7 to 14.0.9.12 patch manually.\r\nHowever after upgrade they were not able to bring the SESMs up. So they decided to do a system rollback but had failure during db restore.\n\r\nSolution:\r\n-	Accessed to the site and checked the current state of the databases they are 14.0.9.12 release.\r\n-	The restore error he was getting was due to the syntax of the restore command;\r\n/var/mcp/db/backup/preUpgradeMCP_14.0.9.12_2012-10-28-1235.tar.gz.20121207_210044.tar.gz: Cannot open: No such file or directory\r\n-	Ive dropped the replication by running cleanupReplication script on primary database.\r\n-	Once the replication was dropped Ive restored the database with backup with dbBackup_THU backup.\r\n-	Since the upgrade was attempted several times there were backups that shows as preUpgradeMCP_14.0.9.12 but they really are the backup of 14.0.9.12 release. Thats why I returned to the backup on Thursday.\r\n-	Once the restore was completed successfully, checked from the database and the db instance table was as 14.0.9.7.\r\n-	Ran resync to bring the replication back.\r\n-	Accessed to the MCP GUI and deployed and started both SMs with 14.0.9.7 load.\r\n-	Customer will perform upgrade with wizard next time, not manually.\r\n-	Also Ege removed the problematic function (GENH323EPUSERID) from the database which was seen during upgrade. He removed this function cause other BT sites do not have this function and investigation will continue on where this function came from.\r\n-	Agreed with Henry and left the conf.','null'),(697,'Senem Gultekin (NETAS External)','AS-OAM','2012-12-10','121210-375119','ATO','Problem Description:\n\r\nER paged OAM GPS for a critical alarm seen on SESM2_1 instance. Customer is ATO and its running on 14.0.9.6 Release.\r\nAlarm is as following;\n\r\nAlarm THLD 401 Threshold crossed: Interworking IOCs:InUse exceeds threshold value of 95000.\n\r\nSolution:\r\n- Since this is ATO theres no site access for any of us.\r\n- Requested ATO engineer to restart the SESM2_1 instance where they were getting critical alarm of THLD 401\r\n- ATO engineer restarted the instance but the alarm didnt clear.\r\n- SESM2_1 server was up for 250 days, requested to restart the SESM2_1 server, after the server came up SESM2_1 was still giving critical alarm.\r\n- Since its only 1 SESM instance issue and restarts didnt work Ive requested from ER to page CALLP GPS team.\r\n- Agreed with ER and he continued to work with CALLP GPS','null'),(698,'Ege Varhan (NETAS External)','AS-OAM','2012-12-09','121208-375077','BT PLC (Manchester)','Problem Description\r\n--------------------\r\nSESM instances can not be up and active after upgrading the system to 14.0.9.12 from 14.0.9.7 via manual patching.\n\r\nSolution Description\r\n--------------------\r\nI have connected to the site and checked SESM\'s work logs. SESM instances couldn\'t be started because of some missing configuration parameters in Database. One of them is below;\n\n\n\r\nThose new parameters must be written into the database during 14.0.9.12 patch. However they couldn\'t be written into DB somehow. I have realized that some other config parameters were also missing that could cause other Network Elements (NEs) to fail to get started. We thought that the DB upgrade was not completed successfully and suggested the customer to restore the backup of A2 14.0.9.7 and try to patch A2 via Upgrade Wizard again.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(699,'Ege Varhan (NETAS External)','AS-OAM','2012-12-08','121208-375077','BT PLC (Manchester)','Problem Description\r\n-------------------\r\nThe customer tried to upgrade A2 from 14.0.9.7 to 14.0.9.12 via \"mcpPatch.pl\" (manual process). SM instances couldn\'t be up in A2 14.0.9.12 after \"mcpPatch.pl\" script was completed. Also, there is a database mismatch occurred during DB upgrade.\n\r\nSolution Description\r\n-------------------\r\nI have connected to the site and  checked the database upgrade logs. I have found that even though there is a problem at the end of the DB upgrade, the database looked like upgraded. I have deployed and started SM instances on 14.0.9.12 successfully and the problem was resolved.\n\r\nGPS Concerns\r\n-------------------\r\n1- The customer shouldn\'t have upgraded A2 via mcpPatch.pl. They should have used Upgrade Wizard.\r\n2- The customer tried to perform the same patch 4 times. In none of them, SM upgrade was performed by the script. When they see a problem, they shouldn\'t have tried to patch the system over and over again.\r\n3- There is a rogue function defined in database. That is the function which caused database mismatch during database upgrade to 14.0.9.12. The name of the function is \"GENH323EPUSERID\". I couldn\'t find that function at our labs and in the official load. Not sure how it exists in their database. I have told Henry Meleg (GTS) to ask it to the customer. If it is custom and defined by someone else, it should be deleted from database immediately.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(700,'Ege Varhan (NETAS External)','AS-OAM','2012-12-06','121206-374724','Thunder Bay Telephone','Problem Summary\r\n------------------\r\nGB SWD contacted ER to report that the Wizard Upgrade tool had stopped at step 17 when attempting to upgrade the A2 from MCP 14.0.9.6 ->14.0.9.12. SM 1 took activity but SM 0 remained down / unavailable. Also noted was that SESM1 0 and RTP Portal VKBlade 1 were also placed in a down / offline / unavailable state\n\r\nSolution Summary\r\n------------------\r\nGPS connected to the site and started SM_0 instance manually via SM GUI. During the upgrade, it is normal and expected for SESM1_0 and VKBlade1 to be offline and unavailable during that step. Because, the service is transfered to secondary elements. But, we had to start SM_0 manually. There is a RCA case opened for us to investigate why SM_0 couldn\'t be started. RCA case 121206-374759 has been opened and dispatched for further review.\n\r\nWe have also observed the whole remaining upgrade steps to make sure that everything is ok. We have left the call after all upgrade steps were completed.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(701,'Senem Gultekin (NETAS External)','AS-OAM','2012-11-16','TBD','Genband','Problem Description:\n\r\nGenband live site was being upgraded from 16.0.2.3 to 16.0.2.4 patch. Doug paged me for a mcpPatch.pl failure as following;\n\r\n________________________________________________________________________________\n\n\r\nLoading Journaling FW triggers\r\n***************************************************************************\r\nDone loading stored procedures and triggers.\n\r\nChecking Status of user objects .....\r\nDECLARE\r\n*\r\nERROR at line 1:\r\nORA-20980: Invalid object found: object type =FUNCTION object name =SQUIRREL_GET_ERROR_OFFSET status=INVALID\r\nORA-06512: at line 8\n\n\r\nDisconnected from Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - 64bit Production\r\nWith the Partitioning, OLAP, Data Mining and Real Application Testing options\r\nAudit/Monitor the database since upgrade failed ..\n\n\r\n________________________________________________________________________________\n\r\nSolution:\r\n-	Asked Doug that why is he using manual upgrade instead of wizard upgrade. He responded that they have been doing manual upgrade for a while. Explained him that wizard upgrade is the supported upgrade procedure, not manual.\r\n-	Asked him which document he his using, he said he is not using an updated one.\r\n-	Requested him to use wizard upgrade.\r\n-	He didnt have the loadlineup file, sent him the 16.0 loadlineup file.\r\n-	Gave the document ID that he should use, which is 630-01217-01.\r\n-	Wizard failed due to ASU file, wizard was trying to see the ASU_MMPCClient_9.0.72 file under /var/mcp/media directory in primary EM server. To pass this step requested from Doug to transfer the ASU file. He was not able download it at that time.\r\n-	Since these are pre upgrade steps, I told Doug that I can transfer this file for you but this pre step is not something I will do at 5AM. Kindly explained him that pre steps can be done during bussiness hours, doesn\'t have any impact to the system by using wizard upgrade and by following the document.\r\n-	Agreed with Doug so he can complete pre upgrade steps first and continue with the actual upgrade. He told me that probably the upgrade will postponed to next week.\r\n-	Also explained that wizard upgrade will not change the fact that the database upgrade is failing. But for supported procedure we should fallow wizard upgrade steps. \r\n-	Left the conference.\r\n-	In the other hand Ive worked with design for the database issue during the week and prepared the database for the upgrade.\r\n-	Ill give details about the database failure once Doug will open the cases.','null'),(702,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-11-20','121120-372019','Global Village Telecom (GVT)','Problem Description:\n\r\nPlatform Linux installation on ATCA blade using PIW script fails with the following error message: \r\nroot@typhoon-base-unit0:/tftpboot/cnp/ssl> ./configNDMForA2E.pl -blade 0 0 8 0 -piw /tftpboot/cnp/ssl/A2GVTRJOinstallconfig.xml EMServer1 \r\nrver1 \n\r\nAn A2E/SSL installation will be performed with following information; \r\nBlade\'s Frame: 0 \r\nBlade\'s Shelf: 0 \r\nBlade\'s Slot: 8 \r\nBlade\'s Sub-slot: 0 \r\nPIW File: /tftpboot/cnp/ssl/A2GVTRJOinstallconfig.xml \r\nPIW Server Name: EMServer1 \r\nPlease make sure that the information above is correct. \n\n\r\n!!!ATTENTION!!! \r\nThe blade will be automatically reset as part of configuration!!! \n\r\nDo you want to continue (Y or N) [N]: Y \n\r\nRetrieving network data... \r\nArgument \"255.255.255.0\" isn\'t numeric in repeat (x) at ./configNDMForA2E.pl line 1086. \r\nNDM does not have a valid netmask. \n\r\nMCP Load: 14.0.0.X\n\r\nSolution of Problem:\n\r\nNeslihan had paged me about above problem and told me the problem description. There a was script failing situation. I have connected customer site and checked the logs and script that was failed. I saw that the script belongs to CVM16 release but the customer\'s VSE version was 5.1. According to VSE version versus A2 adaptation it should be like below; \n\r\nVSE 5.X -> CVM14, CVM15 \r\nVSE 6.0 -> CVM16 \n\r\nI have found this info is already written in the \"configNDMForA2E.pl\" scripts like; \r\n    \" 1.3 Enhancements for adaptation to new VSE load (6.0.0.110607.0)\". \n\r\nSo it means, when you have VSE 6.0 version, this file can be used. \n\r\nBecause of the customer has VSE 5.1 they should either use CVM14 A2 load or they upgrade their VSE to 6.0 for CVM16 A2 load. Then, I left the conference.','null'),(703,'Senem Gultekin (NETAS External)','AS-OAM','2012-11-13','121113-371007','TelstraClear','Problem Description: \n\r\nUpgrade Path 14.0.9.8 to 14.1.0.10. \r\nWizard was returning an error on Screen 6: Extract Upgrade Tools \n\r\nSearching for core load MCP_14.1.0.10_2012-09-20-1829... \r\nError! EMS1_0@MCP_14.0 is currently being configured in another session. \r\nAn error occurred while searching for core load. \n\r\nSolution: \r\nI\'ve accessed to the site and checked the system, I\'ve received the same error myself. \r\nLogged into the primary EM server and restarted NED by running the neinit restart command. \n\r\nOnce the NED was restarted on primary EM server, I\'ve hit retry on wizard for screen Screen 6: Extract Upgrade Tools. And the screen was complete successfully. \n\r\nSWD continued to the upgrade. Left the conference.','null'),(704,'Samet Bilevci ( Netas External )','AS-OAM','2012-11-08','120906-358606','Singtel Optus Pty Ltd','Yuji Onozuka from local GTS team has pages me for log collection activity previously agreed on for already open BC Case from Optus. Case# 120906-358606 about SESM\'s PA Routing not working.\n\r\nPreviosuly at Optus \"Sunshine\" site, after CVM13 (12.0) to CVM14 (14.0) Upgrade the same issue was seen and the RCA could not found from the logs as the issue was occuring at runtime and disappeared after swacting SESMs and have not seen after OPTUS Sunshine site upgraded to CVM16 (14.1)\n\r\nSo, the following logs to be captured if the same issue occurs during the next site Upgrade (Rochedale) to 14.0 load.\n\n\r\nFrom Active SESM\r\n>telnet localhost 21000\r\n>cd trace\r\n>act\r\n>debuglevel set DEBG verbose\r\n>debuglevel set SYNC verbose\r\n>debuglevel set DSYT verbose\r\n>debuglevel set NEI verbose\r\n>debuglevel set SMCM verbose\n\n\r\n=================\n\r\n>start  \r\nWait 5 minutes\r\n>stop\r\n>deact\r\nNOTES!: No need to start / stop the traces as debuglevel verbose already start the debug logs collection to the Active SM oss logs.\r\n===================\n\r\nThen collect the following logs.\n\r\nFrom active SM\r\n/var/mcp/oss/log/SM/all/MCP/SESMx_0\r\n/var/mcp/oss/log/SM/all/MCP/SESMx_1\r\n/var/mcp/oss/log/SM/all/MCP/PROV1_0\r\n/var/mcp/oss/log/SM/all/MCP/PROV2_0\n\n\r\n- A2 CALLP GPS team informed that no need to start / stop above traces.\n\r\n- Yuji collected the logs for 20 minutes timeframe but no clue has been found. We also captured the following outputs for a problematic user:\n\r\nfrom active SESM:\n\r\nDEBUG:root/>cd imdb\r\nDEBUG:root/imdb/>query userprofiledata USERNAME 61732168357@sip03.ipphone.optus.com.au\r\nDEBUG:root/imdb/>qsubr 61732168357@sip03.ipphone.optus.com.au\r\nDEBUG:root/imdb/>query systemprofile NAME adhocconf_profile_2  ---> for all system profiles seen from the output of first command.\n\r\n- A2 CallP GPS reviewed the imdb outputs and informed that; the problem is mixed system profiles; \n\r\nMore specifically, \"Advanced Screening\" has \"adhoc\" system profile as seen below:\n\n\r\n[UserName: 61732168357@sip03.ipphone.optus.com.au][Type: advscr][SystemProfileName: adhocconf_profile_2][DomainProfileName: null]\n\n\r\n- In order to understand if the problem is with the SESM synch or the data itself on the DB we should perform;\n\r\nSTOP both SESM instances at the same time.n\r\nStart just one of them and wait it to become Active\r\nThen start the other one to make if become HotStandby\n\r\n- Agreed and dropp off from the call that the customer could perform SESM stop/starts to correct the issue and the case will be investigated for RCA.\n\n\r\n===Additional Notes===\n\r\n- Customer performed the action to fix the SESM IMDB cache issue. However reported that they still have some users with same issue.\n\r\n- This is the same issue that was observed at Sunshine site (previously upgraded site) and resolved after upgrading to CVM16 (A2 14.1)\r\nHowever, after SESMs stop/start actions suggested by CALLP GPS team, my colleague Ege (A2 OAM GPS) found there are still some users \r\nwho have PA routing problems. They found one user again last night. His IMDB data was wrong. Even though IMDB is wrong, database is correct. \r\nEge verified this for problematic users.\r\n- Additionally, swacting SESMs didn\'t solve the problem completely. We had Salam Samoael from OPTUS to stop both instances of SESM1 and \r\nstart them one by one. We checked the problematic user data from IMDB after the SESM instances were restarted, and it was ok. \r\n- That\'s why OAM GPS thinks that there is a bigger problem with SESMs cache (IMDB), the upgrade procedure didnt cause this problem.\r\nBecause, we restarted both SESM instances completely at that night (Not swacting), but still customer has the problem. \n\r\n- CVM14 SESM needs to be checked. It could be related to only software (SESMs IMDB code). We think that the upgrade is not the main problem. \r\nas the upgrade only brought CVM14 SESM onto the table, and this problem started to occur with it.\n\r\n- Requested 14.0 DB backup created just after the upgrade from GTS Team (Yuji) to work for RCA in the Lab with CallP GPS team.\r\nCase is still under investigation for RCA investigation as well as workaround until site upgrades to CVM16 (14.1) load.\n\n\r\n==As an action moving forward:==\n\r\n1.	GPS has requested Optus to supply backup file (post14DBBackup.tar.gz) from Primary EM server.  (DB backup file created just after 14.0 Upgrade).   \r\nCurrently this has been requested to Optus by GTS. --> Requested for RCA work for 14.0 SESM\n\r\n2. GTS has advised them (in Parallel) to perform the work around suggestion steps(per Eges instructions) to solve the issue until\r\nsite upgrades to CVM16.  (refer to case notes for details)','null'),(705,'Samet Bilevci ( Netas External )','AS-OAM','2012-11-09','121109-370536','TELECABLE DE ASTURIAS S.A.U.','Guido Zijlstra from SWD team has paged me for A2 Upgrade Wizard issue during upgrade (from CVM13 to CVM15) on Telecable live site: \n\r\nUpgrade path: 12.0.12.4 (CVM13 - 7.0sp1) ---> 14.0.9.7 (CVM15 - 8.0BRC)\n\r\nGuido informed that the first half of the system is upgraded. When clicking NEXT in the wizard to start the OS patching on the secondary servers, \r\nthe Upgrade Wizard window totally locked-up and he had to close the window and then tried to restart it again. This did not work.\r\nWhen starting the wizard, he gets Another session exists for the user. When he force-out the other session, it says it is resuming from the last state, \r\nbut the wizard window never appears. After 5 minutes or so it shows the error : Upgrade wizard is unable to resume upgrade.\r\nHe had cleared the Java cache and restarted his machine, but no change.\r\nHe checked the secondary servers, and they are correctly running MCP_14.0.16 platform patch level.\n\r\n====Actions Taken to Resolve====\n\r\n- Accessed to site using VPN access provided by SWD team. Checked and confirmed that Secondary server\'s OS were actually patched to the 14.0.16 (ple1)\r\npatch level. So this explained that Wizard stopped monitoring and working, but the OS patching on secondary started and completed fine at background.\n\r\n- Accessed to customer DB to verify that current DB load (14.0.9.7 after upgrade) and checked wizard_state table for latest saved screen, \r\nverified that it is still on \"PATCH_SEC_PLATFORMS\" screen from DB.\n\r\n- Tried to launch wizard on Debug mode from my PC and got same pop-up as Guido did after a while, after force out the user:\n\r\n==========\n\r\nResume Error Occured:\r\nUpgrade Wizard is unable to resume upgrade, please try again.\r\nIf you see this error message more than once, please contact next level of support for further investigation.\n\r\n=========\n\n\r\n- Checked the Wizard logs on my local PC and the following is seen while trying to launch wizard:\n\r\n012-11-09 02:14:44,478  INFO UpgradeMainController - Wizard load: MCP_14.0.9.7_2012-04-06-1644\r\n2012-11-09 02:14:44,478  INFO UpgradeMainController - Wizard version: 3\r\n2012-11-09 02:14:44,478  INFO UpgradeMainController - Starting screen: PATCH SEC PLATFORMS\r\n2012-11-09 02:14:44,523 DEBUG ScriptManagerImpl - Start script request:\r\nScript Name:\'mcpRelease.pl\'\r\nServer Name:\'EMS1\'\r\nParameters:\'\'\r\n2012-11-09 02:17:45,971 DEBUG ScriptManagerImpl - Start script response:\r\nScript Name:\'mcpRelease.pl\'\r\nServer Name:\'EMS1\'\r\nResult:\'FAILED\', \'Error! EMS1_0@MCP_14.0 is currently being configured in another session.\'\r\n2012-11-09 02:17:45,972 ERROR LoadAnalyzerImpl - Error occurred during getting EMS1 server platform data: Error! EMS1_0@MCP_14.0 is currently being configured in another session.\r\n2012-11-09 02:17:45,973 DEBUG MessageView - Error Dialog : Resume Error Occurred, Upgrade Wizard is unable to resume upgrade, please try again.\r\nIf you see this error message more than once, please contact next level of support for further investigation.\n\n\r\n- Search on the error \"Error! EMS1_0@MCP_14.0 is currently being configured in another session. \" on previous cases and found this to be \r\nNED session issue which should be reolved with latest \"R\" status 14.0.14 (for 8.0BRC - ple1 patch level 14.0.18) and 14.1.4 (for 8.0SP1) Platform MRs that includes NED enhancements. (the upgrade to ple1 OS patch was 14.0.16)\n\r\n- Since the workaround is to restart NED process on primary EM server, I accessed EM server as \"root\" and performed the command:\n\r\n#neinit restart\n\r\n- After that Guido was able to launch the Wizard and continued / completed the upgrade. Agreed and drop off the call.\r\n- Guido informed that this site had the software delivered for a while already and the upgrade was delayed for other issues.\r\nThat is why we do not have the latest platform level.\n\n\r\n- Samet Bilevci\r\nA2 OAM GPS','null'),(706,'Samet Bilevci ( Netas External )','AS-OAM','2012-11-11','121110-370702','VTR Global Com SA','Earl Thomas from ER paged me on this case for the second time.\r\nHe said, after SM swacted to bring NCAS ONLINE / CONNECTED customer was reported provisioning and qsip Ok, but now he is still seeing the same\r\nQSIP issue although NCAS link is fine.\n\r\n==Actions Taken==\n\r\n- Accessed to site and worked with ER to understand the issue. Bounced NCAS link several times, double swacted SM and also swacted SESM instances, with no chance.\r\nER also swacted CMT units, thus restarted CMT applications. \n\r\n- Noticed that the SESM instances being seen as greyed out on MCP GUI (although they were up/working) issue was resolved after the actions performed.\n\r\n- Found out that qsip command works fine for the other users, but we only receive the original timeout error for the newly added users through OSSGate.\n\r\n- As soon as qsip command was run for newly added user, we see the following null expection on SM logs:\n\r\nSM_0 DEBUG 87 INFO NOV11 11:42:24:436 MCP_10.3.2.11\r\n  QSIP: QSIPRequestData[\n\r\n  SM_0 SWERR 801 MINOR NOV11 11:42:24:507 MCP_10.3.2.11\r\n  One or more Software Errors have occurred. The first one is captured below. Pl\r\nease collect the SWERs and the surrounding logs from the log file.\r\n  SWER Reported:\r\n  Task Runner exception:\r\n  priority=3\r\n  task=Task: com.nortelnetworks.mcp.ne.sm.ncas2k.qsip.QSIPTransaction@c77c8\r\n  State: run\r\n  java.lang.NullPointerException\n\n\r\n- When checking the old users, they exist on SESM IMDB cache (qsubr ) however, when we checked the new user, it did not\r\nexist on SESM IMDB. (Although it was added to SSL DB and Prov GUI)  this would explain the swerrs we see on SM oss logs.\n\r\n- Suggested ER to reboot the SESM server one by one, but requested to engage callp team for their suggestion on this. Agreed and drop off the call.\n\r\n- Also ,provided with two old Knova Solutions (attached to the case - documents 8773990 & 8883425) that were followed in order to clear the QSIP timeout issues, \r\nwhich were followed on old cases, for this EOL (SSL 10.3) load.\n\r\n- According to ER timeline; CallP GPS informed that a user needs to register to SESM for once, in order to be seen in SESM IMDB cache and they requested customer to \r\ncollect packet captures and SESM traces to continue investigate the user registration problem and ER agreed with CSAM that this case is not E2 anymore and could be worked\r\non during normal business hours.','null'),(707,'Samet Bilevci ( Netas External )','AS-OAM','2012-11-11','121110-370702','VTR Global Com SA','Bradley Hetzel has paged for sip lines unable to qsip and provisioning issue.\r\nAlso reported that SESM servers was seen as greyed out on MCP GUI > Logical View and no alarms displayed.\n\r\nSite Load: CVM12 - SSL 10.3.2.11 (EOL Release)\n\r\nThis E2 case originally reported for new sip lines unable to register SSL SESM and CALLP GPS team already engaged on that and informed that, \r\nsince no REGISTER message arrives into SESM, it can not be processed and nothing related with that user seen in the SESM logs, so they requested\r\npacket captures from client and SESM to further troubleshoot the issue.\n\r\nLater, customer called into ER and reported that SM0 did not come back up after they tried rebooting it. ER porwer cycled SM0 and able to log into both SM0 and\r\nSM1 server and paged me for OAM issues. Customer now reported that they are unable to provision and unable to qsip from all users are getting the following output while they are able to\r\nsee \"qdn\" output for users.\n\n\r\nSIP DATA CAN NOT BE DISPLAYED DUE TO RESPONSE TIMEOUT FROM CS2000 SESSION SERVER\n\n\n\r\nActions taken to resolve the problem\n\n\r\nER shared access through VMWare.\r\nGPS informed ER that on old loads such as customer\'s current load(10.3), qsip command works thru NCAS link and below is how qsip works:\n\r\nCMT/ OSSGate (input) --> SSL SM ---> SSL SESM  ----> SSL SM ---> CMT /OSSGate (qsip output)\n\n\r\n- Launched MCP GUI (customer & ER with shared remote session) to check the NCAS link status under NE Maintenance > System Manager > CS2000 Integration and even there were no NCAS link seen there.\r\n(NCAS link is not visible on SM GUI)\r\nInformed ER and customer that, NCAS link should be off even if there is a connection problem, but we even did not not see it\'s status.\n\r\nSince customer is not able to perform QSIP and getting timeout, explained it\'s normal if NCAS link is not working.\n\r\n- Also, explained customer: although SESM instances seen as greyed out, we can see them ONLINE/Hot-Stanby under MCP GUI > NE Maintenance > Session Manager\r\nso this is a seperate snmp related issue and would not impact anything.\n\r\n- Request customer approval to perform SM Swact to see if it restores NCAS link visibility on MCP GUI.\n\r\n- After performing SM Swact, we could observe the NCAS and it\'s status as ONLINE / CONNECTED.\n\r\n- Customer reported that they could do provisioning and qsip nor works Ok, however registration issue (original problem) still ongoing.\n\r\n- Agreed with ER that, they would continue working the original registration issue by collecting wireshark traces requested by CallP team, and drop off from call.','null'),(708,'Samet Bilevci ( Netas External )','AS-OAM','2012-11-05','121105-369664','Singtel Optus Pty Ltd','Eric Duke from SWD team has paged me for provisioning issue through CMT OSSGate, after CMT has been upgraded from CVM13 to CVM14 and \r\nA2 still at 12.0.6.16\n\r\nCustomer was getting the following error while adding sip line:\n\r\n=============\r\nEndPoint can not be added to GateWay. System:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing \r\nrequest on MCS-EM for GW:EP O4AT-SSL01-VMG00:LS00/000/1/1007 Details:  Cannot perform operation. OPI.addUser.002\r\nSIP_PACKAGE=package-00\r\n============\n\n\r\nThey already performed the Bulletin:20120705063, Rev 1  - Required actions to prevent SIP line provisioning failures during CVM13 to CVM14 upgrade, However still having the issue.\n\n\n\r\nSince the problem reported after CMT upgrade, SWD team also engaged CMT GPS team.\r\nCMT GPS team reviewed the CMT logs and noticed the following:\n\r\n12.11.05 23:54:24.327 MAJ (MCSEMProxy) [####MCSEMProxy-request_1####] AddTerminationRequest_10_3:rocessRequest - Failed. Check for duplicate request...\r\n12.11.05 23:54:24.327 VRB (MCSEMProxy) [####MCSEMProxy-request_1####] MCSEMRequestTimer::update -- Response received.\n\r\nand on error message as follows: \n\r\nDetails:  Cannot perform operation. OPI.addUser.002\r\nSIP_PACKAGE=package-00\n\n\r\nthey found the following previous case from same customer (OPTUS) which is exactly same issue:\n\r\nCase# 110116-003022 - MJ:OPTUS:CVM13:OSSGATE Unable to provision lines on A2E 12.0.6.0\n\r\nhttps://na4.salesforce.com/5006000000AV9I6?srPos=2&srKp=00a\n\n\r\nFor the previous case, there was CR created to make the error message detailed to point out the lying problem:\n\r\nAfter the CR fix, it would indicate the following:\n\r\n=======\r\n\"Service set () has invalid service profiles (), please correct service set and then try again.\"\r\n=======\r\n(provided for 8.0BRC onward. Not exist on customer\'s current load - 7.0sp1)\n\r\nAnd, the following note was also added to Prov Client NTP document (N48111-125) to highlight this issue and how to fix:\n\r\n===============\r\nPlease note that:\r\nWhen a system profile\'s name is changed:\r\nThe service set ,which the system profile is assigned to, must be updated according to new name of the system profile manually.\r\nIf the service set is not updated after the name of system profile is changed, the users which have already \r\nbeen added with the associated service set can not be modified. And new users with the service set\r\ncan not be added.\r\n================\n\r\nSteps to resolve the issue:\n\r\n- Customer indicated that they changed the service set name of these users from \"package\" to \"package-00\", they did it prior to the CMT upgrade \r\nand it was changed because of the A2 MCP14.0 upgrade coming up as per they were told.\r\nBut they havent applied it to the existing users yet.  (yes so its been changed there but the save button has not been pressed yet)\n\r\n- As per above previous case / CR details with same issue, we suggested applying the changes (save the new service set name) on PROV client.\r\nAfter this action, customer reported everything is all good now. Eric opened case:  121105-369664. \r\nAgreed and drop off the call.','null'),(709,'Tugrul Timorci (NETAS External)','AS-OAM','2012-10-16','121016-366021','VTR Global Com SA','Tony Called me for SESM server Platform Upgrade issue. Path is 12.0.12.4 to 14.0.9.11\n\r\nI logged in to the system and checked the problem. Problem raised after network drop of the upgrade wizard PC. During the platform patch of the SESM server network dropped and Upgrade wizard did not start the platform Upgrade. \n\r\nI Upgraded the SESM server manually and after restarted the UW. UW started work normally and Tony continued to upgrade. \r\nAfter Agreement I dropped the call','null'),(710,'Ken Johnson','AS-OAM','2012-10-30','TBD','BT Gov','Upgrade prep failed on all servers due to missing files.  Issue caught by upgrade wizard preChecks.  Unpacked vaulted image to restore missing packages (tcpdump and JRE 6u16).  After restoring files upgrade precheck succeeded.','null'),(711,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-10-16','121016-366009','VTR Global Com SA','Problem Description:\r\n   Tony had paged me about \"Database upgrade failed during wizard upgrade\". He said me that the wizard gave me error on Upgrading DB and SM instance screen.\n\r\nMCP Load:\r\nUpgrade from 12.0.12.4 to 14.0.9.11\n\r\nSolution of Problem:\r\n   I have connected to site over VPN and checked the wizard. The wizard was failing with below logs; \r\n===================================== \r\nValidating the scripts and files required for upgrade. \r\nDB Type in Props File should be set as SINGLE. \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/mcpUpgrade/ut_mcpUpgrade.EMS1_366601d4-2a19-1b21-b380-001b21182591_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20121015_233055.log \n\r\nWrong DB Type in Props File \r\nDB & SM upgrade failed. \r\n===================================== \r\n   I have changed the DB type said in the logs and clicked the retry button it had failed again. Then, I remembered that this site was being upgraded last week and due to there were limited time for MW, we had cut the upgrade and do resync operation manually. That is why upgrade wizard was taking this problem because upgrade wizard was waiting the replication dropped but it still behaves as resynced. Then, I had dropped the resync manually, then clicked the retry button, then wizard passed the screen and DB and SM had been upgraded successfully. After that I have left from conf.','null'),(712,'Tugrul Timorci (NETAS External)','AS-OAM','2012-10-12','121012-365484','VTR','Eric Duke called me for Upgrade Wizard issue. Upgrade path is 12 to 14.0.9.11\n\r\nI logged in to the server and checked the system . upgrade wizard was failing at cleanupreplication step. I manually dropped the replication but it failed again. NED restarted on both of EM server but failed again with below error\n\r\n00:/var/mcp/upgrade_tools/logs/prepareDB[root@HCISSSLM-00 prepareDB]# cat ut_prepareDB.EMS1_366601d4-2a19-1b21-b380-001b21182591_ut_prepareDB.pl_PREPARE_DB_3.20121011_23\r\n0601.log\n\r\n  Started at  =>  Thu Oct 11 23:06:02 2012\r\nScript invoked with: -bk preUpgradeMCP_14.0.9.11_2012-10-05-0857 -cr\r\n23:06:02 Starting to edit the state file\r\nRead initial properties from: /var/mcp/install/installprops.txt\r\nFound ne.load\r\nFound db.host\r\nFound db.secHost\r\nFound db.neName\r\nFound db.type\r\nFound db.backup\r\nGID = 92 and stats[5] = 92\r\nDirectory permissions are = rwx, rwx, --- \r\n GID = 92 and stats[5] = 92\r\nDirectory permissions are = rwx, rwx, --- \r\nclearRep command: /var/mcp/upgrade_tools/bin/ut_cleanupReplication.pl -nc -m EMS1_366601d4-2a19-1b21-b380-001b21182591_ut_prepareDB.pl_PREPARE_DB_3 -s 192.168.85.22\r\n*** Cleanup Replication FAILED ***\r\nSee Cleanup Replication log for details\n\n\r\nPrepare Database FAILED\r\n  Issuing ned command \"put 192.168.85.22:2100 /var/mcp/upgrade_tools/work/EMS1_366601d4-2a19-1b21-b380-001b21182591_ut_prepareDB.pl_PREPARE_DB_3.state ../../../../../var/mcp/upgrade_too\r\nls/work/EMS1_366601d4-2a19-1b21-b380-001b21182591_ut_prepareDB.pl_PREPARE_DB_3.state ntappsw:dummyPassword cd2d3d7c9cae24ac482026cdf7b7b8e6 3600\"  \r\n 23:06:06 Cleaning ned session files\r\n23:06:08 Running ned cmd: /bin/rm -rf /var/mcp/run/dummyRel.prepareDB\n\n\n\r\nI engaged the Design team for further investigation. Design used workaround solution and passed this error. We communicate to customer and inform us they have 1 hour to complate MW. They do not want to upgrade and wanted reschedule. \n\r\nResync operation applied and system seems at the beginning. \n\r\nI created a jira issue for further investigation. After agreement I left the conference.','null'),(713,'Tugrul Timorci (NETAS External)','AS-OAM','2012-10-11','121011-365243','Emirates Integrated Telcommunications','Rodney called me SM GUI issue. He told me GUI seen grey and does not give response\r\nSystem is working on 14.0.9.7 load.\n\r\nI logged in to the site and checked the SM server. SM1 was active and GUI was not give response. I swact the SM1 and SM0 activated . I logged in to GUI and checked.  GUI was show the alarms and it started to work normally. I started the secondary SM and started successfully.\n\r\nI checked the system for RCA and saw SESM2_0 server continuously pushing alarm message. I suggested to call CALLP pager. CALLP engaged to the pager call started to investigation. There was no A2 OAM related issue. After agreement I dropped the call','null'),(714,'Tugrul Timorci (NETAS External)','AS-OAM','2012-10-08','121008-364321','PalTel Ramallah','Wesley Martin Called me for JMXM 200 alarm. Customer load is 14.0.9.0 \n\r\nCustomer saw memory heap alarm on SM0 server. I executed the top command on SM server. I checked there only Java process with high level.I double swacted the SMs and alarm cleared. \n\r\nAfter agreement I dropped the call. \n\r\nThis issue fixed on 14.0.9.11 patch load. Customer needs to apply this patch and this patch will publish today.','null'),(715,'Samet Bilevci ( Netas External )','AS-OAM','2012-09-25','120925-362119','BT PLC (Manchester)','Camila from SWD team has paged me during Upgrade Wizard prep. step execution at BT site. \n\r\nUpgrade path:  from 12.0.12.X to 14.0.9.X\n\n\r\nCamila informed me that SM_0 was made active SM instance in order to be able to start Upgrade Wizard for Prep steps execution. However, System Managers swacted three times autonomously during the A2 prep. \n\r\n- First swact observed Wizard initial steps. Wizard returned message indicating unknown failure happened. When re-launching the wizard,\'A generic failure occured\'. She noticed the SMs had swacted. SMs were swacted back to proceed with wizard execution. \r\n- However, two more swacts occurred during \"Backup Servers\" screen and following error was returned at wizard: \'Monitoring script failed\". Each time, she had to swact back to SM_0 to continue with Wizard and was able to successfuly get to \"DB MOCK Upgrade\" screen (which is the last Prep step, before actual upgrade)\n\n\r\n- Since this is Wizard Prep. steps, I suggested Camila to postpone DB MOCK step for now to make sure system is stable. I requested her to open a case, capture the following logs from EM1 (SM_0) and EM2 (SM_1) servers and attach them to the case, so that we could investigate and monitor the system the next morning.\n\r\n/var/mcp/oss/log/SM/all/MCP/SM_0\r\n/var/mcp/oss/log/SM/all/MCP/SM_1\r\n/var/mcp/run/MCP_12.0/SM_0/work/SM.log   (on EMserver1)\r\n/var/mcp/run/MCP_12.0/SM_1/work/SM.log   (on Emserver2)\r\n/var/log/messages\r\noutputs of , top, ps -ef, uptime, mcpRelease.pl, df -k\n\r\n- No further SM swacts have been observed at site. \r\nCase:120925-362119 have been opened to investigate and for suggested preventative action.\n\r\nAgreed with SWD team that logs would be reviewed after a while again within the case reported, and SWD team would be advised to proceed, and then drop off the call.','null'),(716,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-09-21','120921-361737','TSTT','Problem Description:\r\n   Timothy had paged about DbSetup.pl script had failed while running mcpPatch.pl script.\n\r\nMCP Load:\r\nUpgrade from 12.0.6.6 to 12.0.6.16\n\r\nSolution of Problem:\r\n   I had connected to site and check the mcpPatch and dbSetup logs and I saw that dbSetup script was failing because of one sql within the code. I had found the this sql which was;\n\n<br />\r\n   exec mcsdb_utl.modify_column(\'DOMAINISN\', \'SUPPORTED_SIP_URI\', \'DEFAULT 0 NOT NULL\');<br />\r\n   exec mcsdb_utl.add_check(\'DOMAINISN\', \'CK3_DOMAINISN\', \'(SUPPORTED_SIP_URI IN (-1, 0, 1, 2, 3))\');<br />\r\n\n\r\nWhile this sql was being run the first line (exec mcsdb_utl.modify_column(\'DOMAINISN\', \'SUPPORTED_SIP_URI\', \'DEFAULT 0 NOT NULL\');) was failing because of the secondary database\'s SUPPORTED_SIP_URI table has \'null\' values. I had notices this than I have checked synchronization between primary and secondary since primary database has no \'null\' value on that table while secondary has. I had run \'testReplication.pl\' and it failed. Then, I took necessary permission from customer and dropped the replication with cleanupReplication.pl and then make the synchronization again with resync.pl script. After synchronization was set again, I had run mcpPatch again and it worked. Then, I had left from the call.','null'),(717,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-09-20','120920-361533','TELENET N.V.','Problem Description:\r\n   Thomas had paged me about 100% CPU occupancy problem occured on secondary EM server.EMServer2 has a critical alarm CPU Occupancy threshold reached.\n\r\nMCP Load:\r\n   MCP_14.0.9.3\n\r\nSolution of Problem:\r\n   I had connected to site and check the oracle and platform levels first of all, both primary and secondary servers was seeming in the correct levels. Then I had run \'top\' command on secondary server and I saw that there are too many separate oracle process running on this server. Actually, these process were forcing the server to throw CPU occupancy alarm. I had said these processes to customer and I wanted to restart oracle database. Due to this server was secondary, there would not be problem for us related with call processing side. The customer gave me permission to restart db and I had restarted db with startDatabase.pl and stopDatabase.pl scripts under /opt/mcp/db/bin directory with ntdbadm user. After restart operation, oracle processes were gone and everything fixed. After that CPU alarm was also gone and I dropped the call.','null'),(718,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-09-18','120918-360985','Axtel','Problem Description:\r\nJeff had paged me about all SESM servers and BCP went to a configured state.\n\r\nMCP Load:\r\n12.0.6.13\n\r\nSolution of Problem:\r\nI had connected to site and launched MCP GUI. Saw that all SESM and BCP servers were stuck in configured state. I had killed the secondary SM instance and deploy it again from MCP GUI. Then, I had started to secondary SM instance. After SM swact many instance were fixed automatically and the problematic remaining instance were  killed, deployed and started with the same way. Then, I had dropped the call since the customer agreed that the problem was resolved.','null'),(719,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-09-17','120917-360789','VTL (UK) LTD','Problem Description:\r\nBulent had paged me about provision does not work from QSIP and registration is failing but it works when it comes from core and the A2 side.\n\r\nMCP Load:\r\nMCP_14.0.9.x\n\r\nSolution of Problem:\r\nI had connected to side and checked that provisioning gui is working properly(whether the lines can be registered or not). I had added and deleted the new record. I said that PROV GUI is working and I need to check SESM imdb that if it has the problematic user in it. I had connected to SESM imdb and noticed that problematic user did not involve in SESM imdb but there is other users in imdb so I suggested SESM swact to Bulent in order to make SESM refreshed and synced with PROV\'s users (I mean database). After double swact of SESM, the problem was resolved and dropped the call.','null'),(720,'Senem Gultekin (NETAS External)','AS-OAM','2012-09-10','120906-358780','Optus','Problem Description:\r\nUpgrade Wizard failed at the pre upgrade steps for load extraction for HT Langley platform.\r\n14.0.9.8 to 14.1.0.8 patch and the platforms are 14.1.2 Platform Only MR.\n\n\r\nSolution:\n\r\nIn the loadlineup file there are 3 different MR versions that can be chosen, 14.1.0, 14.1.1 and 14.1.2. Since the new HT Langley load is for 14.1.2 MR (14.1.10 Platform), during the wizard pre steps it should have been selected as 14.1.2. But in the drop down didnt show up because direct platform upgrade is supported starting with 14.0.9.9 patch.\r\nThis issue was not seen in the Optus lab, as far as I know there is no HT Langley in the labs, and HT Langley was not tested. Theres no change for cc3310 platforms, all of them 14.1.9 for 14.1.0, 14.1.1 and 14.1.2 MRs. Only change is in HT Langley platform.\n\r\nThe official loadlineup file;\n\r\nFor 14.1.0 MR section is;\n\n\n\n\n14.1.9\n14.1.9 ->modified this as 14.1.10\n14.1.9\n\r\nWhere 14.1.2 MR section is;\n\n','null'),(721,'Ken Johnson','AS-OAM','2012-08-29',' 120830-357648 ','BT','A2 Upgrade Wizard reported a failure while applying platform patch 14.0.4.  Part of the patch updated NED which was removed and upgraded ok, but then failed to start (1).  GPS manually restarted NED after ensuring it was stopped which enabled the patch to continue application.\r\n  Following the completion of the manual NED patching activity, the patch apply was retried to complete the remaining patch activities.  However this failed as `mcpRelease.pl` could not be located.  Applying the patch outside of the Upgrade Wizard (2) was successful and completed without issue.\r\n  With the patch manually applied the Upgrade Wizard was restarted to allow the upgrade to continue.\n\r\n  Design Sustainment will be engaged to address these two failures:\r\n   - 14.0.4 fails to restart NED,\r\n   - mcpRelease.pl not found when patch apply initiated via the upgrade wizard.','null'),(722,'Ken Johnson','AS-OAM','2012-08-22','120823-356127','SuddenLink','Upgrading A2 to 8.0.SP1 the 14.1.0.5 MCP database schema failed its dry run upgrade test.  Three different schema corrections were made which resolved the \"Schema mismatch(es) is(are) found\" failures, but exposed a subsequent fault which generated a \"Fails to create duplicated schema from currently deployed schema\" error.  This too prevented the upgrade from progressing.  With the close of shift the upgrade was suspended to allow this database and several other issues to be addressed.','null'),(723,'Ken Johnson','AS-OAM','2012-08-20','120821-355596','Cable Onda','This site experienced a hardware fault during an upgrade which halted the upgrade for 3 weeks while the server was replaced, and reReplaced.  After recovering the failed SESM server the upgrade was to continue.  However there were several questions as to the best way to proceed given the extended upgrade suspension.  Recommended the DB be brought back into sync with the active 10.3 From side, reSplit, and then upgraded.  However this failed and so the upgrade proceeded without resyncing first.  This also failed as critical database files were missing from the standby To side unit.  At this point the upgrade was put on hold for extended recovery of the To side Database.','null'),(724,'Ken Johnson','AS-OAM','2012-08-20','120821-355589','Suddenlink','A2 Upgrade Wizard reported an error while validating the scripts and files required for upgrade.  Logs reported that the ne.config value was not an expected string.\r\n  Changing the /var/mcp/install/installprops.txt ne.config value from \"CS2K-A2_Medium_TIGH2U\" to \"CS2K-A2E_Medium_TIGH2U\" corrected the issue allowing the upgrade to continue.','null'),(725,'Ken Johnson','AS-OAM','2012-08-20','120821-355597','BT Gov','While performing backups via the Upgrade Wizard the HotStandby SM determined that the Active SM had gone down and became active.  This split brain condition causes the previously active SM to restart, which failed during initialization, which caused it to restart, which failed during initialization, which caused it to restart (LOOP).  This caused several rouge instances of the SM and drive up CPU utilization to the point the applications could not be stopped till manually killed to recover and stabilize the system.\r\n  When the Upgrade Wizard was retried the same behavior was produced which swacted the SM and took down the standby SM.  Logs revealed that the swact happened because secondary system manager could not communicate to primary system manager during the backup.\r\n  The Upgrade Wizard then failed to operate which required NED to be restarted on virtually all NEs.  After which the Upgrade Wizard was forced to a previous step, the backups purged and retaken transferring one one backup at a time.  This allowed the backups to complete successfully and the upgrade to continue.','null'),(726,'Ken Johnson','AS-OAM','2012-08-20','120821-355788','suddenlink','Several missing platform files prevented the preUpgrade validation.  These files were manually restored to the appropriate locations to allow the upgrade to continue.  Shortly thereafter, the Upgrade Wizard failed to perform various tasks causing numerous operations to fail on various nodes.  NED was restarted on both SMs and SESM Servers to allow the upgrade to continue.','null'),(727,'Ken Johnson','AS-OAM','2012-08-20','120821-355788','suddenlink','Nine of eleven mirrors on Disk 0 (sda) had fallen out of sync which caused the upgrade preCheck to fail. After checking for hardware problems / logs, I dropped disk 0 and reAdded it to the system which repartitioned the disk and added each partition to the appropriate mirror. One and a half hours later the disk completed syncing and became redundant which allowed the upgrade to continue.','null'),(728,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-08-17','120816-354960','VTR Global Com SA','Problem Description:\r\n    Thomas has paged me about new added sip lines cannot able to qsip properly.\n\r\n    The error was like below;\n\r\n    THE NEW SIP LINES\r\n    >qsip ssl0 00 1 0 6\r\n    SIP DATA CANNOT BE DISPLAYED DUE TO RESPONSE TIMEOUT FROM CS2000 SESSION SERVER \r\n    >\n\r\n    >qsip 8868171\r\n    SIP DATA CANNOT BE DISPLAYED DUE TO RESPONSE TIMEOUT FROM CS2000 SESSION SERVER\n\r\n    THE OLD SIP LINES\r\n    >qsip 9348164\r\n    SIP DATA CANNOT BE DISPLAYED DUE TO RESPONSE TIMEOUT FROM CS2000 SESSION SERVER \r\n    >qsip 9348164 now is ok   \n\r\nMCP Load:\r\n10.3.2.X (CVM11)\n\r\nSolution of Problem:\r\n   I have connected to site with VPN and check the qsip commands\' and MCS GUI\'s alarms logs. There were one SWER on SM_0 instance and below execption on oss logs;\n\r\nSM_0 SWERR 799 ALERT AUG17 00:50:27:356 MCP_10.3.2.11\r\n  Send Qsip response failed.\r\n  java.lang.RuntimeException: Send Qsip Response in JNICALL failed\r\n        at com.nortelnetworks.mcp.ne.sm.ncas2k.scplite.Scplite.processClientMessage(Native Method)\r\n        at com.nortelnetworks.mcp.ne.sm.ncas2k.scplite.Scplite.send(Scplite.java:220)\r\n        at com.nortelnetworks.mcp.ne.sm.ncas2k.link.NCASLink.send(NCASLink.java:336)\r\n        at com.nortelnetworks.mcp.ne.sm.ncas2k.qsip.QSIPTransaction.handleError(QSIPTransaction:208)\r\n        at com.nortelnetworks.mcp.ne.sm.ncas2k.qsip.QSIPTransaction.handleQSIPRequestEvent(QSIP.java:170)\r\n        at com.nortelnetworks.mcp.ne.sm.ncas2k.qsip.QSIPDebugger$QSIPDebugCmd.execute(QSIPDebug.java:82)\r\n        at com.nortelnetworks.mcp.base.debug.DebugCommand.execute(DebugCommand.java:83)\r\n        at com.nortelnetworks.mcp.base.debug.DebugSession.run(DebugSession.java:216)\r\n        at java.lang.Thread.run(Unknown Source)\n\r\nI have investigated the exception and found similar issues on SFDC that were solved with SESM and DB restart. I suggest to customer restart the SESM instances firstly. Since they are on MW, they did SESM swact. Then, I got necessary permissions and restarted DB and its services. After that Thomas warned me that provisioning is working now properly. I agreed to leave.','null'),(729,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-08-17','120816-354960','VTR Global Com SA','Problem Description:\r\n   Salvador has paged me about provisioning sip lines down. He said that the customer cannot able to do provision new SIP lines using OSSGate.\n\r\n    >new $ 886945 ibn IBNstgo 0 64 002 SSLO 00 6 02 79 dgt ddn SIP_DATA+\r\n    >SIP_PACKAGE premium SIP_URI 8869945@maip-voip.ctr.net+\r\n    >SIP_CLIENT_TYPE sip_line SIP_LOCATION maip2 SIP_PASSWD 20230226 +\r\n    >$ DPL y 2 $\n\r\n    Details: Cannot Perform operation. No data source(s) available at this time.   \n\r\nMCP Load:\r\n10.3.2.X (CVM11)\n\r\nSolution of Problem:\r\n    I have connected to customer\'s site and wanted to connect PROV client. After I have connected to PROV client, I saw that adding new user was also not working. Then, I opened MCS GUI and se the SM instances. Then, I noticed that SM0 seemed as Unavailable. Then, I ping the SM0 server and it was not pingable. I have asked why this server is down. Database is on this server and it should be up in every time. Customer did not able to know why this was down. I suggest them to activate this server. After SM0 came Up and running, I said to Salvador and Gerardo Luna Torres to try it again. And it worked. Then, I agreed to leave.','null'),(730,'Ege Varhan (NETAS External)','AS-OAM','2012-08-09','120813-353886','VTR Global Com SA','Site Type: SSL\r\nUpgrade Path: 12.0.12.2 to 14.0.9.0\n\r\nProblem Description\r\n------------------------\r\nDuring the post backup process of the upgrade, Upgrade Wizard produced a timeout error while taking and transferring the backups to EMServer2\n\r\nSolution Description\r\n------------------------\r\nWe have connected to the site and checked the logs. Everything was OK. But  there was still an error on Upgrade Wizard. We have closed Upgrade Wizard GUI with \"Save&Close\" option and re-launched it. The failure/error was disappeared after re-launching Upgrade Wizard. Because, the backup process was completed in the background successfully. \n\r\nUpgrade Wizard has predetermined timeout values for each operations. If the target operation is not completed within predetermined time, Upgrade Wizard simply shows timeout error even if the operation completes successfully later. So, this was expected problem since the servers of the customer were CC3310, and also the network speed was slow.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(731,'Ege Varhan (NETAS External)','AS-OAM','2012-08-10','120810-353509','VTR Global Com SA','Customer: VTR Global Com SA\r\nA2 Version: 14.0.9.9 via patching from 14.0.9.0\r\nSite Type: SSL (CVM14)\n\r\nProblem Description\r\n-----------------------\r\nThe customer has claimed that they are not able to perform SSL provisioning after upgrading A2 from 14.0.9.0 MR to 14.0.9.9 patch. When they tried to add a new line, they received the error message below;\n\r\n> new $ 9833000 ibn IBNSTGO 0 0 002 SSL0 01 1 06 00 dgt SIP_DATA + \r\n> SIP_PACKAGE premium SIP_URI 9833000@lflh-voip.vtr.net SIP_CLIENT_TYPE + \r\n> sip_line SIP_LOCATION lflh SIP_PASSWD 9833000 $ DPL y 3 AIN MZ02_LNP 3wc $ \n\r\nSystem:LineProv; EndPoint can not be added to GateWay. \r\nSystem:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP VMGHFLOSSLI:SSL0/001/1/0600 \r\nDetails: SIP Profile ID not found for Profile sip_line \n\r\nSolution Description\r\n-------------------------------\r\nThey were using incorrect SIP Profile name in the new command of OSSGate. The name of incorrect profile was \"SIP_LINE\". Only alpha numeric characters are allowed in SIP Profile names. Because empty space and other non-alpha numeric characters cause some problems on Session Manager. That\'s why the sip profiles which have incorrect characters are modified during the upgrade from 14.0.9.0 to 14.0.9.9. In customer\'s case, \"SIP_LINE\" profile was modified to \"SIPLINE\". Underscore character was omitted by the upgrade script. However, the customer was still using the old name in his OSSGate command. We have informed the customer about the change, and he was able to perform provisioning with the correct name of the sip profile which is \"SIPPROFILE\".\n\r\nThose behavior and change are already documented. Also, Upgrade Wizard informs / warns the applicator about that change at the end of the upgrade.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(732,'Ege Varhan (NETAS External)','AS-OAM','2012-08-10','120810-353584','Cable Onda','Customer: Cable Onda\r\nA2 Version: 14.0.9.4\r\nSESM Version: sesm_13_ab20_0\n\r\nProblem Description\r\n-------------------\r\nThe customer was not able to perform SSL provisioning and they had to add some lines urgently into the system.\n\r\nSolution Description\r\n--------------------\r\nWhen i connected to the site, i couldn\'t telnet into OSSGate to re-produce the provisioning problem specified in the case notes, because MI2 system was not running. I have tried to restart SESM Service in order to make MI2 running, but it didn\'t work.\n\r\nI have checked \"sesm.properties\" file and realized that \"MCPVersion\" paramater was set to \"14.1\", but in that release, there are no jar files which are bound to \"14.1\". I have corrected it and changed from \"14.1\" to \"14.0\". I have restarted SESM Service again after modifying \"sesm.properties\" file of CMT. This time MI2 service was started successfully. I have logged in OSSGate and tested the provisioning, and it worked. I have informed the customer to test the provisioning and the customer confirmed that it is working.\n\r\nCMT00-unit0(active):/> ptmctl status\r\nSESM STATUS ---------------------------\n\r\n  COMPONENT           STATUS\r\n  ---------           ------\r\n RMI Registry          RUNNING\r\n Snmpfactory           RUNNING\r\n MI2 Server            RUNNING\n\r\n  Current number of SESM processes running: 3 (of 3 )\n\r\n  SESM APPLICATION STATUS: All Applications ready\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(733,'Ege Varhan (NETAS External)','AS-OAM','2012-08-08','120808-352970','VTR Global Com SA','Customer: VTR Global Com SA (Andres Pinilla - andres.pinilla@vtr.cl)\r\nUpgrade Path: 12.0.12.2 (CVM13) -> 14.0.9.0 (CVM15)\r\nCMT /SESM: CVM14 \r\nSite Type: SSL\r\nSWD (Applicator): Tony Pittman\n\r\nProblem Description\r\n--------------------------------\r\nWe have completed the primary side of A2 upgrade. We had just enough time to complete the upgrade of the secondary side and we started to upgrade secondary side of A2 system. While Upgrade Wizard was patching operating systems of the secondary servers, it received the error message below on EMServer2;\n\r\n------------------------------------------------------------------------\r\n2012-08-08 14:33:26,931 DEBUG ScriptManagerImpl - Start script response:\r\nScript Name:\'mcpRelease.pl\'\r\nServer Name:\'EMS2\'\r\nResult:\'FAILED\', \'Command exited with error. Command: perl /var/mcp/upgrade_tools/bin/ut_runSimpleScript.pl -prg mcpRelease.pl   Exit code: 255failed to execute mcpRelease.pl: No such file or directory \'\r\n2012-08-08 14:33:26,931 ERROR LoadAnalyzerImpl - Error occurred during getting EMS2 server platform data: Command exited with error.\r\nCommand: perl /var/mcp/upgrade_tools/bin/ut_runSimpleScript.pl -prg mcpRelease.pl  \r\nExit code: 255failed to execute mcpRelease.pl: No such file or directory\r\n------------------------------------------------------------------------\n\r\nSolution Description\r\n------------------------------------------------------------------------\r\nI have told Tony Pittman to stop the secondary side\'s upgrade at that step and continue in the next M.W. since we didn\'t have enough time to investigate both the problem and complete the upgrade. The customer was informed and agreed about the action plan. So, the secondary part of the upgrade was stopped.\n\n\r\nDuring the day, we had investigated the problem and we have realized that it was a NED (Network Element Daemon) problem. It is a known problem and a JIRA CR has already been opened to address that issue. The CR ID is AAK-27164.\n\r\nWe have patched the platform of EMServer2 manually and set up Upgrade Wizard and A2 System ready for the next M.W. Tony Pittman and the customer have already been informed to continue to upgrade in the next M.W. via case notes of Salesforce CRM.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(734,'Ege Varhan (NETAS External)','AS-OAM','2012-08-08','120802-351992','VTR Global Com SA','Customer: VTR Global Com SA (Andres Pinilla - andres.pinilla@vtr.cl)\r\nUpgrade Path: 12.0.12.2 (CVM13) -> 14.0.9.0 (CVM15)\r\nCMT /SESM: CVM14 \r\nSite Type: SSL\r\nSWD (Applicator): Tony Pittman\n\r\nWe were called by Tony Pittman after the primary side of A2 system was upgraded / completed. Tony told us that the customer received provisioning errors during the provisioning tests on OSSGate. We have connected to the active CMT and realized that it was on CVM14 load. But, it\'s primary OPI address was pointed to an A2 Provisioning Server which is still on CVM13. We have reconfigured CMT\'s OPI and OMI configurations and pointed them to upgraded NEs(Network Elements) which are on CVM14 (14.0.9.0) in order to make them communicated between each others. The customer started to provision again after the configuration was changed and SESM was rebooted.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(735,'Ege Varhan (NETAS External)','AS-OAM','2012-08-08','120808-352919','VTR Global Com SA','Customer: VTR Global Com SA (Andres Pinilla - andres.pinilla@vtr.cl)\r\nUpgrade Path: 12.0.12.2 -> 14.0.9.0\r\nSite Type: SSL\r\nSWD (Applicator): Tony Pittman\n\r\nI was paged by Tony Pittman (SWD) about the timeout error occured which Upgrade Wizard produced during DB-Mock Upgrade step.\n\r\nI have connected to the site and checked the status of DB-Dry Run activity from the activity logs. Even though Upgrade Wizard showed time-out error, the activity was completed successfully in the background. We have re-launched Upgrade Wizard and the problem was solved. I stayed with Tony until the upgrade of the primary side was completed just in case.\n\r\nThere are predetermined time for each activities(Backup database, Patching OS etc.) on Upgrade Wizard. If the target activity is not finished within the predetermined time, Upgrade Wizard shows time-out error to inform / warn the applicator about the possible problems that could occur during the upgrade.\n\r\nCurrently we are discussing with the design team if the predetermined time for DB-Dry Run activity is enough.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(736,'Senem Gultekin (NETAS External)','AS-OAM','2012-08-07','120807-352720','VTR Global','Problem Description:\n\r\nSWD contacted me for an upgrade from 10.3.1 to 10.3.2 issue seen at VTR live customer.  mcpUpgradeMR.pl script was stuck at Stopping SM_1 section.\n\r\nSolution:\r\n-	Requested from SWD to restart NED for both SM servers.\r\n-	Script was still at the same point.  \r\n-	Accessed to the site and checked the databases, there were already upgraded to 10.3.2.\r\n-	Killed the mcpUpgradeMR.pl script and started again. It finished successfully.\r\n-	Upgrade was performed successfully.\r\n-	Since 10.3 is an EOL release, no RCA will be given.','null'),(737,'Samet Bilevci ( Netas External )','AS-OAM','2012-08-01','120801-351716','Avaya','Bill Price from ER has paged me for disk issue upon replacing a SESM server.\r\nThis is a Avaya end customer, site: Northwestern University\n\r\nMCP Load: 12.0.6 \r\nServer type: HP cc3310\n\r\nThey replaced the server running SESM1_1 instance for another issue.\r\nThey put the original disk into new server and had issues booting up the server.\r\nThe server could boot up from disk1 and a new spare disk seated in disk0 and they paged OAM GPS as there were no disk redundancy.\n\r\n==Actions Performed==\n\r\nThe following RAID status existed when ER paged me, with disk1 present and disk0 is spare one, server booted up and SESM1_1 is in HOTSTANDBY mode.\n\n\n\r\nDefined disks:  sda=70007.199(MB)\n\r\n                ***** Software RAID-1 Devices *****\n\r\n  MD Device   |            |  Member-0 |  Member-1 |     Sync/Recovery\r\nName  Size(MB)|   Usage    | Name  Flg | Name  Flg | Mode Speed  Fin  Done\r\n--------------+------------+-----------+-----------+--------------------------\r\nmd0     101.9 | /admin     | .     _   | sda1  U   |  .    .      .     .\r\nmd1     101.9 | /boot      | .     _   | sda2  U   |  .    .      .     .\r\nmd2    2047.2 | swap       | .     _   | sda3  U   |  .    .      .     .\r\nmd3    2047.2 | swap       | .     _   | sda9  U   |  .    .      .     .\r\nmd4    2047.2 | /          | .     _   | sda8  U   |  .    .      .     .\r\nmd5    6141.9 | /opt       | .     _   | sda6  U   |  .    .      .     .\r\nmd6    5122.2 | /var       | .     _   | sda7  U   |  .    .      .     .\r\nmd7   52297.4 | /var/mcp   | .     _   | sda5  U   |  .    .      .     .\n\n\r\n- Since \"sdb\" RAID was not existed tried to add sdb with \"mcpSwRaid.pl -add sdb\" and get disk inaccessible error.\r\nRquested the reboot the server and try again, but the server could not boot up after reboot command --> \"Boot wacthdog timer failed\"\r\n- Customer did another swap for disk1 and left disk0 empty and the server booted up Ok.\r\nCurrent status was:\n\r\nDefined disks:  sda=70007.199(MB)\n\r\n                ***** Software RAID-1 Devices *****\n\r\n  MD Device   |            |  Member-0 |  Member-1 |     Sync/Recovery\r\nName  Size(MB)|   Usage    | Name  Flg | Name  Flg | Mode Speed  Fin  Done\r\n--------------+------------+-----------+-----------+--------------------------\r\nmd0     101.9 | /admin     | .     _   | sdb1  U   |  .    .      .     .\r\nmd1     101.9 | /boot      | .     _   | sdb2  U   |  .    .      .     .\r\nmd2    2047.2 | ???        | .     _   | sdb3  U   |  .    .      .     .\r\nmd3    2047.2 | ???        | .     _   | sdb9  U   |  .    .      .     .\r\nmd4    2047.2 | /          | .     _   | sdb8  U   |  .    .      .     .\r\nmd5    6141.9 | /opt       | .     _   | sdb6  U   |  .    .      .     .\r\nmd6    5122.2 | /var       | .     _   | sdb7  U   |  .    .      .     .\r\nmd7   52297.4 | /var/mcp   | .     _   | sdb5  U   |  .    .      .     .\n\n\r\n- Since the mtc window was already ended, customer wanted to leave the server as it\'s now and continue disk RAID actions in the next mtc window, starting at 19:00 CDT.\n\r\n- tonights action plan is to put the original disk back in 0, unseat disk 1, and attempt to boot. If they are able to boot from disk 0, \r\nthen a spare will be put in disk 1. (then the opposite of \"UP Flaged\" RAID needs to be added: \r\n\"McpSwRaid.pl -add sdX\" ---> X (\"a\" or \"b\") will be the opposite of what\'s UP)\n\r\n- If above action plan would not work, we will have to Swap disks with and try to mirror until succesfully proceed.\r\nie: Have the \"sdb\" under Member-1 UP again, and put a spare disk as disk0 then try to mirror \"sda\" in disk0 etc..\r\nER would page A2 OAM GPS if needed during the next mtc window starting at 19:00 CDT. Agreed and drop off the call.','null'),(738,'Tugrul Timorci (NETAS External)','AS-OAM','2012-07-26','120725-350268','Frontier Telephone of Rochester Inc','Martha Foster Called me for ATCA server boot failure. System running on CVM16 \n\r\nSystem had  JMXM200 Low Memory alarm , and GTS found same symptom in another case. GTS advised to reboot server. This server never came back online and ATCA blade 8 in the A2 went into alarm. They tried to reseat the server but it did not work. \n\r\nI asked the led status of the server and they informed me it has Red OOS and Blue led indicator were blinking on the server. I suggested the reseat server again but it did not worked. I assumed that this could be server problem   so  tried to replace new server but it did not worked again . We replaced old server again. \n\r\nATCA GPS involved the case and checked the ShMM. They confirmed patches _37 and _42 are applied, however the shmm firmware is out of date. They started to work on this issue and I dropped the call','null'),(739,'Tugrul Timorci (NETAS External)','AS-OAM','2012-07-26','120726-350480','VTR Global Com SA','Tony called me for Oracle Migration timeout error on CVM13\n\r\nI logged in to the system and checked the log \n\r\nError occurred executing NE commands (see below): \n\r\n> run err Error! Timer expired while executing: perl \r\nError executing Oracle migration commands. \n\n\r\nSee log file for possible details: /var/mcp/run/install/logs/oracleMigration.log.20120726_020734 \n\n\r\nI cheked the NED log and saw there migration script failed while executing ./getIfInfo.pl script. I manually restarted the NED and after tried again execute ./getIfInfo.pl script manually and executed as expected. I advice to execute migration script again.  1 hour later script finished successfully. \n\r\nAfter agreement I left the conference','null'),(740,'Tugrul Timorci (NETAS External)','AS-OAM','2012-07-24','TBD','British Telecom','Camila called me for Replication alarm , this alarm raised after upgrade wizard prep steps finished (CVM13 to CVM15 ).\n\r\nI asked status of the Upgrade wizard. She told me that MOCK upgrade finished and after this alarm raised. While I try to log in to site this alarm disappeared. We waited few minutes for monitor the system and no alarm raised Database replication related. System was healthy as expected. \n\r\nAfter agreement I dropped the call.','null'),(741,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-07-18','120718-348507','BT PLC (Manchester)','Problem Description:\r\n   Camila had paged me about \'Upgrade wizard fails at screen 6 of prep steps, unable to read A2EC140 disc from primary EMServer\'. She said that it is trying to read MCP core patches disc (A2EC) but it can\'t be read on primary EMServer and mount says no medium.. However, it can  be read with another disc without propblem on that same unit like A2EI (platform patches for CC3310s).\n\r\nMCP Load:\r\n12.0.12\n\r\nSolution of Problem:\r\n   I have asked to Camilla for basic question like did you restart the server, which step are you on exactly etc. to understand problem. Then, she said that this is pre step of the upgrade wizard. According to our agreement these pre and post steps should be resolved with raised cases. I have told this to Camila and demanded a case from her. Then, we have agreed that this issue will be investigated in our business hour with first priority and drop the call.','null'),(742,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-07-17','120717-348247','VTR Global Com SA','Problem Description:\r\n   Tony has paged me about MCP GUI connection drops frequently when he connected to GUI. And he also mentioned that he checked the server disk size  and seemed that \'/var/mcp\' directory was being used with %99 ratio. So, he was not able to maintain MCP GUI connection properly.\n\r\nMCP Load of Customer:\r\n10.3.2\n\r\nSolution of Problem:\r\n   I have connected to site via VPN and checked the both primary and secondary servers\' disk space with \'df -k\'command and I saw that there is something wrong. Then, using \'du -csh *\' and \'find / -type f -size +20000k -exec ls -lh {} \\; 2> /dev/null | awk \'{ print $NF \": \" $5 }\'  | sort -nrk 2,2\' commands I found the file which is larger than 20MB and saw them in a sorted list.\r\n   There was \'repq_data.dbf\' file under \'/var/mcp/oradata/mcpdb\' as 22GB and this should be around 60MB. Then, I applied below procedure to fix this file\'s size problem.\n\r\n- Take Backup of primary EM with dbBackup.sh script.\r\n- CleanupReplication (drops the connection between primary and secondary)\r\n- RestoreEmptyDB on primary EM (remove all database data on primary server)\r\n- Put backup back to primary EM with dbRestore script which is taken in earlier step.\r\n- RestoreEmptyDB on secondary EM (remove all database data on primary server)\r\n- Put backup back to secondary EM with dbRestore script which is taken in earlier step.\r\n- Setup replication again between primary and secondary servers.\n\r\nAfter this procedure, eveything seemed well and opened the MCP GUI without problem and drop the call.','null'),(743,'Ege Varhan (NETAS External)','AS-OAM','2012-07-12','120711-347516','Axtel','Problem Description:\r\n-------------------\r\nSESM2_0 couldn\'t be stopped on System Management Console.\n\r\nInvestigation\r\n-------------------\r\n1- We have connected to the site and logged into SM GUI. The state of SESM2_0 stucked in \"stopping...\" process. I have checked the alarms of Active System Manager, and saw blocked socket connection thread which is related to SESM2_0.\n\r\n2- We have tried to connect to SESMServer3 which has SESM2_0 instance installed, but couldn\'t access it. Even though we were able to ping the server, we couldn\'t connect via SSH. Something was wrong with the server.\n\r\nWe have suggested Axtel to reboot the server since it was not responsive. They were going to reboot the server in M.W. (RECOVERY WORK WILL BE BASED ON THURSDAY 12 00:00 HRS. SCHEDULE OF CENTRAL MEXICO). ER will call GPS back when the M.W. starts.\n\r\nIf we can access to the server after the reboot, we will collect server and SESM2_0 work logs to find out the cause of the problem.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(744,'Ege Varhan (NETAS External)','AS-OAM','2012-07-12','120712-347528','Singtel Optus Pty Ltd','Customer: Singtel Optus Pty Ltd\r\nA2 Load: MCP 12.0.6.16\r\nSite Type: SSL\n\r\nProblems:\r\n----------\r\n-Almost all network elements had \"Out of Sync\" alarms on them\r\n-Prov1 had \"Data Distribution\" alarm\r\n-The customer was not able to perform provisioning. The lines, they thought they provisioned, were not able to register, make calls or queried with QSIP command from Core.\n\r\nInvestigation:\r\n-----------\r\nWe have logged into System Management Console and checked the status of SM (System Manager) instances. The status of both SM instances are below;\n\r\nSM_0 Online        Hot Standby\r\nSM_1 Configure     Active\n\r\nSM_1 was the active instance and it\'s state was not normal (**Configured**).\n\r\nSolution:\r\n-----------\r\n1- We have connected to the server, on which SM_1 instance was deployed, via SSH and killed SM_1 manually as root user.\n\r\n#kill -9  \n\r\n2- When we killed SM_1 instance, SM_0 became active. We have logged in SM GUI again and started the killed SM_1 instance to make it hot standby\n\r\nThe customer performed provisioning tests again and everything worked as expected. They will restart all NEs (Network Elements) which have \"Out of Sync\" alarms in M.W. to make them synced with database again.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(745,'Samet Bilevci ( Netas External )','AS-OAM','2012-07-06','120706-346708','GENBAND','Dave Fowler has page me for Upgrade Wizard issue during Genband Corp A2 \"Lab\" system Upgrade, \n\r\n==Upgrade path==\r\nfrom 14.1.0.6 to 16.0.2.2 - major upgrade.\n\r\nDave was working with Doug Holladay on this Lab upgrade, aiming to complete just before the Genband Corp Live site upgrade to 16.0\r\nThis Lab is small 3 server configuration lab, only 2 Core servers and 1 BCP (1Core/1 BCP as Primary side NE and 1 Core as Secondary side NE - for Upgrade Wizard)\n\n\r\n==History==\n\r\n- I have been informed that one of my colleague has been assisting Dave and Doug during the past week/working hours on this Lab upgrade for Wizard prep steps\r\nand the Upgrade Wizard had some cronic issues at this Lab site. If they have to \"Save&Exit\" from UW, they were not able to launch the Wizard again, \r\nand were getting the following error when trying to launch:\n\r\n\"Wizard State that was saved on this server was invalid. Please contact next level of support\"\n\r\nEverytime they had this error, they had to start the UW from scratch during the Wizard Prep-Upgrade steps as well, by truncating the \"Wizard-State\" table\r\non Database. Because, None of the workarounds for Wizard launch problems would work.\n\n\r\n- Dave and Doug informed me that they started actual Upgrade Steps with Upgrade Wizard. They made it through where it upgrades the DB& SM and \r\nstarts the service.  Then they got an error saying the \"license key is invalid, please correct the key and start the wizard over\"\n\r\nHowever, No license alarm existed on MCP GUI. (they only have one licensekey file, which they already configured for Upgrade Wizard) \n\r\nThey did save&exit due to error message.\n\r\nHere is the corresponding section from Wizard logs:\n\r\nDuring Screen: UPGRADE PRI NEIS\n\r\n2012-07-06 12:06:38,522 DEBUG PanelController - Current step: APPLY_LICENSE_KEY\r\n2012-07-06 12:06:38,522 DEBUG PanelController - Runs step APPLY_LICENSE_KEY.\r\n2012-07-06 12:06:38,544  INFO PanelController - Error occurred : An error occurred while applying license key. Please correct the license key and restart the wizard.\r\n2012-07-06 12:06:38,544 ERROR PanelController - Error occurred while applying License Key : Database Error: null\r\n2012-07-06 12:06:38,545 DEBUG PanelController - Peer NEI Validator is set to  null\r\n2012-07-06 12:12:07,080  INFO MainController - Save and Exit is clicked.\n\n\r\n- They, then Save&Exited the Wizard.  They re-applied their Lab license once more, and it was successfully applied.\n\r\n- When they tried re-launching Upgrade Wizard, the cronic issue re-occurred and Wizard did not launch. So, they decided to page OAM GPS.\n\n\r\n2012-07-06 12:37:24,551  INFO UpgradeWizard - User upgrade has login to wizard.\r\n2012-07-06 12:37:24,552 DEBUG UpgradeWizard - Openning wizard.\r\n2012-07-06 12:37:24,894 ERROR WizardStateManagerImpl - There is a selected load in state, but load lineup file is missing.\r\n2012-07-06 12:37:24,895 ERROR UpgradeWizard - Following error occurred while starting upgrade wizard : fail:INVALID_STATE:null\r\n2012-07-06 12:37:24,895 DEBUG MessageFrame - Error Dialog : Starting Wizard Error, Following error occurred while starting upgrade wizard.\r\nThe wizard state that was saved on the server is invalid. Please contact next level of support.\n\n\r\n==Actions Performed==\n\r\n- Checked the licenkey.txt file being kept for Upgrade Wizard directory under /var/mcp/upgrade_tools/data and compared with the Lab license key file \r\nwhich is valid for 16.0 load. They were exactly same. Could not find any indication on why wizard complained about license at first place.\n\r\n- Since we had to move on with Upgrade Primary NEIs and the Wizard was not launching, I tried the following:\n\r\nrestarting ned on Primary EM server (neinit restart)\r\ntried to launch wizard in Debug mode.\r\nChecked the current wizard state table from Primary DB, it was on Screen: \"UPGRADE_PRI_NEIS\", changed it manually to previous / next screen names,\r\nManually deployed the new 16.0 loads to Primary NEIs and start NEIs (these were the actions performed by wizard in current screen),\r\nbut Wizard still did not launch with any of the actions/workaround trials.\n\r\n- discussed the issue with my colleague and learned, currently Wizard is stuck in a bad state, which it was same experienced during the Prep-Steps.\r\nSince the only known way to recover Upgrade Wizard was to start it over (truncating wizard state table), and we have only 1 server as Secondary side \r\nserver for this Lab site, Doug and Dave agreed and would like to move forward without Upgrade Wizard, agreed me to manually perform the remaining upgrade steps.\n\n\r\nUPGRADE_PRI_NEIS\r\nSWITCH_TO_PRI_NEIS\r\n===> continued manually as Wizard could not launch.\r\nPATCH_SEC_PLATFORMS  ---> patchPlatform.pl on SecondaryEm\r\nPATCH_SEC_ORACLES   ---> \"OraclePatch.pl -secondary\" on PrimaryEm\r\nUPGRADE_SEC_NEIS    ----> Manually deploy/start secondary NEIs with 16.0.0.2\r\nSETUP_DB_REPLICATION --->  ./setupDBReplication.pl  from PrimaryEM\r\nBACKUP_DB  took manual backup.\r\n==started DB monitors manually.\n\n\r\n- Requested Doug and Dave to report this Wizard issue with a Case and we would investigate and report to Design team for investigation.\r\nI collected upgrade wizard logs along with upgrade_tools\' data and logs directory / screeshots and the problematic Wizard State table output from DB.\n\r\nthe following case has been opened for investigating this issue:\n\r\n120706-346708   A2 9.1: FNS: Upgrade wizard rejected license key, then would not continue.\n\n\r\n- After completing upgrade, all instances were running 16.0.2.2 load fine.\r\nAgreed and left the conference.','null'),(746,'Senem Gultekin (NETAS External)','AS-OAM','2012-06-26','120623-343284','Swisscom','Problem Description:\n\r\nER paged me for both unavailable SMs on Swisscom live site where previously Ive suggested the apply a workaround(120621-342781) to bring the SMs up again. ER was going to apply the workaround but at last customer didnt want to stop and start all their instances. \r\nSite is running on 14.0.9.4 Release. \n\r\nSolution:\r\n-	Explained to the customer that this is a complicated issue and we need more time to investigate the issue and to find a solution.\r\n-	Accessed to the site and applied the second workaround to bring SM0 up. Cause customer was concerning provisioning issue as well.\r\n-	Second workaround is just editing the subsystem/xml file under SM0 server and brining SM0 up without some OM related tasks.\r\ncd /var/mcp/run/MCP_14.0/SM_0/data\r\nvi subsystem.xml\n\r\nEdit the file by commanding out the following lines;\n\n\r\n-	After this modification SM0 came up and customer was able to login to MCP GUI.\r\n-	Since the MCP GUI was up and customer was able to provision, customer was fine with us to continue investigation during work hours.','null'),(747,'Senem Gultekin (NETAS External)','AS-OAM','2012-05-22','120522-337261','BT','Problem Description:\n\r\nSWD paged OAM GPS for a platform patch failure during wizard upgrade. \n\r\nUpgrade path is MCP_12.0.12.2 to MCP_14.0.9.7.\r\nError:\n\r\n[*P-Info*] Unpacking (pre-loading) any upgrade directive file specs\r\n[*P-Info*] Appl Target: BackupSnmpdConf\r\n[*P-Info*]    Executing command: \"(/bin/cp -f /etc/snmp/snmpd.conf /admin/snmpd.conf) 2>&1\"\r\n[*P-Error*]    [*P-Error*]    Retcode is     >>0x0100<<\r\n[*P-Error*]    Signal part is >>0x00<<\r\n[*P-Error*]    Main part is   >>0x01<<\r\n[*P-Error*]    System return strings:\r\n[*P-Error*]       >>/bin/cp: `/etc/snmp/snmpd.conf\' and `/admin/snmpd.conf\' are the same file<<\n\r\nSolution:\n\r\n-	Accessed to the site.\r\n-	Checked the logs and investigated wizard related issues.\r\n-	It seemed that there was a problem with the snmpd.conf file under admin directory for every BCP server.\r\n-	In total there were 6 BCPs to be upgraded at the primary side, and as total there are 12 BCPs.\r\n-	All of them seemed that had snmpd.conf file as a link file.\r\n-	Changed that file as a normal file and performed the patch again. It was success.\r\n-	SWD changed the file for every BCP server and now continuing with the upgrade.\r\n-	Since the issue was solved, Ive left the conference.','null'),(748,'Ege Varhan (NETAS External)','AS-OAM','2012-05-21','120521-337021','BT PLC (Manchester)','Customer: BT PLC (Manchester)\r\nA2 Load: MCP_12.0.12  / CVM13\r\nOracle: 10g\n\r\nProblem Summary: Primary database was down. When the site engineer tried to start it, he received the error below;\n\r\n----------------------------------------------------------------\r\nERROR at line 1:\r\nORA-00313: open failed for members of log group 4 of thread 1\r\nORA-00312: online log 4 thread 1: \'/var/mcp/db/data/mcpdb/redo04.log\'\r\n-----------------------------------------------------------------\n\r\nAlso, the alarm below occurred on MCP GUI due to unavailable primary database\n\r\n-----------------------------------------------------------------\r\nAlarmName: DBComm\r\nTimeStamp: Thu May 17 09:15:23 EDT 2012\r\nFaultNumber: 101\r\nShortFamilyName: DBCM\r\nLongFamilyName: DBCOMM\r\nSeverity: MAJOR\r\nProbableCause: communications subsystem failure\r\nDescription: No connection to DB Instance 0.\r\nCorrective Action: Check network connectivity.\r\n-----------------------------------------------------------------\n\n\r\nRoot Cause: Some redo log files were missing under \"/var/mcp/db/data/mcpdb/\" directory. The missing files were \"redo04.log\" and \"redo05.log\"\n\r\nSolution Description: We have recreated the missing redo files via the procedure below;\n\r\n1- Connected to sqlplus via \"ntdbadm\" user\n\r\n#sqlplus /nolog\n\r\n2- Became \"sysdba\" user after getting into sqlplus command line\n\r\nSQL> conn sys/mcp_raptor_1901 as sysdba;\n\r\n3- Shutdown the database completely\n\r\nSQL> shutdown immediate\n\r\n4- Started the database in mount mode\n\r\nSQL> startup mount;\n\r\n5- We checked the status of missing redo files to see if they are active (they are tried to get used by database at that time). Both were INACTIVE\n\r\nSQL>  select * from v$log;\n\r\n5 1 170 314572800 1 NO INACTIVE\r\n105080532 11-MAY-12\n\r\n4 1 169 314572800 1 NO INACTIVE\r\n104833981 08-MAY-12\n\r\n6- We have cleared the log files \n\r\nSQL> alter database clear UNARCHIVED logfile group 4;\r\nSQL> alter database clear UNARCHIVED logfile group 5;\n\r\n7- We opened the database\n\r\nSQL> alter database open;\n\r\nWe left the call after the database was up and running. Also, all database related alarms were cleared on MCP GUI.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(749,'Samet Bilevci ( Netas External )','AS-OAM','2012-05-17','120517-336482','Frontier Communications','Christopher Dejong from SWD Team has paged me as he was not able to start patch upgrade at Frontier site. \n\r\nHe was not able to get SM_0 ACTIVE which is required to be able to launch Upgrade Wizard. Also, when he first accessed to the site he noticed the \r\nfollowing alarm on primary DB (mcpdb_0)\n\r\nDBMN728 - Oracle Broken Job  Desc: The Job[UpdateSeqNumInfo] is broken.\n\n\r\nHe was going to perform patch upgrade from 14.1.0.0. to 14.1.0.5 load. SM_1 was ACTIVE at that time. In order to be able to \r\nlaunch the Upgrade Wizard, he had to swact the SMs, but he was not able to get SM_0 ACTIVE.\n\r\nHe paged me to recover SM_0 and address the SMs wouldn\'t swact issue, firstly. It appeared that a database error that was presant before \r\npatching activity was interfering with current status.\n\r\nChris provided remote access to me and I checked the SM NEIs\' status. SM1 was Active with no issue, but SM_0 was at following states: \n\r\nAdmin: Online  Link: Down and Oper: Unavailable\n\n\r\nI tried to kill the SM_0 instance from MCP GUI and it didn\'t work. I checked the NEI processes from command line of the EMServer1\r\nand there was no process for SM_0 instance (\"neinit -p\") actually.\n\r\nI tried to edit the System Manager_0 from \"Instance\" under System Manager (MCP GUI) . Since SM_0 runs on EmServer1, I selected it and \r\nedited (using -/+ button) and clicked on \"Apply\" button without changing anything. I got an error stating that the DB was in QUISCED mode. \r\n(read only mode)\n\r\nThis was the reason of SM_0 not getting ACTIVE. \n\r\nI connected to primary DB (sqlplus) and performed the following actions to get DB recovered from quiesced mode.\n\n\r\nat \"SQLPLUS>\" prompt:\n\r\n#select distinct gname from ALL_REPOBJECT;\r\n------------------------------\r\nMCSDBSCHEMAREGDESTGRP\r\nMCSDBSCHEMAREPGROUP\r\nMCSDBSCHEMASECWRTGRP\n\n\r\n- Run the following three code block for each of the above repobjects:\n\n\r\nBEGIN\r\nDBMS_REPCAT.RESUME_MASTER_ACTIVITY(gname =>\'MCSDBSCHEMAREGDESTGRP\',override =>true);\r\nEND;\r\n/\n\n\r\nBEGIN\r\nDBMS_REPCAT.RESUME_MASTER_ACTIVITY(gname =>\'MCSDBSCHEMAREPGROUP\',override =>true);\r\nEND;\r\n/\n\r\nBEGIN\r\nDBMS_REPCAT.RESUME_MASTER_ACTIVITY(gname =>\'MCSDBSCHEMASECWRTGRP\',override =>true);\r\nEND;\r\n/\n\n\r\n- After above action, I was able to kill the SM_0 instance actually to get it\'s state to \"Offline\" (Stopped) and then I used \"Start\" button\r\nto start it as \"HOT-STANDBY\".\n\r\n- Swacted the SM instances successfuly to get the SM_0 as the Active instance so Chris could start patching process using Upgrade Wizard.\n\r\n- Also suggested Chris to perform \"Resynch.pl\" from PrimaryDB in order to clear out the alarm on mcpdb_0 (DBMN 728)\n\r\n- Resych.pl cleared the DBMN alarm.\n\r\n- Chris was still not able to launch the \"Upgrade Wizard\" although corrected DB issues and get SM_0 active, due to following additional issue:\n\r\n- When he tried to log into the Upgrade Wizard it was giving the following error in a pop-up window, after he enters username/password into Wizard\r\nlogin window:\n\r\n\"Following error occured while starting upgrade wizard. WebServiceFailed\"  \n\n\r\n- Checked the Upgrade Wizard logs in his local PC produced while getting the error: (will be attached to the case)\n\n\r\n2012-05-17 08:14:40,567 ERROR TopologyManagerImpl - Error occurred while getting topology: \r\norg.apache.axis.AxisFault: ; nested exception is: \r\n                org.xml.sax.SAXException: Invalid element in com.nortelnetworks.mcp.omi.data.network.ServerNaturalKey - longName\r\n                at org.apache.axis.AxisFault.makeFault(AxisFault.java:101) ~[axis.jar:na]\r\n                at org.apache.axis.client.Call.invoke(Call.java:2470) ~[axis.jar:na]\r\n                at org.apache.axis.client.Call.invoke(Call.java:2366) ~[axis.jar:na]\r\n                at org.apache.axis.client.Call.invoke(Call.java:1812) ~[axis.jar:na]...\r\n...\r\n...\n\n\n\r\nPerformed the following actions in order to correct Wizard Launch issue:\n\n\r\n-Cleared the Java Cache from Java settings (Java Control Panel > General Settings) and tried launching Wizard, with no luck.\n\r\n-Copied all of the original contents from \"/var/mcp/loads//upgrade_tools\" directory to \"/var/mcp/upgrade_tools\"\r\n, cleared Java cache and tried again, same error received. (before this action, I created a copy of the current \"upgrade_tools\" directory as a .zip \r\nfor further investigation)\n\r\n-Finally, ran \"mcpInstallFirstLoad.pl\" script and selected current load file (14.1.0.0) as this command distrubutes all mcp related files into corresponding\r\nfolders in the server. After this action, cleared Java cache again and I was able to launch the Upgrade Wizard. Handed over to Chris.\n\r\n- In order to understand the reason of Upgrade Wizard launch issue, I requested Chris to attach/add the following data to the case:\n\r\n  Upgrade Wizard logs, details of upgrade history at this site and the \"upgrade_tool.zip\" which I created before the above actions. \n\r\n- Agreed with Chris and dropped off as Wizard Launch issue resolved.','null'),(750,'Samet Bilevci ( Netas External )','AS-OAM','2012-05-15','120515-336052','Alphawest Services P/L (Carr)','Yuji Onozuka from Australian local team has paged me for the following issue;\n\r\nIssue:\r\nSM unable to start during ATO Lab Rollback: from A2 14.1.0.3 (8.0sp1) to A2 14.0.9.2 (8.0brc)\r\nNo site access was available as this is ATO site.\n\n\r\n-David  Giomi was performing procedure: \"Rolling back the entire system (Full Major Release)\" in order to rollback \r\nATO lab system from 14.1.0.3 to 14.0.9.2 (Doc# NN10440-450 Release: CVM16, Docume Revision: 10.10 , starting at page 1586)\n\r\n-David informed that, step25 Restoring a core server from backup of the procedure had just been completed and accidentaly SecondaryEM server was rebooted instead of Primary server.\n\r\n-He required help on getting MCP GUI back online to continue and requested local team to open a BC case to page A2 OAM Team.\n\r\n-SM was not starting on SecondaryEM (SM_1)\n\r\n-Remote connection was not available/allowed at this ATO lab site. So, assisted David during the day on several steps of the rollback procedure. SM1 was not getting ACTIVE after each action and we were getting following error during SM initialization:\r\n(ref. work logs at \"/var/mcp/run/MCP_14.0/SM_0/work/SM.log/\")\n\r\n\"DBCM/DBCOMM 204 ALERT  MAY15 22:12:33\r\nNo suitable database instance is available for initialization\"\n\n\r\n- Requested David to check the users on secondary DB:\n\r\nSQL> select username from dba_users; \r\nUSERNAME \r\n------------------------------ \r\nSYSTEM \r\nSYS \r\nOUTLN \r\nWMSYS \r\nDBSNMP \r\nDMSYS \r\nEXFSYS \r\nCTXSYS \r\nORACLE_OCM \r\nDIP \r\nTSMSYS \r\n11 rows selected.  \n\r\n- Database should have been restored on secondary server (as Rollback procedure is from secondary to primary) during previous steps, but there were no MCP users in secondary database.\n\n\r\nIn order to recover the system and start SMs, following actions have been performed:\n\n\r\n- On the Primary DB (EMServer1), Oracle was not re-installed yet, so Oracle has been re-installed.\n\r\n- On Secondary DB, DB backup (for 14.0.9.2) has been restored. (confirmed we now see db users for MCP system --> db.user=mcpuser dbapp.user=mcsdbapp)\n\r\nSQL> select username from dba_users; \r\nUSERNAME \r\n------------------------------ \r\nSYSTEM \r\nSYS \r\nMCSDBAPP \r\nMCPUSER \r\n...\r\n...\n\n\r\n- Installed DB script files for the \"rollback to\" load: Running \"dbInstall.pl -fo\" (from Primary server) to install mcpdb_0 and mcpdb_1 DB directories to Em1 and Em2 servers respectively for rollback load (ne.load=MCP_14.0.9.2_2011-12-02-1059) after checking&correcting parameters of \"/var/mcp/install/installprops.txt\" files on both EMservers.\n\r\n- Had to perform restore DB operation on Primary DB using the same backup file we restored on Secondary \n\r\n- Finally, ran \"Resynch.pl\" script to set up resynch between two DBs and, we were able to bring up the SMs on both servers. Since the system had been recovered, agreed with David and dropped off.','null'),(751,'Ken Johnson','AS-OAM','2012-05-09','120509-335131','OneConnect','Site had a failed hard drive which corrupted its mirrored mate rendering the system unbootable.  Recovered mirrored disk, replaced failed disk, rebuilt disk mirrors.','null'),(752,'Samet Bilevci ( Netas External )','AS-OAM','2012-05-09','120509-335017','Videotron Ltee','-Bob Stokoski (GTAC) has paged me for this BC case. He was performig the IM document which is attached to the case (IM: 85-3649, Custom IM for Videotron - title: Videotron ERS8600 SMLT, IST, and VLAN Network Simplification) with Greg Trimeloni (DDI) at site. 8 gb USB drive was inserted to the EMServer1 for this purpose. \n\r\nAt procedure 6  \"Backup the Langley HT A2 Servers\" step 5, \"./bkupSvr.pl -usbdrive\" command was failing as below, so they couldnt not proceed with the rest of the IM steps and the mtc window time frame was running out.\n\n\r\nerror - sh: line 1: 26516 File size limit exceeded/bin/nice /bin/tar cvfbP /mnt/sdc/mcpLoad.VL-QN-SE000. \r\n2012_05_09.02_01_07.tar 20 /var/mcp/loads /var/mcp/media \n\n\r\nWe had yahoo conference with Bob and Greg. Since there was a contivity problem with my new PC, we had to connect the site (SFOYPQ01CA0) from my colleague, Tugrul\'s PC using the old version of contivity. \n\r\n==EMServer1== \r\nMCP Platform Release Level : 14.0.19 (via install) \r\nMCP Platform Hardware Type : Intel-TIGH2U \r\nOracle Version: 10.2.0.4.0 \r\nOracle Patch Level: 19 \n\n\r\n- During execution of \"./bkupSvr.pl -usbdrive\" command, the command was failing due to file size limit while backing up Oracle installer files which is located under /var/mcp/media directory as the error also indicates. \n\r\n- Checked /var/mcp/media directory and noticed that there were old/unused Oracle installer and patch_installer files. \n\r\n- Suggested removing the files which are old and unused for this server (most of them are used for 32bit server and they are older files) : \n\r\ninstaller-mcp-oracle-EE-10.2.0.4-17.LINUX32.tar \r\ninstaller-mcp-oracle-EE-10.2.0.4-19.LINUX32.tar \r\npatch_installer-mcp-oracle-EE-10.2.0.4-16.LINUX32.tar \r\npatch_installer-mcp-oracle-EE-10.2.0.4-16.LINUX64.tar \r\npatch_installer-mcp-oracle-EE-10.2.0.4-17.LINUX32.tar \r\npatch_installer-mcp-oracle-EE-10.2.0.4-19.LINUX32.tar \n\n\r\nAfter removing above files and formatting the USB drive & inserting again, re-tried the \"./bkupSvr.pl -usbdrive\" command and it completed this time without error (attached session output file to the case). Advised Greg and Bob that they can continue with the process and I dropped off from yahoo conference. \n\r\nThanks, \r\nSamet Bilevci \r\nA2 OAM GPS','null'),(753,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-04-30','120430-330707','BT PLC (Manchester)','Problem Description: \r\n   Camila has paged me about upgrade wizard problem on DB Mock upgrade stage. She said altough I tried to \"retry\" button it failed again.\n\r\nUpgrade Path:\r\n    From  12.0.12.1(CVM13)\r\n    To    14.0.9.7 (CVM15)\n\r\nSolution of Problem:\r\n   I have connected customer site via VNC of Camila, and checked the Db Mock upgrade logs. As far as I see from the logs, there did not been started DB Mock upgrade yet. Then, I checked the log seen in the upgrade wizard screen and it was saying;\n\r\n[root@GVSSLEM0 root]# cat var/mcp/upgrade_tools/logs/dryRunDBUpgrade/ut_dryRunDBUpgrade.EMS1_a15f7d20-29f7-1b21-8989-000e0cb865c3_ut_dryRunDBUpgrade.pl_DB_MOCK_UPG_6.20120430_132911.log\n\r\n  Started at  =>  Mon Apr 30 13:29:11 2012\r\nScript invoked with: -l MCP_14.0.9.7_2012-04-06-1644\r\n/var/mcp/run/install/work/lock_file, existing permissions match what was requested => 0660\r\nLock acquired by ut_dryRunDBUpgrade.pl\r\nError: Running multiple instances of DB related scripts (at the same time) is NOT allowed\r\n13:29:11 Cleaning ned session files\n\r\nThen, I run \"neinit restart\" command so that ned session is renewed. After that command, I tried \"retry\" button and it passed.','null'),(754,'Ken Johnson','AS-OAM','2012-04-23','120423-329350','Axtel','Multiple problems during A2 upgrade:\n\r\n   - 9.2.0.6 oracle patch failed to apply when it removed a library it needed for patch patch application, causing it to fail to apply.  I manually compiled the oracle patch on SM1 and applied it while restoring the library so that the patchs subsequent oracle commands would not fail.\n\r\n   - SCSI controller on SM0 caused the system to hang on a hardware interrupt which often required a power cycle to recover.  The site intends to replace SM0 in tonights maintenance window.\n\r\n   - completed the Upgrading the C20-A2 procedure, replicated the database, and upgraded the CMT/IEMS certificates to restore provisioning.  Restarted A2 NEs to resolve the data synchronization issues which arose from the frequent SM0 crashes when no SM1 was present.\n\r\n   - Several Raid Status Check Script Does Not Exist alarms on the BCPs as the 12.0 script does not yet exist.\n\r\n   - Some SIP PBXs were in a down state as the PBX was not responding to the AYT audit. Andy Puetzer determined  the SESM was not sending the AYT audits to the portal\'s socket, but rather to the IP defined in the SIP Via field (See Andys pager report for more detail).','null'),(755,'Ege Varhan (NETAS External)','AS-OAM','2012-04-15','120415-328230','Axtel','Customer: Axtel\r\nA2 Load: MCP 10.3.2.13\r\nPlanned Upgrade: MCP 10.3.2.13 -> A2 12.0.12 MR -> A2 12.0.12.2 via patching\n\r\nSummary\r\n--------------\r\nSWD tried to upgrade the site from MCP 10.3.2.13 to A2 12.0.12.2, but the upgrade was failed for a database problem. We tried to rollback the system to MCP 10.3, but the customer couldn\'t provide working Oracle Software CDs. So, the system was working on  the secondary side only. The situation was E2. There was no redundancy.\n\r\nProblem 1: Serial Port connection of SESMServer3 didn\'t work\n\r\nWe couldn\'t install the core linux installer onto SESMServer3 even though the baud rate was setup correctly, which was 19200 for HTLangley Servers, and agetty process was working successfully. Whenever we tried to boot the system from CD to install linux software onto the server, we had some strange characters on the hyperterminal. When the CD was out, everything started to work. We couldn\'t solve that problem at that time. We have asked the customer if they had KVM at the site. He connected to the server via KVM. So, we guided the customer to install the linux software onto the server. \n\r\nWe couldn\'t install MCP 10.3 to SESMServer3 server, because the serial port connection didn\'t work while the system was booting from CDROM.\n\r\nProblem 2: Oracle Software CDs (Version 9i for MCP 10.3) provided by the customer didn\'t work. \n\r\nAs GSP, we had nothing to do with this problem. We have told the customer that we needed a working set of CDs in order to work/continue to rollback. Jarrad Turner from SWD has tried to transfer them as iso files from genband.com, but that attempt was failed as well. The customer requested working CDs from another site which is located in an another city. The CDs were received sometime later, and we were able to rollback the site to MCP 10.3\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(756,'Ege Varhan (NETAS External)','AS-OAM','2012-04-15','120416-328404','Axtel','Customer: Axtel\r\nA2 Load: MCP 10.3.2.13\r\nPlanned Upgrade: MCP 10.3.2.13 -> A2 12.0.12 MR -> A2 12.0.12.2 via patching\n\r\nProblem 1: Platform patch of SESMServer3 couldn\'t be done. \n\r\nI was paged by Eric Duke from SWD. Eric told me that he was upgrading Axtel site from MCP 10.3.2.13 to A2 12.0.12.2, and performing platform patches of the primary servers. He told me that he couldn\'t perform the platform patch of SESMServer3. I suspected that the CD could be bad, but he told me that he used the same CD for all other servers that were successfully patched. I have connected to the site and checked the platform level of SESMServer3 via the command below.\r\n----------------------------------------------------------------\r\n[root@TORSSLI02 ~]# mcpRelease.pl\n\r\n*** MCP Platform Release ***\n\r\nSystem Type: mcp_core_linux_ple2\r\n----------------------------------------------------------------\r\nWhile all other servers were CC3310, SESMServer3 was HTLangley. So, i have told him to find the correct CD, which includes the linux core installer and patches of HTLangley (ple2) servers.\n\n\r\nProblem2: HTLangley Platform Installation (Serial Port Connection Problem)\n\r\nHTLangley server\'s platform can not be patched during the upgrade from MCP 10.3 to A2 12.0.12. The server must be re-installed from the scratch. I have helped Eric during the installation of the ple2 core linux of HTLangley Servers. In order to install the servers, we needed to connect to the server via serial port connection. Even though the serial port looked like working well when the server is up and running, it didn\'t show anything rather than strange characters while booting from the CD. I have connected to the server via SSH and changed serial port settings from \"/etc/inittab\" file and run \"init q\" to refresh the updated settings. In the following reboot, we were able to see the booting screen of linux installation and installed the ple2 image onto SESMServer3. \n\r\nProblem3: ./mcpUpgradeFrom10To12.pl script failed on primary SM\n\r\nWe have started to upgrade DB and SM instances of EMServer1 after completing the platform patches / installation for ple2 servers of the primary side.  We ran \"mcpUpgradeFrom10To12.pl\" command in order to upgrade the system, but the command was failed with the error below;\n\r\n----------------------------------------------------------------\r\nJServer Release 9.2.0.6.0 - Production\r\ntbsSize = large\r\n*********************************************\r\nCreating MCS DB tablespaces...\r\nTablespace INDXexists.\r\nTablespace MCP_DATAexists.\r\nTablespace MCP_LOB_DATAexists.\r\nTablespace MCP_MID_SIZED_TABLESexists.\r\nTablespace MCP_MID_SIZED_INDXSexists.\r\nTablespace MCP_PICTURE_DATAexists.\r\nTablespace MCP_CPL_DATAexists.\r\nTablespace MCP_LARGEST_TABLESexists.\r\nTablespace MCP_LARGE_TABLESexists.\r\nTablespace MCP_NTWKCALLLOG_INexists.\r\nTablespace MCP_NTWKCALLLOG_OUTexists.\r\nTablespace MCP_NTWKCALLLOG_INDXSexists.\r\nTablespace MCP_LARGE_INDXSexists.\r\nTablespace MCP_LARGEST_INDXSexists.\r\nDone with MCS DB tablespace creation.\r\n*********************************************\r\nDECLARE\r\n*\r\nERROR at line 1:\r\nORA-04021: timeout occurred while waiting to lock object SYS.DBMS_LOCK\r\nORA-06512: at line 7\n\n\r\nDECLARE\r\n            v_cnt number;\r\n         BEGIN\r\n            select count(*) into v_cnt from all_tab_privs where GRANTEE = \'MCPUSER\' and PRIVILEGE = \'EXECUTE\'\r\n               and TABLE_SCHEMA = \'SYS\' and table_name = \'DBMS_LOCK\';\r\n            if v_cnt = 0 then \r\n               execute immediate \'grant EXECUTE on DBMS_LOCK to MCPUSER\';\r\n            end if;\r\n         END;\r\n/\n\n\r\nsqlplus  -S fails at /usr/lib/perl5/site_perl/mcsBase/SysUtl.pm line 400\r\n        mcsBase::SysUtl::pipedCmd(\'mcsBase::SysUtl=HASH(0x86fc530)\',\'sqlplus  -S\',\'ARRAY(0x8cca458)\',\'sys\') called at /usr/lib/perl5/site_perl/mcsBase/SysUtl.pm line 445\r\n        mcsBase::SysUtl::pipedSysCmd(\'mcsBase::SysUtl=HASH(0x86fc530)\',\'sqlplus  -S\',\'ARRAY(0x8cca458)\') called at /usr/lib/perl5/site_perl/DB/OraSQLWrapper.pm line 613\r\n        DB::OraSQLWrapper::sqlCmd(\'AppSqlWrapper=HASH(0x88f518c)\',\'sys\',\'-S\',\'undef\',\'SYS\',\'DECLARE\\x{a}            v_cnt number;\\x{a}         BEGIN\\x{a}            ...\') call\r\ned at /usr/lib/perl5/site_perl/DB/OraSQLWrapper.pm line 551\r\n        DB::OraSQLWrapper::sqlSysCmd(\'AppSqlWrapper=HASH(0x88f518c)\',\'-S\',\'undef\',\'SYS\',\'DECLARE\\x{a}            v_cnt number;\\x{a}         BEGIN\\x{a}            ...\') called \r\nat /var/mcp/run/MCP_12.0/mcpdb_0/bin/util/migrationScripts/../../base/DBApp.pm line 1240\r\n        DBApp::grantObjPriv(\'MigrateDbFrom10To12=HASH(0x89008f4)\',\'SYS\',\'EXECUTE\',\'DBMS_LOCK\',\'MCPUSER\',\'undef\') called at /var/mcp/run/MCP_12.0/mcpdb_0/bin/util/migrationScri\r\npts/../../base/DBApp.pm line 904\r\n        DBApp::grantMCSDBAPriv(\'MigrateDbFrom10To12=HASH(0x89008f4)\',\'MCPUSER\') called at ../bin/util/migrationScripts/migrateDbFrom10To12.pl line 380\r\n        MigrateDbFrom10To12::migrateUser(\'MigrateDbFrom10To12=HASH(0x89008f4)\') called at ../bin/util/migrationScripts/migrateDbFrom10To12.pl line 79\r\n        MigrateDbFrom10To12::start(\'MigrateDbFrom10To12=HASH(0x89008f4)\') called at /usr/lib/perl5/site_perl/mcsBase/BaseOperation.pm line 551\r\n        mcsBase::BaseOperation::main(\'MigrateDbFrom10To12=HASH(0x89008f4)\') called at ../bin/util/migrationScripts/migrateDbFrom10To12.pl line 48\n\n\n\r\nsqlplus  -S fails\n\r\nERROR: migrateDbFrom10To12.pl Terminated at  =>  Sun Apr 15 05:39:33 2012\n\r\n     Reference the following log file for additional details:\r\n      /var/mcp/run/MCP_12.0/mcpdb_0/work/migrateDbFrom10To12.log\n\r\n----------------------------------------------------------------\n\r\nWe have tried to solve the problem, but the customer told Eric (SWD) to stop immediately and rollback the primary side to MCP 10.3 again, because the M.W was about to finish.  So, We have collected the logs, and rollback has been started.\n\r\nProblem4: ./mcpUpgradeFrom10To12.pl script failed on primary SM\n\r\nWe have re-installed 10.3 platforms onto all upgraded servers(primary side) in order to rollback. The next step was to install Oracle software onto EMServer1. While the oracle installation files were being extracted from CDs, the process failed when it was extracting CD#2. We have tried to extract CD#2 a couple of times with no success. I have suspected that the CD#2 was malfunctioning, so i have checked the system logs under \"/var/log/messages\" file and seen some \"IO error\" messages. I have told the customer to find or obtain a working CD set of Oracle Software in order to make us continue.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(757,'Tugrul Timorci (NETAS External)','AS-OAM','2012-04-19','120419-328885','Bell Aliant','Kyle called me for removing SESM1 from the MCP GUI. Customer is performing the MOP (attached to case) and reports that step 5 is failing.\n\r\nI logged in to the site and checked the error message. MCP GUI responds with Delete Operation Failed. This entry is referenced by service data of type MPSessionManagerData\n\r\nI deleted the\r\n                   media portals -> Media Portal Clsuter ->STJHNBSUCL1 -> Session Mangers\r\n                   media portals -> Media Portal Clsuter ->STJHNBSUCL2 -> Session Mangers\n\n\r\nafter this operation I tried to delete SESM again but alarm changed to At least one Voice Mail Server is still associated with this Session Manager\"\r\nI changed the Voice mail servers to SESM2 server\n\r\nAfter deleting deleting all SESM1 instances from VoiceMail server, now error has changed again.I deleted the authorized methods and instances under SESM1 instance. \r\nI tried again and I was able to delete the SESM instance. \r\nI advised to ER stop and start all RTP blades after customer approval. Because RTPs will check the SESM1 instance and will give alarm \n\r\nAfter agreement I dropped the call','null'),(758,'Tugrul Timorci (NETAS External)','AS-OAM','2012-04-16','120411-327702','Shaw CableSystems','Tom Draper called me for Provisioning issue of A2. This issue rised after CVM15 to CVM16 upgrade. \n\r\nI logged in to the system over ER via VNC.I checked the Provisioning client of the customer and system was working normally.  I asked to ER reproduce the issue. Actual problem was adding GW to VMG. This is like a OSSgate issue but  there was a SM error log in the SM oss logs. \n\r\nSM_0 OMIFSEC 621 INFO APR16 11:58:35:327 MCP_14.1.0.3\r\n                    Operation: Add\r\n                    Admin ID: admin@10.63.128.235 failed\r\n                    Resource Type: VMGAppearanceData\r\n                    Resource(s): ,SESM1;sbt60gwc30sesm1\n\n\r\nI engaged the Ege and he checked the prov side and there was no issue from A2 side. He worked same issue before and he understood this is OSSgate issue. OSSgate configuration parameter changed and restarted the CMT. After reboot ER was able to add GW. After agreement I dropped the call.','null'),(759,'Ege Varhan (NETAS External)','AS-OAM','2012-04-13','120413-328016','BT','Customer: BT\r\nA2 Load: 12.0.12\r\nOracle Version: 10g\r\nProblems: Alarms existed on MCP GUI is below;\r\n1- Replication fails (Databases are not replicated). The reason of E2 and the pager call.\r\n2- SM1 is in \"Configured\" and \"Offline\" state\r\n3- FTP Push doesn\'t work\n\r\nI was paged by Bradley Hetzel(ER). Bradley indicated that Edwin Verheij from SWD ran \"setupDBReplication.pl\" to make primary and secondary databases replicated, however the script failed with the error below;\r\n------------------------------------------------------------\r\n                > run output /opt/mcp/ned/bin/nedclient: Error! Could not connect to 62.7.120.197:4891 or 62.7.120.197:4890: Connection refused\r\n                > run output \r\n                > run output \r\n                > run output Error: when execute above command\r\n                > run output \r\n                > run output  ERROR: resync.pl Terminated at  =>  Fri Apr 13 03:06:51 2012\r\n                > run output \r\n                > run output      Reference the following log file for additional details:\r\n                > run output       /var/mcp/run/MCP_12.0/mcpdb_0/work/resync.log\r\n------------------------------------------------------------\n\r\nI though that this is a NED problem, and connected to the secondary database (62.7.120.197) which refused the NED connection in the error message above. I have checked the NED logs and restarted NED (Network Element Deamon) with \"neinit restart\" command. I have performed \"setupDBReplication.pl\" again from the primary database after NED was restarted. The script finished successfully. I have also tested the replication with \"testReplication.pl\" which is located under \"/var/mcp/run/MCP_12/mcpdb_0/bin/util\" directory. It was successful too. All replication related alarms were disappeared on SM GUI.\n\r\nFor the second problem, i have deployed and started SM_1 via SM GUI. SM_1 has became \"Hotstandby\" successfully too.\n\r\nThere were a couple of alarms left and all of them were related to FTP Push. While i was checking the configuration of FTP Push on SM GUI, Edwin stated that the customer was aware of FTP Push problems. I think that it happened in the past because SMs or FTPs were hacked and passwords were changed somehow and that was a known issue for the customer. The customer has acknowledged the alarms via SM GUI and the status window became all green.\n\r\nSince, all problems were resolved, i have left the conference.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(760,'Samet Bilevci ( Netas External )','AS-OAM','2012-04-05','120412-327797','Hawaiian Telcom','Tony from SWD team has paged me during  A2 system upgrade using Upgrade Wizard from 12.0.12.0 to 14.0.9.4 at Hawaiian Telcom live site.\r\nThis was not seen in weekly SWD activity but Tony informed that this is a re-scheduled upgrade.\n\r\nThe Upgrade Wizard has stuck at Upgrading Primary NEs (at specific screen name: Checking NE redundancy)\n\r\nConnected to site and check the instance states all fine. Did Save&Exit and re-launched the upgrade wizard. It resumed from \"Upgrading Primary NEIs\" screen and still hung status.\n\r\nInformed Tony that this is a known issue (occurs if there is no Account Manager at A2 system), there is a bulletin already published for this issue, and it has been fixed with 14.0.9.6 patch and higher. For lower \"upgrade to\" loads, the workaround should be applied as stated in the bulletin.\n\r\nBulletin link:\r\nhttps://c.na4.content.force.com/sfc/servlet.shepherd/version/download/06860000000bGnvAAE\n\r\nTitle: Communications Application Server (A2) Upgrade Wizard May Stall When Upgrading Primary Network Elements\n\n\r\nApplied the workaround (manually deploying new load to primary NEIs and Skipping the current hang screen via Wizard debug mode) and checked if everything is fine. \n\r\nHanded over to Tony and informed him that he can continue with the upgrade.','null'),(761,'Ege Varhan (NETAS External)','AS-OAM','2012-04-10','120409-327177','Axtel','Customer: Axtel \r\nSite Location: Torreon Coahuila, Mx\r\nA2 Load: MCP_10.3.2.13 (CVM11)\r\nOracle Version: Oracle 9i\n\r\nProblem Description:\r\n--------------------\r\n1- BCP blades were all down after losing fans and DC power \r\n2- Once blades cooled, they recovered\r\n3- DB is in and out of sync \n\r\nInvestigation:\r\n--------------------\r\nI was paged by Mark Zattiero for the third problem (DB is in and out of sync). I have already investigated the problem last night in a different call with Tom Drapper(ER) and requested to restart the primary database. But, the customer wanted us to do that in M.W. So, Mark called me to restart the primary database in M.W. I have connected to the site and restarted both primary and secondary databases. I have launched OEM (Oracle Enterprise Manager) to observe replication queue and database jobs for a while after both databases were restarted. We haven\'t see any problems during the observation of the system around 30 minutes. We have requested the customer to perform some provisioning to make sure that everything is ok. The provisioning was successful too.  So, i have left the call with an agreement.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(762,'Ege Varhan (NETAS External)','AS-OAM','2012-04-09','120409-327177','Axtel','Problem Description:\r\n--------------------\r\n1- BCP blades were all down after losing fans and DC power \r\n2- Once blades cooled, they recovered\r\n3- DB is in and out of sync \n\r\nAs i understand, there were 3 problems occurred at the site of Axtel. I was paged by Tom Drapper (ER) for the last problem (DB Replication). Tom stated that the DB replication kept broken, and a couple of DB alarms existed on MCP GUI.\n\r\nInvestigation\r\n-------------------\r\nI have connected to the site via ER\'s VNC. There were 2 alarms on the primary database (mcpdb_0). \n\r\n1- Oracle Replication Link Deferred Transactions\r\n2- Broken Job\n\r\nI have launched OEM (Oracle Enterprise Manager) via internet explorer and login as \"sysman\". I have checked the oracle jobs that are available. I realized that the job that is in charge of pushing the transactions to the secondary database was broken. I have set up and ran it again. The replication started to work, however 10 minutes later the job was broken again. Since, the problem was on the primary, i have requested to restart the primary database. The customer wanted us to restart it on April 9, 2012 at 2300EDT in M.W. So, i have dropped of the call. The ER will call me again in M.W. to restart the primary database.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(763,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-04-03','120403-326144','Yadkin','Problem Description:\r\n   Jeffrey has paged me about Provisining problem on the customer after the upgrade finished. And he also said that customer\'s SESM2 server is lost and it did not be seen in the SESM tab on the left of MCP GUI.\n\r\nUpgrade Path:\r\n    From:14.0.9  (CVM15)\r\n    To  :14.1    (CVM16)\n\r\nSolution of Problem:\r\n   I have connected to site Jeffrey\'s VNC connection. First, I checked the all instances up and running properly. They were good. Theni I opened the PROV client to add a new user. When I add a new user, it was successfull. This means primary database was up and running properly. Since the secondary database is read-only, if the primary was not up, adding a new user will have not been successfull. Then, I look at the SESM instances and there was one pair of them. I also checked the IP addresses from MCP GUI that are there more than 2 IPs of SESM to check if really there is SESM2 server or not. Then, I said to customer that there is not SESM2 before the upgrade. If it were, after the upgrade it should not be removed. Then, Jeffrey said me that he misunderstood the customer. Avtually, they were saying SESM2 server is not monitored. When I learn this, it was working properly. \n\r\nThen, I said Jeffrey that if new user can be added over PROV client, there was not provisining issue here. If your OSSgate command was not working properly, please page the PS A2 SESM (OSSGate) team ann then I left the call as agreed with Jeffrey.','null'),(764,'Samet Bilevci ( Netas External )','AS-OAM','2012-04-02','120402-326038','Dubai Telecom','Camila from SWD team has paged me for upgrade wizard issue. The upgrade wizard was not launching and upgrade path is 12.0.12 to 14.0.9.6\n\r\nBackground:\r\n-SWD created new user/password for Upgrade Wizard as per upgrade document.\n\r\n-When trying to launch with this new user/password, received popup window which states An upgrade session owner by admin is already in progress You can not login (seems someone previously launched Wizard with admin user long time ago. But this was the first upgrade process at this site)\n\r\n-Trying to launch the Wizard with admin/admin using force-out option as per above message, received another popup states An unexpected error occurred. Please contact next level of support.\n\r\n-GPS have been contacted and reproduced the issue on this site with local PC, also not able to launch the Wizard from MCP GUI.  Checked the available logs. Only Upgrade Wizard logs was available (on local PC), there were no upgrade_tool logs created yet on Primary EM. \n\r\n- Connected to customer DB and get a copy of the current Wizard State table (attached) for Design review and requested Upgrade Wizard logs from applicator PC.\n\r\n-Deleted the Wizard State table from DB, and try to login with user/password created for Upgrade Wizard and the Wizard succesfully launched at the Welcome screen. Click on CANCEL button and informed SWD prime that she can proceed with Wizard Upgrade process.\n\r\nRequest a case to be opened for further investigation and agreed with Camila and drop off.','null'),(765,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-03-26','120324-324044','Axtel','Problem Description:\n\r\nBill has paged me about customer was not able to apply license key to their system. In the meantime, their Emserver1 somehow was being unreachable sometimes. After these issues were resolved they were not able to do provisining since the primary database is not reachable by instances. The solution of these three problems will be told in \"solution of the problem\" section.\n\r\nUpgrade Path:\n\r\n    From 10.3 (CVM12)\r\n    To 12.0 (CVM13)\n\r\nSolution of Problem:\n\r\nI have connected to the site and tried to add license key. But it failed and gave the \"Cannot get DB connection\" error. Since, I had to go office, I forwarded this issue Tugrul and Ege who are from A2 OAM GPS and at the office in that time. When I came back to office, Tugrul has said that their Emserver\'s platform patch was 10.1.17 and it should be 10.1.19. Because this old platform (17) has some problems. He said that they need to upgrade their platform level. Since the customer did not found the platform patch CD, we found this patch from Netas server and put this patch to their server from remote. While we were waiting for the patch operation complete, we have looked PROV logs since the provisining problem has occured. PROV logs were like below;\n\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.DataAccess.executeQuery(DataAccess.java:109)\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.DataAccess.executeQuery(DataAccess.java:96)\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.DataAccess.executeQuery(DataAccess.java:218)\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.DataAccess.executeQuery(DataAccess.java:207)\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.PkGenMgr.getKeys(PkGenMgr.java:150)\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.PkGenMgr.getKey(PkGenMgr.java:129)\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.PkGenMgr.getKey(PkGenMgr.java:131)\r\n      at com.nortelnetworks.mcp.ne.share.dataaccess.shared.impl.PkGenMgr.getKey(PkGenMgr.java:131)\n\r\nActually this problem had orginated from the after resync operation completed. Since the table DUALX is not a replicated table and it did not have any records in Secondary Database. When a resync is done from the secondary database to the primary database, table DUALX is empty, when it should have 1000 \'Y\' records.\n\r\nSo, as a solution, we have done following procedure;\n\r\n1. Connect to the primary database server with oracle user.\r\n2. Enter the command sqlplus.\r\n3. Copy the following procedure line by line:\n\r\ndeclare\r\n  i number;\r\nbegin\r\n   FOR i IN 1..1000 LOOP\r\n      INSERT INTO DUALX VALUES (\'Y\');\r\n   END LOOP;\r\nend;\r\n/\r\ncommit;\n\r\n4. Ensure you can add a new user from Provisioning client page successfully.\n\r\nAfter this operation completed, provisining worked properly. Then we applied the platform patch and EMserver1 was working properly anymore and being down for EMserver1 has been never happened again.\n\r\nIn the meantime we applied the license key manually from the db. \n\r\nAfter all these settings done, eveything was seemed as working.\n\r\nWe have agreed with customer and left the call.','null'),(766,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-03-24','120324-324044','Axtel','Problem Description:\r\n   Rodney has paged me about new server installation on Axtel. He had actually started to installation but since the newer issue has been received, he asked me to go on installation while he was in owerflow situation.\n\r\nUpgrade Path:\r\n    From 10.3 (CVM12)\r\n    To   12.0 (CVM13)\n\r\nSolution of Problem:\r\n   I have connected to the site and took over the job from Rodney. I have used the 10.3 installation document to do this job. After linux installation had been finished succesfully, I passed the patch platform, lastly, I have done the oracle installation. \r\n   Since the system was working on secondary side, I started to do resync operation from secondary to primary. The customer wanted to use primary since the secondary was not writable and they were not doing provisioning. After the resync operation was finished, I checked the provisining with adding a new user from PROV client and then gave the system up to the customer. We have agreed and I left the site.','null'),(767,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-03-24','120324-324033','Axtel','Problem Description:\r\n   Tony has paged me about EM server 1 did not come up after OS upgrade done. After he applied the patchPlatform, EMserver was not able to come up properly and there seems foreign characters on the screen. \n\r\nUpgrade Path:\r\n   From 10.3 (CVM12)\r\n   To   12.0 (CVM13)\n\r\nSolution of Problem:\r\n   I have connected to site and checked the Emserver1 is pingable or not. Emserver1 was not pingable from Emserve2. So I suggested them to do power cycle since there is nothing to do while Emserver1 was not pingable. They did power cycle but server was still shows the foreign characters on the screen. \r\n   Then, I suggested them to connect monitor and keyboard to the directly server and watch the server from that monitor. They connected and saw the same error as well. \r\n   Then, I suggested them to take out secondary disk of the server so that I want to see if the disk has a faulty or not. When they took out the secondary disk, the situation was the same. Then, I suggested them to take out primary disk and try the start the server. This time server came up with active situation. So, we understood that primary disk is faulty. Then, I suggested them replace the disk with spare one. They, actually tried to replace the disk but, when I ran \"mcpSwRaid.pl -status\" script it was not able to show the correct place of the disks. The screen was like below, although they put the primary newer disk;\n\r\nDefined disks:  sda=70007.196(MB)\n\r\n                ***** Software RAID-1 Devices *****\n\r\n  MD Device   |            |  Member-0 |  Member-1 |     Sync/Recovery\r\nName  Size(MB)|   Usage    | Name  Flg | Name  Flg | Mode Speed  Fin  Done\r\n--------------+------------+-----------+-----------+--------------------------\r\nmd0     101.9 | /admin     | .     _   | sda1  U   |  .    .      .     .\r\nmd1     101.9 | /boot      | .     _   | sda2  U   |  .    .      .     .\r\nmd2    2047.2 | swap       | .     _   | sda3  U   |  .    .      .     .\r\nmd3    2047.2 | swap       | .     _   | sda9  U   |  .    .      .     .\r\nmd4    2047.2 | /          | .     _   | sda8  U   |  .    .      .     .\r\nmd5    6141.9 | /opt       | .     _   | sda6  U   |  .    .      .     .\r\nmd6    5122.2 | /var       | .     _   | sda7  U   |  .    .      .     .\r\nmd7   52297.4 | /var/mcp   | .     _   | sda5  U   |  .    .      .     .\n\r\nThis means, in primary disk\'s location was empty. So, I suggested them to replace the server.\n\r\nAfter all these situations, time past and customer wants to do rollback since the newer Emserver1 needed to be installed by new mcp platform and oracle installations from the begging. The customer accepted to stay with secondary side active. Then, I agreed with customer to replaced new server and they wanted to call ER for installing new MCP to their newer spare server.','null'),(768,'Ken Johnson','AS-OAM','2012-03-29','120319-322732','DACOM','A2 oracle database replication was down. Execution of the resync operation failed with an ORA-02019: connection description for remote database not found.\r\nThe primary and secondary databases were reinitialized the the customer\'s data restored from backup to the primary.  After which the resync operation was able to complete successfully.','null'),(769,'Ken Johnson','AS-OAM','2012-03-28','120329-325200','Sunrise - Zurich','Several issues encountered during an A2 upgrade attempt from 12.0 to 14.0.\n\r\n{{Upgrade Wizard failed to swact SM}}\r\nUpgrade Wizard reported the swact from SM_0 to SM_1 failed, waiting for SM_0 to restart.  GPS found no indication that a start had been attempted.  After several attempts to recover put the SMs in the correct state and forced the upgrade to the next step.\n\r\n{{Upgrade wizard failed to detect successful platform patch application}}\r\n{{Upgrade Wizard inaccessible performing never-ending operation}}\r\nThe next step to apply platform patches failed on RTP0 due to a missing state file.  GPS found the state file, which showed that the patch had been successfully applied.  With the application in a loop the wizard was exited and subsequent attempts to start the wizard failed as it would not become visible after authentication.\n\r\n{{Upgrade Wizard not visible}}\r\nThe wizard appears to hang iterating through its network elements.  The upgrade wizards state had it performing the step Start_Secondary_AM.  This step hung the wizard as there are no optional AM network elements to start.  Restarting the wizard with \'forceout\' to clear other sessions raised a dialogue box from those other instances and made the invisible wizards visible.  A double swact was performed on the SMs and once past this the wizard was able to launch and become visible.\n\r\n{{Upgrade Wizard unresponsive}}\r\nWith the wizard visible the previously failing RTP0 was now in a success state, EMServer1 and SESMServer1 had also failed.  The wizard was then unresponsive and the retry button could not be clicked.  The wizard was eventually killed and abandoned for the remainder of the sites maintenance window.\n\r\n{{SESM0 down and unbootable following platform patch application}}\r\nSESM0 was down and unresponsive following its platform patch.  The customer drove out to the site to power cycle the system which failed to return it to service.  A console session revealed that there was a disk issue and we pulled disk 0 (primary) to allow the system to boot from the secondary which enabled the system to boot.  With no hardware errors logged GPS will reintegrate the disk in the sites next maintenance window.\n\r\n{{SESM launch failed due to missing Jboss directory}}\r\nAfter booting SESM0 and manually starting the SESM application it became hung in an Activating state.  Found that Jboss, updated by the 14.0 platform, required a patdk directory which didnt exist.  After manually creating this missing directory the application was then able to launch correctly and traverse the activating state to become Active.\n\r\nWith 15 minutes remaining in the maintenance window a swact was performed to ensure SESM0 was healthy.  All customer CallP tests completed successfully and the Upgrade activity was suspended till the next maintenance window.  Efforts to recover the unbootable disk on SESM0, Upgrade Wizard functionality/state, platform patching, and remaining upgrade steps will continue in a yet to be scheduled maintenance window.','null'),(770,'Tugrul Timorci (NETAS External)','AS-OAM','2012-03-16','120316-322448','Yadkin Valley','SWD called me Upgrade failure. I logged in to the system and checked the logs \n\r\nThe following alarm on DB_0 does not allow the upgrade script to start:\n\r\nAlarmName: Database Communication Error\r\nTimeStamp: Tue Mar 13 03:03:39 EDT 2012\r\nFaultNumber: 102\r\nShortFamilyName: DBMN\r\nLongFamilyName: DBMON\r\nSeverity: MAJOR\n\r\nThis issue was related to DB. System has also alarmed the issue 3 days ago.\n\r\nRCA of this upgrade failure DB alarm did not resolved the before upgrade. I have restarted the DB and alarm cleared.\n\r\nThere was also archivelog issue and I have already cleared the archive logs.\n\r\nPreventative action :you need to open case before the upgrade for solve the DB issues.\n\n\r\nThanks\r\nTugrul\r\nNext Update time :23/03/2012','null'),(771,'Tugrul Timorci (NETAS External)','AS-OAM','2012-03-16','120316-322447','Cincinnati Bell','Eric called me for wizard issue. When attempting to run the A2 upgrade wizard for the upgrade after it downloads the .jnlp file and loads the login window, the gui fails to load after login.\n\r\nI have corrected the DB wizard state table. After this action wizard successfully started.\n\r\nRCA: Prep steps done 20 days ago. Recommended action : prep steps execute 7 days before the upgrade.\n\r\nThanks\r\nK. Tuğrul Timorci ,MIS \r\nA2 OAM GPS','null'),(772,'Tugrul Timorci (NETAS External)','AS-OAM','2012-03-22','120315-322292','Shaw CableSystems','Wesley called me for backup issue on wizard. I logged in to the site over Wesley and I checked the system EM2 server disk space was not enough  for take backup \n\r\nChecked the system and found the full space . This issue rised cause of the prep backup screen executed twice (it is in prep step)\n\r\n0;root@cpd2sem2no:/var/mcp/upgrade_bkups/pre[root@cpd2sem2no pre]# ls -larth\r\ntotal 14G\r\n-rw-rw-r-- 1 ntappsw ntappgrp 70K Mar 14 09:01 mcpPlatform.cpd2ssl1no.2012_03_14.09_01_03.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 70K Mar 14 09:01 mcpPlatform.cpd2sem1no.2012_03_14.09_00_59.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 70K Mar 14 09:01 mcpPlatform.cpd2ssl2no.2012_03_14.09_01_04.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 930K Mar 14 09:01 mcpApp.cpd2sem1no.2012_03_14.09_01_22.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 22M Mar 14 09:01 mcpLoad.cpd2ssl1no.2012_03_14.09_01_22.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 22M Mar 14 09:01 mcpLoad.cpd2ssl2no.2012_03_14.09_01_23.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 6.6G Mar 14 09:20 mcpLoad.cpd2sem1no.2012_03_14.09_01_38.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 57M Mar 14 09:22 mcpDB.cpd2sem1no.2012_03_14.09_22_00.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 70K Mar 14 11:41 mcpPlatform.cpd2ssl1no.2012_03_14.11_41_08.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 70K Mar 14 11:41 mcpPlatform.cpd2ssl2no.2012_03_14.11_41_10.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 70K Mar 14 11:41 mcpPlatform.cpd2sem1no.2012_03_14.11_41_06.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 930K Mar 14 11:41 mcpApp.cpd2sem1no.2012_03_14.11_41_28.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 22M Mar 14 11:41 mcpLoad.cpd2ssl2no.2012_03_14.11_41_28.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 22M Mar 14 11:41 mcpLoad.cpd2ssl1no.2012_03_14.11_41_27.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 7.0G Mar 14 12:01 mcpLoad.cpd2sem1no.2012_03_14.11_41_43.tar\r\n-rw-rw-r-- 1 ntappsw ntappgrp 57M Mar 14 12:03 mcpDB.cpd2sem1no.2012_03_14.12_03_02.tar\r\ndrwxrwx--- 2 ntappsw ntappgrp 4.0K Mar 14 12:03 .\r\ndrwxrwx--- 4 ntappsw ntappgrp 4.0K Mar 15 06:26\n\r\nI deleted first taken backups and executed step 35 it worked and passed.\n\r\nRCA : prep backup screen executed 2 times\r\nPreventive action : if you executed this screen few times you have to delete previous taken backups.\n\r\nI am changing status to pending closure\n\r\nThanks\r\nK. Tuğrul Timorci ,MIS \r\nA2 OAM GPS','null'),(773,'Ken Johnson','AS-OAM','2012-03-16','120316-322561','One Connect','MCS5200 (SN09u) outage caused by an exhausted /opt filesystem.  Manually purged oracle archive logs (2+GB).  This enabled Oracle to write and record transactions to the database.  After purging the archive logs users were able to login, provision, etc.  IPCM and SESM service indicate they are no longer in sync with the DB given this access/operational outage.  The site will have to restart these apps tonight in a maintenance window.','null'),(774,'Senem Gultekin (NETAS External)','AS-OAM','2012-03-08','120229-319111','Telecom Liechtenstein','Problem Description:\n\r\nER paged me for a provisioning issue seen at Telecom Liechtenstein. Customer release is 12.0.12. There was a SIPPBX related jar (120229-319111) applied and after the jar, customer reported that they are having provisioning issue, cannot add ISN data with the following error in Provisioning Manager;\n\r\nAdding Domain ISN Data failed: The field(s): IMSDBA.DOMAINISN.NODE_ID must be unique\n\r\nSolution: \n\r\n-	Joined the bridge and checked the error as screenshot they were having.\r\n-	Suggested customer to add the ISN with a different name.\r\n-	During that customer realized that they have not deleted previously needed field with the same name, and they have fixed it.\r\n-	Customer reported that the issue has been solved and they can add ISN data from prov.\r\n-	Left the bridge.','null'),(775,'Senem Gultekin (NETAS External)','AS-OAM','2012-03-07','120307-320307','Telefonica','Problem Description:\n\r\nIve received pager from ER due to OPE and SOPE initializing stuck on Telefonica site after firmware upgrade. Current release is 14.0.6.1 Release.\n\r\nSolution: \n\r\nThe following actions were taken by site engineer with GPS suggestion;\r\n-	Kill problematic OPE instance from GUI.\r\n-	Restart NED from the server which is OPE server.\r\n-	Start OPE instance from the GUI.\n\r\nAfter these actions OPE was up running but SOPE application itself was still stuck in initializing mode in the MCP GUI.\r\nIve accessed to the over Rubens PC and here are the actions Ive performed.\n\r\n-	At that point primary OPE1 was hot standby and OPE2 was in active state. \r\n-	Checked the OPE and SOPE logs, I didnt see anything odd.\r\n-	It seems the previously OPE was killed, but SOPE was not stopped.\r\n-	Ive killed OPE1 and undeployed.\r\n-	Ive undeployed SOPE1.\r\n-	Once both OPE1 and SOPE1 was stopped Ive first deployed OPE1 and started.\r\n-	Later Ive deployed SOPE1 and came up.\r\n-	Both OPEs and both SOPEs are up running properly.\r\n-	Agreed with customer and ended the call.','null'),(776,'Ege Varhan (NETAS External)','AS-OAM','2012-03-01','120228-319007','IUSACELL','Customer: IUSACELL\r\nA2 Load: CVM11/10.3.2.11\r\nProblem: The customer was not able to provision SSL Lines\n\r\nI was paged by Jeffrey Brennan (ER) and told that the customer was not able to perform provisionig for a week. When they tried to add a line from OSSGate, they received the error below;\n\r\n---------------------------------------------------\r\nSystem:LineProv; EndPoint can not be added to GateWay.\r\nSystem:GWEMProxy; EndPoint data can not be added to GateWay.; Reason: Problem encountered when processing request on MCS-EM for GW:EP VMGSSLI01:SSLN/001/9/1012\r\nDetails: Cannot add/modify user without Locale. Please provide the required value. \r\n----------------------------------------------------\n\r\nI have connected to the site and checked Provisioning Server\'s logs. Provisioning Server complained that \"locale\" parameter was empty or null based on the logs. Normally, this parameter is mandatory to add a new line and is injected into the SOAP request by OSSGate automaticly. I have checked the Provisioning Server\'s source code if anything was broken. The source code looked ok to me. It was checking the validity of \"locale\" field and if it is empty or null, it produces the error above which the customer receieves from OSSGate. It was an expected behaviour.\n\r\nI have tried to replicate the same error via SOAP UI web service test tool. When i provided \"locale\" xml tag in the request, i was able to add a line even if the locale is set as empty string or null. So, i have realized that OSSGate didn\'t even put locale xml tag into the SOAP request. I have told ER to page OSSGate team. Mert Cokluk from OSSGate team joined the conference. Mert told that it is a known issue and there was already a patch developed for the problem. The patch name is \"ZER74ONY\". he checked whether or not the patch was applied to the site. But, the patch has already been installed into SESM. He removed the patch and re-applied it again into SESM. The provisioning started to work after re-applying the patch. Somehow, the patch was corrupted. We left the call after the customer started to provision again successfully.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(777,'Ege Varhan (NETAS External)','AS-OAM','2012-03-27','120228-318900','Unitymedia NRW GmbH','SITE NAME: Unity Media Site 3\r\nSITE CLLI (if applicable): DSDFDEUM01D\r\nELEMENT: A2\r\nFROM/TO LOAD: CVM11/CVM13\n\r\nCamila Guzzo from SWD team called me about a problem occured while she performed \"setupDBReplication.pl\". She wanted to replicate the databases, and performed \"setupDBReplication.pl -p installprops.txt\" command. The command failed and produced the error below;\r\n-----------------------------------------------------\r\nDeploying the DB...\r\nError occurred executing NE commands (see below):\n\r\n21:44:16 Error: Invalid value received for operation Parm => PATCH\n\r\nError: Invalid parms received, aborting script\n\r\nNE command exited with the value: 1\n\r\nError executing start commands.\n\r\nSee log file for details: /var/mcp/install/logs/dbInstall.log.20120227_214407  \r\n-----------------------------------------------------\n\r\nI have requested \"installprops.txt\" file from Camila, and realized that some parameters in it were wrong. For instance;\n\r\nne.load=10.3.2.0\r\ndb.operation=PATCH\n\r\nFirst of all, the customer load was 10.3.2.13 at that time. I have requested Camila to change it to 10.3.2.13 and remove \"db.operation=PATCH\" line from installprops.txt file. \"setupDBReplication.pl\" command completed successfully after we corrected \"installprops.txt\" file.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(778,'Ege Varhan (NETAS External)','AS-OAM','2012-03-02','120302-319675','BT PLC (Manchester)','SITE NAME: BT ADASTRAL 2 /IPCC\r\nELEMENT: A2\r\nFROM/TO LOAD: CVM13 (12.0.12) TO CVM15(14.0.9.5)\n\r\nI was paged by Meraz Aziz from SWD. Meraz told me that they have upgraded the primary servers of BT, but they had to rollback them to the old release for an another reason. They wanted to upgrade the primary side again via Upgrade Wizard, however the wizard produced the error below in pre-steps;\n\r\n2012-03-02 10:47:15,209 DEBUG ScriptManagerImpl - Start script response:\r\nScript Name:\'ut_mcpSystemValidation.pl\'\r\nServer Name:\'SESS2\'\r\nResult:\'FAILED\', \'java.io.FileNotFoundException: /var/mcp/upgrade_tools/data/pre_ple1_list.txt (No such file or directory)\'\r\n2012-03-02 10:47:15,211 INFO UpgradePanelController - Retry is enabled.\r\n2012-03-02 10:48:59,271 INFO DefaultDisplayHandler - closeHandler.handle(null)\n\r\nThe upgrade wizard couldn\'t find the file specified below;\n\r\n\"/var/mcp/upgrade_tools/data/pre_ple1_list.tx\"\n\r\nWe have suggested him to copy the file from the secondary EM server to primary EM server manually. The failing step was passed after transferring the necessary file manually. \n\r\nThe case has already been opened. We will follow the problem up with the design team.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(779,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-02-22','120214-315828','Hargray Communications Inc.','Tony has paged me about provisioning SIP lines problem via OSSGATE. He finished the both primary and secondary sides\' upgrade with upgrade wizard but he said that  the problem is going on which Ege was dealt with case 120214-315828. We have connected to the site and run the commands which customer ran and got the error messages. We have taken the permission for the OSSGate restart operation and Tony asked to customer and replied as \"OK\". Then, we restarted OSSGate and tried again same scenario. Everything is resolved after the restart operation. We have asked to customer that they can also run and get the commands properly. When they said problem is resolved then I left the call.','null'),(780,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-02-22','120214-315828','Hargray Communications Inc.','Tony has paged me about provisioning SIP lines problem via OSSGATE. There was a case already GPS engineer (Ege Varhan) dealt with. Tony has mentioned that the customer has finished the upgrade of the primary side and the problem goes on and he asked me that can we go on upgrade or any investigation should be done at this moment. Then, I told him that go on with secondary side upgrade operation since it can be investigated after both primary and secondary sides upgraded.\n\r\nThen, we agreed and I left the call.','null'),(781,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-02-21','120209-315149','Hargray Communications Inc.','Tony has paged me about upgrade wizard problem. He said that in pre-upgrade stage there was some problems on BCP servers. This problem is known issue and its description was like below;\n\r\n- When there is empty slot among BCT blades, one of platform scripts fails since it cannot handle the other remaining servers so that apply the script\'s functionality.\n\r\nSolution is; \r\n- The design team has finished their fix for this problematic issue and  it will be included in next available patch.\n\r\nWhat I did in the customer site;\r\n- I used force next option since it is known issue and no impact to system.','null'),(782,'Tugrul Timorci (NETAS External)','AS-OAM','2012-02-16','120217-316390','Cable Onda','Donnell Called me for upgrade wizard issue. I logged in the site and checked the system \n\r\nI saw below error \n\r\nDuring the Post Upgrade instllation using the MCP Upgrade wizard (Screen 38), the install of the ASU files fail to install correctly \r\n============================================================================================= \r\nFinding the necessary installation files \r\nChecking and creating the ASU and PC Client directories \r\nInstalling Signed ASU File \r\nInstalling PCC JNLP File \r\nInstalling PCC EXE File \r\nChanging the ASU,PC Client, Online Help files and directories permission\n\r\nI checked the primary and secondary system there were wrong ASU files in the path. I put correct ASU file to path and retry the post upgrade. Wizard start to install ASU files.\r\nAfter agreement I dropped the call.','null'),(783,'Tugrul Timorci (NETAS External)','AS-OAM','2012-02-16','120217-316338','Cable Onda','Donnell Called me for upgrade wizard issue. I logged in the site and checked the system \n\r\nI saw below error \n\r\nValidating the scripts and files required for upgrade. \n\r\nValidating the content of /var/mcp/install/installprops.txt failed. \r\nSee log file for possible details: /var/mcp/upgrade_tools/logs/mcpUpgrade/ut_mcpUpgrade.EMS1_2d4c0060-29e9-1b21-a031-000e0caff8ad_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.20120216_222913.log \n\r\nInvalid Properties File \r\nDB & SM upgrade failed. \r\nExecuting DB & SM upgrade \r\nLogs are written to /var/mcp/upgrade_tools/logs/monitored_scripts/monitored.ut_mcpUpgrade.EMS1_2d4c0060-29e9-1b21-a031-000e0caff8ad_ut_mcpUpgrade.pl_UPGRADE_DB_SM_1.20120216_223026.log\n\n\r\nI checked the primary and secondary system installprops.txt\n\r\nPrimary was ne.config=CS2K-A2_Medium_CC3310\r\nSecondary was ne.config=CS2K-A2E_Medium_CC3310\n\n\r\nI corrected the primary one to ne.config=CS2K-A2E_Medium_CC3310 and retry the upgrade. Upgrade wizard start the database upgrade. After agreement I dropped the call.','null'),(784,'Tugrul Timorci (NETAS External)','AS-OAM','2012-02-16','120216-316143 ','Avaya Inc.','Kyle(ER) called me for firmware update issue. \n\r\nI logged in the site and checked the system. HT_langley servers are running on 12.0.25 platform load. When we try to update firmware we faced below log\n\r\nOne-Boot Flash Update Utility Ver 9.70 Build 8\r\nCopyright (c) Intel Corporation 2003-2008\n\n\r\n*ERROR* Unable to read FW version from the BMC.\n\r\nERROR flashupdt tool could not be executed.\n\n\r\nThis will not affect the MR upgrade so this update can apply in other MW. Also I provided the workaround solution to the customer which is Slax live CD update. New case created for this issue and this issue will follow with case.','null'),(785,'Tugrul Timorci (NETAS External)','AS-OAM','2012-02-15','120215-315967','Cable Onda','Donnell Williamson called me for setupDbReplication issue. \n\r\nCustomer upgrade the their site CVM11 to CVM 13. I logged in to the site and checked the logs. I found the below logs\n\n\r\nDeploying the DB... \n\r\nDB Operation Completed. \n\r\nResync from primary DB to secondary DB. This will take a while, please be patient and wait ... \r\nError occurred executing NE commands (see below): \n\r\n/opt/mcp/ned/bin/nedclient: Error! Could not connect to 10.200.6.5:4891 or 10.200.6.5:4890: Connection refused \r\nError: when execute above command \r\nNE command exited with the value: 1\n\r\nI restart the NED both of DB server and I restarted the ./setupDBReplication.pl script. Script executed successfully. After agreement I dropped the call.','null'),(786,'Senem Gultekin (NETAS External)','AS-OAM','2012-02-13','120213-315453','Ivory','Problem Description:\n\r\nER paged me for a primary EM server disk issue seen at Ivory. Customer is running on 14.0.9 MR.  And /var/mcp partition is %100 full as following;\n\r\n____________________________________________________________\r\n[root@GLSWSSLEM01 ~]# df -k\r\nFilesystem           1K-blocks      Used Available Use% Mounted on\r\n/dev/md4               2030672    886008   1039848  47% /\r\n/dev/md8               2030672     57448   1868408   3% /var\r\n/dev/md10            115041144 109106920         0 100% /var/mcp\r\n/dev/md9               3049960     71756   2820776   3% /var/log\r\n/dev/md7               6092288   2424008   3353816  42% /opt\r\n/dev/md6                497765     12229    459837   3% /tmp\r\n/dev/md5                497765    181735    290331  39% /home\r\n/dev/md1                101018     11861     83941  13% /boot\r\ntmpfs                  4077556         0   4077556   0% /dev/shm\r\n/dev/md0                 93207      5726     82669   7% /admin\r\n____________________________________________________________\n\n\r\nSolution:\r\n-	I had some problem with my pc at home, requested 45 min from ER to go to work.\r\n-	Once I was in the office Ive accessed to the site and started to investigate the folders that is maximizing the disk usage.\r\n-	During my investigation SM performed autoswact and alarms were cleared. Primary EM /var/mcp partition was decreased as following;\n\n\r\n[root@GLSWSSLEM01 var]# df -k\r\nFilesystem           1K-blocks      Used Available Use% Mounted on\r\n/dev/md4               2030672    886012   1039844  47% /\r\n/dev/md8               2030672     57448   1868408   3% /var\r\n/dev/md10            115041144  16570460  92532668  16% /var/mcp\r\n/dev/md9               3049960     71768   2820764   3% /var/log\r\n/dev/md7               6092288   2424016   3353808  42% /opt\r\n/dev/md6                497765     12229    459837   3% /tmp\r\n/dev/md5                497765    181735    290331  39% /home\r\n/dev/md1                101018     11861     83941  13% /boot\r\ntmpfs                  4077556         0   4077556   0% /dev/shm\r\n/dev/md0                 93207      5726     82669   7% /admin\n\n\n\r\n-	Collected logs and agreed with customer that I will investigate the logs for RCA.\r\n-	Ended the call.','null'),(787,'Senem Gultekin (NETAS External)','AS-OAM','2012-02-10','120211-315431','Cypress','Problem Description:\n\r\nSWD delivery paged me for a wizard upgrade issue seen at Cypress live site. Upgrade path is 12.0.12 to 14.0.9.0. Screen 39 of A2 upgrade wizard: Backup of Network Elements. The following error occurred.\r\n____________________________________________________________\r\nMONITOR_SCRIPT_COMPLETED ut_bkupSvr.pl EMS1 FAILED Estimated space required for backup: 5012477 KB\n\r\nRemote disk space check failed 2012-02-11_05:48:34 2012-02-11_05:48:48\r\nMONITOR_SCRIPT_COMPLETED ut_bkupSvr.pl EMS2 FAILED Estimated space required for backup: 4305036 KB\r\nCreating tar file for backup set \"mcpPlatform\"\r\nBackup set \"mcpPlatform\" written to local disk\r\nSending backup set \"mcpPlatform\" to remote server\r\nBackup set \"mcpPlatform\" sent to remote server\r\nCreating tar file for backup set \"mcpApp\"\r\nBackup set \"mcpApp\" written to local disk\r\nSending backup set \"mcpApp\" to remote server\r\nBackup set \"mcpApp\" sent to remote server\n\r\nLocal disk space check failed 2012-02-11_05:48:38 2012-02-11_05:49:40\r\nSCREEN_RETRY_ENABLED POST_BACKUP 2012-02-11_05:49:46\r\nSCREEN_FAILURE POST_BACKUP 2012-02-11_05:49:46\r\nSCREEN_SAVE_EXIT POST_BACKUP 2012-02-11_06:04:21\r\n____________________________________________________________\n\r\nSolution:\r\n-	I was investigating the issue but customer wanted to apply 14.0.9.4 patch asap  due to maint window overflow.\r\n-	Applied 14.0.9.4 patch with SWD.\r\n-	Since taking backup is a post upgrade step, customer agreed that this issue can be hanled later with GPS investigation.\r\n-	Collected the logs and investigation will continue on weekdays by GPS.','null'),(788,'Senem Gultekin (NETAS External)','AS-OAM','2012-02-10','120211-315429','Cypress','Problem Description:\n\r\nSWD delivery paged me for a wizard upgrade issue seen at Cypress live site. Upgrade path is 12.0.12 to 14.0.9.0. Section is; screen 27 of A2 upgrade wizard: Upgrade Primary DB and SM.\r\nThe following error occurred.\r\n____________________________________________________________\r\nError occurred executing NE commands (see below):\n\r\nNE command exited with the value: 1\n\r\nError at Updating schemas in the DB.\r\n  Issuing ned command \"put 216.246.200.132:2100 /var/mcp/upgrade_tools/work/EMS1_2833e666-29e8-1b21-ab56-000e0cc14b45_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.state ../../../../../var/mcp/upgrade_tools/work/EMS1_2833e666-29e8-1b21-ab56-000e0cc14b45_ut_mcpUpgrade.pl_UPGRADE_DB_SM_0.state ntappsw:dummyPassword f1a52666c084cccdd1d7a48af460c314 3600\"  \r\n 02:23:22 Cleaning ned session files\r\n 02:23:24 Running ned cmd: /bin/rm -rf /var/mcp/run/dummyRelmcpUpgrade\r\n____________________________________________________________\n\r\nSolution:\n\r\n-	Ive accessed to the site and checked the logs, it seemed that there was a problem with ned.\r\n-	Restarted ned for both EM servers.\r\n-	Hit retry from Upgrade Wizard for screen 27 and it passed.\r\n-	SWD continued with the upgrade.','null'),(789,'Senem Gultekin (NETAS External)','AS-OAM','2012-02-10','120211-315428','Cypress','Problem Description:\n\r\nSWD delivery paged me for a wizard upgrade issue seen at Cypress live site. Upgrade path is 12.0.12 to 14.0.9.0. Section is; screen 21 of A2 upgrade wizard: Switching Service to Secondary Network Instances.\n\r\nWhen the A2 wizard was attempting to disable and shutdown BCP 3 it was unable to do so.\n\r\nSolution:\n\r\n-	Ive accessed to the site and checked the logs, BCP3 was stuck at Stopping, it was not going on to Offline mode.\r\n-	Accessed to the MCP GUI and killed the current BCP3 instance manually. It was turned to Offline mode. \r\n-       Also I\'ve restarted the ned for BCP3 server.\r\n-	Hit retry from Upgrade Wizard for screen 21 and it passed.\r\n-	SWD continued with the upgrade.','null'),(790,'Tugrul Timorci (NETAS External)','AS-OAM','2012-02-13','120213-315589','Singtel Optus Pty Ltd','ER (Jeff Brennan)called me for SEMS2 unit 1 was cycling between initializing  and synchronizing issue. \n\r\nI recommended to stop both of SESM2 instance to recover based on previous cases. SESMs did not start 30 mins. I recommended to stop both of SESM2 instance again and start primary one of the SESM2 server instance and said to ER page CALLP pager. \n\r\nBasar involved the pager as CALLP team and he advised to wait 15 -20 mins. This issue is known from OPTUS side and also they have a case for this issue (120201-313489)\n\r\nThis is not related to OAM team. After agreement I left the conference.','null'),(791,'Senem Gultekin (NETAS External)','AS-OAM','2012-02-09','120209-315009','Axtel','Problem Description:\n\r\nSWD paged for a BCT Management Module login issue during CVM12 to CVM13 upgrade. Since management modules have problem SWD cannot login and run install kvm for BCP servers.\n\r\nSolution:\n\r\n-	Logged into the site and tried to login to the 2 Management Modules(172.16.134.69 and 172.25.130.4). One does not come up after login window and the other one is unreachable.\r\n-	Restarted the Management Module(172.16.134.69) and issue has been cleared, we were able to login.\r\n-	For the 172.25.130.4 management module, Ive requested to customer to check their network and vpn configuration. It seemed that 172.25.130.4 was not in that vpn.\r\n-	Customer checked and added 172.25.130.4 to the vpn.\r\n-	SWD was able to reach both management modules and continued to the upgrade.','null'),(792,'Tugrul Timorci (NETAS External)','AS-OAM','2012-02-04','120201-313473','Frontier Communications (CZN)','Ege  told me that the customer replaced one of the RTM cards on ATCA Server, and he tried to install A2 software into the replaced card. \n\r\nWe installed the Linux to ATCA server. However, he had a problem with setting up the environment and received some error messages.Currently, the platform OS was installed successfully, however they didn\'t have the MCP Core Bundle CDs at the site, so they couldn\'t install the oracle software on it. They will install oracle software and MCP software when they get the MCP Code bundle CDs.','null'),(793,'Senem Gultekin (NETAS External)','AS-OAM','2012-02-04','120204-314213120204-314213','Cypress','Problem Description:\n\r\nSWD paged A2 OAM for a platform patch failure on SESM server at Cypress live site. MR upgrade path is from 12.0.6.16 to 12.0.12.\r\nEM1 and EM2 has been platform patched properly, but SESM server platform patch was failing with the following error;\n\r\nMounting image \"/var/mcp/platform/os/images/mcp_core_linux-12.0.21.patches.r-1.iso\"...\r\nValidating image...\r\nImage validation successful.\r\nInvoking patch image driver...\r\nThe following raid-1 devices are faulty:\r\n\"md0\"\r\n\"md1\"\r\n\"md2\"\r\n\"md3\"\r\n\"md4\"\r\n\"md5\"\r\n\"md6\"\r\n\"md7\"\r\nPlease correct faulty software raid-1 device(s) and re-execute the patching tool.\n\r\nSolution:\r\n-	Accessed to the site and check the disk/partitions. It seemed that sdb was all down. There was only sda up.\n\r\n***** Software RAID-1 Devices ***** \n\r\nMD Device | | Member-0 | Member-1 | Sync/Recovery \r\nName Size(MB)| Usage | Name Flg | Name Flg | Mode Speed Fin Done \r\n--------------+------------+-----------+-----------+--------------------------\r\nmd0 101.9 | /admin | sda1 U | . _ | . . . . \r\nmd1 101.9 | /boot | sda2 U | . _ | . . . . \r\nmd2 2047.2 | swap | sda3 U | . _ | . . . . \r\nmd3 2047.2 | swap | sda9 U | . _ | . . . . \r\nmd4 2047.2 | / | sda6 U | . _ | . . . . \r\nmd5 6141.9 | /opt | sda7 U | . _ | . . . . \r\nmd6 5122.2 | /var | sda8 U | . _ | . . . . \r\nmd7 52297.4 | /var/mcp | sda5 U | . _ | . . . .\n\r\n-	Tried to add sdb with the following command. \r\n#mcpSwRaid.pl add sdb\n\r\n-	But it return with the following error;\n\r\nDevice \"/dev/sdb\" either not defined or inaccessible\n\r\n-	It seemed like server was not able to see sdb at all. Rebooted the server.\r\n-	Ran mcpSwRaid.pl add sdb again and it started sync up, it took around 2 hours.\r\n-	During SESM server was being synced, SWD continued with the other servers platform patch (SESM1 and BCPs)\r\n-	Ive waited and assisted until the sync and upgrade (oracle patch, core upgrade and instances deploy) was completed successfully.\r\n-	Cypress is running on 12.0.12 MR now.','null'),(794,'Ege Varhan (NETAS External)','AS-OAM','2012-02-05','120205-314227','Axtel','Customer: Axtel\r\nLoad: MCp 10.3 (Upgrading to MCP 12.0.6.13)\n\r\nI was paged by Wesley from SWD team. Wesley told me that he finished upgrading the primary side, and all NEs on the primary side were deployed on MCP 12.0.6.13. But SESM instances in the new load couldn\'t be initialized. We had to  make them up, so the customer could test them in the new load, otherwise we had to rollback the system quickly. I told Wesley to call CallP pager. Ozan Terzi from CallP team got involved. Ozan suggested us to take down both SESM instances (for ex. SESM1_0 and SESM1_1) and then restart SESM1_0 in MCP 12.0.6.13 (upgraded load). We did what Ozan suggested and SESM1_0 was started and became active. The customer made call tests and the upgrade continued.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(795,'Ege Varhan (NETAS External)','AS-OAM','2012-02-05','120205-314229','Axtel','Customer: Axtel\r\nLoad: MCP 10.3 (During upgrading to 12.0.6.13)\n\r\nWesley Reisdorph from SWD ran mcpUpgradeFrom10To12.pl. However, it took almost 1.5 hours to complete.Wesley though it was stucked and called me. I have connected to the site and checked the logs. While i was checking the logs, the script was completed. So, i have left the call.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(796,'Ege Varhan (NETAS External)','AS-OAM','2012-02-04','120204-314219','UPC Netherland','Customer: UPC Netherland\r\nLoad: MCP 10.3 (Upgrading to MCP 12.0.6.16)\n\r\nI was paged by Guido Zijlstra from SWD. Guido told me that he was going to start an upgrade from MCP 10.3 to MCP 12.0.6.16, and both databases were not replicated. So, he wanted to set up the replication first and then started the upgrade. He told me that he performed \"setupDBReplication.pl\" and waited for it to complete almost 2 hours.\n\r\nI have connected to the site and checked the logs. The script seemed to be hang. Killed the script and suggested him to perform \"resync.pl\" instead. He performed \"resync.pl\" and completed the replication within 40 minutes. He has tested the replication with \"testReplication.pl\" and confirmed that both databases became replicated. I have suggested him to continue to upgrade and left the call.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(797,'Ege Varhan (NETAS External)','AS-OAM','2012-02-04','120204-314202','Axtel','Customer: Axtel\r\nLoad: MCP 10.3 (Upgrading to MCP 12.0.6.13)\n\r\nI was called by Wesley Reisdorph from SWD team. Wesley told me that he wanted to start the upgrade from MCP 10.3 to MCP 12.0.6.13, however he saw a lot of deferred transactions (more than 1000, and was increasing very fast). According to the upgrade procedure, there must be less than 50 deferred transactions in the system before the upgrade can be started. I have found out that there was a oracle broken job on OEM console. I have fixed the job and number of the deferred transactions started to decrease. However, the same oracle job kept borken within 2 minutes. Then, i have realized that there is a problem with the listener configuration, and fixed it and then restarted both primary and secondary databases. After the configuration change and database restart, the problem was solved. So, i have left the call.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(798,'Ege Varhan (NETAS External)','AS-OAM','2012-02-04','120204-314220','UPC Nederland','Hello,\n\r\nI was paged by Guido Zijlstra from SWD. Guido told me that they have already finished the half of the upgrade from MCP 10.3 to MCP 12.0.6.16. They tried to start SESM3_0 and SESM7_0 instances in the new load (12.0.6.16) for the customer to test. But, SESM instances couldn\'t be started. Their status were changed constantly from \"Synchronizing\" -> \"None\". Since, the time was very limited (1.5 hours left for M.W to end), The customer wanted to rollback. Unfortunately, they performed a rollback because of this problem.\n\r\nThe same problem also was observed on Axtel\'s  upgrade on the weekened. But, that time we performed a workaround and continued to upgrade.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(799,'Ege Varhan (NETAS External)','AS-OAM','2012-02-03','120201-313473','Frontier Communications (CZN)','I was called by ER (John Kishner). John told me that the customer replaced one of the RTM cards on ATCA Server, and he tried to install A2 software into the replaced card. However, he had a problem with setting up the environment and received some error messages. We helped the customer during pager call step by step, also on the weekend, we continued to support. Currently, the platform OS was installed successfully, however they didn\'t have the MCP Core Bundle CDs at the site, so they couldn\'t install the oracle software on it. They will install oracle software and MCP software when they get the MCP Code bundle CDs.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(800,'Ege Varhan (NETAS External)','AS-OAM','2012-02-03','111207-303790','VENTELO SVERIGE AB','Software: MCS 9.0 (EOL)\n\r\nKyle Mawst (ER) called us and stated that Account Manager was not able to start on MCS 9.0.\n\r\nI have connected to the site and connected to the server which has account manager installed via SSH. I have performed \"#neinit -p\" to find out the process id of the account manager, and killed with \"kill -9 process_id\". I have started to account manager from MCP GUI again after it was killed. This time, it has started successsfully, and i left the call.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(801,'Ege Varhan (NETAS External)','AS-OAM','2012-02-02','120202-313721','TelstraClear','ER (Kyle Mawst) contacted me to report that after the customer started a system backup on the active SM (SM 0), the system initiated a swact on the SM\'s and SM 0 is now down. the customer noted that the same problem occured several weeks back (ref case 120116-310893). Kyle  logged into SM 1 and was able to ping SM 0 but was unable to SSH to SM 0. Kyle  had site power cycle SM 0 to recover SM 0. After SM 0 recovered, SM 1 was reporting two major alarms - Blocked threads detected and No connection to DB Instance 0. The SESM servers and RTP Portals were also reporting \"No connection to DB Instance 0 alarms\". Kyle  requested the customer to restart the PROV1_0 instance but when they issued a stop, PROV1_0 got stuck in stopping state. Customer tried a Kill on PROV1_0 but it stuck in a killing state. I contacted to the site and performed a neinit restart on SM 1 followed by a kill -9 on the PROV1_0 instance and PROV1_0 recovered. Then i stopped the active SM 1 to initiate a swact. SM 0 took activity but SM 1 never went offline. Then I issued a kill on SM 1 to bring SM 1 offline. SM 1 was then restarted and the SM alarms cleared. Next, the SESM\'s and RTP portals were restaretd to clear the No connection to DB Instance 0 alarms on these nodes. Finally, I restarted the secondary database to clear an outstanding DB alarm. The MCP GUI showed no active alarms at this point. \n\r\nSınce all alarms were cleared, i have left the call.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(802,'Ege Varhan (NETAS External)','AS-OAM','2012-02-02','120202-313748','Hargray Communications Inc.','I have a pager call from Munir Ahmad(SWD) about config backup script. Munir told me when he tried to take a backup, he received the error message below;\n\r\n------------------------------------------------------------------------\r\nCan\'t exec \"/usr/bin/ssh-agent\": No such file or directory at /usr/local/bin/sshUtl.pm line 2304.\r\n0;ntsysadm@ptvlscsslm00:~[ntsysadm@ptvlscsslm00 ntsysadm]$\r\n------------------------------------------------------------------------\n\r\nActually, this problem has been observed in the past and solved in the newer releases. Also, there is a workaround for the problem. I have applied the workaround below on all servers.\n\r\nThe workaround\r\n-------------------------------------\r\nroot@REGNDS1-PA-0:~[root@REGNDS1-PA-0 root]# ln -s /usr/local/bin/ssh-agent  /usr/bin/ssh-agent\r\nroot@REGNDS1-PA-0:~[root@REGNDS1-PA-0 root]# ln -s /usr/local/bin/ssh-keygen  /usr/bin/ssh-keygen\r\nroot@REGNDS1-PA-0:~[root@REGNDS1-PA-0 root]# ln -s /usr/local/bin/ssh-add  /usr/bin/ssh-add\n\r\nThis problem has already been fixed and implemented into 12.0.12 MR.\n\r\nThanks,\r\nEge varhan\r\nA2 OAM GPS','null'),(803,'Ege Varhan (NETAS External)','AS-OAM','2012-01-31','120105-308044','Hong Kong Broadband','Customer: Hong Kong Broadband\r\nProduct: MCS 3.0\r\nProblem: There is no replication between primary and secondary databases.\r\nSolution: We have applied the replication procedure at the site.\n\r\nNormally, replication was failing because of the 19 GIGABYTE repq_data.dbf file. We have modified the replication scripts at the customer\'s site, and applied the replication procedure. It was completed successfully.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(804,'Ege Varhan (NETAS External)','AS-OAM','2012-01-31','120131-313410','Optus','Customer: Optus\r\nCaller: Ronald Mesina\r\nProblem Description: One of the Session Manager Instances can not be sync with the active one.\r\nSolution Description: We have stopped both SESM instances (SESMx_0 and SESMx_1) and started them one by one. It took almost 45 minutes for an instance to become active or hotstandby.\n\r\nRonald paged me and told me that one of the SESM instances is not able to become \"Hotstandby\". I have connected to the site and checked OSS logs. I have seen some checkpoint related error messages. I have told the customer to stop both SESM instances, and then tried to start them again one by one. He stopped both SESM instances, and started SESM1_0. It took almost 45 minutes for that instance to come up again. During that time we were in E1 situation, so Ronald has called ER (Tom Drapper) and callp team (Ozan Terzi).  We have checked the logs constantly, waited for the instance to become \"Active\". It has became \"Active\" after 45 minutes passed. When the first instance was Active, we started another instance. It took 45 minutes for it to become \"Hot standby\" too. The case is investigated by Call Processing team currently.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(805,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-01-28','120128-312977','Millington Telephone Company','Rodney has paged me since PA servers were down on customer\'s site. I have connected to site and check the instances on their system. All MCP instances were up and running except Personal Agent. I have checked the personal agent instance but there was no instance created. So, I have asked to customer why your instance was not found in the list or was someone removed it. They did not know the cause of the lack of PA instance but they said that there was no problem on PA yesterday. I have demanded for EMServer1 and EMServer2\'s server password so that I can find any log related with PA instance but they said that \"We do not know the password of users\' on servers.\" I have also asked for the which server they want to create PA instance and the server type of that server so that I will create the PA instance instead of them, but again the customer said \"We do not know which server is suitable to create this PA and its server type.\"  As if, the customer does not know the password, server type and which server PA will be created as well, so I said to Rodney that this should not be pager call since the system up and running without problem. PROV client enables the user PA features as well. So, I said to Roney that please talk with customer and persuade them for to look this case on Monday with fisrt priority. Because the details of the site is not clear and the situation is not pager needed. I have agreed with ER and left the call.','null'),(806,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-01-27','120125-312493','Rogers Communications','Aaron paged me about upgrade wizard issue which is \"Error on finding the ASU MOC installation file\". They had upgraded their system 8.0 SP1 and their state was post-upgrade step. The upgrade was not finding the ASU MOC files under /var/mcp/media/MO-Client_8.0.2101 directory since their permissions and name was not same with upgrade wizard wants to have. Because upgrade wizard is looking for only one specific name for ASU files in that directory. \n\r\nSolution of the Problem;\r\n1) I have changed all files\' permission in all servers in which upgrade wizard failed.\r\n2) I have changed all files\' name to correct one. As if, before the names were like \'MMPCCClient_8.0.2101_20111005_Release.zip\' then it is changed to \'MMOfficeClient_8.0.2101_20111005_Release.zip\' since the core bundle have the updated names of them within it.\r\n3)These files should be under \"/var/mcp/media/MO-Client_8.0.2101\" directory so that upgrade wizard can see them. They are already in that directory. So, I have never changed anything there.\n\r\nAfter all these changes, I have run \"Retry\" options and Upgrade Wizard passed that screen.\n\r\nThen, I agrreed with Aaron and left the call.','null'),(807,'Mehmet Salim Demir (NETAS External)','AS-OAM','2012-01-25','120125-312324','K-OPTI.COM','Yuji Onozuka has paged me at 8:25 with Istanbul time. Then, we started conference with customer whose name is Moh Mei. Since the contivity server did not work properly on customer\'s site, I could not connect to site. Then, we started yahoo conference with Moh Mei in order to further investigation. \n\r\nThe problem was like below; \n\r\nThe customer installed standalone A2 system, but the EMserver, 2 MAS server and PA servers were not pingable by other servers within the same network. But these other servers can ping to Contivity VPN server, SBC and RSM servers. \n\r\nAs a GPS, we suggest Moh Mei to run some scripts (\"ifconfig\", \"tracert problematic IP\") on EMserver and MAS server in order to learn what was problem originated from. Customer sent us the results of this scripts and we investigated that there should be problem related with L2 switch. \n\r\nThe customer was using Preliminary documentation to do this installation and there was one EMServer on their system. So, I warn her that there should be two EMserver in order to assure redundancy between EMservers. But she said that, this was decided by customer since this is just customer trial and they decided to go for 1 server only for EMserver. \n\r\nThe solution and suggestions to find the problem were like below: \n\r\nWe suggest Moh Mei to replace the switch with dummy one (simple switch which will not need any configuration on it) and try the same scenario. After she changed the switch it failed again. We wanted to learn VLAN adresses and IPs of each server so we suggest her to run \"cat /admin/userinfo.txt\" on each server and sent us the results. After we have taken the results we saw that the mtceVlanID is different comparing with MAS and EMservers. Then, we suggest to remove mtceVlanID and use only same vlanID for both EMserver and MAS servers. After we removed the mtceVlanID and we have same vlanID for both servers, it failed again. Because of the difference subnet address of the Moh Mei\'s laptop, it did not ping the servers. Then, Moh Mei changed the L2 switch with newer one and applied same settigns on this switch. After all these settings were done, the servers were pingable from the laptop of Moh Mei which is in the same subnet and same gateway with these servers.','null'),(808,'Tugrul Timorci (NETAS External)','AS-OAM','2012-01-20','120117-310932','Cable Onda','Munir Ahmed called me for platform patch issue of the system.\n\r\nHe was trying to run \"patchPlatform.pl -force \" but system does not find the -force option on the system.\n\r\nThis system is LAB so I said I will work in working hours. after agreement I dropped the call.','null'),(809,'Tugrul Timorci (NETAS External)','AS-OAM','2012-01-17','120117-310932','Cable Onda','Munir Ahmed (swd) called me for upgrade issue. \n\r\nHe was trying to upgrade to 10.3.2 MR but he cannot able to upgrade system cause of the raid issue. \r\nI suggested to run \"mcpraidad.pl summ\" script to system for checking the failed hard drive. He said sda failed. \n\r\nI suggested to run mcpraidad.pl add sda script. SWD typed the script but failed with disk size error. \r\nI was starting  the investigate this issue but I learned this site is LAB. \n\r\nI said please create a Case and send our queue.  After this point I left the call.','null'),(810,'Tugrul Timorci (NETAS External)','AS-OAM','2012-01-17','120116-310893','TelstraClear','Tom Drapper (ER) called me for Loss of redundancy of the System Management Server\n\r\nSystem was running on 14.0.6.0\n\r\nI logged in the system over ER. I checked the system. SM1\'s status stacked at stopping. I logged in the server and run neinit p but it did not response the command. I restarted the ned and I checked the system again . \r\n SM1 status changed to online up not stopped. I stopped the instance from MCP GUI, stopped . SM_0 change to active , I started the SM1 and started to work normally. \n\r\nCustomer wanted to try again same scenario, and passed. \n\r\nAfter agreement I dropped the call .','null'),(811,'Senem Gultekin (NETAS External)','AS-OAM','2012-01-14','120113-310306','Globalcom, Inc.','Problem Description:\n\r\nER paged me for a Prov issue on Standalone system with 10.2 Release. Platform is Sun solaris and there is only one Prov on the sytem. Prov was offline and it was not coming up. Also there were sync alarms on their IPCM.\n\n\r\nSolution:\n\r\n	ER center was facing some trouble with their own system so it took some time to recover that by ER.\r\n	Accessed to the Customer site via Bomgar. Checked MCP GUI and Prov was offline and unavailable.\r\n	When I hit start button it was giving java related errors in the work logs.\r\n	Checked the ned for prov server, it was working normal.\r\n	Deleted Prov instance from the server by hitting the undelpoy. And deployed again.\r\n	Start button worked and PROV it came up as online.\r\n	Also restarted the IPCMs to clear the alarms.\r\n	Customer ran some test provisioning  and test calls, all passed successfully. \r\n	Customer Agreed to close the case.','null'),(812,'Senem Gultekin (NETAS External)','AS-OAM','2012-01-14','120114-310603','Cypress','Problem Description:\n\r\nSWD paged me for a DBdryrun script failure during prep steps of the upgrade. Cypress is planning to upgrade from 12.0.6.13 to 14.0.9.0 Release.\n\r\nError is as fallowing;\n\r\n****************************************************\r\nRUNNING SCRIPT A00024396_sippbx_ext_pa_support_4.sql\r\n****************************************************\r\n      SELECT node_id, domain_id, username, passwd, charge_dn, tel_uri, global_e164, conn_mode, supported_sip_uri\r\n                                                                                               *\r\nERROR at line 3:\r\nORA-06550: line 3, column 96:\r\nPL/SQL: ORA-00904: \"SUPPORTED_SIP_URI\": invalid identifier\r\nORA-06550: line 3, column 7:\r\nPL/SQL: SQL Statement ignored\r\nORA-06550: line 2, column 11:\r\nPLS-00341: declaration of cursor \'C_DOMAINISN_DATA\' is incomplete or malformed\r\nORA-06550: line 9, column 21:\r\nPL/SQL: Item ignored\r\nORA-06550: line 21, column 76:\r\nPLS-00364: loop index variable \'V_DOMAINISN_DATA\' use is invalid\r\nORA-06550: line 21, column 76:\r\nPL/SQL: ORA-00904: \"V_DOMAINISN_DATA\".\"DOMAIN_ID\": invalid identifier\r\nORA-06550: line 21, column 7:\r\nPL/SQL: SQL Statement ignored\r\nORA-06550: line 22, column 37:\r\nPLS-00364: loop index variable \'V_DOMAINISN_DATA\' use is invalid\r\nORA-06550: line 22, column 7:\r\nPL/SQL: Statement ignored\r\nORA-06550: line 23, column 88:\r\nPLS-00364: loop index variable \'V_DOMAINISN_DATA\' use is invalid\r\nORA-06550: line 23, column 105:\r\nPL/SQL: ORA-00984: column not allowed here\r\nORA-06550: line 23, column 7:\r\nPL/SQL: SQL Statement ignored\r\nORA-06550: line 24, column 85:\r\nPLS-00364: loop index variable \'V_DOMAINISN_\n\n\r\nActions:\r\n	Checked the logs and realized that this issue came us few days ago from another customer.  \r\n	Requested to postpone the upgrade until the rca is found. Since dbdryrun is just a prep step of the upgrade, this failure will not affect the customer at all.\r\n	Agreed with SWD and ended the pager call.\n\r\nInvestigation & Solution:\r\n	Upgrade should have had performed from the latest released patch level which is 12.0.6.16 now, not 12.0.6.13.\r\n	We have tested dbdryrun from 12.0.6.16 to 14.0.9.0 in our labs and its working properly.\r\n	Customer will upgrade the system to the latest released patch 12.0.6.16 first and later on perform 14.0.9 upgrade.','null'),(813,'Senem Gultekin (NETAS External)','AS-OAM','2012-01-11','120111-309743','Clearwire','Problem:\r\nSWD paged me for oracle patch issue seen at Clearwire Atlanta site.  Clearwire is upgrading from 12.0.6.9 to 12.0.12.\n\r\nError seen during oracle patch is;\n\r\n> run output  Database was successfully patched.\r\n> run output \r\n> run output \r\n> run output  patchDB.pl Completed at  =>  Wed Jan 11 00:59:35 2012\r\n> run output \r\n> run output      Reference the following log file for additional details:\r\n> run output       /var/mcp/db/install/patches/../../logs/patchDB.log\n\r\nError occurred executing NE commands (see below):\n\r\n patches/Security/CPUJul2011/12419249/11787763/files/lib32/libnnz10.a/ssl_hshk_priv_onerror.o\r\n patches/Security/CPUJul2011/12419249/11787763/files/lib/libnnz10.a/ssl_hshk_priv_onerror.o\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPageInvalidRequest.class\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPage.class\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPageInvalidRequest$__jsp_StaticText.class\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPage$__jsp_StaticText.class\n\r\nTerminated at  =>  Wed Jan 11 00:59:36 2012\n\n\r\nSolution:\r\n-	Checked the oracle patch logs.\r\n-	Oracle patch was completed successfully however there was error messages at the end which is not related to oracle. This is a known issue and basically its just a info message not an error. It should be ignored.  Latest upgrade documents (NN10437-500 02.13 for 7.0SP1 and NN10437-500 03.06 for 8.0 BRC) have been updated and before oracle patch, mcpInstallFirstLoad.pl script should be ran. This will change the error output of the end from;\n\r\npatches/Security/CPUJul2011/12419249/11787763/files/lib/libnnz10.a/ssl_hshk_priv_onerror.o\r\npatches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPageInvalidRequest.class\r\npatches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPage.class\r\npatches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPageInvalidRequest$__jsp_StaticText.class\r\npatches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPage$__jsp_StaticText.class\n\r\nas; \n\r\nIgnoring STDOUT => onerror\r\nIgnoring STDOUT => errorPage \r\nIgnoring STDOUT => errorPage \r\nIgnoring STDOUT => errorPage \r\nIgnoring STDOUT => errorPage \n\n\r\n-	SWD continued with the upgrade, tracked the steps with him.\r\n-	MCP upgrade and all instances upgrade completed it successfully. Clearwire is running on 12.0.12 MR now.','null'),(814,'Senem Gultekin (NETAS External)','AS-OAM','2012-01-10','120110-309487','Clearwire','Problem:\r\nER paged for PA issue on Clearwire Atlanta site. Customer is performing upgrade from 12.0.6.9 to 12.0.12 MR. After platform patches they were not able to reach PA server. \n\r\nSolution:\r\n-	PA server was not pingable\r\n-	Requested power cycle to be performed by someone on site.\r\n-	Since there was no one at the site at that time customer agreed to perform power cycle next day.\r\n-	Next day customer performed power cycle and PA server came up properly. Its up and running now.','null'),(815,'Senem Gultekin (NETAS External)','AS-OAM','2012-01-09','120109-308716','Telefonica','Problem:\n\r\nER paged me for server issue at Telefonica 3.0 site. Both Prov servers were down. System is running on sun solaris servers. Please note that 3.0 is almost 10 years old release and has been EOL years ago.\n\r\nSolution:\n\r\n-	Accessed to the site and rebooted the servers but it was still getting stuck, disks were problematic. \r\n-	Ken Johnson from RTP GPS joined the bridge, he is experienced in sun solaris systems.\r\n-	On Prov1 the disk1 had failed and it was not able to synchronize with disk 0. This failure was not enabling the mirroring to become writable and it was causing boot to fail. Deleting mirroring on the disks recovered the disk 0 to be writable. Thus the server boot up properly.\r\n-	On Prov0 the disk1 was having the same issue. Removed the problematic disk from the server.\r\n-	Stopped and started prov instances from server. Customer logged into Prov GUI.\r\n-	Currently both Prov servers are up and running with only 1 disk.  Customer will contact sun solaris support and request new disks for both Prov servers.\r\n-	Agreed with customer and left the conference.','null'),(816,'Ege Varhan (NETAS External)','AS-OAM','2012-01-07','120107-308655','Avaya','Lee Taylor (Avaya) was preparing for the upgrade of A2 system of a customer from MCP 12.0.6.0 to  MCP 12.0.6.13 through MCP 12.0.6.6 as a middle path. Before he started to patch the system, he performed a pre-check script(mcpSystemValidation), and received the error below;\n\r\n[root@SSLEMS1 log]# cat mcpSystemValidation_20120106203558.log\n\r\n-------------------- MCP SYSTEM VALIDATION TOOL - Ver 1.2 --------------------\n\r\nExecution Time: 01/06/2012  20:35:58\n\r\nHardware Environment: IBM-X3550\r\nPlatform Environment: ple2\n\r\n------------------------------------------------------------------------------\n\r\nHostname Validation : ............................................... [PASSED]\n\r\n------------------------------------------------------------------------------\n\r\nFAILED : IBM-X3550 can not be used with ple2 platform environment\r\nExiting the program...\n\n\r\nThe pre-check scripts stated that IBM-X3550 can not use ple2 image, but that server was already using ple2 image at that time. I have checked the oracle level of the customer and found that it was Oracle 10g. Since, they use Oracle 10g, i suggested them to upgrade the MCP system to MCP 12.0.6.13 directly, and skip MCP 12.0.6.6 and platform patch available. He upgraded the system and tested all call scenarios after upgrade was completed. All of the call scenarios were successful. So, I left the call.\n\r\nThanks,\r\nEge','null'),(817,'Ege Varhan (NETAS External)','AS-OAM','2012-01-04','120105-308044','Hong Kong Broadband','The product is MCS 3.0 (EOL). We tried to give the best effort since it is too old for us to handle.\n\r\nProblem Description: While the customer performed re-sync, the re-sync failed. After it failed, Some Network Elements tried to communicate with the secondary database even though the primary database is up and running. Additionally, the secondary database had some missing data because of the failed re-sync. The databases were not replicated.\n\r\nSolution Description: We have opened MCS GUI, and restarted all the network elements which had database errors. Because, those elements tried to communicate with the secondary database, and  thought that the primary database was down even though it was up and running at that time. \n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(818,'Ege Varhan (NETAS External)','AS-OAM','2012-01-06','120105-308044','Hong Kong Broadband','This is the following of the problem(E2) happened on January 5, 2012.\n\r\nThe customer scheduled a M.W to perform re-sync, however re-sync was failed. Since, we didn\'t have enough time to complete it, we canceled the progress. After i left the M.W., i was called by John (ER) indicating that some of the users are not able to get a service package from Provisioning Servers. So, the customer wants to restart Prov instances. Since, they are not able to open MCP GUI because of the re-sync related problem, they have to do it from the console via ssh commands. But John told me that they don\'t know how to do it from the command prompt. I have connected to the site, and found \"startWeb\" and \"stopWeb\" scripts, and provided the procedure to the customer. Since, the release is too old (MCS 3.0!), it took time for us to find and test the necessary commands.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(819,'Ege Varhan (NETAS External)','AS-OAM','2012-01-04','120104-307841','RMobistar Belgium','Problem Description:\r\n--------------------\r\nI was called by Edwin Verheij, who is SWD. Edwin told me he performed an oracle \r\npatch onto Primary EM Server, and script finished with some error messages. The error messages are below;\n\r\nISSUE: After Oracle patching following errors are seen seen:\n\r\n-----------------------------------------------------------------\r\nError occurred executing NE commands (see below):\n\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPageInvalidRequest$__jsp_StaticText.class\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPage$__jsp_StaticText.class\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPage.class\r\n patches/Security/CPUJul2011/12419249/10325885/files/dv/jlib/dva_webapp_jsp.jar/_errorPageInvalidRequest.class\r\n patches/Security/CPUJul2011/12419249/11787763/files/lib/libnnz10.a/ssl_hshk_priv_onerror.o\r\n-----------------------------------------------------------------\n\r\nSolution Summary:\r\n-------------------\r\n1- Firstly, I have connected to the site, and check the current oracle patch level with the script below;\n\r\n#showversion.pl\n\r\nThe script shows that the oracle patch (21) is applied successfully.\n\r\n2- Secondly, I wanted to confirm %100 if the patch is really applied to oracle database. So, i have performed the commands below;\n\r\n#cd $ORACLE_HOME/OPatch\r\n#./oPatch lsinventory\n\r\nThe output showed me that on January 3, 11:45 PM, the specified oracle patch was applied to the system successfully.\n\r\nI have told Edwin that everything looks ok despite of the error messages. I have suggested him to continue to MR upgrade.\n\n\r\nNext Actions:\r\n--------------\r\nA new case(120104-307841) is already opened for the error messages received during the oracle patch process. We will investigate the root cause of them.\n\r\nRegards,\r\nEge Varhan\r\nA2 OAM GPS','null'),(820,'Ege Varhan (NETAS External)','AS-OAM','2012-01-02','120102-307634','Australian Tax Office','This problem was redirected by the CallP team.\n\r\nProblem Summary\r\n-----------------\r\nThe customer was upgrading it\'s A2 system from  MCP 12.0.6.17 to MCP 14.0.9.2. There were two problems observed.\n\r\nProblem 1: Prov2 instance couldn\'t be deployed and started with the new upgraded load which is MCP 14.0.9.2 \n\r\nProblem 2: SESM1_1 instance ccouldn\'t be started with the new upgraded load.\n\r\nSolution Summary\r\n-----------------\n\r\nSolution of the Problem 1: \r\n--------------------------\r\nAdmin state of Active SM (System Manager) instance was \"Offline\" even though the SM was up and running. Because of this, David Giomi couldn\'t deploy and start Prov2 instance on EMServer1 with the new load. So, we instructed him to connect to EMServer1 via SSH, and kill the active SM instance with linux kill command(#kill -9 process_id). When the active system manager was killed, hotstandby instance became active, and we were able to deploy and start Prov2 instance via the GUI of that instance.\n\n\r\nSolution of the Problem 2: \r\n--------------------------\r\nSESM1_1 instance couldn\'t be deployed onto SESMServer2 server. We instructed David to connect to that server and check the applied platform patches. We have realized that some platform patches were missing in the system. We told him to run \"patchPlatform.pl\" script to upgrade the platform  to desired level which is 14.0.16. However, the patch was failed. We couldn\'t find the root cause of the patch failure, and didn\'t have enough time to fix it. Since it was a Session Manager server, we decided to re-install the platform, which is an easier path to fix the problem. We sent the necessary documents to David to re-install the platform from scratch. After the platform was re-installed, he was able to deploy and start SESM1_1 instance with MCP 14.0.9.2 load.\n\n\r\nSince, the customer is Australian Tax Office (ATO), we weren\'t allowed to connect to the site because of the security reasons. We tried to help David via Yahoo chat and E-mail messages with screenshots. That\'s why the pager duration took longer than expected.\n\r\nRegards,\r\nEge','null'),(821,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-12-23','111223-307098','VTL (UK) LTD','Dean has paged me on friday about not opening MCP GUI in the VTL customer. I have connected to the site and check the MCP GUI. But it was given certificate error when I try to login. First of all, I tried to stop secondary SM and try to open MCP GUı on prinmary SM. But it did not be successfull. Then, I changed the certifacate in the database to default certificate. Then, I tried to login and it was successfull. The cause of the problem was wrong or corrupted certificate was used on the customer. After MCP GUI was opened, I agreed with ER and left the call.','null'),(822,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-12-23','111223-307059','Ziggo','John has paged me in the morning about qsip issue that customer could not do qsip over ossgate. I took the site connection data and check the PROV client that if any user can be added and read from database. It was successfull to add test user and find it over PROV GUI. I have checked the oss logs and saw the below exception;\n\r\njava.lang.NullPointerException \r\nat java.lang.String.(Unknown Source) \r\ncom.nortelnetworks.mcp.ne.share.ncas2k.qsip.data.QSIPResponseData.copy(QSIPResponseData.java:594) \n\r\nI have searched the exception in the salesforce and found that there is one case similar to this opened before.(111214-305358) I have restarted the SM instances and then SESM instances after the permission was taken from customer which was the action plan of the found case above. The issue is resolved after that and customer said to us that we can do qsip over ossgate anymore. I said to John that the oss logs under the both emserver should be taken and attached to the case for further investigation about this case. Then, I have agreed with ER and left the bridge.','null'),(823,'Tugrul Timorci (NETAS External)','AS-OAM','2011-12-16','111216-305946','UPC','Suna called me for UPC Provisioning issue on UPC LAB. \n\r\nWhen they try and add a new domain in the PROV GUI, they faced the following error:\n\r\nA data access error occurred while adding domain: No datasource(s) available at this time \n\r\nI logged in to the system and check the PROV ,for every change above error message rised.\n\r\nLogged in to MCP GUI and wanttedt ot change somethink but there was DB access error.I tried to log in primary EM server but it was un pingable and down. I asked EM server status. They send me below message\n\r\n\"SSLM0, slot 8 in the ATCA (i.e. application blade in the ATCA). Allen has tried re-seating it, powercycling it, etc. and it always has the red OOS light lit.  There is no response from the console port.\"\n\r\nI told them , there is HW issue and PROV needs to connect for data change or data add. Primary DataBase is on SSLM0. I suggest server change.\n\r\nAfter agreement dropped the call .','null'),(824,'Tugrul Timorci (NETAS External)','AS-OAM','2011-12-15','111215-305481','Thunder Bay Telephone','Jarrad SWD called for Upgrade failure. \n\r\nChecked the site, patch was failing with db.app.user \"mcsdbapp\" login error. \n\r\nIt was due to CREATE SESSION privilege absence for this \"mcsdbapp\" user in Oracle 10g database. Furthermore when checked I saw that ssdvdb schema user did not have any tables associated with it. \r\nWe restored the DB to backup taken by patch script automatically in the beginning of the patch upgrade. All permisions and ssdvdb tables are recovered. \r\nAfterwards mcpPatch.pl run without any issues.\n\r\nRoot Cause analysis: \n\r\nWhen mcpPatch.pl was run for the first time at 02:15 the script could not complete and gave the following error before it was able to restore SSDVDB tables and system privileges on Oracle DB. \n\r\nERROR at line 1: \r\nORA-04020: deadlock detected while trying to lock object SSDVDB.MCSDB_REPGROUP \r\nORA-06512: at line 7 \n\r\nThis indicates that the patch script was run while Replication was still active however replication should have been cleaned up prior to the upgrade. \n\r\nI see a resync log for setting up the DB replication on Dec14 \r\n> run output resync.pl Completed at => Wed Dec 14 22:50:06 2011 \r\n> run output \r\n> run output Reference the following log file for additional details: \r\n> run output /var/mcp/run/MCP_12.0/ssdvdb_0/work/resync.log \n\r\nyet no cleanupReplication log is present therefore most likely this documented step was missed. \n\r\nAfter agreement we dropped the call. \n\r\nThanks Emre Bertan for your help.','null'),(825,'Tugrul Timorci (NETAS External)','AS-OAM','2011-12-15','111209-304613','Claro Codetel','SWD Munir called me for Oracle migration issue. \n\n\r\nHe started the Oracle migration script but it failed after 80 mins later. \r\n> run err Error! Timer expired while executing: perl Error executing Oracle migration commands.\n\r\nI logged in the system over SWD. And checked the logs. There is no more information just there was timer expired error message. I executed the migration script again. After 30 mins I received below message on the primary server. \n\r\n[ntdbadm@SSLMMEL0 db]$      \r\nMessage from syslogd@SSLMMEL0 at Thu Dec 15 01:59:13 2011 ...\r\nSSLMMEL0 lra: Please check group Voltage Probe on this machine  \n\n\r\nI checked the dmesg but there was no hardware error. I advised the checking voltage values. There was no one for check voltage value. \n\r\nAfter 80 mins migration script finished successfully. After agreement I dropped the call','null'),(826,'Tugrul Timorci (NETAS External)','AS-OAM','2011-12-15','111215-305481','Thunder Bay Telephone','SWD called me for DB connection issue. He wanted to resync DB but he faced below issue\n\r\nDeploying the DB...\n\r\nDeploying files to the Secondary DB\n\r\nDeploying the DB...\r\nError occurred executing NE commands (see below):\n\r\n> config err Error! Could not get user id ntdbsw from password file.\r\n> unknown cmd Error! Instance not configured.\r\n> unknown cmd Error! Instance not configured.\n\r\nI tried to log in to the system but bomgar was really so slow So I wanted logs from SWD. We checked the secondary side and logs , but secondary side had not  installed Oracle. I suggest install oracle and patch to SWD. After agreement I dropped the call.','null'),(827,'Tugrul Timorci (NETAS External)','AS-OAM','2011-12-14','111214-305262','Shaw CableSystems','ER called me for PROV instance could not be online \n\r\nI loggen in the system and check the instance. It was configured and online state. I killed the process. I undeployed , deployed and started the instance. It started succesfully.\n\r\nAfter agreement I dropped the call .','null'),(828,'Tugrul Timorci (NETAS External)','AS-OAM','2011-12-13','111213-305041','Belize Telemedia Limited','Er Bill Price called me resync issue. System was on 12.0.6.13 laod. \n\r\nPrimary side was reinstalled and they want to resync system as final step\n\r\nI checked the resync logs and I saw there below issue.\n\r\nPrimary DB IP:         190.197.38.68\r\nSecondary DB IP:       190.197.38.69\r\nDatabase Username:     MCSDBSCHEMA\r\nOperation to Perform: Resync\r\nContinue with these settings?(Y/N): [N]y\r\nERROR:\r\nORA-01017: invalid username/password; logon denied\n\r\nERROR:\r\nORA-01017: invalid username/password; logon denied\n\r\nSP2-0306: Invalid option.\r\nUsage: CONN[ECT] [logon] [AS {SYSDBA|SYSOPER}]\r\nwhere  ::= [/][@] | /\r\nSP2-0157: unable to CONNECT to ORACLE after 3 attempts, exiting SQL*Plus\n\r\nI run restoreEmptyDb script for cleanup the password issue on primary side but this did not solve the issue. After that step I checked the secondary db and db password type and DB users are not same. File had mcpschema user but there was no same user in the DB\n\r\nI logged in the primary EM server /var/mcp/install I changed name of the .dbuserdata because this file stored the DB users and passwords. Also I changed the name of . dbuserdata under /var/mcp/run/MCP_12.0/mcpdb_0/data\n\r\nAfter this change I executed  the ./dbInstall.pl fo script and restored the old DB backup to secondary DataBase. \n\r\nI run resync.pl script and finished normally. \n\r\nAfter resync I undeployed and deployed the SM instances and started to work. I advised to Stephen undeploy /deploy /start all instances. He finished this procedure successfully. \n\r\nAfter agreement I dropped the call.','null'),(829,'Senem Gultekin (NETAS External)','AS-OAM','2011-12-08','111208-304285','Belize Telecommunications','Problem Description:\n\r\nSWD paged me for a cleanupReplication script stuck, before 12.0 to 14.0 upgrade at Belize site.\n\n\r\nSolution:\n\r\nAccessed to the site \n\r\nThere has been complexity in the primary server directory owners. Someone has changed them all to ntappsw:ntappgrp as the following command. \n\r\n689 chown -R ntappadm:ntappadm mcp \r\n690 ls -l \r\n691 chown -R ntappsw:ntappgrp mcp \n\r\nThese commands shouldnt have been ran. Database related directories should have been stayed as ntdbdadm. Due to this permission problem primary database cannot come up, it give directory and file permission errors such as. \n\r\nORA-01180: can not create datafile 1 \r\nORA-01110: data file 1: \'/var/mcp/db/data/mcpdb/system01.dbf\' \n\r\nThe above error is just one of the errors, there are lots of permission problem related errors in the server. \n\r\nPrimary EM server needs to be re-installed. \n\r\nThe following actions provided to CSAM (Bob Mckerlie)\n\r\nHere are the steps that needs to be performed for reinstallation \n\r\n- Copy the database backup of primary server to your local PC which is post_12.0_upgrade.tar.gz under /var/mcp/db/backup. \r\n- Install Linux Platform of 12.0 Release to the primary server (190.197.38.68)  Ive attached the Installation document, please check (12.x_platform_installation.doc) \r\n- Once you complete linux platform installation start to Oracle installation on primary server(190.197.38.68). Attached (Install_Oracle_10_2_0_4(modified).doc) \r\n- Perform resync from Secondary to Primary database. Login to secondary server (190.197.38.69) and perform resync. Attached document (Resync_the_Database.docx).\n\n\r\nAbove actions will be performed by Network Integrator','null'),(830,'Senem Gultekin (NETAS External)','AS-OAM','2011-12-07','111207-303790','Ventelo','Problem Description:\n\r\nER paged me for and E2 issue seen on Ventelo site. Site has no redundancy and ER is not able to launch the GUI. Customer is running on 9.0 which is and EOL Release.\n\r\nSolution:\n\r\n-	Ive accessed to ERs PC and started the investigation.\r\n-	Theres 2 SM servers but only primary SM is up and running. Secondary SM was not even deployed.\r\n-	To deploy secondary SM I had to launch the GUI but due to java issue it was not opening. Since 9.0 is a very very old release its using again a very old version of java.\r\n-	During this time customer requested to continue next morning.\r\n-	Ended the call.\r\n-	\r\nNext Morning\n\r\n-	Accessed to the site.\r\n-	From GPS PC we have deleted all the newer version newer version javas, and installed java 1.4. we were able to launch to MCP GUI.\r\n-	Added the secondary SM to EMServer2 Server, but I was not able to deploy it cause I received license key error.\r\n-	Suggested customer to contact KRS team for new license key.\r\n-	Customer accepted and ended the site investigation.','null'),(831,'Senem Gultekin (NETAS External)','AS-OAM','2011-12-06','111206-302991','Axtel','Problem Description:\n\r\nI was paged by Gerardo Luna for dbcom  for every instances and replication error on database servers. Customer is running on 10.3.2.0\n\r\nSolution:\n\r\nExplained Gerardo that they should perform resync during maint window. Customer agreed to perform it in our morning 8:30 AM. Left the call.\n\r\nNext Morning:\r\n-	Accessed to the site and ran resync but it failed with the following command.\n\r\n------------------------------------------------------------------------------------------------------------\r\n/opt/mcp/oracle/product/9.2.0/bin/agentctl: error while loading shared libraries: libclntsh.so.9.0: cannot open shared object file: No such file or directory\r\nlsnrctl: error while loading shared libraries: libclntsh.so.9.0: cannot open shared object file: No such file or directory\r\nFail to start oracle listener\r\nsqlplus: error while loading shared libraries: libclntsh.so.9.0: cannot open shared object file: No such file or directory\r\nFail to start database\r\n------------------------------------------------------------------------------------------------------------\n\r\nSomehow libclntsh.so.9.0 file was not on the secondary server, so I copied the libclntsh.so.9.0 from primary and put to secondary.\n\n\r\n-	Ran resync command but it failed again with a different error;\n\r\n------------------------------------------------------------------------------------------------------------\r\n> run output ERROR at line 1:\r\n > run output ORA-23320: the request failed because of values MCPUSER.WATCHLIST and \r\n> run output \"ensure_repprop_exists\"\r\n > run output ORA-06512: at \"SYS.DBMS_SYS_ERROR\", line 95 \r\n> run output ORA-06512: at \"SYS.DBMS_REPCAT_ADD_MASTER\", line 343 \r\n> run output ORA-01403: no data found \r\n> run output ORA-06512: at \"SYS.DBMS_REPCAT_ADD_MASTER\", line 2224 \r\n> run output ORA-06512: at \"SYS.DBMS_REPCAT\", line 1173 \r\n> run output ORA-06512: at line 35\n\r\n  I performed the following steps to solve above error\n\r\n-	Took the database backup of primary database\r\n-	Ran restore_ empty_db command and wiped out all the data on primary and secondary database\r\n-	Restored the database backup to primary database\r\n-	Ran resync from primary database to secondary and it completed successfully.\r\n-	Customer agreed and I left the site.','null'),(832,'Ege Varhan (NETAS External)','AS-OAM','2011-12-02','111202-302428','Ed Kucbel / 	 Thunder Bay','Initial Problem:\r\n--------------------\r\nPrimary System Manager server was rebooted. After the reboot, the server couldn\'t get started. It stucked at a screen while booting and showed the error message below;\n\r\nUSB Legacy........... Enabled\r\nATAPI CD-ROM PM-DW-224E-B\n\n\r\nSince, the site is SSL, primary EM server has database, primary SM instance, and prov2 instance. All of them were unavailable at that time.\n\r\nSolution Description\r\n---------------------\r\nWhen we connected to the site, the customer has already replaced the original disks with the spare ones. We checked the server from SM GUI, and all of the elements on the server looked down and unavailable. Initially we though that this is a hardware problem and requested the customer to replace the server with the spare one. The customer replaced the server and re-installed the server. However, when we rebooted the newly installed server, we received the same problem. After this point, we have realized that the problem could be related to the serial port, which the customer is connected to the server. Indeed, the problem was related to it. Even though the server was up and running, serial port connection didn\'t show us the real state of the server.\n\r\nWe have informed the customer about the serial port problem and requested him to connect to the server via KVM and checked the state of the server. The customer has checked the server status, and confirmed taht everything is ok with the server.  After the confirmation, we have left the call.','null'),(833,'Senem Gultekin (NETAS External)','AS-OAM','2011-12-05','111205-302844','Turk Telekom','Problem Description\n\r\nI was paged due to a resync issue seen after 12.0 to 14.0 upgrade at Turk Telekom site. Customer is running on MCP_14.0.6.6_2011-08-04-1601 Release. They performed primary to secondary resync and received failure as below; \n\r\n> run output /opt/mcp/ned/bin/nedclient: Error! Could not connect to 88.255.54.149:4891 or 88.255.54.149:4890: Connection refused\r\n> run output \r\n> run output \r\n> run output Error: when execute above command\r\n> run output \r\n> run output  ERROR: resync.pl Terminated at  =>  Mon Dec  5 21:57:09 2011\r\n> run output \r\n> run output      Reference the following log file for additional details:\r\n> run output       /var/mcp/run/MCP_14.0/mcpdb_0/work/resync.log\n\r\nSolution:\n\r\n-	Ive investigated the resync logs.\r\n-	Accessed to the site.\r\n-	Checked both EM servers and pinged them from each other, they were able o communicate. \r\n-	It seemed that there was a problem with the ned since the error was coming from /opt/mcp/ned/bin/nedclient. \r\n-	I restarted ned on both EM servers as follow;\n\r\n#neinit restart\n\r\n-	Customer ran resync.pl script again and it completed successfully.\r\n-	Ended the call.','null'),(834,'Emre Bertan (NETAS External)','AS-OAM','2011-11-26','111126-301466','UPC Nederland','ER called and stated that customer is claiming they are having an E1 outage because their SM server is not responding due to Java Heap Exhaustion alarms.\n\r\nI told Er that this is not an E1 issue, and the customer has an open case (110526-224107) about it which is currently worked by Design. Customer said normally SM was swacting itself due to this JavaHeapExhaustion and this time it did not do it automatically.\n\r\nI logged in to site and performed swcat manually and then closed the server monitors as their case suggests 110526-224107. They have monitored their site for an hour and then let us go. I suggested Er to tell them apply what their case suggest before calling Er for an already known issue by their site.','null'),(835,'Emre Bertan (NETAS External)','AS-OAM','2011-11-24','111124-299576','BT Ireland','PROBLEM (Why ER called):\r\nER contacted me saying that the customer having issues in the server replacement process of SESM server running on 8.0 BRC (14.0.6.6). \r\nDuring replacement the customer had reinstalled the server with the document provided by GPS. They said that along the installation process the install-script had asked for MaintenanceVLAN IP and because they did not have this information they have entered bogus IP at this step, claiming that that is causing problems on the setup of the server.\n\r\nREAL PROBLEM:\r\nActually there were several problems which had nothing to do with the MaintenanceVLAN IP.\n\r\n1) When questioned the customer had admitted that they had used the document provided up to where mcpExtractContent -aat is run. -HOWEVER- it was instructed to  use the procedure in the document between pages 24 to 30 Procedure 5  Manual Platform Linux Installation and a 17 step of the procedure marked in a table. The document clearly states when the procedure ends on page 30 at step 17 which is the last row of the table, yet the customer continued on the document which is in a different table under a different procedure (Procedure 6  Manually Patching the Platform Linux) thus there was no problem really.  \n\r\n2) The new installed SESM server was not able to ping any other server and SESM instance could not be deployed from SM.\n\r\nSOLUTION:\r\nChecked IP settings and compared with the working pair of the SESM instances.As expected (lesson learned from problem 1) The person who installed the server, had provided different subnets and gateways for this SESM during the installation. As a result the server was not able to ping anywhere.\n\r\nSESM1_0 (working one)      => inet addr:10.130.231.37 Bcast:10.130.231.47 Mask:255.255.255.240\n\r\nSESM1_1 (new installed one)=> inet addr:10.130.231.38 Bcast:10.130.231.63 Mask:255.255.255.224 \n\r\nI had run reconfigure.pl to correct the gateway and subnet values of the fresh installed server to match the working one.\n\r\nAfter that we had checked the routing information of Both servers:\n\r\nWorking SESM1_0:\r\n[root@DBLBTA2SESM1 sysconfig]#   ip route show\r\n10.130.231.32/28 dev bond0  proto kernel  scope link  src 10.130.231.37 \r\n169.254.0.0/16 dev bond0  scope link \r\ndefault via 10.130.231.33 dev bond0 \n\r\nNew Installed SESM1_1:\r\n[root@DBLBTA2SESM2 sysconfig]# ip route show\r\n10.130.231.32/28 dev bond0  proto kernel  scope link  src 10.130.231.38 \r\n169.254.0.0/16 dev bond0  scope link\n\r\nAdded the last line \"default via 10.130.231.33 dev bond0\" to SESM1_1 since it was missing by \"ip route add default via 10.130.231.33 dev bond0\" command.\n\r\nAfter this the fresh installed SESM1_1 server was pingable from SM so I have deployed and started the SESM instance without any issues.Another pager which only requires basic network knowledge and which does not require A2 OAM GPS skill-set has been solved.','null'),(836,'Emre Bertan (NETAS External)','AS-OAM','2011-11-23','111123-299310','Compania Dominicana de Telefonos C por A','I was called while I was currently working at another call. ER provided \r\ncontact info to me so that I can contact NI Engineer when I handed over the other one to one of my team mates. \n\r\nPROBLEM: \r\nDuring CVM12 MR upgrade from 10.3.1.19 to 10.3.2.0 the upgrade script failed at the end part due to SM_0 unavailable problem as below:\n\r\nStarting MCP_10.3/SM_0. \r\nError occurred executing NE commands (see below): \r\n> run err Error! Timer expired while executing: perl getInstanceState failed. \r\n> config ok \r\n> run err Error! Timer expired while executing: perl \r\n> exited \n\r\nSOLUTION:\r\nI logged in to site and checked current platform patch levels. According to the release notes for 10.3.2.0 MR, they need to have 10.1.10 Platform MR installed.\r\nFor HPcc3310 servers this means: 10.0.19 patch for mcp_linux\n\r\nHowever the site had the following: 10.0.10 patch for mcp_linux\n\r\nI told the site engineer to apply the 10.1.10 Platform MR and try to upgrade again. However they did not have the Platform Patch CDs on site.\n\r\nI transferred the platform patch iso file to customer site and run the patchPlatform.pl script to apply the platform MR.\n\r\nIt failed with the following error:\r\nError (255) executing patch application targets\r\n[*PD-Error*]    Command: \"cd /var/mcp/platform/os/mnt/10.1.12.p-1;./mcp_core_linux-10.1.12.p-1.pl -install img=mcp_core_linux-10.1.patches_1.r-21 2>&1\"\r\n[*PD-Error*]    Error applying patch \"10.1.12.p-1\"\r\n[*PD-Error*]    Retcode is     >>0xff00<<\r\n[*PD-Error*]    Signal part is >>0x00<<\r\n[*PD-Error*]    Main part is   >>0xff<<\r\n[*PD-Error*]    System return strings:\r\n[*PD-Error*]        Capture not attempted.\n\r\nContacted Tugrul Timorci GPS and Arda Askin from Platform Design team for the error. Platform design suggested to remove ism manually and running the platfomr patch again by:\r\n[root@SSLMMEL0 patch_logs]# rpm -qa |grep ism\r\nism-5.5.5-1\r\n[root@SSLMMEL0 patch_logs]# rpm -e ism-5.5.5-1\n\r\nIt failed again. At this point I handed over the case to Tugrul Timorci  while Design was investigating the problem.\n\r\nThey killed the ISM thread on the Linux OS and run manual remove procedure above followed by the patchPlatform script. It worked this time.\n\n\n\r\n03:09 kmawst ER contacted paul for an update. GPS is now working with Paul indicates that the \r\nproblem with the platform patches \r\nPaul Loots: according to the release notes we need 10.1.10 MR \r\nPaul Loots: the SSLm have: \r\n[root@SSLMMEL0 root]# mcpRelease.pl \n\r\n*** MCP Platform Release *** \n\r\nSystem Type: mcp_core_linux \r\nRelease Level: 10.1.10 (via patching \n\r\nPaul Loots: but .... 10.1.10 MR is not 10.1.10 \r\nPaul Loots: 10.1.10 MR is 10.1.19 \r\n03:10 kmawst GPS is downloading 10.1.19 MR to SSLM now \r\n04:31 kmawst Paul states that 10.1.19 MR application failed and GPS is contacting design. \r\n06:16 kmawst Emre from A2 OAM GPS states that he handed case over to Tugrul Timorci and he is \r\nworking with design on that front. \r\n07:51 kmawst Pual indicates that his managed to load the Platform patches \r\nPaul Loots: however we ran out of time \r\nPaul Loots: I will continue the MR application tonight \r\nPaul Loots: we successfully applied the 10.3.2.0 load to the SM\'s and we \r\nleft the SSLI\'s(SESM\'s) at 10.3.1.19 \r\nPaul Loots: the Customer agreed to this since the SM issues did not affect \r\ntraffic \r\nPaul Loots: but we need another Maintenance window to complete the rest \n\r\nThe MCP GUI is now working after applyingthe platform patches to the SM servers. \r\nIt appears that the missing platform patches were causing the MCP GUi to fail. \r\nPaul stated to close case.','null'),(837,'Emre Bertan (NETAS External)','AS-OAM','2011-11-24','111123-299305','Unitymedia NRW GmbH','PROBLEM:\r\nAfter customer Core Servers  were upgraded to  CVM11 => CVM13, SWD started upgrade of BCP blades in a new maintenance window. \n\r\nSWD called because at the first BCP upgrade they had failure because Primary EM server and BCP_0 blade was not able to ping each other whereas the same BCP_0 blade was able to ping all other servers.\n\r\nSOLUTION:\r\nI logged in to customer site the Primary EM server without a problem however BCP_0 blade was not responding my ssh login via putty. BCP Blade was available only through IBM ManagementConsole. I could not login through IBM MM because of java problem but SWD was able to launc Remote Control for BCP_0.\n\r\nI started ping from EM_0 to BCP_0 and it was failing. However I can ping other BCP blades from EM_0. At this point SWD stated that the customer was using IPSec. EM_0 had IPsec running but BCP_0 did not have the IPSec settings. The documentation was stopping the IPSec in Secondary EM, and that explained why BCP_0 was able to ping EM_1 but not EM_0. All other BCPs had IPsec setup on them and they were not upgraded yet (still runnig on CVM11).\n\r\nI told SWD to copy /admin/ipsec contents from other Blades and start ipsec again from ipscfg.pl script. The transfer of files took about 2 hours and in the end this attempt was not successful. SWD told me at this point that while upgrading (done by fresh install) this unit there was a driver problem so they had to stop the install in the middle and then re run it, but this time they had to enter all server information manually. I thought reentering IPsec conf would solve the issue but at  the same time I had another pager call from ER. Because the business day started in Istanbul office, I handed over this issue to A2 Gateway team, Adem Yavuz Aydin and Yunuz Ozturk to look up the other pager.','null'),(838,'Emre Bertan (NETAS External)','AS-OAM','2011-11-24','111124-299551','WELCOME ITALIA SPA','Earlier I was called by SWD for a regitration problem occured after CVM13 to CVM15 half upgrade(111124-299545). During primary server tests (running on MCP_14.0.6.6) after I verified every thing was okay from OAM perspective I routed the pager call to Callp team so that they can investigate the SESM NE behavior.\n\r\nThis time SWD called me because they had decided to rollback the system and they had came across the following problem during CVM15 -> CVM13 half rollback procedure:\n\r\nWhile rolling back the Primary EM Server (HPcc3310) from CVM15->CVM13, they had reloaded the old Linux OS from the CD and during that process, they had selected the option next to Re-install (restore) with platform data stored on remote server to re-install the server to a previous configuration.Then they pointed it towards one of the backup files on the Secondary EM Server still running on CVM13. Afterwards the document routed them to \"With procedure Restore the Primary Element Management Server from the Secondary EMS. For detailed steps, see Restoring a core server from backup (page 1610)\" [NN10440-450_09.05].\n\r\nPROBLEM:\n\r\nAt this point configBkup script failed with the following error:\r\nCan\'t exec \"/usr/bin/ssh-agent\": No such file or directory at /usr/local/bin/sshUtl.pm line 2\n\r\nI logged in an checked and there was no  /usr/bin/ssh-agent on the fresh installed server System Type: mcp_core_linux ; MCP Platform Release Level :  12.0.19 (via install). Yet it existed on untouched server i.e. secondary EM running on CVM13 System Type: mcp_core_linux ; MCP Platform Release Level :  12.0.19 (via patching).\n\r\nSOLUTION:\r\nI copied all ssh related files under /usr/bin from Secondary EM to Primary EM and rerun the configBkup script. It passed successfully. I told SWD to continue the rollback from the next step and exited the call.','null'),(839,'Emre Bertan (NETAS External)','AS-OAM','2011-11-24','111124-299545','WELCOME ITALIA SPA','After CVM13 to CVM15 upgrade had performed in primary half of the customer site  during testing stage of the upgrade before starting the secondary servers, customer complained that SESM on CVM15 server (running on MCP_14.0.6.6) was taking a long time to initialize and when it was active it responded very slowly to registration attempts thus causing call failure. after I verified every thing was okay from OAM perspective I routed the pager call to Callp team so that they can investigate the SESM NE behavior.','null'),(840,'Emre Bertan (NETAS External)','AS-OAM','2011-11-22','111122-299110','Axtel','ER called as customers request. Customer states that SESM3_1 running on CVM13 is down after they were performing maintenance on the unit to change the admin state to online. SESM 3_1 was in an admin state of configured and operational state hot standby and they were trying to change the admin state to online.\n\r\nI have connected to site through ERs VNC connection. Checked the MCP GUI status and saw that SESM3_1 was in configured state. I had Deployed the NE from MCP GUI and started the instance. It started normally with no issues. After agreement we closed the call.','null'),(841,'Emre Bertan (NETAS External)','AS-OAM','2011-11-21','111121-298997 ','Ziggo','SWD is upgrading from CVM13 to CVM15 and while they were upgrading I got called because oraclePatch was failing on secondary server.\n\r\nI have checked the site. Both Servers were patched to MCP14.0 patch level so that was good. There was a mistake made during the upgrade and SWD engineer had tried to patch secondary server by running the script from Secondary Server, however it should have been run from the primary. \n\r\nI logged in to Primary EM server (sslem00) and run oraclePatch.pl script from there. Oracle Patch Completed Successfully.','null'),(842,'Kerem Gunduz (NETAS External)','AS-OAM','2011-11-19','111119-298897','Axtel','ER paged me due to SESM start problem. I tried to connect to SESM but but it was not pingable.Customer told me that they noticed SESM was unreachable then they performed power cycle. I told to perform power cycle again. Server came up and it was reachable. I connected to the SESM server and checked the work logs. There were several JAVA exceptions. I checked the Database and noticed Oracle version was 9i which should be 10g with 12.0. Customer told me they performed upgrade a short time ago. I told customer to complete Oracle Migration which is inside the Post Upgrade Steps. I also collected SESM work and OSS logs to provide them CallP team since SESM restart problem will be investigated by CallP team.','null'),(843,'Kerem Gunduz (NETAS External)','AS-OAM','2011-11-17',' 111110-297180','CenturyLink','Customer reported that SESM could be started after upgraded to 12.0.6.13 from 10.3. SESM goes to offline state after initializing. \r\nI checked SESM work log and noticed that there are missing engineering parameters that required for sesm to be an active.I checked upgrade logs and saw sm upgrade was failed during the patch upgrade(12.0.6.0->12.0.6.13).\r\nI restored 12.0.6.0 backup and tried to perform patch upgrade again, sm part was failed again. Tried to run smPatch.pl, it was failed too. Finally tried to run smUpgrade.pl and it was completed successfully and added missing eng parms.\r\nThen tried to start sesm on mcp gui, sesm went to active state.\n\r\nDropped the call with an agreement.','null'),(844,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-11-12','111114-297817','Türk Telekom','Gokhan has paged me because of they are taking error during mcpUpgradeFrom10to12.pl script. I have connected to the site and checked the logs. I have learned which step did they do lastly. I have checked the installprops file and I saw that the user name written in installprops is different than the user recoreded in database so it could not connect database to run script. I fixed it with changing installprops file then I have ran dbInstall -fo to sync with database with installprops. Then first problem is resolved but I took one error more then, I have asked them that did you do cleanupReplication and they said that we have skipped it accidentally. The second problem was caused because of this. They were wanting to do rollback because of the less maintenance window hour remained but I suggested them as a GPS that this system can be work with loss of reduncy properly on 10.3 system so the customer could read data from database but not write. If we wait until business hour we can solve this problem more quick than rollback. Then, they accepted.\n\r\nWhen I came to office, Emre was concerning with the case and we tried to drop replicatiobn with manual methods but it failed. Then,we took backup and then  restore the same database operation to drop replication. Then we tried to run mcpUpgrade10to12.pl script. Then, it worked. We have notified the customer that you can go on with patch appliying.','null'),(845,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-11-12','111112-297746','Axtel','Donnell has paged me about the upgrading from 10.3 to 12.0 platform and the SESM instances did not come up to new load. \n\r\nI have connected to the site and connected to the MCP GUI. I have checked the SM and PROV, they were correct state but SESM instances\' were at old load. But SESM instances\' on old load were working properly. I have checked the SESMs\' work logs. I suggested them to stop 10.3 SESMs and then try to start 12.0 SESMs. They need to talk with upper management because this will cause outage for 1 minute. They took permission from upper management and they tried on SESM5 first. It worked and then tried all other SESMs and they could pass the correct load one by one. \n\r\nThen I have agreed with customer and then left from the call.','null'),(846,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-11-12','111112-297743','Axtel','Donnell has paged me during platform installation on 12.0. He said that the customer could not able to run instaal-kvm script in the beginning of the installation.\n\r\nI have connected to the site and learned about the server type. It was HT-Langley. I have asked the customer that are you sure to use the correct CD for this server and they said yes. Then I have asked that is there any spare CD for this server. The customer replied as no. Then I suggest them to connect one monitor and keyboard to the server which will be upgraded, then try to power cycle server and try to write install-kvm command. They did this scenario and it passed this time. Sometimes it would be useful to connect a keyboard and monitor directly to the server.\n\r\nThen, I have agreed with customer and left from call.','null'),(847,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-11-11','111115-298025','Turksat','Mahmut has paged me about upgrade from 10.3 to 12.0. He said that;\"we are trying to upgrage the customer system. Actually we have finished the primary side but PROV1 did not come up in new load. I have noticed that I accidentally deployed the PROV1 on EMS1 but according to documentation PROV1 should be on EMS2.\"\n\r\nI have said to him that this is not problem for now you can go ahead with upgrading secondary side then you can change the PROVs loaction on servers.\n\r\nHe went on upgrade and finished the secondary side to 12.0. Then he deployed the PROv1 to EMS2 and PROV2 to EMS1 and they came up successfully with no problem.','null'),(848,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-11-10','111110-297180','CenturyLink','John has paged me about SESM coming up problem. The customer was wanting to upgrade thier system from 10.3 to 12.0. They have did platform and oracle patch and then they runned the mcpUpgrade 10to12.pl script. SM, PROV was come up with new load (12.0) but SESM was down and it did not come up with new load. The system was working on secondary site meaning of call processing. \n\r\nI connected the site and check the work logs of SESM. I tried to make the server come up but it did not. I agreed with customer to open case for further investigating and remain the system with loss of reduncy . They have accepted this and I left from call. Then, I asked a co-worker from CALL processing team after I came to office and he said that during upgrading from 10.3 to 12.0 sometimes SESM instance can not come up. There is also a case we are already working on but as a workaround; the customer can  stop SESM on 10.3 load then they can try to start SESM on 12.0 load. I said this Chuck, they will have tried this. If this workaround does not worked, we will go on investigating this case.','null'),(849,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-11-05','111105-296440','Oneconnect','Gary has paged me because of the \"single user mode\" problem after replacing HDD on customer\'s side. Customer has changed the HDD with spare one and they took a problem and they could not able to start their server properly.\n\r\nI have connected to site and I have active call with customer because of the ERs\' overflowed. I have asked some question to customer in order to understand situation clearly. Craig did not know exactly that this changed HDD was spare or new HDD. \n\r\nI tried to pass this error with doing power-cycle the server but it did not work. The system wa 9.0 and the server type was Sun T4000 because of this release EOL and the server is old, I haven\'t started the server properly. \n\r\nI have asked to Craig that how many HDD  you replaced? And if you knıw the disk that was changed, can you take out that disk and try to reboot the server again, if it does not work then try to take out the other one and reboot it again to understand that if one of the disks has a problem or not. This did not also work because of Craig said that there is a lot of mirror disk here and I don\'t know which one is correct one to take out. Then, I suggested them to do fresh install their system and it they wish they can take help from SUN firm because of the server is old. \n\r\nThen, I left from call agreed with customer and customer did fresh install and now their server is running.','null'),(850,'Tugrul Timorci (NETAS External)','AS-OAM','2011-11-03','111103-295701','Axtel','ER called me for SM0 is not up and bouncing . \n\r\nI logged in the system and checked the SM logs. I saw there below message \n\r\n[nortel@MTYSSLM00 staging]$ \r\n./smUndeploy.pl -p installprops.txt\r\n ********************************************************************\r\n Load directory and all sub-directories MUST HAVE read/execute privileges for the group user. Current Group permission are => --- for /var/mcp/loads/MCP_10.3.2.0_2010-01-21-1023 ******************************************************************** \r\nAborting due to invalid load permissions See log file for possible details: /var/mcp/install/logs/smUndeploy.log.20111103_011705\n\n\r\nI deleted the MCP_10.3.2.0_2010-01-21-1023 and I executed the ./neStart.pl script but this failed. I checked the SM.logs there were /opt/mcp/nif/bin\r\nopt/mcp/certs/external/MCP_10.0_keystore.jks error messages. \n\r\nWe started to deeply investigate the system and realized the all the SM , PROV script has 20201 user privileges. These scripts have to Nortel user privileges.  We started to recover these files permissions manually. After some changes SM0 successfully started\n\r\nI tried to deploy new load to PROV1 and successfully started to work. I checked the Prov2 but that instance was unavailable I deployed new load to Prov2 instance but it did not started. I deleted Prov2 instance and redeployed but it did not worked. I checked the Prov logs with Emre Bertan and we saw there same permission errors . We found one of the script for correct all the system permissions. Which is find / -uid 20201 -exec chown nortel \'{}\' \\;\n\r\nWe executed the script and corrected SM0 system. Tried to start PROV1 instance and it worked successfully. \n\r\nI informed to customer their system has customer defined user. They need to correct them before upgrade the SESM instances. After agreement I dropped the conference.','null'),(851,'Tugrul Timorci (NETAS External)','AS-OAM','2011-11-01','111031-295111','Emirates (Dubai)','ER (Donnie Young) called me SESM1_1 can not be ACTIVE. Load is 12.0.6.7\n\r\nI logged in the site and checked the system. System was running healthy. I tried to restart SESM1_1 instance , it started but took over 5 mins. I checked the oss logs of the SESM1_1 and I saw there some of SIP PBX related swerr and Alarm. And this swerrs contains imdb alarm. I tell to ER call SIP PBX pager.\n\r\nSIP PBX joined the conf. after agreement I left the conference','null'),(852,'Senem Gultekin (NETAS External)','AS-OAM','2011-10-30','111030-295035','Axtel','Problem Description\n\r\nSESM2_0 was stuck in killing and warm stanby status. System is on 10.3 and servers are CC3310.\n\r\nSolution\n\r\n-	Accessed to the site and checked the GUI. SESM2_0 was in killing and warm stanby state, it was not able to it kill stop or start.\r\n-	Accessed to the server via ssh and ran neStop neStart commands. Instance was back at Hot Standby again. But maint status was still on killing.\r\n-	Explained to the customer that this Killing maint status will not affect the system, since the state is Hot Standby its working properly.\r\n-	To clear the Killing state a SM swact is needed. Customer will perform that during maint window.\r\n-	Agreed with customer and left the bridge.','null'),(853,'Senem Gultekin (NETAS External)','AS-OAM','2011-10-27','111027-294348','CenturyLink','Problem Description\n\r\nCustomer was performing 10.3 to 12.0.6.13 upgrade. Half upgrade was completed but they were facing SIPPBX issue, so the customer decided to rollback. At the beginning of the rollback they understood that they can see this issue cause the A2 upgrade has not completed fully, so they decided to continue with the upgrade and cancel the rollback. At that time they have already ran the following scripts.\n\r\n./changeEngConfTo10x.pl\r\nAnd\r\n./add_10_x_mcpned.pl\n\n\r\nCustomer was confused and they didnt know how to continue with the upgrade.\n\r\nSolution\n\r\n-	Customer was following NN10440-450 07.54 document, Ive checked and verified which step were they stuck exactly. \r\n-	Requested access info,  ER was accessing the site but there were some connection issues and it took very long to access to the primary database server.\r\n-	Once I was accessed Ive updated Engineering Configuration from the database as follow;\n\r\nupdate NEINSTANCE set ENG_CONFIG = \'A2E-AS_Large_HS20\' where ENG_CONFIG = \'BCT_Standard\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'CS2K-A2E_Medium_4GB2Core\' where ENG_CONFIG = \'SAM-XTS_Standard\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'CS2K-A2E_Medium_CC3310\' where ENG_CONFIG = \'CC3310_Standard\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'AS5300-PBX1_Medium_X3550\' where ENG_CONFIG = \'X3550_Standard\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'AS5300-PBX1_Small_X3550\' where ENG_CONFIG = \'X3550_Micro\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'RTP_Portal\' where ENG_CONFIG = \'UAS_Conf_Server\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'_Legacy_N240_Standard\' where ENG_CONFIG = \'N240_Standard\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'_Legacy_T1400_Standard\' where ENG_CONFIG = \'T1400_Standard\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'_Legacy_V100_Standard\' where ENG_CONFIG = \'V100_Standard\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'_Legacy_V100_Micro\' where ENG_CONFIG = \'V100_Micro\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'_Legacy_BCT_Micro\' where ENG_CONFIG = \'BCT_Micro\';\r\nupdate NEINSTANCE set ENG_CONFIG = \'_Legacy_CC3310_Micro\' where ENG_CONFIG = \'CC3310_Micro\';\r\ncommit;\n\r\nThese updates are to reverse changeEngConfTo10x.pl script.\r\n-	add_10_x_mcpned.pl script was just creating Nortel and mcpned users to be used in 10.3 release, so no need to reverse this script.\r\n-	Suggested the customer to continue with the upgrade by patching the SM with 12.0.6.13 load and follow the NN10440-450 document.\r\n-	Agreed with the customer and left the bridge.','null'),(854,'Senem Gultekin (NETAS External)','AS-OAM','2011-10-28','111027-294348','CenturyLink','Problem Description\n\r\nCustomer was performing 10.3 to 12.0.6.13 upgrade. Half upgrade was completed but they were facing SIPPBX issue on 12.0.6 load. SIPPBX GPS was already working to solve the issue but customer wanted to switch where there service on secondary where its 10.3 load. They were having problem to bring secondary part up SESM and Prov instances.\n\r\nCustomer was confused and they didnt know how to continue with the upgrade.\n\r\nSolution\n\r\n-	Accessed to the site over ER.\r\n-	Deployed and started the 10.3 instances for secondary SESM and secondary PROV.\r\n-	Stopped 12.0.6 instances for primary SESM and secondary PROV.\r\n-	Customer made test calls and they passed.\r\n-	Customer was fine working on a non-redundant system. There plan was to fix the SIPPBX issue and continue with the upgrade later.\r\n-	As OAM GPS my part was completed. The system was running on 10.3 with secondary servers. Agreed with the customer and left the bridge.','null'),(855,'Senem Gultekin (NETAS External)','AS-OAM','2011-10-27','111027-294188','Windstream','Problem Description:\n\r\nJames Young paged me for an EM0 server issue at Windstream site.  Site is running on 12.0.4.11 Release. Servers are CC33010. After 12.0.12 to 12.0.19 platform patch server didnt come up.\n\r\nSolution:\r\n-	This issue came earlier and disk was recovered which is sda. But it seems that sda was faulty again.\r\n-	Requested from the customer to pull out the faulty disk sda. Server came up with only one disk.\r\n-	Customer found a spare disk and inserted to the empty slot.\r\n-	After the reboot server came up and sda was not synced with sdb.\r\n-	SWD ran mcpSwRaid.pl -add sda and added the partitions with the new disk.\r\n-	This raid action took around 2 hours and was completed successfully.\r\n-	Currently all server platforms have been upgraded to 12.0.19 level successfully. \r\n-	SWD continued with the 12.0.6 MR upgrade.','null'),(856,'Senem Gultekin (NETAS External)','AS-OAM','2011-10-26','111026-293815','Windstream','Problem Description:\r\nBill Price paged me for a EM0 server issue at Windstream site.  Site is running on 12.0.4.11 Release. Servers are CC33010 and platform level is 12.0.12\r\nSWD saw faulty partition at sdb and ran remove sdb and was not able to add again, tried a reboot and server didn\'t come up back. \n\r\nSolution:\r\n-	Customer was able to login to the server via console access. \r\n-	Requested from the customer to pull out sdb disk from the server and start it, It didnt comeup.\r\n-	Swapped the disks as sdb and sda, but that didn\'t work as well.  The following error was appearing; \r\n__________________________________________________\r\ndevice md7 contains filesystem  with error \r\n                  device md 7 ... \r\n                  1 node reported corrupt orphaned link list found\n\r\n                  device md7 \r\n                  unexpected inconsistency .. run fsck manaully \r\n                  example: w/o -a or -b options\n\r\n                  md 0 clean \r\n                  failed \r\n                  an error occurred during fsck\n\r\n                  drops to shell and says system will reboot when exits\r\n_____________________________________________________\n\r\n-	At last we ran fsck (files system check) and with a reboot server came up back. Customer was able to provisioning again.\n\r\n-   I logged in to EM0 server via ssh and check the disk status, sda was down as follow; \n\r\n***** Software RAID-1 Devices ***** \n\r\nMD Device | | Member-0 | Member-1 | Sync/Recovery \r\nName Size(MB)| Usage | Name Flg | Name Flg | Mode Speed Fin Done \r\n--------------+------------+-----------+-----------+-------------------------- \r\nmd0 101.9 | /admin | sdb1 U | . _ | . . . . \r\nmd1 101.9 | /boot | sdb2 U | . _ | . . . . \r\nmd2 2047.2 | ??? | sdb3 U | . _ | . . . . \r\nmd3 2047.2 | ??? | sdb9 U | . _ | . . . . \r\nmd4 2047.2 | / | sdb6 U | . _ | . . . . \r\nmd5 6141.9 | /opt | sdb7 U | . _ | . . . . \r\nmd6 5122.2 | /var | sdb8 U | . _ | . . . . \r\nmd7 52297.4 | /var/mcp | sdb5 U | . _ | . . . . \n\n\r\n-	I ran mcpSwRaid.pl -add sda to sync up sda. Took almost 2,5 hours and it completed successfully. Both sda and sdb partitions were up. \n\n\r\n***** Software RAID-1 Devices ***** \n\r\nMD Device | | Member-0 | Member-1 | Sync/Recovery \r\nName Size(MB)| Usage | Name Flg | Name Flg | Mode Speed Fin Done \r\n--------------+------------+-----------+-----------+-------------------------- \r\nmd0 101.9 | /admin | sdb1 U | sda1 U | . . . . \r\nmd1 101.9 | /boot | sdb2 U | sda2 U | . . . . \r\nmd2 2047.2 | swap | sdb3 U | sda3 U | . . . . \r\nmd3 2047.2 | swap | sdb9 U | sda9 U | . . . . \r\nmd4 2047.2 | / | sdb6 U | sda6 U | . . . . \r\nmd5 6141.9 | /opt | sdb7 U | sda7 U | . . . . \r\nmd6 5122.2 | /var | sdb8 U | sda8 U | . . . . \r\nmd7 52297.4 | /var/mcp | sdb5 U | sda5 U | . . . .\n\r\n-	Windstream was going to upgrade to 12.0.6 MR, requested from SWD to check the disk status of EM0 before the upgrade again.\r\n- Agreed with customer and left the bridge.','null'),(857,'Emre Bertan (NETAS External)','AS-OAM','2011-10-24','111023-293252','WIND Telecom','ER called and explained the issue. I have send them the instructions on how to aplly licencekey and asked them to contact KRS.\n\r\nER asked customer to reapply the license key with my provided instructions mean while asked me to stay on line. The customer could not find the licence key thus Er tried to contact KRS.\r\nER did not have any info about KRS team. I told them once Dave Tanner was responsible for KRS issues. They tried to look up his number yet only his office number was available. \n\r\nI dropped off the call.','null'),(858,'Emre Bertan (NETAS External)','AS-OAM','2011-10-24','111023-293252','WIND Telecom','CallP GPS called again OAM pager and wanted to give the OAM pager number to Er so that I can explain the issue to them. I said OK and wait for ERs call.','null'),(859,'Emre Bertan (NETAS External)','AS-OAM','2011-10-24','111023-293252','WIND Telecom','CallP GPS called again OAM pager to ask how does the license key is applied. I told him how it is done and repeated that KRS needs to be contacted besides it is not an outage so it should be handled as a case which does not need pager support. Told him that if ER persists they can escalte the issue.','null'),(860,'Emre Bertan (NETAS External)','AS-OAM','2011-10-24','111023-293252','WIND Telecom','CallP GPS called again OAM pager to ask how does the license key is applied. I told him how it is done and repeated that KRS needs to be contacted besides it is not an outage so it should be handled as a case which does not need pager support. Told him that if ER persists they can escalte the issue.','null'),(861,'Emre Bertan (NETAS External)','AS-OAM','2011-10-24','111023-293252','WIND Telecom','CallP GPS was contatced by ER because the customer was claiming that they had upgraded siplines license from 10000 to 12044 two days ago however they can not provision new lines because it was seen as 10000. Koray had asked me if this falls to our area and I informed him that ER needs to contact KRS for new licence generation. \r\nOAM can help if there is a functional problem with license upgrade, i.e. error seen during applying the license. But here the problem is with license itself so it is KRS\'s responsibility to generate licenses.\r\nI dropped off the call.','null'),(862,'Emre Bertan (NETAS External)','AS-OAM','2011-10-21','111021-293065','CenturyLink','During CVM12 (MCP10.3.2.13) ->CVM13 (12.0.6.0) upgrade during patching the platforms customer tried to patch their server from 10.1.10 platform level to 12.0.15 platform level, however got \"No applicable patch(es).\" screen.\n\r\nContacted Platform Design support to verify the issue. Normally all customers must be at the latest platform patch level before doing a release upgrade.\n\r\nThe latest platform level for CVM12 is 10.1.11MR level which is in HPcc3310 Servers 10.1.19 level. Because the customrs OS is in a very old platformlevel the patching to CVM13 level was not supported.\n\r\nTold customer to apply 10.1.11MR first then upgrade to CVM13. Customer stated that the required CDs were not found on site so I have placed the required iso file under NetasFTP server for them to download.\n\r\nCustomer decided to perform download and continue the upgrade tomorrow.','null'),(863,'Emre Bertan (NETAS External)','AS-OAM','2011-10-21','110929-287038','Verizon Communications','Customer had a problem while executing the predefined steps for Resync system recovery on Primary DB running on MCP10.3.2.0 (CVM12). \r\nI had connected to site and checked Primary DB. It was not up so I started Oracle instance and recovered DB through RMAN. After restoring the latest backup and running Resync procedure along with config_snmp all alarms were cleared from customer site and DBs are replicated again.','null'),(864,'Emre Bertan (NETAS External)','AS-OAM','2011-10-19','111019-292350','Singtel Optus Pty Ltd','Called customer (Neil Kelly) and connected to site through their team viewer account. \n\r\nThey were using PROV servers as PA and routing public PA URL to SM server floating IP. Checked PROV status from SM GUI both instances were up and running. \n\r\nWe verified PA was reachable from their network but was not reachable from internet (end customers). We also checked JBOSS access logs and we couldn\'t find any entry coming from public network as expected.\n\r\nAsked customer to check their DNS and PROXY settings. The customer spotted that one of their PROXY servers was problematic correctly after a load balancing operation. They corrected the issue and we dropped off the call.','null'),(865,'Emre Bertan (NETAS External)','AS-OAM','2011-10-18','110928-286736','INFORMATION SYSTEMS JET','Customer has 4 BCPs in BCT chassis. Two BCPs stuck in configured state. After confirming the issue is GW related I have shared the GW pager number with GTS and dropped off the call.','null'),(866,'Ege Varhan (NETAS External)','AS-OAM','2011-10-13','111014-291263','Cincinnati Bell','Problem Description\r\n------------------------\r\nThe customer has forgot all passwords (ntsysadm, ntappadm, root etc.) of a session manager server recently. The root password recovery document has been provided, however when they performed the procedure, they recieved the error below;\n\r\n\"kernel image not found\"\n\r\nSolution Description\r\n-----------------------\r\nThe recovery document provided has some missing points. Initially the customer was using \"A2 12.x Core Linux Installer\", however they should have used \"MCP 9.x Core Linux Installer\". We have told them to use \"MCP 9.x Installer CD\" instead, but after this point, we have faced other problems. Since, it is a SESM server, we suggested the customer to re-install the server from scratch, which is the best way to solve the problem and recover the password. The necessary documents have been provided. The customer re-installed the server successfully today.\n\r\nThanks,\r\nEge Varhan\r\nA2 OAM GPS','null'),(867,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-10-04','111002-287491','Cypress Communications','Mark has paged me that the MCP Database server was down on MCP_10.2 system. \n\r\nI have connected to the system and checked the type of the servers with and taht was SunOS atl-56m-mcs-db0 5.9 Generic_122300-13 sun4u sparc SUNW, Netra-240. Then, I tried to login to oracle and run sum select command but it did not work because of the DB was down. Then, I tried to run shutdown immediate and shutdown abort commands but both ot them were failed with the error messages. Then, I decided to restore system with RMAN. I tried to run shıutdown abort with RMAN, then sqlplus was worked with shutdown abort command. Finanly, I started up database with startup mount command and restore database back. Then, I said to Mark that after all these you should run resync command in order to resycn two databse each other.  I disconnected from the system. After half an hour later Mark said me that resysnc operation finished with no errors.','null'),(868,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-10-04','111005-288000','Videotron Ltee','Tony has paged me this morning because of he encountered a problem related with platform patch. This error was like below;\n\r\n***** Applying patch\n\r\nApplication part #55\r\nAppl Target: SysadminArchTar\r\nArchiving home directory /home/sysadmin\r\nUnrecognized escape \\g passed through in regex; marked by <-- HERE in C:\\Documents and Settings\\galexand\\Desktop\\iems091_patch\\$/ at ./mcp_core_linux-10.1.13_u_12.0.7.pl line 1824.\r\nUnrecognized escape \\i passed through in regex; marked by <-- HERE in m/^C:\\Documents and Settings\\galexand\\Desktop\\i <-- HERE ems091_patch\\$/ at ./mcp_core_linux-10.1.13_u_12.0.7.pl line 1824.\r\nForce-recursive deleting \"\"/home/sysadmin/C:\\Documents and Settings\\galexand\\Desktop\\iems091_patch\\\"\".\r\nsh: -c: line 1: unexpected EOF while looking for patch part #55 exited with an error (0).\n\r\nThe mentioned path in the error which is C:\\Documents and Settings\\galexand\\Desktop\\iems091_patch directory is not related A2 product. By the way Tony was talking with customer, when we were trying to solve this issue. Tony wanted from customer that can you remove that that directory from your local computer. Whne customer removed that directory, it worked fine. This problem may probably related with the computer and its settings which is trying to do platform patch on.','null'),(869,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-10-04','111005-287998','Videotron Ltee','Tony paged me tuesday morning for the CRITICAL SIP threshold alarm seen on MCP GUI. While the customer think to upgrade their system from CVM12 to CVM13 they saw a business critical alarm on their MCP GUI and they wanted to make it clear before the upgrade.\n\r\nI have connected to the system and check the alarm. The detail of the alarm was like below; \r\nShortFamilyName: SIP\r\nLongFamilyName: SIPPRTCL\r\nSeverity: CRITICAL\r\nProbableCause: threshold crossed\r\nDescription: The number of 500 Server Internal Error responses to SIP\r\nINVITE requests in OM group SIP_Inbound_Response_Report exceeds the specified percentage of responses.\n\r\nThis alarm was not obstacle for the alarm and I have controlled the SESM instance that they were working or not. They were working properly and I stopped active one and restarted it. Then, the other one which was waiting on \"Hot standby\" turned into \"Active\". Then, I have stopped the active one again and alarm was cleared and SESMs were turned into the situation that I have in the first.','null'),(870,'Tugrul Timorci (NETAS External)','AS-OAM','2011-09-27','110927-286503','NTT','SWD called me fail of the upgrade screen 24. (14.0.9.0 to 14.1.0.0)\n\r\nDuring the Upgrade screen 24: Upgrading Primary Network Element Instances, while trying to deploying and start SESM1_0 the following error is encountered:\r\n=========================================================================================================\r\nSESM1_0: Admin State: Offline -> Configured\r\nSESM1_0: Load Name Change: MCP_14.1.0.0_2011-09-06-2100\r\nSESM1_0: Maintenance State: None -> Deploying\r\nSESM1_0: Admin State: Configured -> Offline\r\nSESM1_0: Maintenance State: Deploying -> None\r\nSESM1_0: Maintenance State: None -> Starting\r\nSESM1_0: Connection State: Up\r\nSESM1_0: Admin State: Offline -> Online\r\nSESM1_0: Maintenance State: Starting -> None\r\nSESM1_0: Connection State: Down\r\nNEI is already under maintenance operation.\r\nError occurred during validating NEI : NEI Administrative state should be ONLINE. NEI Operational state should be\n\n\r\nthis is like interface issue. ETH could not take the correct mac adress. I run /opt/mcp/atca/mcpEthMacBind.pl script and entered the default values. After reboot SESM1_0 worked successfully\n\r\nAfter agreement I dropped the call','null'),(871,'Tugrul Timorci (NETAS External)','AS-OAM','2011-09-27','110927-286496','NTT','SWD called me fail of the upgrade screen 13. (14.0.9.0 to 14.1.0.0)\n\r\nDuring the Pre-Upgrade screen 13: Backup Server, all backups completed successfully except SESM1Server1 failed with the following error:\r\n=========================================================================================================\r\nError during screen 13: Backup Servers:\r\nEstimated space required for backup: 428617 KB\r\nCreating tar file for backup set \"mcpPlatform\"\r\nBackup set \"mcpPlatform\" written to local disk\r\nSending backup set \"mcpPlatform\" to remote server\n\r\nI login the system and checked the ned status and restarted the NED instance. It did not solve the issue. after I rebooted the server butit did not solve too. \n\r\nI called the Yalcın Tosun for this issue. He logged in the system and checked.\n\r\n /var/mcp/upgrade_bkups  directory was created with ntsysadm user. He corrected the directory to ntappsw. \n\r\nI retry this step and it passed successfully\n\r\nafter agreement I dropped the call','null'),(872,'Tugrul Timorci (NETAS External)','AS-OAM','2011-09-27','110927-286484 ','NTT','SWD called me cause of the missing loadline up file.\n\r\nWhen trying to retrieve the the release notes and load line-up file for A2 8.0 SP1, the listed address (http://cust.genband.com) my login does not work, which is my genband username and password. I also try to pull those files from http://support.genband.com but only the 8.0 BRC load line-up file is available. \n\r\nI called the Design team for take this file from them. Yalcın Tosun provided me the LLU file. I sent this file to Donnell, he tested the this file and worked\n\r\nafter agreement I dropped the call','null'),(873,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-09-21','110921-284073','TWT SpA','Earl has paged me on wednesday and he said me that customer cannot provisioning lines on the SSL via OSSGATE on 12.0.6.8. They tried to add a new user via OSSGATE but they got the failure message. We have asked that PROV client is working or nor and if there is only problem on OSSGATE, one person from OSSGATE team shoul be involved to the conference. Mert Çokluk was joined to the conference from OSSGATE team. He was also investigated logs like us.\n\r\nWe checked PROV client to add a user. At the first, we succeeded to add a user but when we double check the OSSGate command we saw that customer wanted to add with different service set than we did. Then, we tried to add with the same configuration parameter on OSSGATE command. Then, we saw that hotline service caused this problem on the \"basic\" service set. This service was hardcoded to be a PAY-FOR service. Since the Hot Line service was not enabled on the A2, any new line with the basic service set was failing to be added anymore. So, we unchecked this service from the \"basic\" service set and it resolved the problem adding a new user on the both PROV client and OSSGATE.\n\r\nWe said this to the Earl Thomas and agreed each other to finish pager call.','null'),(874,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-09-14','110921-284035','Videotron Ltee','Jim has paged me on wednesday and he said me that the SM_0 is down before the migration from CC3310 to HT-Langley server. The customer had started to prepare the backup but server was down incidentally. \n\r\nWe have connected to the site and check the SM_0 was up or not. We saw some problems on SM_0 so, we suggested to them to restart their server with Power cycle. After the restart operation server was up.\n\r\nThe customer wanted from us to look the alarms were raised. There were alarm on PROV_2 server that was not connected with database. So we restarted PROV to be cleared these alarms. But it did not resolved. We suspected from the SM that it may not be up properly or SM GUI may not take the configurations properly so we swacted SMs. After that it resolved. There was resyncronisation problem between database instances and this was the one the alarms of the MCP GUI. We did a resync between these databases and it resolved. There was also snmp alarm on MCP GUI and we restarted snmp services and rebooted EMServer 2 in order to reolved this issue as well. Finally, all alarms were cleared.\n\r\nThen, we have agreed with the customer to finish our pager call.','null'),(875,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-09-19','110919-282761','UPC Nederland','Jim has paged me on Monday for this issue and he said that the customer was unable Start/Stop of NCAS link on MCP12.0.6.0. QSIP link between core and SSL-M goes down every two days. Start/Stop of NCAS link does not help only way to get it back into service is to switch over SSL. \n\r\nActually there was a case related with this issue for the same customer and there was a workaround for this. We applied the workaround to be sure it is solving this issue or not. We informed the customer that we will try the workaround on your system and if it works, you should wait this fix for the other opened case. When we tried this workaround, we saw that it was working properly.\n\r\nSo, finally we informed the customer and we were agreed each other to finish the pager call.','null'),(876,'Emre Bertan (NETAS External)','AS-OAM','2011-09-12','110912-280609','Paltel','Upgrade prestep is failed due to RAID problem. When Checked we saw that SDA disk of the server was faulty.\n\r\nSite did not have any spare disks, worked with site to steal a Hard Disk from the other SESM pair, and Sync and allow the upgrade to complete. Then once that node was complete, steal back the Disk and re-Sync with the original SESM Disk to allow that upgrade to complete.\n\r\nHigh Level execution plan (with Details in black):\r\n1.	Remove sdb(Disk 2) from RMLASSL10 raid setup\r\n[root@RMLASSL10 proc]#  mcpSwRaid.pl -remove sdb\n\r\n2.	Physically remove  sdb(Disk 2) from RMLASSL10 and replace it with sda (Disk 1) in RMLASSL00\n\r\n3.	Add sda (Disk 1) in RMLASSL00 in raid setup\r\n[root@RMLASSL00 proc]#  mcpSwRaid.pl -add sda\n\r\n4.	Apply platformPatch.pl on RMLASSL00\n\r\n5.	Remove sdb(Disk 2) from RMLASSL00 raid setup\r\n[root@RMLASSL00 proc]#  mcpSwRaid.pl -remove sdb\n\r\n6.	Physically remove  sdb(Disk 2) from RMLASSL00 and replace it with sdb (Disk 2) in RMLASSL10\n\r\n7.	Add sdb (Disk 2) in RMLASSL10 in raid setup\r\n[root@RMLASSL10 proc]#  mcpSwRaid.pl -add sdb\n\r\n8.	Apply platformPatch.pl on RMLASSL10\n\r\n9.	Request a new disk for sdb(Disk 2) from RMLASSL00\n\r\n10.	When obtained physically  add sdb (Disk 2) in RMLASSL00 and add it in raid setup\r\n[root@RMLASSL00 proc]#  mcpSwRaid.pl -add sdb','null'),(877,'Kerem Gunduz (NETAS External)','AS-OAM','2011-09-16','110909-280477','OneConnect','Customer had an issue with server and replaced it, then they need to perform platform installation on new server which is Netra 240. They were trying to install 9.1 plarform. Ken assist customer to install platform and perform patches since I had another page at that time. Installation and patch applying were completed successfully.','null'),(878,'Kerem Gunduz (NETAS External)','AS-OAM','2011-09-17','110916-282116','Cincinnati Bell','ER paged me to ask if we can provide customer which users were unregistered from  1st July to now. I told him we don\'t save any date information on Database and IMDB dont have such complex query capability. ER also told me customer wants to know who deleted domain. I told ER we got IP adress and username information and we will provide it to the customer. Agreed and dropped the call.','null'),(879,'Kerem Gunduz (NETAS External)','AS-OAM','2011-09-16','110916-282116','Cincinnati Bell','Cincinnati Bell reported that one of their domains (Cinvoip.fuse.net) was lost on Provisioning Client and none of the domain subscribers was able to register or make a calls.  \n\r\nA2 GPS was called and GPS confirmed that that domain did not exist in neither Provisioning Client nor Database. We suspected it might have been deleted manually by intentionally or accidentally and collected all logs to analyze when/who deleted the domain.\n\r\nIn order to recover, the only option was to restore a previous Database backup that was known to be working well. However, the recent Database backup was taken in July with the previous MCP load (12.0.6.8) and no newer backup was available. We informed the customer that they would lose all data since July and they accepted. \n\r\nA2 GPS restored the database taken in July and confirmed the domain (Cinvoip.fuse.net) was recovered, and upgraded to the desired MCP Patch load (12.0.6.9). Then GPS restarted all PROV, SESM instances and  performed Database Resyncronization (DB resync) to get both Primary and Secondary DBs working fine. \n\r\nThe customer made tests and verified that the issue was recovered and outage ended.\n\r\nWhen  analyzing the logs at hand, A2 GPS identified that the domain was deleted by someone from a specific IP, which well provide the details on the timestamp and IP address on Monday. The customer should follow-up who was the responsible for that change. Also, they have been suggested to schedule DB backups to take automatically at regular intervals.','null'),(880,'Kerem Gunduz (NETAS External)','AS-OAM','2011-09-15','110915-281499','Paltel','Customer had issue during SESM server re-installation. There was bad sector on disk and it causes platform installation failure. \r\nWe tried to switch SDA and SDB then re-try installtion but it failed again. We involved design team to get their feedback and decided to perform installtion with LabMachine option. At this time installation was finished successully. But I told customer to switch disks between servers to have same sized disks on each server and perform installation on related SESM server again without LabMachine option since that option may have side effect.\n\r\nDesign has told that it was fixed with 14.0.13 platform release but customer already tried to install 14.0.13 and faced upwith this issue. I raised a CR about it Q02207676.','null'),(881,'Emre Bertan (NETAS External)','AS-OAM','2011-09-08','110908-280084','Videotron Ltee','PROBLEM:\r\nAfter completing the upgrade successfully. SWD engineer checked the status of the custome certificates for the PROV. The status was not showing OK, Certificate Status shows: Path does not chain with any of the trust anchors. \n\r\nRESOLUTION:\r\nAlthough the issue can be tracked witha simple case (not a pager call issue) GPS connected to site and verified that the intermidiate chain certificate was not applied to the truststore of the MCP system.\n\r\nVeriSign\'s public intermediate certificate for chained certificate is downloaded from the following URL and applied to the MCP  System and the issue is resolved.\n\r\nhttps://knowledge.verisign.com/support/ssl-certificates-support/index?page=content&actp=CROSSLINK&id=AR1513','null'),(882,'Kerem Gunduz (NETAS External)','AS-OAM','2011-09-13','110913-280919','Videotron','SWD paged me due dryrunDBUpgrade.pl was in progress for 1hour and 30 mins without any progress indication. I connected to the site and terminated current execution. Then I executed script again but it fails with NED error. Performed reboot on Secondary Database where dryrun runs, then execute script again. I check the log from /var/mcp/run/MCP_/_1/work/OracleMethod_upgrade_dry_run.log\n\r\nScript was completed successfully. I told Tony that this was NED issue and NED enhancement will be implemented on 9.0 release then we dropped the call with an aggrement.','null'),(883,'Kerem Gunduz (NETAS External)','AS-OAM','2011-09-14','110913-281195','Unity Media','SWD paged me due to SESM instance startup failure. He noticed that two SESM instances were stopped by itself.\r\nI advised him to reboot one problematic SESM server and monitor the result. \r\nHe also noticed Java Heap/Memory problem inside the work logs. I told him monitor the system and if the problem occurs again, page CallP team since they are the right team for SESM instance problems.\n\r\nWe dropped the call','null'),(884,'Kerem Gunduz (NETAS External)','AS-OAM','2011-09-14','110914-281246','Verizon','Er paged me due to SM stop problem during MR upgrade. Sheila tried to stop SM1 from SSH but it was stuck at \"Stopping\" state on MCP GUI. I advised to kill instance from ssh but we noticed that NED is not working properly. \n\r\nNED was restarted and smStop.pl was performed again. Current operation was updated on MCP GUI.\n\r\nWe performed remaining steps till section 4.6.3.Then dropped the call with an agreement.','null'),(885,'Garrett Yates','AS-OAM','2011-09-01','110901-278954','Gary Schultz','Customer had callP issues after upgrade and decided to roll back.  Due to problems on a previous roll back they requested it be done by GPS.\n\n\n\r\nSolution Summary:\n\r\nRolled the system back by uninstalling the db on the primary after setting the install props to Single, doing a files only db install on the primary after changing the load name in the installprops to the old patched load (10.3.1.16) and restoring the db to the version that was backed up before the upgrade. After the restore completed I changed the installprops.txt back to Replicated db and to the base 10.3.1.0 load. The next step was to stop SM 0 from CLI via smStop, then smUndeploy followed by smDeploy and smStart to get it back on the old load. From here I launched the MCP GUI and downgraded the other elements. The DB was then resynchronized via setupDBReplication. After resync was completed the customer installed their custom jar files and made test calls and verified all was successful.','null'),(886,'Senem Gultekin (NETAS External)','AS-OAM','2011-08-18','110818-276272','Corporacion Telemic CA','Problem Description:\n\r\nER paged me for a E1 issue seen after during 9.1 to 10.1.7.0 upgrade. According to customer all type of calls were failing.\n\r\nSolution:\n\r\n- Accessed to the site and checked the system. Its a standalone BCP 7100 system, just 1 SM and 1 DB running, theres no redundant.\r\n- Tried to restart the BCPs but it didnt work, checked the platform levels and realized that BCP servers were not patched yet. \r\n- Since its a BCP related issue Ive contacted Huseyin Yavuzturk from gateway team and he took on the work.','null'),(887,'Senem Gultekin (NETAS External)','AS-OAM','2011-08-19','110817-276012','VENTELO SVERIGE AB','Problem Description:\n\r\nER paged me for a problematic Management Console. System is running on MCP_9.0.7.0_2007-01-31-1245 load.\r\nOnce they click Launch MCP System Management Console link they get the following error;\n\r\nError: The following required field is missing from the launch file: \n\n\r\nSolution\r\n-	Accessed to the site and checked the system. Its a Standalone system, SM and databases are on different servers. Servers are Sun Solaris N240.\r\n-	Primary SM was crashed and secondary SM seemed to have a ned related issue. Tried everything on secondary server(change permissions, restart ned, reboot server) but didnt work.\r\n-	On secondary SM, SM was not coming up. Worked with design team as well but it was problematic.\r\n-	Later on primary SM came up with reboot, but that was not stable as well.\r\n-	Recommended to reinstall SM servers.\r\n-	Handed over the work to RTP GPS (Garrett Yates and Ken Johnson)','null'),(888,'Senem Gultekin (NETAS External)','AS-OAM','2011-08-18','110818-276241','Corporacion Telemic CA','Problem Description:\n\r\nSWD paged me for a mcpUpgradeNextRel script failure during 9.1 to 10.1.7.0 upgrade. \n\r\nmcpUpgradeNextRel script failed:\r\nzip -d /var/mcp/staging -p installprops.txtl.pl -l MCP_10.1.7.0_2009-04-16-1302. \r\nExtracting contents of /var/mcp/loads/MCP_10.1.7.0_2009-04-16-1302.zip\r\nValidating contents of unzip\'d file\r\nUpdated /var/mcp/staging/installprops.txt file with new load => MCP_10.1.7.0_2009-04-16-1302\r\nUpdated /var/mcp/staging/installprops.txt file with new db type => Single \n\r\n--- Invoking mcpUpgrade => to upgrade the SM & Database ---\n\r\n--- Upgrading the Database ---\n\r\nDeploying the DB...\n\r\nUpdating schemas in the DB...\r\nError occurred executing NE commands (see below):\n\r\n   23:35:01  Error: moveTablespaces.pl must be executed prior to DB Upgrade.\n\n\n\r\nSolution\r\n-	Accessed to the site and checked the system. Its a standalone BCP system, just 1 SM and 1 DB running, theres no redundant.\r\n-	Ran moveTablespaces.pl script manually under /var/mcp/run/MCP_9.1/mcpdb_0/bin/util/\r\n-	Wesley ran mcpUpgradeNextRel script again and it completed successfully.\r\n-	MoveTablespaces is a known issue during 9.1 to 10.1 upgrades, output error is assisting the user correctly.\r\n-	Please note that 9.1 and 10.1 releases have been EOL long time ago.','null'),(889,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-08-22','110817-276012','VENTELO SVERIGE AB','Problem Description\r\n-------------------\r\nThe Sharwanda Spivey (ER) called me about this issue that she was owerflow. So I took this issue and call the customer. The customer could not connect MCP Gui because of the SM is not available on both primary and secondary server. The customer provided us the site connection data. When we check the servers, we have seen that the primary server is not reachable and the SM was not up on the secandary server as well.\n\r\nSolution Description\r\n--------------------\r\nWe try to reach primary server but it did not work. Then, we try to deploy SM_1 on secondary server in order to work on secondary server. We have seen that when we deploy and start SM_1 on secondary server it was starting the SM_1 but after a few minutes later it was removed and SM_1 was not up.\n\r\nSo, we suggest the customer to do re-install their both servers. Because of they  have sun solaris server, they should follow sun solaris installation guide in order to do re-install. So we send a sun solaris documentation via e-mail and said that can you check your 9.0 CDs is available on your hand.','null'),(890,'Senem Gultekin (NETAS External)','AS-OAM','2011-08-17','110817-276011','Avaya','Problem Description:\n\r\nAvaya SWD has upgraded from 10.3.1 to 10.3.2, but customer requested rollback due to time/planning issue. They didnt have enough time to run their tests during the maint window after the upgrade. Thus, SWD performed rollback from 10.3.1 to 10.3.1.16. But they were not able to bring SESMs up, call failures came up and outage started.\n\r\nSolution:\n\r\n-	Accessed to the site and checked the database. Even though SM was running on 10.3.1.16, database was still on 10.3.2.0. \r\n-	Checked the MR and Patch Upgrade MOP which SWD was following, it was referring to a downgrade MOP which should have been used for a rollback. In a normal rollback procedure previous database backup should be restored. So basically SWD should have to used the downgrade MOP, not the upgrade MOP for a rollback.\r\n-	Ive ran cleanup replication script from primary database and dropped the replication\r\n-	Ran restore empty db script to wipe out all the data from the primary database.\r\n-	Ran database restore script from /var/mcp/run/MCP_10.3/mcpdb_0/bin/util  returned the database 10.3.1.16 release.\r\n-	At this point customer reported that their SESMs were active and up, calls were successful.\r\n-	Ran resync from primary to secondary to put the replication back.\r\n-	Also sent the 10.x system downgrade MOP to AVAYA people, to be used next time if rollback is necessary.\r\n-	Left the bridge.\r\n-	Please note that there were no issues during the upgrade, customer didnt want to continue with test calls and decided to rollback. This is only a planning/timing issue, not a product issue.','null'),(891,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-08-15','110815-275485','Chad Strand','Mark Seawell paged us about the failure on CVM13 to CVM14 upgrade, when the customer run \'configBackup.pl\' scripts on their system. We connected to the site and investigated the problem. After a minute, we found the the solution that resolves the customer\'s problem.\n\r\nSolution of the Problem:\r\n------------------------\r\nRunnig below scripts on EMServer1, EMServer2, SESMServer1 and SESMServer2;\n\r\n[root@SXFLsslm002 root]# ln -sf ssh-agent /usr/bin/ssh-agent\r\n[root@SXFLsslm002 root]# ln -sf /usr/local/bin/ssh-agent /usr/bin/ssh-agent\r\n[root@SXFLsslm002 root]# ln -sf /usr/local/bin/ssh-keygen /usr/bin/ssh-keygen\r\n[root@SXFLsslm002 root]# ln -sf /usr/local/bin/ssh-keyscan /usr/bin/ssh-keyscan\r\n[root@SXFLsslm002 root]# ln -sf /usr/local/bin/ssh-add /usr/bin/ssh-add\n\r\nAfter we have runned above scripts the problem was resolved and I have left the conference.','null'),(892,'Emre Bertan (NETAS External)','AS-OAM','2011-07-28','110728-236839','WIND Telecom','All calls were failing following a S/W upgrade from MCP 10 to 12. Upgrade was rolled back to MCP10 and calls were successful. There were 7012 subscribers affected (100% of node) for 49 minutes. There are two redundant SESM unit pairs however the software upgrade placed the node in a simplex mode which removed the redundancy, the disk space of the Simplex SESM server was filled up 100% by SESM core dump files. Outage cleared after removing those files since disk usage fall back to 8%.\n\r\nA follow up case 110728-236886 is opened and directed to A2 CALLP GPS.','null'),(893,'Emre Bertan (NETAS External)','AS-OAM','2011-07-27','110729-237427','Midcontinent Communications','The customer got this error because they were trying to upgrade their system from 10.3 to 12.0.6.11 without migrating the Oracle 9i to Oracle 10g. This migration has to take place prior to 12.0.6.6 patch. We had upgraded their system to 12.0.6.6 first then have them patch to upper patch level to overcome the issue.\n\n\n\r\nBulletin is the following: https://support.genband.com/library/view.htm?docid=5456','null'),(894,'Tugrul Timorci (NETAS External)','AS-OAM','2011-08-11','110810-274514','Axtel','Er Manger Wesley Martin called me Axtel issue. There was no available ER engineer so He shortly told me the issue and he gave me customer yahooIM\n\r\nI contacted to customer (Raul) over yahoo. He told me some minutes ago was applied a swact to del CMT, but the issue remains on sam21 gui ,and gave me alarm log of the sam21gui \n\r\nTime:     Thu Aug 11 01:38:50 CDT 2011\r\nReason:   Remote Node Communication Failure\r\nCategory: communications\r\nCause: unknownCause\n\r\nI checked it and this was CMT related issue. I contacted to SESM GPS and invited to conference. We agreed to this is CMT issue and I left the conference.','null'),(895,'Tugrul Timorci (NETAS External)','AS-OAM','2011-08-11','110727-236636','Windmax','Donnel Williamson Called me for Windmax upgrade. \n\r\nThis site\'s upgrade failed and rollbacked the system 10.3 load (mcpUpgrade10to12.pl script failed.). Cause of this failure found by GPS and we agreed to correct it manually. \n\r\nI logged in the system and I corrected the wrong data. \n\r\nrun.zip\\bin\\upgrades\\rel12.0\\base_files\\ prov.scripts\n\r\nELSIF(gwtag_name_lower = \'mxsipapp\') THEN\r\n                                UPDATE physicalnode SET tagdescription_id = \r\n                                (select oid from sip_media where profile_name = \'Mxsipapp\')\r\n                                 WHERE tagdescription_id = physicalnode_tagdesc;\n\r\n                Change \'Mxsipapp\' to \'MxSipApp\'\n\r\nELSIF(gwtag_name_lower = \'mxsipapp\') THEN\r\n                                UPDATE physicalnode SET tagdescription_id = \r\n                                (select oid from sip_media where profile_name = \'MxSipApp\')\r\n                                WHERE tagdescription_id = physicalnode_tagdesc;\n\r\nAlso changed m1                 \r\n                                ELSIF(gwtag_name_lower = \'m1\') THEN\r\n                                UPDATE physicalnode SET tagdescription_id = \r\n                                (select oid from sip_media where profile_name = \' nortelcs1000\')\r\n                                WHERE tagdescription_id = physicalnode_tagdesc;   \n\r\nChange \' nortelcs1000\' to \' m1\'\n\r\nELSIF(gwtag_name_lower = \'m1\') THEN\r\n                                UPDATE physicalnode SET tagdescription_id = \r\n                                (select oid from sip_media where profile_name = \' m1\')\r\n                                WHERE tagdescription_id = physicalnode_tagdesc;   \n\n\r\nI saved the changed data and inform to SWD for go on procedure. He finished the upgrade Successfully. and I dropped the call','null'),(896,'Emre Bertan (NETAS External)','AS-OAM','2011-07-27','110727-236636','WIND Telecom','CVM12 (MCP_10.3.2.11_2010-12-07-2327) to CVM13 (MCP_12.0.6.0_2010-04-19-1428) upgrade fails while running mcpUpgradeFrom10To12.pl\r\nduring upgrading DB Schema after DB migration is made. \n\r\nDbSetup.log (/var/mcp/run/MCP_12.0/sdvdb_0/installLogs/DbSetup_12.0.6.0_2010-04-19-1428.log): \n\r\n**************************************************** \r\nRUNNING SCRIPT delete_gwtag_table.sql \r\n**************************************************** \r\nDECLARE \r\n* \r\nERROR at line 1: \r\nORA-01407: cannot update (\"SDVDB\".\"PHYSICALNODE\".\"TAGDESCRIPTION_ID\") to NULL \r\nORA-06512: at line 25 \n\r\nat the time of the pager it was not possible to determine why it was failing because Db rollsback the changes automatically.\r\nDue to time limit constraints GPS and SWD had to rollback the customer system.  \n\r\nLater on while working on the case at GPS lab the RCA is found. The script crashes because the following query does not return any value from the customer Database \n\r\nselect oid from sip_media where profile_name = \'Mxsipapp\'; \n\r\nThe upgrade script SQL should look for \'MxSipApp\' instead. Once this value is changed in upgrade load (Q02206432-02 [mcpUpgradeFrom10To12.pl fails with Null Pointer Exception]) and the issue is resolved.','null'),(897,'Ege Varhan (NETAS External)','AS-OAM','2011-08-04','110802-272755','Swisscom','Problem Description\r\n----------------------\r\nAfter primary core servers upgrade to MCP 14.0.6.0 from MCP 12.0.6.13, customer reports that CLIP sent to the PSTN during calls is not correct. For all trunk calls, you\'ll see the main trunk number instead of the expected number. \n\r\nSome previous configured data of Number Qualifier service, haven\'t been transfered correctly during the upgrade.\n\r\nI have connected to the site and checked if any other important data were lost or incorrectly transfered during the upgrade. Other settings and the records in the system seems to be correct. \n\r\nI got the database back up of the customer system in MCP 12.0.6.13, and restored into one of our labs, and compared the data before and after the upgrade. I have found out that the structure of Number Qualifier table has been changed and modified in MCP 14 by the design team. I think that the upgrade script was not able to transfer the data into the newly modified database table in MCP 14.0.6.0 correctly.\n\r\nAfter collecting the logs, database backup and outputs of some database tables, I left the call with an agreement.\n\r\nThe problem is not affecting the basic functionality of the calls and will be escalated to the design team for further investigation.\n\n\r\nRegards,\r\nEge Varhan','null'),(898,'Ege Varhan (NETAS External)','AS-OAM','2011-08-03','110803-272847','Oneconnect','Jim Georgaras, who is Software Delivery,  performed a patch and upgraded the system of Oneconnect from MCP 14.0.6.0 MR to MCP 14.0.6.5. The default route started not to work and route the incoming calls to voicemail box of the user after the patch is applied.\n\r\nWe have checked the SESM logs and seen a couple of sweers occured. Since, this is a call problem, we involved the CallP team, and redirect the case to their queue.\n\r\nThe customer wanted to rollback the system to MCP 14.0.6.0 in which the default route and voicemail routing work. Since, the problem was not easy for us to solve quickly, we had to let them rollback to MCP 14.0.6.0.\n\r\nThey rollbacked the system to MCP 14.0.6.0 successfully. We left the call after they completed the rollback and call tests.','null'),(899,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-08-01','110801-272413','LG Nortel','Problem Description:\r\n--------------------\r\nThe customer was not able to access MCP GUI in MCP 3.0. \n\r\nWhenever they try to connect they receive the error message like below;\n\r\nJavax.net.SSL.SSLHandShakeException:No Trusted Certificate found.\n\r\nRoot Cause:\r\n-----------\r\nWe found out that the certificate which SM and MCP GUI use was expired on 30 July 2011. That\'s why they can not connect to the server.\n\r\nSolution Description:\r\n---------------------\r\nThey need to renew the certificate. Since the release is very old, GPS does not know the correct procedure for this operation and suggested to the customer to upgrade their system newer version.\n\r\nStill we are discussing with the design team about procedure.','null'),(900,'Mehmet Salim Demir (NETAS External)','AS-OAM','2011-07-19','110719-235338','Telenet','Meraz has paged me about dryRun script\'s failure on Telenet. When we connected to the site dryRun Script was running but he said that I am waiting for an hour to be completed. We checked the logs, but logs were not being written in the real time while script is runnig. So, we have decided to stop the script and re-run again. After this re-run operation script was finished and the logs were written successfully.','null'),(901,'Kerem Gunduz (NETAS External)','AS-OAM','2011-07-22','TBD','GVT','SWD paged me because they received \"ERROR: the given directory \"/var/mcp/upgrade_bkups\" does not contain any suitable files\" error during performing platform upgrade on HT Langley servers. After initial investigation, we found that local site engineer selected wrong option while following the document.\n\n\r\n Answer questions as prompted by the installer, when asked to select type of installation, choose: Option 3 (Configure the server).  On the Administration Partition Data screen, choose Option [2] (backed up platform data is on a remote server) to re-install the server to a previous configuration. \n\r\nEnginner has selected option 2 instead of option 3. After trying to perform upgrade again with selecting option 3 upgrade was completed successfully. \n\r\nAfter couple of minutes, SWD paged me due to getting error while performing prepMCPUpgrade.pl script. There was an error related with loadsdir10zipfile. I adviced SWD to unzip that file, then script completed successfully.','null'),(902,'Kerem Gunduz (NETAS External)','AS-OAM','2011-07-20','110719-235457','Cabovisao','ER paged me due to SESM server could not connect with SIP Server and due to this problem soft phones could not make call.\r\nWe checked if SESM server can not connect to other Servers or not we noticed there is no problem about connecting to other A2 Servers. After making further investigation, decided that this issue is related with ERS. Customer performed reboot to ERS0 and then SESM server was able to connect with Sip Server.\n\r\nWe dropped the call with an aggrement.','null'),(903,'Senem Gultekin (NETAS External)','AS-OAM','2011-07-15','110715-234875','SaskTel','Problem Description:\n\r\nSWD paged me for a 12.0.6.6 patch issue seen at Sakstel. After 12.0.6 MR upgrade Donnell was trying to apply the 12.0.6.6 mcp core patch to the system by putting the .patch file under /var/mcp/dropbox. In normal circumstances the patch will be generated under /var/mcp/load as zip file in 5-10 min. But according to Donnell this was not happening.\n\r\nSolution:\n\r\n-	We have suspected that .patch file may have been corrupted.\r\n-	Customer transferred the .patch file under /var/mcp/dropbox by himself from the site.\r\n-	Patch load was created under /var/mcp/load successfully. So it seems that at the first attempt the .patch file was corrupted.\r\n-	Donnell performed 12.0.6.6 patch successfully.','null'),(904,'Senem Gultekin (NETAS External)','AS-OAM','2011-07-14','110708-233779','Bell Canada','Problem Description:\n\r\nER paged me for Solaris patch issue at Bell Canada on SESM0 server. Platform patch is from 9.0.7 to 9.1.13. Previously customer had lost their 9.1 CDs and Ive provided the 9.1.13 platform patch via FTP server. But, platform patch procedure was failing.\n\r\nSolution:\n\r\n Accessed to the site and applied the following procedure (section in Maintenance Release Upgrade Guide) to mount the platform patch iso file but it failed.\n\r\n__________________________________________________________________________\r\nmount -F hsfs -o ro `lofiadm -a /filename.iso>` /mnt\n\r\ne.g.  \r\nmount -F hsfs -o ro `lofiadm -a /export/home/sysadmin/filename.iso` /mnt\n\r\nNote: If you get an error message when you try to mount the ISO image file, try to restart devfsadm by following these steps:\n\r\n/etc/init.d/devfsadm stop\r\n/etc/init.d/devfsadm start \r\nlofiadm\n\r\nIf the ISO file is listed, then first disable the loopback file system: \r\ne.g \r\n# lofiadm\r\nBlock Device             File\r\n/dev/lofi/1              /export/home/sysadmin/filename.iso\n\r\nsudo lofiadm d /filename.iso\r\n___________________________________________________________________________\n\r\n-	Normally this platform patch should have been applied from the CD. Thus, Ive told the customer the they should burn the iso file to a CD and apply the platform patch.\r\n-	There was no one at the site so this action was left to the next day.\r\n-	Next day, Ive accessed to the site again and applied the platform patch from the CD. SESM0 was successfully upgraded to 9.1.13 platform level.\r\n-	SESM0 instance started properly and it was up in Hot-Standby.\r\n-	Our job was completed for the server replacement activity. It took 3 days in total but completed successfully. And please keep in mind that 9.1 release has been End Of Life long time ago, its not a supported release.','null'),(905,'Senem Gultekin (NETAS External)','AS-OAM','2011-07-13','110708-233779','Bell Canada','Problem Description:\n\r\nER paged me for the previous N240 Installation at Bell Canada for a 9.1 SESM0 server. According to customer installation was done and they were requesting GPS support to check the server and bring SESM server up\n\r\nSolution:\n\r\n-	Accessed to ERs PC. \r\n-	Customer was having console issues to provide server access to us.\r\n-	Waited for 1 hour and told to customer that we will continue the next day on work hours.\r\n-	Next day during work hours we Ive accessed to Jeffs PC and tried to bring up the SESM0 instance which have been newly installed.\r\n-	It was not coming up, checked the platform levels and realized that all the servers are running on 9.1.13 platform and this new SESM0 server is running on 9.0.7 platform. Which is not a supported configuration and its normal that SESM0 instance is not coming up.\r\n-	Told the customer that they should apply the platform patch to 9.1.13 platform. But they reported that they do not have any platform patch CDs, it seems that they have lost them.\r\n-	Ive uploaded the solaris platform patch (mcp_core_solaris-9.1.13-upgrade.iso) to our Netas FTP Server. Customer will download the iso file to the SESM0 server.\r\n-	Also sent the platform patch procedure to Jeff so he will mount the iso file and run the platform patch.\r\n-	Left the conf. with agreement.','null'),(906,'Senem Gultekin (NETAS External)','AS-OAM','2011-07-13','110708-233779','Bell Canada','Problem Description:\n\r\nBell Canada 9.1.14 site SESM0 was down and server replacement was required. Servers are Sun solaris N240. New server was at the site but customer requested installation to be provided by ER. Since 9.1 has been EOL long time ago, ER doesnt have any N240 installation method guide.\n\r\nSolution:\n\r\n-	Sent MCP 9.1 240 Installation & Commissioning guide to Earl. \r\n-	He was going to do the installation and if any issue come up he will page me.\r\n-	Ended the call.','null'),(907,'Senem Gultekin (NETAS External)','AS-OAM','2011-07-13','110713-234413','Unitymedia','Problem Description:\n\r\nER paged for a SNMP agent alarm seen on secondary database of Unitymedia site. The main issue was unregistered 20.000 subscribers and they have fixed it, but as a last step they were not able to get rid of the SNMP agent alarm. Customer is running on 10.3.2 MR\n\r\nSolution:\r\n-	Accessed to the site, checked the database and MCP GUI. There was only snmp Agent alarm on secondary database.\r\n-	Ran the config_snmp script under /home/oracle/bin.\r\n-	Checked from the GUI and the alarm was still on secondary database.\r\n-	Stopped and started the secondary database monitor from the GUI.\r\n-	Alarm was cleared.\r\n-	Left the conf.','null'),(908,'Senem Gultekin (NETAS External)','AS-OAM','2011-07-12','110712-234099','Videotron','Problem Description:\n\r\nVideotron was performing upgrade from 12.0.6.9 to 14.0.3. During prep steps, DB Dry run script failed due to a Schema mismatch error on the database. DB dry run is just a prep step and this failure will not affect anything on the current system.\n\r\nSolution:\n\r\n-	Checked the logs and its clear that theres a schema mismatch issue.\r\nThe upgrade scripts in load MCP_14.0.3.0_2011-01-05-2100 \r\nwas tested against the current database, the test result is \r\nFAILED!!!! \r\nthe reason for the failure is: \r\nSchema mismatch(es) is(are) found. \r\nPlease review the result file: \r\n/var/mcp/run/install/work/DBUpgradeDryRunResult_MCP_14.0.3.0_2011-01-05-2100.zip \r\nat EM/SM server(10.154.38.16). The zip file contains: \r\nDBDryRunUpgradeSummary: \r\nResult sumary of the DB Upgrade Dry-Run. \r\nUpgradeDryRunLogs/OracleMethod_upgrade_dry_run.log: \r\nUpgrade Log. \r\nUpgradeDryRunLogs/OracleMethod_upgrade_dry_run_debug.log: \r\nDebug log from mcsdbUtl Package. \r\nFiles in UpgradeDryRunLogs/dataChanges directory: \r\nThose files(if any) contain important data changes during \r\nupgrade that the end user need to know, or additional \r\nactions the end user need to perform after \'TRUE\' upgrade.\n\r\n-	Checked our released MRs and realized that our latest MR is 14.0.6 MR. Customer was trying to upgrade to 14.0.3 MR which is an older MR.\r\n-	Wesley checked the latest MR with the customer and they had it. \r\n-	Wesley ran the DB dry run script again for 14.0.6 MR and it was completed successfully.\r\n-	Even though the maint.  window was a little over the track customer preferred to start the upgrade, and upgrade completed successfully.\r\n-	Ended the conf with Wesley.','null'),(909,'Tugrul Timorci (NETAS External)','AS-OAM','2011-07-09','110708-233779','Bell Canada','ER called me for SESM1 was unreachable. System was on MCS3.0\n\r\nThere was a red alarm indicator on the server. Server was not able boot up. customer tested the servers disk on another server. Another server did not boot with problematic server\'s disks. This was a disk issue and customer need to change disks. Customer had a spare disk but they did not want to change with spares and they wanted to involvement of the SUN / Solaris. \n\r\nIn this point I gave the action plan which was the disk replacement. \r\nAfter the agreement  I dropped the call cause of customer wanted to involvement of SUN.','null'),(910,'Tugrul Timorci (NETAS External)','AS-OAM','2011-07-08','110708-233528','Sprint Communications Company LP','ER called me for SM1 was unreachable and MCP GUI was not open. System was on MCS3.0\n\r\nI logged in the system over ER. I tried to ping secondary server but that was not ping able. I suggested to hard reboot server. Customer was not on the site so We wait customer. In parallel we tried to connect SM over SM0. I run below scripts but it did not work. \r\n/IMS/mgmtsvr/bin/mgmtsvr/MgmtSvrShutdown.pl\r\n/IMS/mgmtsvr/bin/mgmtsvr/MgmtSvrConfigSetup.pl\n\r\nI checked the uptime of the server that server\'s uptime was 1213 day. I suggested to reboot this server but this server did not up. I offered the reinstall this server but Ken knew the workaround solution. He took control and delete old files which were /var is full of millions of little files. while the amount of data in them is small, they each use a block on the disk. so the disk is only 70% full of data, but ALL blocks are in use. He deleted these files and tried to reboot this server . Server booted up. \n\r\nI tried again below scripts worked MCP GUI started to work\r\n/IMS/mgmtsvr/bin/mgmtsvr/MgmtSvrShutdown.pl\r\n/IMS/mgmtsvr/bin/mgmtsvr/MgmtSvrConfigSetup.pl\n\n\r\nCustomer arrived the site and hard rebooted the server. But this did not work. We tried to boot from redundant disks but these attempts did not worked too. We suggested to reinstall server but customer does not have the CDs. They will get the CDs and reinstall the server next week. \n\r\nAfter agreement we dropped the call.','null'),(911,'Emre Bertan (NETAS External)','AS-OAM','2011-06-20','110621-228631','R Cable y Telecomunicaciones','About 15 minutes into running oracleMigration.pl script, putty session / connection to the Primary System Manager was lost, script was not finished at that point so SWD engineer contacted OAM GPS Pager.\n\r\nGPS investigated the database and confirmed the migration wasn\'t yet complete. Although oracle was at 10g, patching was still at the previous level 23, so GPS recommended the oracleMigration script to be re-executed. \n\r\nGSD re-excecuted the script successfully, constantly sending characters to the putty session to ensure session was not lost again. \n\r\nScript completed in 30 minutes and problem is solved.','null'),(912,'Emre Bertan (NETAS External)','AS-OAM','2011-06-20','110620-228520','Embarq Midwest Mgmt Services Co','A2 load: CVM14\n\r\nCustomer site was upgraded to MCP_14.0.3.1_2011-01-11-1024 recently from MCP_12.0.6.9_2011-03-14-2155 and the site engineer performed resync operation however before the upgrade the replication is dropped therefore it has to be set up again before it can be resynced. \n\r\nNN10440-450 08.19 indicates on its Upgrading the core servers in the CS 2000-A2E/SSL system section, step 28 that: \n\r\n28 Set up database replication. \r\nFor detailed steps, see Re-synchronizing the two databases \r\nfrom Primary to Secondary (page 1754). \n\r\nCustomer is advised to use the document and run setupDBReplication.pl from PrimaryEM server as stated in the upgrade guide.','null'),(913,'Kerem Gunduz (NETAS External)','AS-OAM','2011-06-18','TBD','CenturyLink','Kenneth paged me about upgrade failure. I noticed that he tried to change NEs load to 14.0 onMCP GUI while SM and DB load at 12.0. I told him first perform upgrade the system to 14.0 then change NEs load to 14.0 on MCP GUI.\r\nWe performed platform and core upgrade together, then he was able to change loads to 14.0','null'),(914,'Kerem Gunduz (NETAS External)','AS-OAM','2011-06-18','TBD','','Kenneth paged me due to load problem, I checked the /var/mcp/loads folder and noticed that .patch file was put there instead of /var/mcp/dropbox directory. \r\nI put the load manually to the /var/mcp/loads folder. Then I noticed that he is trying to upgrade 12.0.6.7 before completing Oracle migration. I told him to complete migration before performing patch. We agreed and dropped the call.','null'),(915,'Kerem Gunduz (NETAS External)','AS-OAM','2011-06-15','110615-227695','Puerto Rico Telephone Co Inc','SWD paged me due to could not access to the secondary EM server. I told im to get direct console access but he told me that they can\'t get it. So as a lost option, we wanted customer to perform the hard reboot at the site, after they performed we could logged in successfully.\n\r\nThe root cause of this issue known and this issue fixed on 12.0.11 Platfom Patch.\n\n\r\nWe dropped the call with an agreement.','null'),(916,'Kerem Gunduz (NETAS External)','AS-OAM','2011-06-15','110615-227683','Puerto Rico Telephone Co Inc','SWD paged me due to failure of patchPlatform.pl script. \n\r\nWhen I checked the logs , I noticed that script try to create folder named upgrade_bkups under the /var/mcp/ folder but there was file upgrade_bkupscauses failure.\n\r\nI renamed upgrade_bkups folder then run script again and script completed successfully.\n\r\nWe dropped the call with an agreement.','null'),(917,'Kerem Gunduz (NETAS External)','AS-OAM','2011-06-15','110615-227678','Claro','SWD paged me due to failure of patchPlatform.pl script, when I checked the logs I noticed script try to add new user group with existing one there.\n\r\nThis issue occured because customer added customuser groups to that related server before. IDs with custom users and ids with new platform users were the same. Due to this id conflict, script was failed. We already do not support adding manually any user groups to server.\n\r\nI deleted two user group that was added manually from the server then run script again and it was completed successfully.\n\r\nRoot cause : Custom user groups with same id with newly added platform user groups.\n\r\nPreventative action :Do not adding custom user groups to the server.\n\r\nWe dropped the call with an agreement.','null'),(918,'Kerem Gunduz (NETAS External)','AS-OAM','2011-06-15','110615-227649 ','Videotron','SWD paged me about could not ssh to the both EM serbers.\n\r\nSWD had dthe direct console for EM0 so I advised him to run for:\n\r\n>service sshd restart\n\r\nFor EM1, we performed hard reboot then we could reach both EMs via SSH.\n\r\nI suspect about SCSI Raid problem about being root cause, I sent the bulletin to how to check and resolve it. If it is not related with SCSI,  it may be the network related problem and there is no log about it. But the workaround is quite simple as I mentioned above, just  have direct console then run command above.\n\r\nWe dropped the call with an agreement.','null'),(919,'Kerem Gunduz (NETAS External)','AS-OAM','2011-06-14','110614-227457','','SWD paged me about upgrade failure from CVM11 to CVM13.\r\nI checked the installprops.txt and noticed that patch load was put as a ne.load property value. I told Wesley to change it to the MR load which 12.0.6.0. Then we noticed that load folder was removed since document says delete it. Now we are checking the document if there is wrong thing or not. \r\nWe extracted load zip file with running mcpExtractLoad.pl script and then try to perform upgrade again.\n\r\nUpgrade finished successfully, we dropped the call with an aggrement.','null'),(920,'Senem Gultekin (NETAS External)','AS-OAM','2011-06-09','110609-226823','Thunder Bay Telephone','Problem Description:\n\r\nSWD upgraded the system from 10.3.2.4 to 10.3.2.13 patch level. Due to SM alarms they have rebooted the EM0 server. Afterwards they realized SESM alarms as follows;\n\r\nMajor Standard Recording Stream Unavailable alarm was raised \n\r\nAlarmName: Standard Recording Stream Unavailable \r\nTimeStamp: Thu Jun 09 01:39:43 EDT 2011 \r\nFaultNumber: 101 \r\nShortFamilyName: RTA \r\nLongFamilyName: RECTRAGT \r\nSeverity: MAJOR \r\nProbableCause: transmit failure \r\nDescription: Number of Standard Recording Stream for log system is Down: 1/1. Stream: SESM1_1. \r\nCorrective Action: Check the state of the element manager. Bring up the element manager if it is not active. Otherwise, check network connectivity. --- Patching the SysMgr (SM) --- \n\r\nSolution:\n\r\n-	Accessed to the site and checked the system.  \r\n-	There were over 100 logs under /var/mcp/spool/logs in SESM1_1. It seemed that logs were stuck there. \r\n-	Restarted the SESM1_1 instance from the MCP GUI.\r\n-	Alarm was cleared out. \r\n-	Left the bridge.','null'),(921,'Senem Gultekin (NETAS External)','AS-OAM','2011-06-09','110609-226799','Thunder Bay Telephone','Problem Description: \n\r\nSWD was upgrading from 10.3.2.4 to 10.3.2.13 patch level. Script got stock deploying SM1 as follows; \n\r\n--- Patching the SysMgr (SM) --- \n\r\nStopping SM_0. \n\r\nQuerying SM instance data from the database. \n\r\nUndeploying MCP_10.3/SM_0. \n\r\nDeploying SM_0 with MCP_10.3.2.13_2011-03-22-0131. \n\r\nUpgrading configuration parameters to MCP_10.3.2.13_2011-03-22-0131 in the database. \n\r\nUpdating SM instance(s) load data to MCP_10.3.2.13_2011-03-22-0131 in the database. \n\r\nStopping SM_1. \n\r\nUndeploying MCP_10.3/SM_1. \n\r\nDeploying SM_1 with MCP_10.3.2.13_2011-03-22-0131. \r\n*** SM Patch FAILED *** \r\nSee SM Patch log for posible details \n\n\r\n*** SM/DataBase Patch FAILED *** \r\nUse of uninitialized value in concatenation (.) or string at ./mcpPatch.pl line 257,  line 2. \r\nSee log files in for possible details \n\n\r\nAlso SWD reported that MCP GUI cannot open. \n\r\nSolution: \n\r\n- Accessed to the site and checked the system. \r\n- Ive investigated the logs and it seemed that SM1 was problematic. \r\n- I logged in to the database and the database was already upgraded to 10.3.2.11 as following; \r\n- OID INSTANCE_NUMBER LOAD LOG_DB ---------- --------------- -------------------------------- \r\n- ---------- SERVER ---------- \r\n- 1245 0 MCP_10.3.2.13_2011-03-22-0131 229 340 \r\n- 1249 1 MCP_10.3.2.13_2011-03-22-0131 229 343 \n\r\n- Ive started SM0 manually from the server and GUI started successfully. \n\r\n- Once Ive opened the MCP GUI I saw that SMs are already on 10.3.2.13 level. Ive tried to start SM1 but it wasnt coming up. \n\r\n- Ive accessed SM1 via ssh checked the SM1 directory and realized that not all the directories were there which should come with deploy. \n\r\n- Ran the neinit p command. No respond from the ned. It seemed that there was a problem with SM1 ned. \n\r\n- Restarted SM1 ned manually by neinit stop and neinit start. \n\r\n- Deployed SM1 again and all the necessary directories were placed in SM1 server. Started SM1 and it was able to come up. \n\r\n- SM0 was Active and SM1 was Hot Standby running successfully on 10.3.2.11. \n\r\n- Left the bridge and SWD continued to upgrade other instances (SESM, PROV etc.)','null'),(922,'Tugrul Timorci (NETAS External)','AS-OAM','2011-06-01','110601-224720','R-Cable','I was paged by Mark Zattiero who is ER. He told that after 10.3 to 12.0.6.6 Upgrade calls dropped. \n\r\nI wanted to check system. I suspected the sesm were down. I connected to site over ER via vnc. SM , DB and SESMs were up and working. This could be specific CallP issue. I Called CallP pager and invited to Bridge. Koray Joined to conferance and he said that this issue solved on 12.0.6.10 but customer could not patch to that load. Because they had to migrate Oracle 9i to 10g and system was half upgraded at this point they could not migrate the oracle. Koray did not know the work aroud solution for this issue. \n\r\nCustomer wanted to rollback their system. SWD started to rollback. After agreement I dropped the call.','null'),(923,'Ege Varhan (NETAS External)','AS-OAM','2011-05-25','110524-223810','WIND Telecom','I was paged by Bill Price at 10 AM on May 25, 2011. He indicated that both System Managers were unavailable at the site of WIND Telecom. \n\r\nProblem Description\r\n--------------------\r\nBoth System Managers were unavailable.\n\r\nSolution Desciprion\r\n--------------------\r\nI have connected to one of the system managers via SSH, and launched the instance of the system manager with neStart.pl script. After it was started, I was able to access to System Management Console (SM GUI), however I couldn\'t start the another System Manager, which was unavailable, via SM GUI. I tried to connect to the server via SSH and indented to start the instance via scripts directly, but i couldn\'t connect to the server. The server was unreachable. We have suggested the technication, who is in the site, to power cycle the server. He said that he did it, but still the server was not reachable. And then, we suggested him to connect to the server via console port. They did it in the evening and restart the server successfully. After I was able to access to the server, I managed to start the secondary SM, and other instances, which are located in the secondary server. I checked the system and it seemed to stable. We left the call with an agreement.\n\r\nRegards,\r\nEge Varhan','null'),(924,'Senem Gultekin (NETAS External)','AS-OAM','2011-05-20','110518-222576','UPC Nederland','Problem Description:\n\r\nER paged for a critical alarm seen on UPC.\r\nCustomer has replaced the secondary SM server which is a HT Langley server.\n\r\nAlarmName: LKEY_HARDWARE_MISMATCH_ERROR_CRITICAL_754 \r\nTimeStamp: Thu May 19 17:46:00 CEST 2011 \r\nFaultNumber: 754 \r\nShortFamilyName: LKEY \r\nLongFamilyName: LKEYSYS \r\nSeverity: CRITICAL \r\nProbableCause: file error \r\nDescription: License key problem detected: License key does not match hardware. \r\nCorrective Action: Install a valid license key. \n\r\nAction:\r\n- Every server has its own MAC address. Once a replacement has performed a new MAC address will come for the system.\r\n- License Keys contain MAC addresses of SM servers. Since the replaced server is one of the SM servers the new MAC address needs to be added to the License Key.\r\n- A new License key needed to be generated by KRS team. I\'ve forwarded the ER to KRS team. Customer site was running on primary SM server, and the replaced server was secondary server. Thus, customer was fine to apply the license key later. KRS team can provide license key during business hours.\r\n- I left the bridge with customer approval.\r\n- Next day, KRS team generated a new license key, customer applied to their system and alarm has been cleared.','null'),(925,'Senem Gultekin (NETAS External)','AS-OAM','2011-05-19','110518-222738','Telecom Liechtenstein AG','Problem Description:\n\r\nER paged me for SIP PBX issue seen after the 12.0.6.4 to 12.0.6.11 upgrade. \r\nCustomer reported that SIP PBX was failing after the upgrade, it was working fine on 12.0.6.4 patch level.\n\r\nSIP PBX Alarm: \r\nAlarmName: SIP PBX/SIP RS Unreachable \r\nFaultNumber: 101 \r\nSeverity: Major \n\r\nActions:\r\n- Requested from the customer to test Provisioning from Provisioning Client to see if there are any issues on database. It was successful.\r\n- Since it was a SIP PBX issue, I\'ve told ER to page SIP Lines pager. But the pager number on the list was not valid. \r\n- There was a previous case (110330-213715) from the same customer which CALLP team worked on it. ER paged CALLP team. Engin Bacanak was involved.\r\n- Later on SIP Lines pager prime Dan Stahlman was involved. They have investigated the issue.\r\n- Per to customer request I stayed online just in case if they would like to turn back to the older load and if they see any OAM related issues.\r\n- At last, the issue fully was SIP PBX related and Dan Sthalman was working on it, I\'ve left the bridge with customer approval.','null'),(926,'Senem Gultekin (NETAS External)','AS-OAM','2011-05-16','110511-220848','UPC','Customer was having JMXM 200 MAJOR Memory use is approaching critical levels alarms previously. This alarm was cleared before. According to GPS investigation heap dump should need to be collected on the next alarm appearance. Alarm was generated again and GTS requested procedure how to collect heap dump. Ive provided the procedure and dropped the call.','null'),(927,'Kerem Gunduz (NETAS External)','AS-OAM','2011-05-04','110504-219130','VTR Global Com SA','SWD paged me due to failure of setupDBReplication.pl script. \r\nI checked logs and noticed that several of Oracle paramaters are wrong. The reason for that is the trying to perform 12.0.6.9 patch yesterday.(110503-218978)\n\r\nI replaced current initmcpdb.ora file with Oracle 9i 23 patch initmcpdb.ora. Then run setupDBReplication.pl again and script completed successfully.','null'),(928,'Kerem Gunduz (NETAS External)','AS-OAM','2011-05-03','110503-218978 ','VTR Global Com SA','SWD paged me for 12.0.6.9 patch apply was failed. I checked the Oracle version and it was 9i. As we told before the latest patch load that can be applied before completing Oracle migration is 12.0.6.6. However in this case 12.0.6.9 patch was applied even Oracle migration was not completed yet so script was failed.\n\r\nI corrected oracle configuration file and advised SWT to keep system on 12.0.6.0 until completing Oracle migration.','null'),(929,'Tugrul Timorci (NETAS External)','AS-OAM','2011-04-27','110427-218278','British Telecom','Camila Guzzo Called me for they can not add user add over OSSGATE. \n\r\nI told her quick test for understand the issue is from our side. I suggested to user add over PROV client. She was able to add user over PROV. This means this is not related to OAM. I suggested to call CMT (OSSGATE)team. \n\r\nAfter the agreement I dropped the call.','null'),(930,'Ege Varhan (NETAS External)','AS-OAM','2011-04-20','110420-217579','Hong Kong Broadband Network','I was paged by Donald Young, who is ER.  Donald indicated that the customer couldn\'t ping multiple servers via Management Server. We connected to the site, and started to investigate the problem. Since, the release was so old (MCS 3.0), it took sometime for us to understand the differences and get used to it. While we were investigating the problem, we realized that the source of the problem was BPS servers (BPS0 and BPS1).We suggested the customer to reboot the servers for understanding the issue is related to Network interfaces. Since BPS servers are supported by Avaya, Donald got involved an ER from Avaya. Avaya team also started to investigate the problem. After Avaya team suggested a server replacement to the customer and found that the problem was related to BPS servers, we left the call with an agreement.\n\r\nRegards,\r\nEge Varhan','null'),(931,'Ege Varhan (NETAS External)','AS-OAM','2011-04-22','110419-217419','Mobistar','I was paged by Çiğdem Erol Sonmez from NTS. She indicated that she changed the process and session values of the databases of mobistar, and restarted the database instances including both primary and secondary databases. After restarting databases, she realized some minor alarms occurred on some of Session Manager\'s instances on MCP GUI. I have connected to the site and checked the logs. I couldn\'t find anything wrong in terms of the OAM\'s scope. Since the alarms were Session Manager related, I redirected her to CallP team to get better information about the minor alarm. While Ozan Terzi, who is from CallP team, was investigating the problem, I left the call.\n\r\nRegards,\r\nEge Varhan','null'),(932,'Ege Varhan (NETAS External)','AS-OAM','2011-04-21','110421-217749','Mobistar','I was called by Camila from Software Delivery. While she was performing an upgrade at one of mobistar\'s site, after she ran setupDBReplication.pl, there was an alarm occurred on MCP GUI. The alarm info is below;\n\r\nAlarmName: Oracle Replication Link Errors\r\nFaultNumber: 727\r\nShortFamilyName: DBMN\r\nLongFamilyName: DBMON\r\nSeverity: MAJOR\r\nProbableCause: unspecified reason\r\nDescription: Conflicts or incorrectly formed transactions have caused 7 errors on link [\r\nMCPUSERREPGROUP ]\r\nCorrective Action: Contact next level of support if the problem persists.\n\r\n1- First, we ran \"viewRepConflicts.pl\", which is located under \"/var/mcp/run/MCP_12.0/mcpdb_0/bin/util\". The script output indicated that all conflicts occurred for updating \"AdminInfo\" table\n\r\n2- We checked \"AdminInfo\" table from both primary and secondary database via \"sqlplus\" utility. We didn\'t see any difference  between two tables.\n\r\n3- Since, there was no difference in terms of the data between two tables, we decided that it is safe to delete the conflicts and the alarm from the system. We ran \"deleteRepConflicts.pl\" to delete the conflicts from the system. This script is also located in \"/var/mcp/run/MCP_12.0/mcpdb_0/bin/util\"\n\r\nWe left the call with the agreement.\n\r\nRegards,\r\nEge Varhan','null'),(933,'Ege Varhan (NETAS External)','AS-OAM','2011-04-19','110322-211644','Earthlink (deltacom)','I was paged by Donald Young about the shelf-replacement of one of the Session Manager of Earthlink. The customer wanted GPS to observe the replacement process and stay during the replacement. We joined the conference with the customer, and checked and observed the steps, which the customer took. After the hardware replacement was done, we restarted the Session Manager, and tested it. Everything looked ok and then, left the bridge with an agreement.\n\r\nRegards,\r\nEge Varhan','null'),(934,'Ege Varhan (NETAS External)','AS-OAM','2011-04-19','110419-217396','Mobistar (former KPN Belgium)','I was paged by Camila Guzzo from Software Delivery. She indicated that she was performing an upgrade at the site (Mobistar), and she was waiting for mcpUpgradeFrom10To12.pl script to complete for over an hour.\n\r\nI connected to the site via her VNC server, and checked the logs. There was no error messages in the logs. While i was checking/investigating the logs, the script was completed successfully.\n\r\nI have checked the status of the system, and apparently the script completed successfully without having any problems.\n\r\nI left the call with the agreement.\n\r\nCurrently we are checking if it is normal for the script to finish its job around an hour.\n\r\nRegards,\r\nEge Varhan','null'),(935,'Senem Gultekin (NETAS External)','AS-OAM','2011-04-14','110414-216811','110414-216811','Problem Description: \n\r\nSWD paged me due to an upgrade failure during 12.0.4.5 to 12.0.6 MR.  When Josh ran mcpUpgradeMR.pl he saw the following error;\n\r\nBEGIN mcsdb_utl.modify_column(\'NORTEL_MOBEXTN_SPD\', \'AUTO_CALL_PARK_ALL_CALLS\', \'DEFAULT \'\'N\'\' NOT NULL\'); END; \n\r\n* \r\nERROR at line 1: \r\nORA-01442: column to be modified to NOT NULL is already NOT NULL \r\nORA-06512: at \"MCPUSER.MCSDB_COMMON\", line 115 \r\nORA-06512: at \"MCPUSER.MCSDB_COMMON\", line 129 \r\nORA-06512: at \"MCPUSER.MCSDB_UTL\", line 634 \r\nORA-06512: at line 1 \n\r\nAction:\r\n- Josh told me that this is not a live site, but it will be live next week.\r\n- I told that we only give pager support to live site issues and I will continue working on the case the first thing in the morning with high priority, which was 4 hours later.\r\n- I requested permission from the customer to perform some actions during the day, since it was not a live site customer was fine with us working during office hours.\r\n- Left the bridge and worked during the work day, details are in the case logs.','null'),(936,'Emre Bertan (NETAS External)','AS-OAM','2011-04-07','110406-215198','Axtel','Raul Zarate (Axtel) was replacing a SESM server (HTLangley) using replaced servers harddrives and wanted GPs support on the issue.\n\r\nAfter server was replaced and booted up normally, the SESM instance on MCP GUI (Management Console) was shown as => SESM 4_0 Admin state offline . link up and Operational state Hotstandby.\n\r\nIssued a undeploy and deploy operation and afterwards SESM 4_0 was up and running normally with Admin state Online . link up and Operational state Hotstandby. E2 was over.','null'),(937,'Emre Bertan (NETAS External)','AS-OAM','2011-04-04','110322-211644','Earthlink','One of the SESM servers (running on HPcc3310 hardware) was shutdown unexpectedly on site and the investigation showed a voltage related problem causing the server failure. The server was up and running after powercyle. E2 was over.\n\r\nAs a resolution customer had checked their system and verified that the power coming to the server was normal. Therefore the power units were replaced. There are still alarms seeing on the hardware leds therefore the server will be replaced (customer decided to perform this activity at midnight 04/19). GPs had provided both power unit replacement and unit replacement documentation to the field and the customer.','null'),(938,'Kerem Gunduz (NETAS External)','AS-OAM','2011-04-02','110322-211644 ','Earth Link','This issue started on 3/22/11 and looked like just a SSH problem.  After investigating further it was determined that there was a problem with SESM1_0 and decision was made last night to power-cycle the server.  After the power-cycle was performed, the customer could ping the server, but could not deploy.  ER was then contacted and logged into the MCP.  See that the operational state of SESM1_0 is unavailable.  ER asked the on-site tech to check LED status of the server and found LEDs lit, but it did not appear the server is powered up.  On-site tech checked power connections and for any tripped breakers, and did not find any.  ER sees the NEComm alarm stating that communication is down from the SM to SESM1_0.  ER recommended setting up a mtce window to troubleshoot the issue further.  The customer does have a spare server at site that can be used as an replacement if needed. ER engaged GPS (Kerem Gunduz) during maintenance window activity. SESM1_0 was then power cycled and ER and GPS observed the bootstrap from console and the SESM1_0 did come up online hotstandby, but with chassis yellow/minor power alarm. The SESM1_0 was then accessible via SSH within the network. GPS collected the message logs from /var/log directory and observed some logs that needed further investigation from design. Kerem with GPS will engage design and investigate the message logs and contact ER to give update and possible go forward plan. ER will then update customer and if needed, setup offshift to perform required corrective actions on SESM1_0. ER will monitor SESM servers(SESM1_0 stiopped inactive) until Monday\'s update from GPS/design.','null'),(939,'Kerem Gunduz (NETAS External)','AS-OAM','2011-03-30','110330-213547','British Telecom','SWD started to apply upgrade procedure from CVM11 to CVM13 and it was completed successfully. Then mcp patch upgrade was performed to upgrade system to 12.0.6.7. It was failed due to database upgrade script. \r\nI advised customer upgrade to 12.0.6.4 since that upgrade script does not exists on that patch. Upgrading to 12.0.6.4 patch was stucked during \"\"\r\n\"Updating DB Schema\" stage. It was occured due to NED error. The next day I rebooted server then tried to perform mcpPatch.pl again, it was succeeded without any issue.\r\nAggreed with SWD and I left the conference. SWD performed rest of upgrade steps.','null'),(940,'Tugrul Timorci (NETAS External)','AS-OAM','2011-03-24','110324-212507',' Cable Onda','Donnell Williamson called me for mcpPatch.pl fail. \n\r\nHe was trying to upgrade\n\r\n*** SM Patch FAILED \r\n*** See SM Patch log for posible details \r\n*** SM/DataBase Patch FAILED \r\n*** Use of uninitialized value in concatenation (.) or string at ./mcpPatch.pl line 257,  line 2. See log files in for possible details\n\r\nSolution :\r\n==================\n\r\nI run same script for see detailed logs but this time script worked successfully\n\r\nAfter the agreement I left the conferance','null'),(941,'Tugrul Timorci (NETAS External)','AS-OAM','2011-03-26','110326-213049','Cypress Commmunications','Problem Description: \r\n===================== \r\nER reported that the customer was trying to connect one of the BCP blades with root user and realized the root password doesn\'t work. To apply a recovery procedure to change the root password from GRUB screen customer did a restart and after that the prompt never appeared and got stuck on a screen which shows the MAC addresses. \n\n\r\nSolution: \r\n====================== \n\r\nI connected the site and I checked the recover procedure. Every thing seems ok for recovery procedure. There could be another issue for BC-T. I called the GWC pager for further support. \n\r\nAyse Ozer joined the conferance. After agreement I left the conferace','null'),(942,'Tugrul Timorci (NETAS External)','AS-OAM','2011-03-24','110325-212822 ','BT','Camila Guzzo (SWD) called me for BC-T fails\n\r\nProblem:\n\r\nAfter Firmware Upgrade server gives DIMM issue. \n\r\nSolution :\n\r\nI checked the system. System was working but there were DIMM failures. It means there was no service impact for customer and not Pager call require. \n\r\nAfter agreement She created case and dispatched to GWC queue.','null'),(943,'Tugrul Timorci (NETAS External)','AS-OAM','2011-03-24','110324-212507','Cable Onda','Donnell Williamson (SWD) Called me Upgrade issue 9.1 to 10.1\n\r\nProblem:\n\r\nDB upgrade failure. \n\r\nSolution:\n\r\nI connected the and I checked the fail logs. I saw below log message\n\r\nDeploying the DB...\n\r\nUpdating schemas in the DB...\r\nError occurred executing NE commands (see below):\r\n  23:33:07  Error: moveTablespaces.pl must be executed prior to DB Upgrade.\r\nNE command exited with the value: 1\r\nError executing start commands.\n\r\nI tried to run  moveTablespaces.pl but script failed with tns lsnr fail. I tried to connect DB but DB was powered off. I started the DB and I run moveTablespaces.pl script. It finished succesfully. \n\r\nAfter agreement I dropped the call.','null'),(944,'Tugrul Timorci (NETAS External)','AS-OAM','2011-03-23','110323-212256      ','BT','Camila SWD called me for cleanup replication issue. Customer was on 10.3 load. \n\nProblem :\nShe complained for script finished but mcpgui shows relication. \n\nSolution:\n\nI connected to site and I checked the system it showed the replication on DB monitor tab. I connected the server and I ran test replication script on the system. Script failed. It mean replication dropped. I said her \"you can continue upgrade\" but she said that complained the loss of the time. So she did not want to continue upgrade. \n\nI started the resync but it failed cause of the corrupted tnsnames.ora file. \nI called Kerem Gunduz for solving issue. \n\nThe reason why this the file is corrupted is existing invalid characters inside of it. During resync operation setupDBAlias is being run and it updates contents os tnsnames.ora, then it checks if connection can be established to both pri and sec dbs, in our situation, at that time it failed since it could not connect to the sec db, this makes sense because tnsnames.ora file is being used when you try to connect sec db from primary and it was corrupted. \nThese invalid characters were not created by setupDBAlias,resync or cleanupReplication. resync and cleanupreplication scripts don\'t edit tnsnames.ora. \nsetupDbAlias only adds required info to the tnsnames.ora, thats it. The only possible reason of this situation is editing the file manually. It looks like tnsnames.ora file was edited for some reason and some invalid characters were left inside of it, invalid characters were many unneeded spaces middle of the file. Handling of these characters occurs on Oracle side, so we can not make any robustness to handle spaces etc..','null'),(945,'Ege Varhan (NETAS External)','AS-OAM','2011-03-15','110315-210199      ','Emilio Cantu','Problem Summary\n\nDuring the \'./mcpUpgradeFrom10To12.pl\' script the following error was encountered at Axtel site:\n\n================================================================\n--- Upgrading the Database ---\n\nDeploying the DB...\n\nUpdating schemas in the DB...\nError occurred executing NE commands (see below):\n\nNE command exited with the value: 1\nError executing start commands.\n\n\nSee log file for possible details: /var/mcp/run/install/logs/dbInstall.log.20110314_215301\n\n*** DataBase Upgrade FAILED ***\nSee DB Install log for possible details\n\n\n\nSee log file for possible details: /var/mcp/run/install/logs/mcpUpgrade.log.20110314_215255\n\n*** SM/DataBase Upgrade FAILED ***\n\n================================================================\n\nSolution Summary\nGPS investigated issue and noticed that moveTablespaces.pl script was not run therefore some tables do not exist in the correct tablespaces. \n\nGPS run following command to resolve the issue and then customer could finish upgrade successfully.\n\nalter table ADDRESS move  tablespace MCP_LARGEST_TABLES;\nalter table ADDRESSBOOK move  tablespace MCP_LARGEST_TABLES;\nalter table ADDRESSBOOKPHONE move  tablespace MCP_LARGEST_TABLES;\n\nalter index IND_ADDRESSBOOKPHONE rebuild;\nalter index PK_ADDRESSBOOKPHONE rebuild;\nalter index IND_ADDRESSBOOK3 rebuild;\nalter index IND_ADDRESSBOOK2 rebuild;\nalter index IND_ADDRESSBOOK rebuild;\nalter index PK_ADDRESSBOOK rebuild;\nalter index UK_ADDRESSBOOK rebuild;\nalter index PK_ADDRESS rebuild;\nalter index IND_ADDRESS rebuild;\nalter index IND_ADDRESS1 rebuild;','null'),(946,'Emre OVA (NETAS External)','AS-GW','2019-04-18','190418-182844','LIBERTY GLOBAL SERVICES BV','Customer AS Relese / MCP Load: (AS 10.4: 17.0.22.20)\n\r\nCustomer (LG/ Vodafone Ziggo) were having issues in monitoring BCP instances for a long time.  BCP instances were not able to be managed by SM properly. It was affecting BCP instance monitoring on MCP GUI, Logical View panel. BCP unit status were looking as grey out and BCP units were producing spool files. Produced files were not able to be transferred to SM as OSS file. \n\r\nAfter comprehensive troubleshootings and design debugging actions, we were able to detect that blocked ports in firewall triggers the problem. For this reason, GPS had prepared a procedure in collaboration with design architects. In summary, given actions were including the following actions\n\r\n1- Enable the following blocked ports in FW; \n\r\nall ports between 12,100  12,200 (for SM)\r\nall ports between  14,000  14,100 (for BCP)\r\nall ports between  21,000  21,100 (for SESM)\r\nall ports between 49,152  65,535 (for communication between NEs)\r\nPort 3904 for Media Portal and GWC.\n\r\nNote: Since customer have STRICT security and firewall rules, they had requested us to provide specific IP/Port range for bringing the monitoring of BCPs back. \n\r\n2- Stop BCP instances one by one through BCP server CLI.\r\n3- Clean up spool files on each BCP Server.\r\n4- Run service network restart on each BCP server as root.\r\n5- Undeploy, deploy and start BCP instances one by one.\r\n6- Double swact all SESM pairs.\n\r\n After customer performed the recommended actions above, BCP monitoring and spooling issue was resolved as we expected. We were able to monitor BCP instance alarms via MCP GUI- Logical View panel anymore and spool files could start to be processed properly and transferred to SM. On the other hand, another media portal-related case (181113-160664 - Bouncing Media Portal alarms on SESM units) was resolved after enabling the given port ranges. Recommended GPS actions resolved that problem as well. SESM alarms are still stable and no more bouncing MP alarms found on the system.\n\r\nAfter completing these maintenance actions, while performing call tests customer reported that some specific calls were failing. (from SIP Line to TDM). Specifically T-Mobile calls were failing according to initial customer test calls. Customer told that its an intermittent call issue. Some of calls were passing whereas some of them was failed. Customer were complaining that they cant hear ring back tone and media. (RCA could not be identified yet). \n\r\nPossible reasons: \r\na- GWC may struggling to insert BCP.  \r\nb- Customers STRICT FW Rule settings may caused to create such kind of network trouble.\r\nc- An external IP routing issue may triggered the problem. \n\r\nLots of product support engineers involved in problem analysis and worked on the outage. (GWC, C20, G9, AS OAM, AS CallP GPS engineers).\n\r\nDue to the fact that customer suspected that the problem might be related to made FW changes, they requested us to make an investigation on BCP units. As a first action, we checked the raised alarms on BCP instances and found that there are RTPB 804- Critical initialization and configuration error on BCP3,5 and 9. In order to recover those units, we had rebooted BCP Server3,5, 9 and all BCP alarms had been cleared in this way. There was no any critical/major alarm on BCP instances anymore. After all alarms were cleared on BCPs, we collected PCAPs, decoded RTP and UDP packets and we could hear ring back tone and ongoing calls through BCP 3 and BCP 5. Apart from this, we could not observe any exception on BCP OSS and work log files. Given GPS recommendations for media portal maintenance was NOT already including any step in terms of BLOCKING any port or TOUCHING ON callp ports. On the contrary, those actions were taken for recovering unmonitorable BCP units and fixing BCP spool issues by ENABLING BLOCKED PORTS in FW. We could already confirm that those problems had been resolved after taking these suggestions. BCP instance alarm monitoring was turned into normal state and SM/ BCP communnication was fixed for preventing spool file issues. Customer tried to revert FW changes back and performed a few call tests but the result was same and calls were still failing. \n\r\nAs a result of the outage recovery; after FW changes had been reverted back, we rebooted BCP servers (located on the same cluster) at the same time and customer reported that audio is passing fine. After rebooting all BCP servers (cluster by cluster at the same time), customer verified that all calls are OK and passing fine. In order to investigate the root cause, we collected all logs associated with SM and BCP. Currently, since the FW changes reverted back, BCP instances are still monitoring as grey out and their status looks as Offline  Down  Unavailable on MCP GUI (although they are running fine in fact). At the end of this maintenance windows, at least its been verified that the RCA of BCP Spool&Monitoring (180830-148677) and Bouncing Media Portal alarms case (181113-160664) was blocked ports in FW. After enabling those in FW, we could clearly see that actually those are able to be managed by SM and these problems were resolved. Since just all changes reverted back for recovering the call failures (E1 situtation), currently there are still BCP visibility in terms of instance alarm monitoring and processing BCP spool file issue on site. \n\r\nEven though the fact that we could not observe any BCP-related issue during outage time and-provided previous mtce MP action plan has no any impact on CallP related ports/configuration, we are going to keep our discussion with AS design architects to understand the possible impacts of taken BCP actions on lastly encountered outage. In paralell, although we confirmed that prepared procedure worked and fixed BCP monitoring and spooling issues, we will be discussing the alternative action plan ways for recovering the BCP monitorings without creating a possible call failure. All alternative resolution ways and theories will put on the table in design discussions. Next GPS updates will be given on existing case notes. (180830-148677).','null'),(947,'Emre OVA (NETAS External)','AS-GW','2019-01-25','190124-170228','U.S. Air Force Falcon AFB','MAS Version: 16.0.0.858\n\r\nBrent Combs paged me out from ER and reported that Falcon have intermittent meet me outages. John Shamer and Brent provided all details about outage and I started to work on my analysis.\n\r\nThere were totally 4 MAS servers of customer which include meet me service. (MAS5,6, 7 AND 8)\n\r\nAccording to initial tests; its observed that problem does not occur when just MAS8 is primary (Active).While MAS5,6 or 7 is primary, customer was not able to hear anything for meet me calls.\n\r\nBefore Ive been paged out by ER, full debug logs were already taken on four MAS applications. I quickly monitored the MAS alarms on GUI but there was no any alarm that we can quickly catch. \n\r\nSince the problem only occurs when MAS8 is active, I requested customer to perform a reboot action on MAS5,6 and 7 in order to make all services and processes fresh. They accepted this recommendation but stated that it can be done 2 hours later. Additionally, they requested a root cause analysis until performing the suggested actions. \n\r\nI downloaded the collected MAS debug logs on BC case and found the problem details on scDebug files. According to the following logs, it seems that inbound sessions are rejecting and session reoffer process fails as follows and MAS was returning 480 Temporarily Unavailable error.\n\r\n(FSMEngineManager) Screen Incoming Event [78 = SESSION_REOFFER_RES] \r\n(24 17:00:31.243) (74) (MCSessionEngine) Cleared SEH entry [161935 : 2191] SESSION_REOFFER_ACT (0xf4de5ee0). Entries = 45 \r\n(24 17:00:31.243) (74) (MCSessionEngine) Execution of [SESSION_REOFFER_ACT] completed with result [SessionEventFailed]. \r\n(24 17:00:31.243) (74) (FSMMainDispatcher) Processing event : [SESSION_REOFFER_RES] = FAILED : Sid (74), Tid (4140),\n\r\nsiphdr=17:Event: refer;id=1 \r\nsiphdr=30:Subscription-State: terminated \r\nsipmime=82:mmc_type=15:message/sipfrag \r\nmmc_data=39:SIP/2.0 480 Temporarily Unavailable \n\n\r\nAt this point, I was suspicious about three items:\n\r\n1- A hung process on specifically MAS5,6 and 7 may be causing that problem and server reboot can provide the stability of MAS services. Apart from this, high uptime of running applications may require a reboot in terms of makign the all running services and processes fresh. \n\r\n2- Invalid License keys: 480 Temporary Unavailable message generally appears when currently used license key does not include the requested MAS service. For this reason, I requested to take currently used MAS licensed on MAS5,6,7 and 8 in terms of analyzing that whether the used services are included or not.\n\r\n3- Incorrect port assignment/port conflicts: Sometimes customized port settings may cause port conflicts on MAS and trigger this kind of problems. Since some MAS processes use 5xxxx ports for non-media events, there might be port-conflicts which triggered the problem. Normal default value for conferencing is 31100. In order to identify which port is using by customer, I requested them to check \"System Configuration --> Media Processing --> Advanced Settings --> Conferencing\" section on MAS5,6,7 and 8 and check the \"Starting Port for Conferencing\" port. ER requested these details and attached to case.\n\r\nAfter completing my initial RCA investigation and recommending next actions to resolve intermittent meet me outages, I dropped from conversation with ER. As far as I can see on the case notes of created BC case this morning, it seems that after performing the recommended reboots, meet me service started to work and turned into normal state again as stable without any issue. Since the all problems are resolved associated with meet me and other MAS services, case severity had been decreased from BC to Major for RCA. I put my all investigation findings into case notes and case had been assigned to Bill Picardi from NA MAS GPS. Root cause analysis will be ongoing in case (190124-170228).','null'),(948,'Emre OVA (NETAS External)','AS-GW','2019-01-11','190103-167116	','EASTLINK','John Shamer reported that Easlink have Intermittent MAS Meet-Me Dead Air Issues last week. We had provided an action plan to restore the faulty MAS but customer was requesting GPS assistance to perform the recommended actions during the next MW and they scheduled this MW for tonight. It was a planned MW.\n\r\nProblem description: MAS Intermittent Dead-Air issues were seen in customers meet me calls. Their MAS applications were running in a cluster (MAS2) and (MAS5) were running in the same cluster. Problem was occuring when only MAS2 is active. When MAS5 is active, the same intermittent problem was not happening. \n\r\nGPS analysis: We have analyzed the issue and found that there is a stuck Data Sync alarm on MAS2 and we though that it may cause the Intermittent Issue. For this reason, we had recommended some actions (MAS service reboot, application restart, so on) but none of them worked for clearing the stuck Data Sync alarm. So, we decided to take a MAS backup from healthy MAS and restore it to problematic one. (MAS2). In spite of the provided MOPs, customer did not feel confident and request GPS assistance. In this regard, we have worked with customer tonight and taken the following actions.\n\r\nActions taken:\r\n-------------------\n\r\n- Before starting the MW actions, we have taken MAS backups from MAS2 and MAS5 and saved their License Keys to our local against  any potential issue.\r\n- We confirmed that Data Sync in Progress alarm was exists on MAS2 whereas there is no any alarm on MAS5. \r\n- Then, we have transferred the MAS5 backup to MAS2 servers default  backup directory.\r\n- After that, we have restored MAS2 with MAS backup taken from MAS5. At this point, we were expecting that sync operation will be completed successfully, but we encountered different problems. This time, MAS started to suppose itself as MAS5. In other saying, MAS5s UID, server IP and cluster configs were saved in MAS2. According to my observation, it seems that MAS backups can only be restored on where it is taken from. In other words, if a MAS backup taken from MAS5, it should only be restorable on MAS5. We were not able to restore it into another MAS even if the MAS versions match with each other. \r\n- Since this restoration attempt failed, we restored the MAS2 with its initial backup. (backup file taken before starting the MW)\r\n- At this point, we suspected that the MAS2 data may be corrupted/ or include any bad file, it needs to be restored with a health DB backup, because Data Sync alarm was not never clearing in spite of the taken different actions. (Server reboot, MAS restart, so on.)\r\n- We have checked the existing backup list of MAS2 and found that there is a taken backup named as POSTMR. It was taken on 2018, February. So, we decided to restore MAS2 with this backup. \r\n- In order to prevent any data loss problem, before starting that restoration we have defined the MAS2 as Secondary MAS in cluster. It was Primary before taking the related action. \r\n- So, we have re-defined the cluster and made the MAS5 Primary and MAS2 Secondary. In this regard, we have restored the MAS2 with another old healthy backup. After restoring is completed, healthy data was successfully synced from MAS5 to MAS2. \r\n- Now, we verified that all alarms cleared on MAS5 and MAS2. After verifying that all related alarms cleared and both MAS applications are working with healthy data, we have re-configured the cluster configuration and made the MAS2 Primary and MAS5 Secondary. \r\n- After we informed customer the all MAS appplication are working fine and test calls could be retried.\r\n- Chris told that some Critical alarms appeared on PROV / PA instances about MAS. These alarms were started to disappear one by one slowly but one of the critical alarm was left on PA4. (MASM103  Media Application Server Unreachable). We have tried to restart PA instance but alarm was appeared again.\r\n- Then, Chris stated that the load sharing between MAS APPs should be re-configured because it was configured as MAS5 (%100), and MAS2 was out of the logical entity (%0). \r\n- I have got the PROV/PA/MCP GUI IPs and attempted to correct these settings, but due to the VM failure, it was difficult to reach Logical Entity services, etc. Anyway, its resolved after logout and re-login that VM and I was able to correct those settings. Currently, MAS5 and MAS2 shares the same load (%50, %50).\r\n- After setting the correct Weight values to MAS apps, all PA alarms were cleared as well. \r\n- Since the all MAS and MCP GUI alarms cleared, we wanted customer to re-test the problematic call scenarios. Chris handled meet-me test calls and confirmed that tests were OK in terms of call-quality. Intermittently any other issue was not observed. \n\r\nIn the light of the performed actions above, we decided to keep this BC case opened in PC status and monitor 5-6 days more. If customer doesnt report any further issue/confirm that everything looks fine, BC case will be closed with customer approval. We agreed with customer to keep monitoring and dropped from the bridge.','null'),(949,'Emre OVA (NETAS External)','AS-GW','2019-01-04','190103-167116','EASTLINK','John Shamer from Ribbon GTS team paged me out to report that Eastlink has an Intermittent Meet Me Service issue on their MAS applications. \n\r\nIn order to identify the root cause, I have connected to customer site via GTS VMs and reviewed the case notes of created BC case (190103-167116).\n\r\nCustomer was complaining that when MAS2 is active, intermittent meet me dead air issues occur. When they take the MAS2 out of logical entity or stop that application, MAS5 was getting into active status and the same issue was not observing. \n\r\nAfter get the related details from John and case notes, I have connected to site and checked the current cluster configs and status of MAS applications located under the same logical entity.\n\r\nFirst of all, I have checked the cluster status and confirmed that there are two MAS applications under the same cluster. (MAS2 and MAS5). Both of MAS2 and MAS5 was working fine and all services were running. There was no any not working service .\n\r\nWhile I\'m looking for the reasons for why the problem occurs when only MAS2 is active, I have checked the alarms and seen that there is a Data Synchronization in Progress alarm on Primary MAS application. (MAS2). I strongly suspected that it may trigger the issue and it needs to be synchronized properly with other MAS. Since I suspected that it might include a corrupted or non-synched data on this MAS, when it\'s active it might be causing this kind of issue. We were able to see this alarm on MAS2 while there is no any alarm on MAS5. I think that\'s the reason for why the same intermittent issue does not occur when MAS5 is active.\n\r\nIn order to make the all alarms fresh on problematic MAS cluster, I recommended to stop both MAS (2 and 5) at the same time, execute \"service jboss restart\" and complete the synchronization progress. Due to the fact that customer is not currently in maintenance window, John will update the case with our findings and proceed with GPS recommendations. Additionally, we have collected MAS debug logs and PCAP and dropped from the GTS VM. We expect that the given actions will clear the alarms on both MAS and intermittent issue will be resolved permanently. In this way, even if MAS2 is active, intermittent issue should not be seen again. \n\r\nAs a result, BC case will be handled by GPS on next week and detailed RCA will be provided on this issue.','null'),(950,'Omer KIRCALI','AS-GW','2018-10-16','180308-120980','SINGTEL OPTUS PTY LTD','Ozan kaya paged me about he couldn\'t be able to reach the MAS GUI in SOGPS lab. He state that a JAR file should be given to OPTUS and a test should be run before it.\n\r\nCustomer is on MCP_19.0.1.0\n\r\nI ssh to specified MAS server and notice that after I enter the true password, It is kick me up.\n\r\nI connect to the MAS server via NED and changed the ntsysadm and root passwords. Then, I run \"service jboss restart\" on the server. The problem is resolved.\n\r\nI dropped after problem is resolved on the MAS side.','null'),(951,'Omer KIRCALI','AS-GW','2018-09-21','180920-151943','CHUNGHWA TELECOM CO. LTD.','Peter Meloney paged me about the licance key alarm on the MAS VM; The customer is on 16.0.2.8. MAS:MAS_16.0.0.707\n\r\nAlarmName: Invalid License Key\r\nTimeStamp: Fri Sep 21 01:52:22 CST 2018\r\nFaultNumber: 309\r\nShortFamilyName: MAS\r\nLongFamilyName: MAS\r\nSeverity: CRITICAL\r\nProbableCause: configuration or customization error\r\nDescription: Invalid License Key;Component ID:SC\r\nCorrective Action: Check documentation for appropriate action on this fault number. The alarm will clear automatically when the error condition is resolved. \n\r\nI have connected the site and notice that , after the reboot to HOST2, since MAS2\'s MAC adress was changed , the licance key alarm was appeared. When I checked , I notice that there is no .setVfMac under the /admin file of the HOST2. After recreated the .setVfMac the issue has been resolved. I have done this action for all MAS VM\'s and their HOST servers.\n\r\nThe reason of why only MAS VM was suffered from the issue is other VM\'s calls getHostMAC which gets the host IP from /admin/userinfo.txt, then SSH to the host and does an ifconfing on the host to get the MACs. However MAS VM was checking the MACs and that should defined on the setVfMac on HOST.\n\r\nIn addition while fixing the MAS VM\'s (there is 4 MAS VM) MAS 3 (called meetme1 in customer) was down. I run below command;\n\r\nvirsh destroy \r\nvirsh start \n\r\nAfter perform this action the MAS server is become up and I perform the fix onto this MAS VM as well. \n\r\nSince the issue is resolved with there actions, I dropped from call.','null'),(952,'Emre OVA (NETAS External)','AS-GW','2018-06-29','180626-138326','TSTT','MAS Version: 16.0.0.854\n\r\nGary Norwood from ER paged me out to report that meetme service doesn\'t work properly and returns \"Invalid access code\" error. \n\r\nFirst of all, there was another problem for meetme server before this problem.. When I launched the MAS GUI, I realized that /var/mcp/ma/MAS/platdata/MySQL/data/ibdata1 occupies 35 GB space.. In other words, it was using the %99 of total disk space.\n\r\nThere were totally 4 MAS servers in the customer system. I\'ve compared the MAS versions and observed that only MAS version of problematic meetme server is 16.0.0.854 whereas all other MAS servers\' version is 16.0.0.860. At this point, I was suspicious about incorrect MAS version. In order to make sure that which MAS version is correct and should be installed for customer\'s current MR load, I\'ve checked the 17.0.32 MR release note. In this way, I could confirm that correct version should be 16.0.0.860. In other saying, problematic meetme server was installed with an old MAS version.\n\r\nDue to the fact that ibdata is corrupted and application needs to be re-installed with proper MAS version, I\'ve deleted the MAS application and MAS platform.. In parallel, customer was downloading the 16.0.0.860 installer files. Since the VPN connection is too slow, downloading process took a long time.\n\r\nApplication uninstallation was completed by choosing \"Do not preserve data\" option. While re-installing the application and platform with 16.0.0.860, I had difficulty in platform installation step. After running the MAS_Platform_Installer.bin, script was ended with the following error:\n\r\n\"Installation Cancelled, Can not determine the install type. Please check the logs.\"\n\r\nWe could overcome this failure by deleting the /etc/mas.properties file manually.\n\r\nIn this regard, we were able to re-install the MAS platform and application with correct MAS version. (16.0.0.860).\n\r\nWhile customer performing the meetme service tests, it\'s seen that meetme service doesn\'t work properly and meetme conferencing was returning \"Invalid Access Code\" error.\n\r\nIn order to check the availability of meetme access codes, I took a look at the existing access codes under Media Management --> Provision Media on MAS GUI but there was no any access code under problematic domain.. \n\r\nI\'ve configured the secondary MAS as primary in cluster replication config then tried to perform resync but it did not work for bringing the access codes back. \n\r\nThe only resolution way was looking as restoring the application from a backup file. There was an application backup which has taken 4-5 days ago but we had difficulty in restoring this file. Restoring operation was failing and we were unable to restore the newly upgraded MAS application with this backup file.\n\r\nI was suspecting that this backup may not be appropriate for current customer MAS version. (16.0.0.860), because this backup was taken from 18.0.0.854 version.\n\r\nIn order to confirm that backup failure is associated with the version check, I\'ve tested to restore this backup in our GPS lab which is installed with .860 version. I was able to reproduce the same scenario here. Then, I performed a roll back our MAS application from .860 to .854 and retried to restore this backup in this version. This time I was able to restore backup operation successfully. So, I could confirm that this backup can only be restored on .854 MAS version. In this regard, I\'ve decided to roll back the customer system from .860 version to .854 version. Roll back was successfully completed and I could restore the old backup into customer application successfully.\n\r\nAs a last action, customer application had been upgraded from .854 to .860 version but this time \"Preserve Data\" option was selected.. So, we could recover the all access codes and other required application data successfully.\n\r\nAfter the all tests passed, I dropped from bridge by receiving customer approval.','null'),(953,'Mustafa YUKSEK (NETAS External)','AS-GW','2018-06-26','180626-138359','TSTT','I have paged by James Wilkerson from SWD about uninstall error on MAS during uninstalling platform. When I checked the logs I saw below error; \n\r\n\"Uninstall Error   \r\n ---------------   \r\n Removal of the GENBAND Media Application Server Platform has failed. Found   \r\n running app.\"  \n\r\nIn this respect, I checked the mas.properties directory and confirmed that Uninstall_MAS_Applicatins script has not been completed properly. \r\nThen I remove the mas.properties manually and tried to start uninstalling MAS platform script one more time and it has been completed successfully','null'),(954,'Emre OVA (NETAS External)','AS-GW','2018-06-06','180606-135143','TELEFONICA MOVILES','FROM/TO LOAD: 16.0.0.800 / 16.0.0.854 \r\nLinux platform upgrade from 17.0.11->17.0.20 \n\r\nDavid Bartlett from Ribbon Software Delivery paged me out to report that Music on Hold and Announcement services don\'t work after performing a MAS upgrade.\n\r\nCustomer has 3 x MoH (MOH1, MOH2 & MOH3) - these are single servers, not clustered \n\r\nAdditionally, customer has 2 x Announcements (ANNC1 & ANNC2) - these are single servers, not clustered \n\r\nUpgrades of MOH1 and Announcement1 were completed successfully and tests of these services were passed without any problem.\n\r\nWhen SWD paged me, MOH2 and Announcement2 was not working properly. In order to start the investigation, I\'ve downloaded the MAS debug traces but there was no remained time for maintenance window. For this reason, just Announcement2 MAS platform and application had been rolled back to old version. (We were able to recover MOH2 by restarting the jboss service and restarting the MAS application)\n\r\nWhile performing a rollback, SWD was unable to re-install the platform due to the following failure:\n\r\n16.0.0.800 is already installed.\n\r\nAlthough it looks that this platform is already installed, he was not able to re-install the application. /var/mcp/admin directory was looking empty in spite of the fact that /etc/mas.properties file is populated. In order to recover this issue and re-install the platform and application, I had manually deleted the /etc/mas.properties files then we had re-attempted to perform roll back operation. So, we could roll back the ANNC2 to old load successfully.\n\r\nThen, customer told that MOH1 and ANNC1 is not working properly even if we did not touch on the servers where MOH1 and ANNC1 works. Then, we could identify that it\'s a connectivity issue between SESM and MAS and problem is related to customer\'s network routings.. After customer add required routes and fix the network connectivity issue, this problem had been resolved also. Currently, both MOH and ANNC services are working fine.','null'),(955,'Feridun Bircan SUBASI (NETAS External)','AS-GW','2018-06-01','180601-134494 ','University of Alabama at Birmingham','Upgrade Path:16.0.0.854 to 16.0.0.860\n\r\nI have been paged by Kyle Mawst from ER and he stated that MAS2 is down after upgrade. We connected the site and checked the problematic MAS guest\'s situation. The state of MAS2 was shut down. We tried to run this guest but we faced with this error; \"Unable to allow for disk space path /dev/sr0: no such file or directory\". We found the similar case with the same error and realized that inserted virtual CD caused that error. Since the virtual CD was mounted in the virtual machine, reboot operation was blocking accordingly. Once the virtual CD was demounted, the run command was successful. The MAS 2 server was then unlocked from the MAS 2 GUI and customer reported that all test calls were successful.','null'),(956,'Emre OVA (NETAS External)','AS-GW','2018-03-22','TBD      ','BT Spain','David Paniagua paged me out to report that MAS migration from IBM BCT to MA-RMS has been completed successfully by following the IM-74-3277 but Music-on-Hold service was not working. David was suspected that it might be related to the applied license key, because he was stated that only ADhoc conference is licensed on the license key they got. In this respect, he asked me that how they can re-generate a license key. Accordingly, I\'ve stated that they should contact to KRS team with a text file which includes the all interface MAC addresses in the output.\n\r\nSince it\'s an urgent issue, I\'ve shared the mobile & work phone number and e-mail address of KRS Contact Manager (Bernhard Sauer) with David. Accordingly, he was able to contact to Bernhard for re-generate a license key. Bernhard provided the new license key within an hour quickly and it has been re-applied to the newly migrated MAS of the customer. Then, it has seen that all tests are passed... Currently, all services are working fine on MAS and customer has no any issue with the newly migrated MAS on the MA-RMS.\n\r\nApart from this issue, even if it\'s not a big issue David asked me that how he can change one of the guest domain names and I provided the required action plan to change it via virtual machine manager. Then, he could successfully change the domain name of the VM as he wishes... \n\r\nAfter the issue has been resolved and all tests passed, I\'ve dropped from the bridge.','null'),(957,'Emre OVA (NETAS External)','AS-GW','2018-03-22','TBD','Cabovisao - Televisao por Cabo, SA','OS Upgrade Path (FROM/TO): 14.0.26->17.0.20\n\r\nDavid Bartlett from SWD paged me out to report that patchPlatform.pl script has not been completed and hung while updating the RPM group during the Application part #82 [kernel1-2-6.18-194.el5.x86_64.rpm].\n\r\nI\'ve connected the site by sharing a Bomgar session with David and started to investigate the issue. \n\r\nFirst of all, I\'ve checked the working processes on the server by executing ps -ef command and confirmed that patchPlatform.pl script is still working on the MAS2Server..  I\'ve recommended to wait a few minutes but David stated that it\'s hung and stucked in the same situation for approximately 1 hour.. In parallel, I\'ve checked the uptime of the server and seen that this server is up for a long time (approximately 400 days.)\n\r\nIn order to make the running processes fresh on the server, I\'ve recommended to reboot MAS2 server and retry to execute patchPlatform.pl since the uptime of the MAS2 Server is so high. \n\r\nSWD rebooted the server and re-run the patchPlatform.pl script. Accordingly, this time patching operation has been completed successfully. \n\r\nI\'ve confirmed that the current patch level on the server is 17.0.20 and dropped from the bridge.','null'),(958,'Emre OVA (NETAS External)','AS-GW','2018-02-15','180208-568976','U.S. Air Force Falcon AFB','Glen Anderson from GTS paged me out to report that MAS GUI access to UCVM1 is too slow. \n\r\nI\'ve joined the trillian conversation group with Glen and Vikram and asked some questions. Accordingly, they stated that logging to MAS GUI for UCVM1 is taking too long time. Apart from this, I asked that the same problem is seen for UCVM2 or not, but they confirmed that UCVM2 is working fine. By the way, there was a user (Fred, who are already logged in the GUI) and we could confirm that there is no any alarm on the MAS GUI with the help of him.\n\r\nWe\'ve checked the uptime of UCVMServer1, but the server was up just for 4 days.. Also, Glen stated that CPU spike problem has been observed for UCVMServer1. Customer was attached the output of top command. So, it has seen that a java process was using %99 CPU, then it was decreasing to 30% levels..\n\r\nAt this point, I\'ve suggested to reboot UCVMServer1 and customer was okay to proceed with this way. After the reboot operation is completed, GUI was accessible and it\'s working was normal.. Currently, customer is able to reach MAS GUI and it\'s not too slow anymore.','null'),(959,'Yunus Ozturk','AS-GW','2018-02-09','180209-569171','Midcontinent Communications','Problem Description:\r\n====================\n\r\nAfter the last 3 of 6 MASs were upgraded customer noticed that meetme was not working. Meetme was tested on the first 3 and worked.\n\r\nOnce the MAS upgrade is completed, customer stopped the MAS 1,3,5 and kept MAS 2,4,6 in-service for testing purpose but the Meetme calls did not work.\n\r\nActions Taken:\r\n===============\n\r\nAccessed the site and checked the configurations. We have noticed that customer is using GEO-Redundancy for MAS Servers. MAS 1,3,5 locates on the Active site and MAS 2,4,6 locates on the GEO site. \n\r\nAs per the configurations, MAS 2,4,6 does not have any Meetme data as the customer never took the backups of the Active site and restored them to GEO Cold Standby site. \n\r\nAdditionally, customer needs to change the logical entity configurations on Prov GUI for the corresponding domains. If they would like to use 2 4 6, the logical entity configurations need to be changed and corresponding domains should be assigned to the logical entity where MAS 2 4 6 exists. \n\r\nCustomer also needs to take periodic backups from MAS 1 3 5 and restore them to MAS 2 4 6. This is how GEO-Redundancy works on MAS.\n\r\nIf that is not done, there will be data mismatch between Active and Cold-Standby sites which was the existing situation on system.. There was no data on the GEO MAS Servers (2,4,6)\n\r\nDue to these issues, the behaviour is normal. MAS 2,4,6 cannot process the calls unless the required manual actions are applied.','null'),(960,'Yunus Ozturk','AS-GW','2018-01-30','180130-662309','Guyana Telephone and Telegraph Company','Problem Description:\r\n====================\n\r\nGTS paged out GPS and reported that one of the MAS Servers on customer site is down and they could not communicate with the corresponding MAS Server. \n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and verified that Secondary MAS Server was not reachable from the Primary MAS Server. We also could not reach it through NED connection which means that the Secondary MAS Server was down for some reason. \n\r\n- Connected to the Host Server where the problematic MAS Server is running on and noticed that the MAS Server was at \"Stopped\" status on the \"virsh list --all\" output. \n\r\n- I have tried to start this MAS Server with the command \"virsh start MASServer2\" command but I got the following error;\n\r\nError starting domain: cannot create file \'/var/run/libvirt/qemu/MASServer2.xml.new\': No space left on device \n\r\nTraceback (most recent call last): \r\nFile \"/usr/share/virt-manager/virtManager/asyncjob.py\", line 44, in cb_wrapper \r\ncallback(asyncjob, *args, **kwargs) \r\nFile \"/usr/share/virt-manager/virtManager/asyncjob.py\", line 65, in tmpcb \r\ncallback(*args, **kwargs) \r\nFile \"/usr/share/virt-manager/virtManager/domain.py\", line 1050, in startup \r\nself._backend.create() \r\nFile \"/usr/lib64/python2.6/site-packages/libvirt.py\", line 619, in create \r\nif ret == -1: raise libvirtError (\'virDomainCreate() failed\', dom=self) \r\nlibvirtError: cannot create file \'/var/run/libvirt/qemu/MASServer2.xml.new\': No space left on device\n\r\nThis error means that there is no inode space left on the Host Server which prevents the MAS Server (running on top of it) to be started. \n\r\n- I have checked the \"df -i\" output and noticed that inode usage of \"/var\" directory was %100. \r\n- In order to save inode space, I have executed following command which took around 45 minutes to be completed;\n\r\nfind /var/spool/postfix/maildrop/ -type f -exec rm {} \\;\n\r\n- Once the script was completed, inode usage issue was resolved.\n\r\n- This time I was able to start the MAS Server with the command \"virsh start MASServer2\"\n\r\n- To find out the RCA, I have checked the ipmi tools installed on this server with the command \"rpm -qa | grep -i ipmi\" and noticed that the following 3rd Party ipmi tool was installed on this server. \n\r\nipmiutil-2.9.2-1.el6.x86_64\n\r\nIt appears that ipmiutil tool was installed on this server in the past to be able to troubleshoot some hardware issues and it was not removed. The tool that is officially supported on our platforms is ipmitool tool, not ipmiutil tool. The 3rd Party ipmiutil tool is causing this kind of inode issues. So that, we have removed this ipmiutil tool with the command \"rpm -e ipmiutil-2.9.2-1.el6.x86_64\".\n\r\nAfter clearing the inode usage and removing the ipmiutil tool, the issue was resolved.','null'),(961,'Oktay ESGUL','AS-GW','2018-01-04','180103-658759','BlackBox','David Hedge from ER paged me out to report Voicemail failures at Blackbox.\n\r\nThey have two VM servers and one of them was not working properly based on initial description.\n\r\nEr opened a bridge and we discussed with customer to understand the details of the issue.\n\r\nVM1 was up,yet it was sending 480 response to ucomm invites per  SESM traces. \n\r\nRequested MAS debug traces to understand why MAS sends 480 Temporary Unanavailable. \n\r\nCustomer enabled the traces and made some test calls which all passed somehow.\n\r\nBased on the test,we realized that all production lines are working fine and the issue is just seen at test lines.\n\r\nCustomer performed several more tests with production lines and all passed.\n\r\nAll these proved that the issue is a test line problem and there is no system wide issue. \n\r\nAs there is not a critical system wide problem, we all agreed to track the test line problem via a case instead of pager  and then dropped.','null'),(962,'Yunus Ozturk','AS-GW','2017-11-30','171130-655512','Bragg Communications (Eastlink)','Problem Description:\r\n====================\n\r\nSWD paged out GPS to report the following problem..\n\r\nUpgrade Path : From 18.0.30.1 To 19.0.4.1\n\r\n5 critical alarms were noticed after the A2 upgrade had completed. \n\r\nMAS10 - Internal Component Shutdown (streamsource) \r\nInternal Component Shutdown (Contentstore) \n\r\nMAS1 - SIP offline \r\nInternal Component shutdown (SIPMC) \n\r\nActions Taken:\r\n==============\n\r\nIt seems that some of the MAS Applications could not start up properly since their host servers have been rebooted after the Host OS upgrade which was a part of the A2 Upgrade. \n\r\nWe have restarted the MAS Applications on MAS EM GUI for the MAS Servers that raise alarms and the issue is resolved.','null'),(963,'Oktay ESGUL','AS-GW','2017-11-12','171112-653399','CIBC','Donnell paged me out to report IMM MAS OS patch failure during IMM upgrade.\n\r\nUpgrade Path:\n\r\nFrom:\n\r\nMas Application:16.0.0.846\r\nServer Platform: 17.0.19\n\r\nTo:\n\r\nMas Application : 16.0.0.863\r\nServer Platform : 18.0.11\n\r\nOS patch was failing since jre.x68_64 could not be removed by running \"rpm -e jre.x86_64\" as a part of rpm update during OS patching.Script was failing with \"jre.x86_64 specifies multiple instances\" error.\n\r\nIn order to investigate the issue, I have take a look installed jre packages and observed 4 jre installed which is two for regular MAS servers.\n\r\nThen, I have manually tried to uninstall the exising rpm ,yet all attempt  failed with same error for each jres.\n\r\nThen, searched the internet and found out below parameters which forces the uninstallation for multiple instances .\n\r\n\"rpm -ev --allmatches jre.x86_64\" .\n\r\nI removed all jre packages one by one by adding \"-ev --allmatches\" parameter then re-tried the patchPlatform.pl which completed succesffully.\n\r\nDonnell will continue to rest of IMM3 and IMMMAS3 upgrade.\n\r\nThanks','null'),(964,'Burak Biyik','AS-GW','2017-10-28','171028-651777','Shaw CableSystems','As part of meetme call failures reported, Shaw rebooted Primary node of a MAS cluster providing meetme service and they could not log into its GUI after then. \n\r\nI logged into system and found an ERROR in JBOSS logs. The jboss service did not seem to be initialized properly following reboot.\n\r\nAfter restarting jboss service with below command, the GUI was accessible.\n\r\n#service jboss restart\n\r\nAgreed and dropped the call.','null'),(965,'Yunus Ozturk','AS-GW','2017-09-26','170926-648023','Rogers Communications Canada Inc.','Problem Description:\r\n====================\n\r\nSWD paged out us and reported the following problem about the Genband Media Server upgrade;\n\r\nFROM/TO LOAD: 1.4.2.35 / 10.0.10.7.B \n\r\nAfter running the ./swmsconsole command on GMSH2 (BMTN3MSHVM04) and rebooting the server, the server was unreachable via the GUI or direct ssh. virsh console command also did not work to connect the corresponding GMS unit through the Host Server.\n\r\nActions Taken:\r\n===============\n\r\nTo recover the console access connection issue, we have executed the following command on the Host Server;\n\r\n/etc/init.d/libvirt-bin restart\n\r\nAfter applying this command, we were able to access the GMS unit with console through Host Server.\n\r\nHowever, we noticed that we were still not able to access the GMS unit directly and the GUI was not working.. \n\r\nWe decided to re-run the ./swmsconsole command and did the configurations from scratch. At that point, we noticed that SWD typed a wrong IP address and that was the reason of network connection problem as that IP was not able to reach the Default GW IP.. \n\r\nAfter correcting the IP, the issue is resolved.','null'),(966,'Burak Biyik','AS-GW','2017-08-18','170817-643833','Videotron','As a part of H/W replacement activity (170817-643833), MAS (v.16.0.0.854) server were successfully back in service after booting up with old disks.\n\r\nThere were critical \"Invalid License\" alarm raised as expected due to new MAC addresses introduced.\n\r\nInitially provided license did not clear the alarms as there were missing MAC addresses in the license.\n\r\nThe output of the \"#ifconfig -a\" command was provided to KRS Team to generate a final license including all interfaces.\n\r\nThe alarm got cleared after the final license application.\n\r\nAgreed and droppped the call.','null'),(967,'Burak Biyik','AS-GW','2017-08-16','170816-643680','Videotron','After replacing faulty HT-Langley server, customer was not able to boot the server with old disks that data exists. This was one of the MAS servers providing adhoc,announcement and MusicOnHold services.\n\r\nThe server was trying to boot up from DHCP based on the console output. On-site enginner power cycled the server and I accessed BIOS to check if there is any configuration missing. I disabled OS Watchdog Timer as specified in Installation Method.\n\r\nHowever, the problem was something else. There were no disks listed in the boot priority list so that the server was trying to boot from PXE since there was also not any CD in CD-ROM.\n\r\nAfter power cycling the server several times, we followed the console logs and noticed that two Seagate hard drives are detected by RAID controller indeed, yet those were not visible to BIOS. None of the reseat and swap actions helped.\n\r\nWe inserted new disk drives (delivered with shipped server) to understand if this is disk related but observed the same behavior.\n\r\nIt was not possible to either boot the server with old disk (no OS install required) or new disk (OS Install required) since disk drives are not visible to BIOS.\n\r\nWe told customer to reset BIOS to factory settings and follow the IM for reconfigure BIOS again. In parallel, they will be checking A2B Installation CD in case disk drives are visible.\n\r\nIf OS Install will be required, GPS assistance will be provided.\n\r\nAgreed and dropped the call.','null'),(968,'Yunus Ozturk','AS-GW','2017-08-08','170808-642703','Canadian Imperial Bank of Commerce (CIBC)','Problem Description:\r\n=====================\n\r\nThe CIBC was unable to receive, leave or access any voicemails.  \n\r\nActions Taken:\r\n================\n\r\nOnce GENBAND logged into the CIBC IMM server IMM GPS team noticed that the IMM1 and IMM2 servers had the same service IP, which was causing a split brain scenario. IMM1 was running slow and very unresponsive to commands being entered. At this point the IMM GPS rebooted IMM1 guest VM from the command line which forced the activity to IMM2. Unfortunately at this point they had entered into two separate threads. IMM1 did not come up after the reboot and secondly the IMM2 server was not responding as it should have as users were able to get to the Voicemail but unfortunately receiving dead air after connection. \n\r\nIMM GPS tried one more attempt to reboot the IMM1 Guest server but this time through the IMM1 Host Server. However, IMM1 Host Server did not take react to the command. \n\r\nConcentration at that point was on IMM2s dead air issue, MAS GPS worked with the IMM GPS to diagnosis the trouble. During this time MAS GPS rebooted the MAS2 Guest Server that ended in the same results, dead air, something was corrupted in the MAS that prevented playing of the announcements or there was a communication problem between IMM2 and MAS2 that was blocking the MAS2 to play the announcements. MAS GPS collected and investigated the logs and noticed that \"The audio server play prefetch failed\" and \"error.badfetch, http://:8082/voicemail/vm\"errors were being generated for several wav files. That was sounding a communication problem between IMM2 and MAS2. Due these errors, wav files on IMM2 were re-mounted again onto MAS2 and NFS Service that is used to transfer these wav files from IMM to MAS was restarted. However, these actions did not recover the issue. In paralel, MAS GPS removed the cached wav files (4 wav files) under /var/mcp/ma/MAS/platdata/IvrMP/urlcache/pcmu/ directory on MAS2 as we were thinking that some files might have been corrupted for some reason on cache. After removing these wav files from the corresponding directory, customer performed some more test calls but they failed again. However, they also informed us that after some period of time, the VM calls partially started to work. \n\r\nWe will investigate if removing these cached wav files fixed the issue on MAS2 or there was another communication issue between IMM2 and MAS2 at that point as we were seeing several bad fetch errors between IMM2 and MAS2. This part will be tracked under the case (170810-643000).\n\r\nWhile the IMM and MAS GPS Teams were working on IMM2 and MAS2, OAM GPS team was paged out to check the IMM1 Host Server and see why IMM1 Guest Server did not come up after IMM1 Guest Server reboot. After reviewing the system and digging through the events OAM GPS performed IMM1 Host server reboot action. At this point the IMM1 Guest server communication issue is resolved and IMM GPS continued to work on IMM1 Guest Server to recover the services on it. After some time, IMM1 Application is recovered and IMM GPS took the activity to IMM1/MAS1 and in paralel they restarted the services on IMM2 and rebuild replication between IMM1 and IMM2.\n\r\nAfter all these actions, VM calls started to work successfully through both IMM1/MAS1 and IMM2/MAS2 pairs.','null'),(969,'Burak Biyik','AS-GW','2017-08-07','170807-642610','U.S. Air Force Falcon AFB','Falcon AFB contacted GENBAND ER to report that voicemail service to all users was failing. Some attempts to retrieve vmails would go to dead air, others would get a response saying the service was not available. There were two MAS servers (16.0.0.858) in the cluster. The application on the primary MAS unit was restarted to restore service. To make sure there were no residual issues on the secondary server it was restarted as well after the primary server was fully data synced (it took long time due to high number of mailbox). Data has been collected for RCA.','null'),(970,'Oktay ESGUL','AS-GW','2017-05-30','170529-633156','AXTEL','Dean Gilbert paged me out to report one of BCP blades can not recovered after reboot of the server.\n\r\nSystem Load: MCP 17.0.22.6\n\r\nThe initial problem definition of the case is choppy voice quality. GTS/ER had worked the issue before paging GPS.\n\r\nThere are 3 BCP clusters, with 9 servers.\n\r\nBased on their initial investigation, they recommended customer to reboot the BCP blades since they are UP over than 600 days.\n\r\nThey had started rebooting servers  with Blade2 (Cluster2) which recovered succesfully. yet when they rebooted Blade7  it did not .\n\r\nIn order to recover, a site technician had sent to the site. He had re-seated the blade , yet this did not help. \n\r\nI asked customer to share Management Module access.Once the MM info is shared, I have connected to MM and powered off/on the problematic Blade7 which recovered the Blade7.\n\r\nAfter server became up, I have performed kill/start  for the application and it became active without any issue.\n\r\nSince this choppy voice is not reported from all sites , customer did not want to continue with rebooting the rest of the 7 blades.\n\r\nCustomer will continue monitoring the site and contact with GTS/ER if any issue occurs.','null'),(971,'Kemal AYDEMIR (NETAS External)','AS-GW','2017-05-12','170512-631152','CIBC','Problem Description:\n\r\nGTS reported that users are open a bridge and when participants want to join, they get put on hold as if the meet-me bridge has not started.\n\r\nThe below alarm can be seen on the all meet-me MAS clusters.\n\r\nAlarmName: Database Overload \r\nTimeStamp: Fri May 12 11:09:44 EDT 2017 \r\nFaultNumber: 365 \r\nShortFamilyName: MAS \r\nLongFamilyName: MAS \r\nSeverity: CRITICAL \r\nProbableCause: application subsystem failure \r\nDescription: Database Overload;Component ID:SC \r\nCorrective Action: Check documentation for appropriate action on this fault number. The alarm will clear automatically when the error condition is resolved.\n\r\nMAS load :16.0.0.858\n\r\nActions Taken:\n\r\nI have connected to each MAS GUI and performed Pending Lock-Restart-Unlock operation and that action cleared all the alarms on the MAS instances and resolved the outage situation.\r\nCustomer performed their test successfully.\n\r\nFor the root cause investigation, I have requested logs via Log Capture tool on the MAS GUI.\r\nThen dropped from customer bridge.','null'),(972,'Kemal AYDEMIR (NETAS External)','AS-GW','2017-03-14','170310-622673','CIBC','We have taken permission from CIBC side and SWD started to work on the recovery plan.\r\nBut after SWD performed recovery plan, he reported that still see the alarms and platform rollback is failed.Then escalated this issue to GPS.\n\r\n-------\n\r\nI have logged in to the MAS VM and seen that Linux OS is successfully rolled back.\r\nBut MAS was still unable to start due to previously seen errors.\r\nAfter we have rolled back the Linux OS of IMM MAS HOST(from 18.0.11 to 17.0.22), we have recovered the outage for SCC site.(MAS is running on 17.0.25 MR(Mas Only) right now)\r\nCurrently, MAS is running stable at 846 load and there is no more alarms on the MAS GUI.\n\r\nFor the root cause, we are thinking that IMM MAS HOST is upgraded via Upgrade Wizard(MAS hosts should be excluded during upgrade) and kernel updates between 18.0.10 and 18.0.11 Linux OS is impacted MAS stability.\n\r\nIn order to prevent future issues in this site, I have recommended to align IMM MAS host/VM\'s with the below supported loads which is written on the 18.0.28 MR release notes.\n\r\nSCC:\n\r\nA2 Core = 18.0.28.1\r\nIMM load = Currently 3.9. Should be 4.0 (misaligned)\r\nIMM MAS HOST -Linux OS = 17.0.22. Should be 18.0.11 (misaligned)\r\nMAS VM -Linux OS =17.0.19.Should be 18.0.10 (misaligned)\r\nIMM MAS Load = 16.0.0.846 . Should be 16.0.0.857 (misaligned)','null'),(973,'Kemal AYDEMIR (NETAS External)','AS-GW','2017-03-13','170310-622673','CIBC','I have been paged by IMM GPS that calls are failing because the IMM MAS 3 is down.\n\r\nA2 Core : 18.0.28.1\r\nMAS load(platform&application) : 16.0.0.846\r\nIMM : 3.9\n\r\nWe saw that IMM MAS host OS and MAS VM OS are patched on March 8.\r\nWe have found that the IMM MAS 3 had MAS applications installed. IMM MAS should not have MAS applications.\r\nBased on the logs, MAS application on IMM MAS was installed on 9/27/2016. The reported failing calls in this case should have been failing all the way back to 9/27/2016.\n\r\nFor the recovery action we have recommended to perform below actions:\n\r\n1-Uninstall MAS application(preserve data=no)\r\n2-Uninstall MAS platform(preserve data=yes)\r\n3-Install MAS platform with installer 16.0.0.846\n\r\nWe are waiting for approval from CIBC side in order to start restore process.','null'),(974,'Yunus Ozturk','AS-GW','2017-03-10','170310-622608','Global Village Telecom (GVT)','Problem Description:\r\n=====================\n\r\nA2: MAS: MAS 4 fails to recover after reboot during s/w upgrade from 16.0.0.778 TO 16.0.0.857\n\r\nSWD called to advise that he was upgrading the MAS OS, the MAS Platform and the MAS APPs (4 blades / IBM BladeCenter) from 16.0.0.778 TO 16.0.0.857. Blades 1 - 3 were upgraded without issue but when he upgraded blade 4 the MAS APP upgraded with errors and the MAS GUI did not work so he uninstalled the MAS APP and rebooted the blade. The blade failed to recover after booting. Peter had site perform a hard reset on the blade but the blade still would not come up. Blade 4 could not be reached and ping failed. ER was able to get the remote KM connected to the console and it appears there was a fs issue in that we saw an error reporting that there was an error detected during fs check and booting halted.\n\r\nActions Taken:\r\n===============\n\r\nGPS was able to run the fsck command (fsck -A -y) to clear the fs error and allow the blade to boot up but further investigation showed that the fs and MAS install files are corrupted. GPS has stated that the OS needs to be reinstalled from scratch. Then they need to upload the MAS installers and install it from scratch as well to ensure that all corruption has been removed. GPS also notes that they need to have the license file on their hand for MAS4 and that the data can be synch-ed from the other adhoc server.','null'),(975,'Yunus Ozturk','AS-GW','2017-03-09','170308-622312','Nuvia','Problem Description:\r\n=====================\n\r\nNuvia Ops Team paged out GPS and requested assistance about a MoH file configuration for the Meetme Conference calls..\n\r\nCustomer informed us that tickets are starting to pile up from people reporting the problem and think that it should be of concern due to conference participants not knowing if their call is active or not while waiting on the conference chair person to open the bridge. \n\r\nActions Taken:\r\n===============\n\r\nSince this was not an outage, just a configuration issue, GPS rejected this pager call and asked the customer to raise a case..','null'),(976,'Burak Biyik','AS-GW','2017-02-03','170203-617306','Geisinger Health System','I was paged by GTS to report `invalid access code` complaints after dialing meetme bridge. \n\r\nCustomer has 4 MAS servers to form a cluster and one of them seemed to have a problem in its MySql database as one of the db files (ibdata1) fills \"/var/mcp\" directory up to 100%.\n\r\n##########################################################################\r\n[root@MAS2 ~]# cd /var/mcp/ma/MAS/platdata/MySQL/data \r\n[root@MAS2 data]# ls -la \r\ntotal 99794228 \r\ndrwxrwxr-x 10 root root 4096 Feb 2 17:22 . \r\ndrwxrwxr-x 4 root root 4096 Apr 9 2015 .. \r\ndrwx------ 2 root root 4096 Apr 9 2015 eam \r\ndrwx------ 2 root root 4096 Apr 9 2015 emapp \r\ndrwx------ 2 root root 4096 Feb 2 17:22 emarchive \r\ndrwx------ 2 root root 4096 Apr 9 2015 emcstore \r\ndrwx------ 2 root root 4096 Apr 9 2015 emlog \r\ndrwx------ 2 root root 20480 Feb 3 10:20 emplatcore \r\n-rw-rw---- 1 root root 102065262592 Feb 3 12:22 ibdata1 \r\n-rw-rw---- 1 root root 8388608 Feb 3 12:22 ib_logfile0 \r\n-rw-rw---- 1 root root 8388608 Feb 3 12:22 ib_logfile1 \r\ndrwx------ 2 root root 4096 Apr 9 2015 jasperserver \r\n-rw-rw---- 1 root root 7371413 Feb 2 17:22 MAS2.geisinger.edu.err \r\n-rw-rw---- 1 root root 5 Feb 2 17:22 MAS2.geisinger.edu.pid \r\ndrwxrwxr-x 2 root root 4096 Apr 9 2015 mysql \r\n###########################################################################\n\r\nWhen launching MAS Element Manager, the page was giving the following error despite jboss service was restarted several times: \n\r\n############################################################################\r\nYou have been denied access to the requested resource. \n\r\nThis may be because: \r\nYou do not have a valid Single Sign-On ID in your request, or \r\nThe account you used to log in is not granted the privileges to access the requested resource. \r\nClick the following link to go back to the main navigation page. You may be asked to log in again. \n\r\nhttps://10.20.6.167:8443/\r\n############################################################################ \n\r\nThe inability to launch MAS GUI as well as intermittent meetme failures (when calls hit this MAS) is likely to linked with size of this file.\n\r\nI made a quick search on the internet and found the following info for that file:\n\r\n----------------------------------------------------------------------------\r\nThe file ibdata1 is the system tablespace for the InnoDB infrastructure.\n\r\nIt contains several classes for information vital for InnoDB\n\r\nTable Data Pages\r\nTable Index Pages\r\nData Dictionary\r\nMVCC Control Data\r\nUndo Space\r\nRollback Segments\r\nDouble Write Buffer (Pages Written in the Background to avoid OS caching)\r\nInsert Buffer (Changes to Secondary Indexes)\r\n----------------------------------------------------------------------------\n\r\nI did not want to delete or try rebuilding this file since it is one of the key files for mysql db.\n\r\nWe were agreed to investigate this issue with a BC case by consulting Design Support.\n\r\nDropped the call.','null'),(977,'Burak Biyik','AS-GW','2017-01-31','170131-616746','Singtel Optus Pty Ltd','ER reported that BCP blades are bouncing continuously for the Optus site running 14.1.15.3. Further looking at the BCP logs, following exception was found:\n\r\njava.lang.UnsatisfiedLinkError: /opt/mcp/mediaportal/lib/libHalHeartBeater.so: libACE.so.6.1.0: cannot open shared object file: No such file or directory\r\n        at java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n        at java.lang.ClassLoader.loadLibrary0(Unknown Source)\r\n        at java.lang.ClassLoader.loadLibrary(Unknown Source)\r\n        at java.lang.Runtime.loadLibrary0(Unknown Source)\r\n        at java.lang.System.loadLibrary(Unknown Source)\n\r\nAfter talking to customer, it was found that the site was upgraded from 14.1.0.12 to 14.1.15.3 on June 2016. HT-Langley blades were excluded from the upgrade for some reason, whereas IBM blades were upgraded. (Customer has total of 14 BCP blades : 8 Langley + 6 IBM)\n\r\nWhen customer realized that HT-Langley BCPs runs 14.1.0.12, he deployed and started all of them with 14.1.15.3. Since old platform (14.1.9, ple3) does not have the latest library files, BCP instances could not activate themselves.\n\r\nWe rolled back BCP instances to old load and gave action plan to customer to patch HT-Langley servers first (14.1.15, ple3 )and then deploy the instances again.\n\r\nGPS will try to understand the history of this upgrade path.\n\r\nAgreed and dropped the call.','null'),(978,'Oktay Esgul','AS-GW','2017-01-27','170127-616143','Black Box Network Services','Robert Johnson paged me to report that customer tests failed even I have rebuilded the cluster conf.\n\r\nSummary: Customer upgraded the mas cluster and meetme started to fail when the both unit is active/\n\r\nUpgrade Path:\n\r\nFrom :16.0.0.8.43 \r\nTo : 16.0.0.54\n\n\r\nI have connected site and monitored the active calls at MAS2 .\n\r\nThen, collected traces for success scenerio.\n\r\nIn order to test and validate cluster conf is fine, activated the primary MAS as well.They reported calls are failing again.\n\r\nThen , I have restarted the both application simulteneously.\n\r\nCustomer performed several meetme tests after  both unit activated and informed that calls are passing .\n\r\nPotentially,this issue is seems triggered by missed steps bycustomer during mas upgrade.\n\r\nCurrently both units are active an system is stable .\n\r\nThanks','null'),(979,'Oktay Esgul','AS-GW','2017-01-27','170127-616143','Black Box Network Services','Kyle Mawst paged me to report that Black Box has meetme issues after they upgraded the MASes on their own instead of SWD.\r\nThey have ~100 meetme userS per cstore data at MAS.\n\r\nUpgrade Path:\n\r\nFrom :16.0.0.8.43 \r\nTo : 16.0.0.54\n\r\nThey have  two MAS servers in same clusters.\n\r\nFirst of all we reported all meetme scenerios are failing. \r\nI have take a look at the event logs and alarms at gui. The server was alarm free ,yet there are several license related warning like below/\n\r\nEvent Id: 5042	Severity: Warning	Origin: MAS02-1.yorku.ca\n\r\n                   Date & Time: (EDT)2017-01-26 19:05:56\r\n                   Class: Info\r\n                   Category: General\r\n                   Instance Count: 1\r\n                   Description:  \r\n                   Unable to license service. The service name was [meetme::vid], the async license\r\n                   request was for 1 licenses.\r\n                   Probable Cause: The service is not licensed.\r\n                   Corrective Action: License or redirect the affected request.\r\n                   Application Id: \r\n                   Custom Id: \r\n                   Document Reference Link:\n\r\nIn order to clear this warnings out, we have disabled video capability at MAS servers and restarted the application which cleared the warning.\n\r\nWith respect to failures, there are different scenerios for polycom,cell phone and deskphone  with different symptoms.\n\r\nThe symptoms:\n\r\nCell Phone can login to meetme with access code ,yet it dropped automatically.\r\nDesk Phone can not login to meetme with access code.\r\nPolycom can login,yet dropped automatically.\n\r\nI have compared the cstore data at MAS and prov access code and verified all good.\n\r\nThen, in order to collect mas debug logs I have stopped the secondary MAS instances. \n\r\nIn the meantime, we realized that when the MAS2 is stopped ,meetme is working fine for cell phone and polycom , (Deskphone option was eliminated as it is not one of customer use cases normally)\n\r\nAll access code entrance are accepted by primary MAS properly, which made me suspect that there might be a cluster conf issue between MAS1 and MAS2 that causing cstore sync issues.\n\r\nI recommended customer to rebuild the cluster conf,yet as they are working  for 15 hours they wanted to leave the site as the system is stable with only primary MAS.\n\r\nWithin today, I will rebuild the mas cluster configuration and keep the secondary down till the customer arrive to office to test again.\n\r\nThanks','null'),(980,'Kemal AYDEMIR (NETAS External)','AS-GW','2017-01-13','170110-613272','HKBN','Problem Description:\n\r\nI have been paged by GTS for the for the boot up issue for the MAS-Langley HT server.\n\r\nGTS has given an action about to reboot the server then it was hunging during the filesystem check of /var/mcp partition.\n\r\nChecking all file systems.\r\n[/sbin/fsck.ext3 (1) -- /] fsck.ext3 -a /dev/md4 \r\n/dev/md4: clean, 33780/524288 files, 268161/524096 blocks\r\n[/sbin/fsck.ext3 (1) -- /var] fsck.ext3 -a /dev/md8 \r\n/dev/md8: clean, 201/524288 files, 32329/524096 blocks\r\n[/sbin/fsck.ext3 (1) -- /var/mcp] fsck.ext3 -a /dev/md10\n\r\nActions taken:\n\r\n1)Given an action about to enable the quick boot/fast boot option on BIOS in order to skip the filesystem check.However customer could not find this option on the BIOS.\r\n2)Given an action about to plug-out/plug in the hard drives then power cycle one by one to see if there is a failure at hard drive.During that, also requested to prepare Mas installation CD/DVD (A2B0M170) if this action fails.\r\n3)sda disk is found to be faulty. Replacement action has been given to the customer for this disk.','null'),(981,'Kemal AYDEMIR (NETAS External)','AS-GW','2016-12-21','161219-611096','Eastlink','Problem Description:\n\r\nAfter host replacement,customer reported that when establishing a bridge(meetme) on MAS02(Primary MAS) and you are pointed to MAS05(secondary MAS) you are transitioned over to MAS02.But if you establish a bridge on Mas05 and you are pointed to MAS02 it does not transition to MAS05 it will stay on MAS02.\n\r\nCustomer also reported that he had rebooted the MAS servers but problem still persists.\n\r\nActions Taken:\n\r\nIf there is more than 15seconds delay between primary and secondary MAS,there can be Refer issues between MAS clusters.\n\r\n1)Connected to the site and checked the ntp configurations with Garrett.\r\nntpstat command was giving timeout on MAS02 server.\n\r\n[root@HLFX0MAS02 ~]# ntpstat\r\ntimeout\n\r\n2)Started the ntp config from stratch for MAS02 and MAS05.Seen that Host servers are configured as clock source but it was not.As a result,we have seen sync problems between MAS clusters.Then, we have changed the NTP source for both MAS servers and restarted ntpd service.\n\r\n3)Customer made the test calls about problematic scenario and all are passed succesfully.','null'),(982,'Kemal AYDEMIR (NETAS External)','AS-GW','2016-11-23','161123-607771','Tbaytel','Problem Description:\n\r\nSWD was making MAS upgrade to 16.0.0.854 following the IM NN10324-510 04.18 \n\r\nCustomer stated they were unsure if they had custom announcements.\r\nStated need clarity in identifying any specific files to to restore as the document seems to suggest. \n\n\r\nActions Taken:\n\r\nAltought the IM document indicates the below statement we have been paged about this.\r\n********\r\n\'If you have the AnnouncementsCustom directory, it means that \r\nyou might be using custom announcement files. If this is the \r\ncase, ensure that you have a copy of the custom announcement \r\nfiles you wish to restore on your Local PC or on a Storage server.\' \r\n********\n\r\nRecommended to use all the custom Announcement directory for backup which is under /var/mcp/ma/MAS/platdata and continue with the restore process as indicated on the IM.','null'),(983,'Yunus Ozturk','AS-GW','2016-11-17','161117-607032','Tbaytel','Problem Description:\r\n====================\n\r\nWhile upgrading the BMC Firmware on BCP2 Blade, the blade stayed at stuck status and switched to Kernel Panic Mode. This issue caused the blade to lose its network connection. \n\r\nSolution:\r\n=========\n\r\nAs per the logs generated on the IBM BCT GUI, we noticed a hardware related problem happened on BMC module (baseboard) of the blade  \n\r\n25   I    SERVPROC  11/17/16, 05:17:24 TAM MNR alert for Event (ID = 0x00216000) Problem communicating with BMC. \r\n26   W    BLADE_02  11/17/16, 05:17:24 () Problem communicating with BMC.  \r\n27   W    SERVPROC  11/17/16, 05:17:24 BMC in firmware mode - Self Test Results for slot = 2: BMC operational code corrupted \n\r\nSince this was a hardware problem, we have asked the customer to plug-out this blade and plug-in the previous one. After this action, we have seen that the previous blade was Ok but the OS running on the blade was crashed and requires OS re-installation. \n\r\nWe have asked the customer to insert the installation CD into the Media Tray and we have re-installed the previous blade from scratch. Since most of the firmwares were already up to date on the previous blade, SWD has just upgraded the NIC Firmware of this blade.\n\r\nAfter applying this missing firmware, SWD was able to pass the upgrade wizard screen.','null'),(984,'Oktay Esgul','AS-GW','2016-11-03','161102-605053','Oneconnect','Johg Kishner paged A2 GW pager to report IPCM2 devices are disconnected after IPCM restart.\n\r\nMCP : 17.0.22.1\n\r\nThe main issue reported from customer was activation key list empty at IPCM cluster in PROV. So, in order to recover this issue GTS  had recommended them to\r\nrestart IPCM and PROV in a MW. When they restarted the IPCM2 , all devices connected to this instance had become disconnected and no chance to recover.\n\r\nI have connected site and try ti register user from MCP gui , it did not help.\n\r\nThen, research the MAC address of some users in PROV and validate that mac adresses are associated with a user in the system, all seemm goods.\n\r\nThe only thing, activation code list which is defined for each domain at IPCM cluster portlet is empty. Then, in order to double checked I have connected \r\ncustomer DB after performing several tests to find out corresponding database table for activation keys.\n\r\nPer investigation, figured out that ESMDOMAINMAP table is being used to store activation keys of ipcm clusters. Then, connected customer DB and validated that somehow\r\ntheir ESMDOMAINMAP is empty .\n\n\r\n          SQL> select * from esmdomainmap;\n\r\n               no rows selected\n\r\n          SQL> \n\r\nAfterward,I have researched DB server and found out ftpBackup directory where customer`s daily backups are located. In order to ensure before restoring the DB,\n\r\nI have copied the latest 3 days backups and transfer them to my local pc.Then, restored them one by one to our DB test lab and checked the esmdomainmap table.\n\r\nThe latest backup which has all data in was from 31th October.\n\r\nEr had started a bridge and I explained customer  the action plan to get their approval.\n\r\nIn the meantime, Ken has collected db related logs to use for RCA investigation.\n\r\nAfter getting approval, I started with getting a new db backup (It failed and we spent a lot time on this issue, then dbora stop/start solved the issue)\r\nThen, dropped the replication between primary and secondary database.\r\nThen, restored the Monday backup to primary db and validated the corresponding data is restored.\r\nThen, set the replication between primary/secondary db again.\n\r\nOnce above actions completed, we have restarted IPCM and PROV managers  and then all client became connected status.\r\nEnd of outage.\n\r\nThank you','null'),(985,'Burak Biyik','AS-GW','2016-10-02','TBD','NUVIA NA Collab','Phil reported that users can not login or make any calls in NUVIA Collab system (TAG_VC3_4_2_077) after re-IP activity.\n\r\nThe activity covers private (10.156.X.X) to public (192.169.X.X) IP conversion for all Collab Routers and Gateways to avoid NAT and access from the internal GB Network so that they will be reached only via the external side of the NUVIA datacenter.\n\r\nIn addition to IP re-assignment, domain names of the servers are changed from \"genband.com\" to \"nuviacloud.com\" since \"genband.com\" is controlled by GENBAND IT whereas \"nuviacloud.com\" can be managed locally by NUVIA Operations team.\n\r\nExample FQDN change:  collabsrv06-ucc.genband.com    ---> collabsrv06-ucc.nuvicloud.com\n\r\nAfter further checking, we realized that system license was still using old Portal FQDN \"collabprtcl-ucc.genband.com\" so that Portal could not allocate a resource for incoming request from \"nuviacloud.com\".\n\r\nWe reached Vidyo Support team and we were told that retrieval of new license might take up to 2 days.\n\r\nThen, we reverted all domain names back to \"genband.com\" and the FQDN change will be applied as soon as the new license is provided by Vidyo.\n\r\nAgreed and dropped the call.','null'),(986,'Oktay Esgul','AS-GW','2016-08-08','160808-592665','GENBAND','We have paged by Gerry to report smart office  outage due to unexpected portal reboots.\n\r\nNormally,collaboration servers works as active/hotstandby,even the both of units were active/hotstandby, after 240 seconds hotstandby unit reboots itself automatically. \n\r\nWe tried to investigate platform side yet due to insufficient logging we could not proceed well (Logging mechanism is really insufficient for investigation) \n\r\nAs a next item, we tried to enable system audit logs from portal gui, yet it could not be succeeded as the servers reboots itself before log collection completed.\n\r\nThen ,tried to reboot the secondary hot standby unit ,it did not help neither.\n\r\nIn paralell to our investigation, Phil paged the vidyo support to involve. Once they joint the bridge, vidyo engineer focused on the ssl certificates, and he updated existing certificates with new ones which did not help neither.\n\r\nBtw, I have observed that physical host servers (Red-hat KVM) were up almost 300 days which is pretty high and recommended to host server reboot after ssl operation completed.\n\r\nIn the meantime, vidyo engineer  had asked next level of vidyo support team to involve , and they figured out that  the vidyo proxy is enabled somehow with same ip adress of portal which causes ip conflicts.This ip conflicts blocks the portals to be sync and get floating ip successfully.Then, we disabled vidyo proxy  from portal gui and this solved the issue.\n\r\nFor rca analysis  Phil is gonna reboot the secondary hot standby portal to see if the proxy becames enabled after reboot.Then we ll continue investigation.','null'),(987,'Oktay Esgul','AS-GW','2016-04-01','160331-574983','Hargray Telephone','Gary  Norwood from ER paged me out to report that one of BCP Blade went down during ERS8600 maintenance activity.\n\r\nI have connected site and  took a look at the system.BCP cluster:1+1\n\r\nBCP server was up and running yet the applciation was stuck at configured status.So that tried to restart ned and deploy again , it failed.\n\r\nThen,kill the application at server then tried again, it failed.\n\r\nAs a next action, I asked customer to swact SM and attempt to deploy again,we swacted sm to host stand by instance and they deployed the BCP bladed without any issue. \n\r\nBCP instance became active and all alarms gone.\n\r\nAs the customer is running on A2_8.0 BRC, we aggreed not to provide RCA , then dropped the call.','null'),(988,'Yunus Ozturk','AS-GW','2016-03-26','160325-574290','Genband Nuvia','Problem Description:\r\n=====================\n\r\nUpgrade Path : MAS_16.0.0.840 to MAS_16.0.0.849\n\r\nUninstalling the MAS platform generated the following error;\n\r\nRemoval of the GENBAND Media Application Server Platform has failed. Found \r\nrunning app. \n\r\nAll installed applications must first be removed. \n\r\nYour system has not been modified. \n\r\nPRESS  TO CONTINUE: \n\r\nActions Taken:\r\n===============\n\r\n- Even though all the MAS Applications uninstalled successfully on the previous step, the MAS Platform could not uninstalled and it was still detecting some installed applications and preventing the MAS Platform uninstallation\n\r\n- Checked the /etc/mas.properties/ file on MAS Server and noticed that MAS Meetme Application (meetme_app=16.0.0.840) could not be removed from this file for some reason\n\r\n- Manually edited this file and removed the part (meetme_app=16.0.0.840)\n\r\n- After editing this file and removing the stuck application data from the /etc/mas.properties/ file, MAS Platform uninstallation completed successfully.','null'),(989,'Burak Biyik','AS-GW','2016-03-20','TBD','NUVIA','I was paged by SWD team to take a look at meetme outage raised following MAS Upgrade (16.0.0.840 to 16.0.0.849). We created a yahoo IM bridge including Phil Karam since he performed the upgrade.\n\r\nThere are two MAS servers (pri + sec) in the cluster. MAS2 was upgraded first (after its platform was patched) and everything was working fine with only MAS2 (MAS1 had been stopped to ensure that the calls were hitting MAS2). Then, the weight of MAS1 was set to 0 for routing all calls to MAS2 till the upgrade was completed for MAS1. After completing the upgrade of MAS1, the weights were modified again to allow the traffic go to both MAS. However, after unlocking MAS1 following to upgrade, MAS was playing \"You will now be placed into conference\" and then \"The conference is now ending\" upon joining a conference and dropping you.\n\r\nI connected to MAS serves through Kloud Screen Connect and we performed few tests calls with Phil by joining different bridges, yet all calls were successful. There was nothing in the event logs indicating conference placement failures other than expected upgrade-related entries. It was hard to find out what could cause this inability for a short time (10 min). After monitoring the system for a period, we were agreed to end the call since the cluster started to give service with two redundant nodes.','null'),(990,'Oktay Esgul','AS-GW','2016-02-25','160225-570039','NetFortris','Eric reported that IA-RMS host server where IMM-MAS-FAX virtual servers are located has lost all interface configuration after patching server platform.\n\r\nFrom Patch: Ple4 17.0.17\r\nTo Patch :  Ple4 18.0.6\n\r\nEven the primary host server patched succesfully without any issue, secondary one did not boot up after rebooted during patching progress.\n\r\nWe tried to connect terminal server for a while and then connected to server.I have checked and verified that all network interfaces/bonds configuration are somehow lost .\n\r\nAs the host server platform upgrade is an optional operation during MAS upgrade and not mandatory for applications, customer insisted to solve the issue asap to be able to complete IMM MAS upgrade in maintenance window that is why I have rollback platform to previos level and all network configuration restored and server became reacbable again.\n\r\nAfterward, Eric had continued the upgrades of IMMMas server platform and applciation and completed in MW without any issue.\n\r\nThank you','null'),(991,'Oktay Esgul','AS-GW','2016-02-25','160208-567408','NetFortris','Eric paged me in order to clarify what MAS load should be used for Netfortris MAS upgrade. He told that he is little bit confused due to even the recommended load is 16.0.0.846, the latest available load 16.0.0.849.\n\r\nI  adviced him to upgrade to latest available loads since it will cover required fix and all new fixes, then we dropped the call.','null'),(992,'Kemal AYDEMIR (NETAS External)','AS-GW','2016-02-04','160204-566900','Ziggo','Problem Description :\r\n=====================\n\r\nI have been paged by Guido Zijlstra due to MAS Platform installer fails after the execution on the MAS server during the MAS upgrade.\r\nMAS servers was preparing to upgrade from MR 17.0.19 to MR 17.0.25.( from 16.0.0.814 to 16.0.0.846)\r\nCustomer has two MAS servers providing announcement service only and first ones (MAS03) upgrade was completed successfully.\r\nGuido informed me about the problematic one(MAS02) which had a sudden issue with the disk during the MAS Platform installer. /var/mcp filesystem was read-only at that moment when SWD executing MAS platform installer\r\nThe installer did the first part ok and then suddenly had some error pointing at the disk and the installer stopped. Now it thinks it is already installed, but cannot uninstall it, as the /var/mcp/admin/ partition is not exists.\n\r\nActions Taken :\r\n==============\n\r\nWe do not have  VPN for the connection to the system so I have requested Teamviewer information of SWD engineer.\r\nConnection to the MAS server was available on BladeCenter-T remote console.\r\nWe have looked into etc/mas.properties file on the MAS server in order to check currently load of MAS\r\nThe output was showing the new load but we all know that installation was unsuccessful.\n\r\nmas=16.0.0.846\r\ninst_dir=/var/mcp\r\ninstalled=yes\r\nmysql=/var/mcp/ma/MAS/platdata/MySQL/\n\r\nWe have deleted this file and then executed MAS Platform installation once again. Installation was completed successfully but MAS platform data was not visible from EM GUI although data was preserved during uninstallation. This showed that since the initial installation was interrupted due to corrupted file system, somehow platform data was not restored. \n\r\nThe action plan for MAS recovery is provided below;\n\r\n1.	OS installation from scratch (to fix file system)\r\n2.	Install old MAS release and restore previously taken backups.\r\n3.	Upgrade to MAS to new release by following official procedure.\n\r\nThe required IM and steps were provided to SWD and this activity will be tracked with a case as agreed.','null'),(993,'Tugkan INCE','AS-GW','2015-12-17','151217-560875','NY Presbyterian Hospital','I was paged by Antti Kahkonen from Software Delivery due to an issue observed during MAS upgrade operation in NY Presbyterian Hospital site.\n\r\nCustomer has 4 MAS servers having them in 2 clusters with 1+1 configuration. 3 of the MAS servers were upgraded successfully. However secondary server of the secondary cluster was failing on platform patch operation.\n\r\nPlatform patch script was returning failure on raid check operation. According to platform patch logs, secondary disk was faulty.\n\r\nWhen I engaged the problem, I first suggested rebooting the server. When the server came up we have tried executing platform patch again.\n\r\nAfter receiving the same failure, I have tried checking raid configuration via script \"mcpSwRaid.pl -status\" verifying that some of the devices are not accessible over the operating system. I have tried executing the final command to verify which disk was faulty.\n\r\nAfter executing mcpSwRaidCheck.pl we have analysed secondary disk was faulty on the server. We have suggested customer replacing the secondary disk if they had any spares, yet unfortunately they did not.\n\r\nSo we have stopped secondary MAS of the cluster #2 to prevent any mismatches within the cluster and told customer to order new disks and replace their secondary disk in the server.\n\r\nWe agreed with Antti to end the pager call','null'),(994,'Oktay Esgul','AS-GW','2015-12-13','151210-559925','PALTEL','Thomas Combs from ER paged me in order to report there is voice noise issues due to high network traffic usage due to undesired DNS trafic generated \r\nby 4 of 7 MASes in PALTEL network. The case was opened on thursday 10.12.2015. There was a conf call and I joint as well.\n\r\nEven there is Q20 (SBC) in the system, MAS servers are on public network  which may cause security vulnerabilities( Network schema : MAS --> ERS8600 -->Fortinet Firewall).\n\r\nBtw, there was not any voice quality issue at SIP to SIP calls, the problem is observed at SIP to ISUP calls.\n\r\nI checked MAS EM  Gui monitoring portlet to see active calls and stats seems normal.There were just 10 active calls which proved \r\nthat this is not a high service usage problem.\n\r\nAs a first preventative action,we  shut down problematic MAS servers after collecting tcpdump logs in order to \r\nclarify network trafic from the MAS Servers and avoid service outage due to network bandwidth lack.\n\r\nWhen I checked the traces , observed below continues google dns queries from problematic MAS which  are undesired.\n\r\n34594 75.73797100082.213.24.73 -> 8.8.8.8 DNS 78 Standard query A gg.m0662.com\r\n35246 77.165953000 82.213.24.73 -> 8.8.8.8 DNS 72 Standard query A gg.m0662.com\r\n171405 375.453021000 82.213.24.73 -> 8.8.8.8 DNS 78 Standard query A gg.m0662.com\r\n171902 376.811490000 82.213.24.73 -> 8.8.8.8 DNS 72 Standard query A gg.m0662.com\n\r\n34517 75.579456000 82.213.24.73 -> 8.8.8.8 DNS 72 Standard query A gg.m0662.com\r\n34528 75.593474000 82.213.24.73 -> 8.8.8.8 DNS 78 Standard query A gg.m0662.com\r\n171494 375.692259000 82.213.24.73 -> 8.8.8.8 DNS 72 Standard query A gg.m0662.com\r\n171700 376.179124000 82.213.24.73 -> 8.8.8.8 DNS 78 Standard query A gg.m0662.com \n\r\nIn order to compare problematic and non-problematic MAS servers ,just checked /etc/hosts and /etc/resolv.conf file in both of servers and they were quite similar\r\nexcept hostableConfig.pl script statement , this is one of suspicios point but not seem exact problem.\n\r\nMAS1 , NO Problem.\r\n[root@rmlamas02 ~]# cat /etc/hosts\r\n127.0.0.1 localhost.localdomain localhost\r\n82.213.24.72 rmlamas01.ramallahcs2k.palte.ps rmlamas01 rmlamas01\n\r\nMAS2  Problematic one\n\r\n[root@rmlamas02 ~]# cat /etc/hosts\r\n127.0.0.1 localhost.localdomain localhost\r\n82.213.24.73 rmlamas02\r\n82.213.24.73 rmlamas02 rmlamas02 # Do Not Modify; managed by script hostTableConfig.pl\n\r\nIn order to proceed ,I have checked all MAS documents to find out if there is any specific dns setup may cause the issue,yet could not see any configuration \r\nthat may be a potential issue.\n\r\nIn paralell to document research,tried to find similar cases in salesforces and fortunately we found out the exact same problem with same symptoms reported by\r\nAXTEL (150619-536066).\r\n-----------------------------------------------------------------------------------------\r\nHere is the case description and solution from the similar case:\r\n-----------------------------------------------------------------------------------------\n\n\r\n\"Axtel contacted GENBAND ER to report bad voice quality on 100% of their SIP lines in the Monterrey EXPERiUS.\r\nIncoming and outgoing calls experienced the same bad quality. Axtel reported the voice quality as sounding \'robotic\'. \r\nThe cause was tracked down to multiple IPs coming into the Monterrey site from unknown sources outside the Monterrey site. \r\nAxtel created an access group in the Alteon Firewall (NAS) and then blocked that list of IPs. \r\nThey also created and access list of allowed IPs through the NAS. After those changes were made in the NAS all calls were clear. \r\n100% of calls were affected for 16 hours and 39 minutes.\n\r\nConfirmed Cause	\n\r\nThe cause was tracked down to multiple IPs coming into the Monterrey site from unknown sources outside the Monterrey site.\n\r\nSolution Summary	\n\r\nInvestigation concludes that observed traffic from MAS is induced from unexpected traffic sources. \r\nSince MAS IPs are public, they are more vulnerable to Internet attacks.\r\nSeveral recommendations were delivered to customer to improve network security and avoid external traffic to reach MAS blades.\"\r\n-------------------------------------------------------------------------------------------\r\n-------------------------------------------------------------------------------------------\r\nAs per above case , we informed customer that there may be a network attack and this has to be handled by firewall and the unknown/undesired IP adresses (for example \r\n188.138.33.79 adress is one of the suspected IP from Germany ) should be blocked to access MAS servers.\n\r\nIn addition , I have found a security bulletin that supports our theory regarding network attack (Distributed Denial-of-Service (DDoS) attack Notice)\n\r\nFor preventative action, customer network team decide to block all trafic to google DNS (8.8.8.8) IP and decided to keep monitoring till tomorrow to see the results.\r\nIn order to avoid further voice problem due to high network usage by hacked MAS servers, we shut down all problematic MASes and left 3 MAS active which we do not observed any\r\nproblem so far.\n\r\nAcording to monitoring results till tomorrow, we will activate the problematic MAS server after ensuring dns attack is totally blocked.Otherwise we may need\r\nERS team support to find out an internal solution in network.\n\r\nThank you','null'),(995,'Kemal AYDEMIR (NETAS External)','AS-GW','2015-11-05','151028-553660','Hargray Communications Inc.','I have been paged by David Berry due to all MAS instances has been crashed.\n\n\r\nProblem Description :\r\n=====================\n\r\nWhile system was stable, Both MAS were working on 14.0.0.284 load. Ken Johnson was already involved the issue from scratch. He informed me about all MAS instances were unavailable state. In order to recover MAS instances, he had found some solutions about Baud Rate Speed to make serial port connection to MAS server. \n\r\nBios was configured for 9600, install cd was defined 19200 Baud Rate Speed. He had informed me about install-kvm screen have not appeared with 9600 baud rate. So we tried 19200 as install cd defined but it had stucked after executed install-kvm command as below.\n\r\nLoading vmlinuz.................................\r\nLoading initrd.img..............................\r\nReady.\n\n\r\nActions Taken :\r\n==============\n\r\nThis forced us to performing re-installation using KVM is the only option for recovery operation. In order to perform our action plan, we have mentioned to ER that we need to know if customer has custom announcements for MAS. ER contacted with customer and informed us they have no custom announcements.\n\r\nWe continued Linux installation on one of MAS instance with onsite engineer. Then moved forward with set up MAS platform and application in order to reduce outage level from E1 to E2 condition. The customer seems using MAS just for announcement service without any custom file. I have connected PROV GUI and checked all settings and logical entities, could not see any particular configuration. Reported to ER that customer can make test calls. Customer performed some basic test calls and since verified system is stable, E2 condition is started.\n\r\nI have performed the same processes to other MAS and recovered. Customer confirmed that test calls have materialized successfully.\n\r\nWe agreed with ER and dropped the call.','null'),(996,'Burak Biyik','AS-GW','2015-08-18','150818-544006','Geisinger Health System','Nate Lewis from TS NA team paged me to report a meetme failure from Geisinger.He reported that newly opened BC case has high visibility as the Geisinger CEO experienced the issue first hand.\n\r\nProblem Description: Meetme conference calls allowing 2 chairpersons to exist in the same bridge even if one of them does not even press 1 to start the conference.The participants hear \"your conference will end in 1 minute\" in the middle of the conference when one of the chairperson left the conference (depending on PA setting).\n\r\nCustomer load: MAS 16_0.0.840\n\r\nI quickly searched similar cases from salesforce since Geisinger had experinced similar issues before. The identical issue was experienced in another customer (140603-475996) and resolved by providing a fix with MAS 16_0.0.828 load which is below customer release.\n\r\nGeisinger had reported the same failure before (131108-440308) and the issue was resolved by tuning MAS engineering parms;however they are now all set to recommended values.\n\r\nThere was no such quick action to be taken in a pager call.\n\r\nThe failing scenario is available in case comment with associated MAS debug logs.I explained customer that this needs further investigation by analyzing attached logs deeply and MAS Design involvement.\n\r\nThey were agreed to have GPS on top of the issue as a first action tomorrow and we dropped the call.','null'),(997,'Burak Biyik','AS-GW','2015-08-05','150805-542332 ','BSkyB','Garry Dean called GW pager to report intermittent meetme failures on SKY GEO site which was running 8.0 BRC (EOL) Relase with MAS load 14.0.0.284\n\r\nThe customer has two MAS clusters whose locations are physically separated. The Basingstoke Cluster is default ACTIVE cluster whereas Reading MAS cluster is COLD-STANDBY and should be ready for taking the calls anytime in case of primary site failure.To keep cold-standby MAS instances up-to-date(with latest access code/chair pin data), application data should be taken from LIVE site and restored to BACKUP site periodically.\n\n\r\nCluster1 (LIVE)\r\n--------\r\nLocation: Basingstoke, UK (BNGSENEA01S)\n\r\nPrimary    (10.244.122.38) MAS02\r\nSecondary  (10.244.122.39) MAS03\r\nStandard   (10.244.122.40) MAS04\n\n\r\nCluster2 (BACKUP)\r\n----------\r\nLocation: Reading, UK (RDNGENEA01S) \n\r\nPrimary    (10.244.122.4) MAS05\r\nSecondary  (10.244.122.5) MAS06\r\nStandard   (10.244.122.6) MAS07\n\n\r\nGPS was asked to join existing bridge with customer to discuss the current situation, recent changes on the system and the action plan to resolve the issue.\n\r\nThe same site had suffered from MAS cluster issue a day ago. The problem was having some of the MAS instances down/unreachable and the calls are recovered after power cycling down MAS servers. The details of this incident can be found in 150804-542178.\n\r\nThe latest meetme failures seemed to result from different cause after performing some checks on site. When I navigate to Cluster Configuration -> Server Designation in MAS02 (primary), the current cluster setup was showing MAS05 (10.244.122.4) as secondary MAS of the LIVE cluster even though MAS05 is actual primary MAS server of BACKUP cluster.\n\r\nThen, we suspected possible backup&restore activity.From Tools -> Backup and Restore -> History Log, we were able to see that full backup task (full backup = application content + system configuration) was created for MAS03 on 4th August:\n\n\r\nTask Name               Task Type               Status                   Date(GMT)                         Elapsed Time\r\n-----------------      ------------            ---------          ---------------------                    -------------\r\nSys_and_app_backup         Backup               completed          2015-08-04 16:11:31.0                      00:00:29\n\r\nAnd this backup seemed to be restored on MAS05 as below:\n\r\nTask Name                  Task Type                   Status                   Date(GMT)                          Elapsed Time\r\n------------              ------------              -----------         ----------------------                  --------------\r\nFullBackup           Restore System Configuration    completed          2015-08-04 16:42:22.0                     00:00:09\r\nSys_and_app_backup   Restore System Configuration    completed          2015-08-04 16:18:19.0                     00:00:09\r\nSys_and_app_backup   Restore Application Content     completed          2015-08-04 16:15:04.0                     00:00:02\n\n\r\nEvent logs on MAS05 were proving this activity too:\n\r\n\"\"2015-08-04 16:18:19 BackupRestore I 14800 RestoreTask: Data Restore Successful\"\" {MAS03 config data restored to MAS05}\n\r\nSo, full backup was taken from Secondary MAS of the LIVE cluster and restored to primary MAS of the BACKUP cluster. Since full backup includes system configuration (copies cluster member configuration (IP,UUID, etc.)), restoring secondary MAS full backup to primary one changed cluster configuration of primary MAS. After this operation, MAS02 was seeing MAS05 as secondary of LIVE cluster and then we were seeing following alarms on MAS02 (currently lost its secondary member due to wrong restoration) \n\r\n2015-08-04 16:42:49 CStore I 14600 Alarm Activated: Mirror Messaging Connection Unavailable \n\r\nThe current situation were causing meetme calls not to REFER the MAS server hosting chair person due to wrong cluster information.\n\r\nThe following actions were taken to correct the issue:\n\r\n- Until the cluster configuration is corrected, all meetme calls were routed only to primary MAS (from Logical Entity portlet) not to cause any REFER activity\r\n- Navigated to Cluster Configuration -> Server Designation on MAS02 and cluster table was updated with correct IPs (MAS restart was needed for changes to take effect)\r\n- Customer restarted MAS02 during low traffic time, which updated cluster information on the other members of the LIVE cluster too.\r\n- Customer reported no issues after cluster IP correction.\r\n- Agreed to update related documents with backup&restoration procedure for GEO MAS (GPS is already working on a bulletin about this) to prevent next issues.\n\r\nAgreed with this action plan and dropped the call.','null'),(998,'Senem Gultekin','AS-GW','2015-08-02','150727-541162','Axtel','Problem Description:\r\nER paged me out for a BCP issue seen at Axtel site. They had 4 BCPs and 2 of them were not taking calls even though they were up and running. Issue is related to and existing BC case (150727-541162)\r\nCustomer is running on 17.0.22.6 Release.\n\r\nSolution:\r\n-	Accessed to the site over ERs VMServer.\r\n-	BCP3 and BCP4 are active and online right now, however they are not taking any call.  The table is empty when we run mptool -l on the servers. \r\n-	BCP3 was reinstalled on Friday. Seems like BCPs have not taken proper cluster configuration. \r\n-	Ive killed BCP3 and BCP4 and started at the same time, but still the cluster was not recovered. There are also Cluster alarms on BCPs. RTPB 804  Critical  RTP Media Protal Configuration/Initialization Error\r\n-	To bring back in cluster all BCPs need to be killed and started. However Axtel didnt want to apply them during the pager call. They will perform it in the next maint window.\r\nAction: \r\n1- Login to MCP GUI. \r\n2- Open BCP1, BCP2, BCP3 and BCP4 from \r\nNetwork Elements -> Media Protals -> RTP Portals -> NE Maintenance \r\n3- When you open 4 of the NE maintanence windows, hit kill on all of them. \r\n4- Once they are all offline. Start them at the same time. \n\r\nThis procedure needs to be performed in maint window. It will have an impact to the ongoing traffic. Customer will apply it.','null'),(999,'Scott Roland','AS-GW','2015-07-21','150721-540298','Alteva (f.k.a. Warwick Valley)','Was paged (walk to cube) by GTS management to help recover the BCP issue for Warwick. I helped resolve BCP issue last week and requested assistance with this case.\n\r\nThe symptoms of this weeks issue were different. There were no integrity events, however customer experienced no speech path issues.\n\r\nFollowing recovery method from previous week, customer attempted to troubleshoot the BCP Blades. Blade 1 was OFFL, due to previous case and being RMA\'d. Blade 3 showed a memory error. Customer took a spare blade and moved Blade 3 hard drive to spare blade and put spare blade into the BCT. Blade 3 appeared to recover from BCT Module Management perspective, however speech path issue was not resolved.\n\r\nDue to no VPN access, connected to customer\'s PC via Bomgar. Checked the BCT Module Manager and attempted to connect to Blade 2, Blade 3 and Blade 4 via remote console.\n\r\nFound that Blade 2 was not reachable via console and got odd behavior from Blade 3 and 4 as well. Decided to OFFL, Un-Deploy, Deploy and Start Blade 2, Blade 3 and Blade 4.\n\r\nConfirmed that Active BCP Blades had active connections via command \"mptool -l\".\n\r\nCustomer indicated that no speech problem persisted. Collected DPT GWC Tapi of a failed call. Found that call was engaging and using BCP resources correctly, however End of Call Stats indicated that no packets were Sent from IW-SPM to BCP but packets were being sent from far end to BCP to IW-SPM.\n\r\nIW-SPM experts in RTP advised to PROT switch the GEMS in the IW-SPM. Once executed, calls again had 2way speech.\n\r\nBelieve problem initially was GEM IW-SPM issue, however due to customer\'s actions, BCP Blades had to be recovered before IW-SPM issue could be identified.\n\r\nScott Roland\r\nA2/GW Genband Product Support','null'),(1000,'Scott Roland','AS-GW','2015-07-13','150713-539181','Alteva (f.k.a. Warwick Valley)','I was paged by ER because GWC GPS noticed that DPL and DPT GWC were not communicating with the BCP Media Portals. Assisted GWC GPS in troubleshooting the BCP Blades from the BCT Manager Module GUI. It appears that blade 1, 3 and 4 were down. Attempted to power cycle Blade 3 which did not change it\'s status. Had site unseat Blade 3. Attempted to power cycle Blade 4 which also did not change it\'s status. Had site unseat Blade 4. Attempted to power cycle Blade 1 which did not change its status. Could not connect to the Remote Control Console due to Java issues, but Customer was able to connect a monitor and keyboard to the KVM on the Management Module. We were able to assign the KVM manually to the individual blades. We had site insert Blade 1 and found that nothing was on the console. Asked them to unseat Blade 1 again. Then insert Blade 3, Blade 3 was now recognizable from the Management Module and we were able to Power on Blade 3. Confirmed in the console that Blade 3 did get to a login prompt. Inserted Blade 4, and it came up same as Blade 3. With Blade 3 and Blade 4 working, GWC GPS confirmed that RSIP MCP Packets were coming into the GWCs from the BCPs. Test calls were made core was no longer producing Xpacket logs. However, GWC alarm of Media Portal Unresponsive was still present.\n\r\nCustomer agreed outage was over and agreed that Blade 1 was bad and needs to be RMA\'d.\n\r\nAttempted to gain access to the CMT GUI to modify the BCP Portal configuration to remove Blade 1 but failed due to Java versions.\n\r\nCustomer agreed alarms are present due to missing Blade 1 but agreed outage was over due to Xpacket logs stopped and calls were successful on the DPL and DPT GWC.\n\r\nFound the RMA process and provided to ER to provide to the customer.','null'),(1001,'Nuri INCE','AS-GW','2015-06-23','150623-536362','Suddenlink (Cebridge Connections Inc)','I was paged by Keith Marshall from Software Delivery team due to a failure received during patching the linux platform of MAS4 Server of Suddenlink as part of the MAS16 Upgrade activity.\n\r\nIn the error message, it was reported that the platform patching could not be completed unless the software raid between the disks is up and disks are synchronized. We have checked the software raid and verified that the raid was not up for secondary disk sdb with the following command:\n\r\n#mcpSwRaid.pl -status\n\r\nThe command output was reporting that sdb was not even recognized as a device in operating system level. Hoping that it might solve the problem, I have rebooted the server with customer approval to at least check if the operating system can recognize sdb as a device or not. \n\r\nServer failed to boot up after the reboot, so we requested assistance of customer to perform hard reboot on the problematic server.\n\r\nMeanwhile, in case of any necessity of re-installation on the hardware -using the backups-, customer has been asked to prepare installation media, KVM or console and spare disks.\n\r\nCustomer provided the terminal console connection in around 40 minutes, in the console output we have seen that the server got stuck in a boot loop. So we asked customer to hard reboot the server. After the hard reboot, server booted up successfully. \n\r\nVerifying that the server was up, I have checked the raid status of the disks and analysed that raid was still not up between disks. However, after hard reboot, operating system was able to recognize the disk as a device. I have started the software raid synchronization between disks with the command below:\n\r\n#mcpSwRaid.pl -add sdb\n\r\n[root@tyrm-mas4 ~]# mcpSwRaid.pl -status\n\r\nDefined disks:  sda=146813(MB)  sdb=146813(MB)\n\r\n                ***** Software RAID-1 Devices *****\n\r\n              |            |           |           |     Sync & Recovery\r\n  MD Device   |            |  Member-0 |  Member-1 |      Speed Finish Done\r\nName  Size(MB)|   Usage    | Name  Flg | Name  Flg | Mode (MB/s) (min)  (%)\r\n--------------+------------+-----------+-----------+--------------------------\r\nmd0      94.0 | /admin     | sda1  U   | sdb1  U   |  .    .      .     .\r\nmd1     101.9 | /boot      | sda2  U   | sdb2  U   |  .    .      .     .\r\nmd10 115976.9 | /var/mcp   | sda11 U   | sdb11 U   |  .    .      .     .\r\nmd2    4094.6 | swap       | sda3  U   | sdb3  U   |  .    .      .     .\r\nmd3    4094.6 | swap       | sda9  U   | sdb9  U   |  .    .      .     .\r\nmd4    2047.2 | /          | sda6  U   | sdb6  U   |  .    .      .     .\r\nmd5     501.9 | /home      | sda12 U   | sdb12 U   |  .    .      .     .\r\nmd6     501.9 | /tmp       | sda5  U   | sdb5  U   |  .    .      .     .\r\nmd7    6141.9 | /opt       | sda10 U   | sdb10 U   |  .    .      .     .\r\nmd8    2047.2 | /var       | sda7  U   | sdb7  U   |  .    .      .     .\r\nmd9    3074.8 | /var/log   | sda8  U   | sdb8  U   |  .    .      .     .\n\r\nAfter the synchronization had been completed, Keith retried patching the linux platform of the server. This time, linux platform patched successfully. We agreed with Keith and I dropped the pager call.\n\r\nAs a GPS recommendation, I will enter a case note to the 150623-536362, suggesting that their secondary disk on MAS4 should be replaced.','null'),(1002,'Burak Biyik','AS-GW','2015-06-16','150610-534656','NUVIA','Phil Karam paged me to report a IMM MAS outage in Nuvia Production site. The issue initially occured 9th of June and several outages were experienced following days. The system has IMM 3.9 running with MAS 16.0.0.805\n\r\nCustomer stated that IMM dies without failing over (which is a separate IMM issue) during peak hours where 400-500 calls (19-20 concurrent sessions) take place in an hour. The reason behind losing voicemail service is likely to be related to IVRMP module of MAS. Both from EM GUI as well as MAS debug logs, we are able to see that IVMRP is shutdown and then activated in a short time but sometimes it is not recovered so that IVR sessions in voicemail service are not established. A sample log given below;\n\r\n(15 14:34:12.354) NWConnMonitorManager: Connection Details\r\n(15 14:34:12.354) IP Version............IPv4\r\n(15 14:34:12.354) Type..................Client\r\n(15 14:34:12.354) Trans Protocol........TCP\r\n(15 14:34:12.354) App Protocol..........IVRMP-MSLINK\r\n(15 14:34:12.354) State.................Connecting\r\n(15 14:34:12.354) Status................Remote Port Not Open\r\n(15 14:34:12.354) Src  Address..........127.0.0.1\r\n(15 14:34:12.354) Dest Address..........127.0.0.1:4001\r\n(15 14:34:12.354) Notes.................connect failed to MAS IVR Media Processor -- retrying\r\n(15 14:34:12.354) Service Status........2\r\n(15 14:34:12.354) Last Conn Timestamp...2015-06-15 18:34:12\n\r\nAlthough there are more or less 20 active sessions on MAS, there are more than 400 established TCP sessions which seemed to be very high. It is more like MAS is not terminating TCP sessions when it is done with it. Destination socket (10.156.61.40:8082) is IMM where MAS would fetch .wav files requested in vxml message. A part of \"netstat\" output: \n\r\ntcp        0      0 10.156.61.44:52938          10.156.61.40:8082           ESTABLISHED 0          29367241   26839/vxmli\r\ntcp        0      0 10.156.61.44:52937          10.156.61.40:8082           ESTABLISHED 0          29367179   26839/vxmli\r\ntcp        0      0 10.156.61.44:52936          10.156.61.40:8082           ESTABLISHED 0          29367007   26839/vxmli\r\ntcp        0      0 10.156.61.44:52951          10.156.61.40:8082           ESTABLISHED 0          29369340   26839/vxmli\n\r\nAlso, in vxmliAppDebug.txt logs, there are bunch of resource errors which made me think that whenever IVR Media Processor is down, there is no resource left in IVR pool that might cause an outage.\n\r\n(12 15:08:04.557) (106) EVENT|2|content=Error: error.noresource\n\r\nSince Design analysis was obviously needed, we were agreed to drop the call as long as providing an update for tomorrow. Possible solutions such as MAS Upgrade or fixing a bug (if there is) are already in discussion. I will stay in touch with IMM team to get some updates from them too.','null'),(1003,'Seren Batmaz','AS-GW','2015-04-30','150430-528657','Unitymedia','Problem Description:\n\r\nSWD paged me for two problems occurred during BCP upgrade. A2 was previously upgraded and BCP was excluded during the A2 upgrade. Upgrade path was MCP_12.0.12.8->MCP_14.0.16.3. There were 2 clusters, both of them having 2+1 cluster configuration.\r\n- In cluster named as \"FRA\", only one of the BCPs was upgraded. Remaining two BCPs was still running on MCP_12.0.12.8. All calls passing through this cluster were failing\r\n- In cluster named as \"DORT\", all of three BCPs were upgraded. However, there were cluster alarms on first and second BCPs. Calls passing through this cluster were successfull.\n\r\nSolution:\n\r\nI connected to the site and checked the system. For the first cluster, when I see that only one of the BCPs were upgraded, I suggested to upgrade remaining two as well, since the cluster was set up improperly due to BCP instances having different loads. To upgrade the BCPs, we ran the following operations:\n\r\n- Ran \"patchPlatform.pl\"\r\n- From MCP GUI, changed the load of BCP instances, deployed and started.\n\r\nUpgrade of these two BCPs completed susseccfully. I checked if calls started passing through BCPs of FRA cluster with \"mptool -n\" command and saw that there were used ports which means calls were not failing any more. However, there were Invalid Cluster Configuration alarms on FRA BCPs, just like the alarms on DORT BCPs.\n\r\nTo clear those alarms on both clusters, I ran the following operations from MCP GUI:\n\r\n- Kill the instance\r\n- Undeploy\r\n- Deploy\r\n- Start\n\r\nAfter these operations were completed, we saw that alarms had persisted. So, I checked if the instances were running properly on servers. What I saw was there was a BCP for each cluster on which HAL wa running on MCP 12.0 instead of MCP 14.0:\n\r\ntsysadm@DORT-bcp00blade3:/home/ntsysadm[root@DORT-bcp00blade3 ntsysadm]# neinit -p\n\r\nRelease         Name             Pid\r\n-------         ----             ---\r\nMCP_12.0        HAL_0            551\r\nMCP_14.0        DORTb3_0         31259\n\r\nSo, for problematic instances, I ran the following operations:\r\n- Killed the BCP and HAL instances.\r\n- Undeploy\r\n- Deploy\r\n- Start\n\r\nAs soon as these operations were completed, cluster alarms on all of the BCPs were cleared. When the customer agreed that all of calls were successfull, we ended the call.','null'),(1004,'Senem Gultekin','AS-GW','2015-04-22','150422-527409','City of Los Angeles','Problem Description:\r\nER paged out for any registration issue seen at City of Los Angeles. They are running on 17.0.18.4.\r\nThey had 50 polycom phones and only 2 of them were able to register.\n\r\nActions:\r\n-	Accessed to the site and check the MCP GUI to see if there are any NE comm or DB releated alarms on the instances. They were all good.\r\n-	Suggested customer to perform SESM swact, and they told me that they have already done and there is no change.\r\n-	Performed Softphone reboot on Polycom Phone, but the result was still the same.\r\n-	Checked the oss logs for SM and SESM.\r\n-	Suggested ER to collect SESM traces and engage CALLP GPS for call flow investigation.\r\n-	Kimberly Jones from CALLP GPS joined the investigation and she has found out there are null point exceptions on SESM.\r\n-	Kim continued with her investigation, later on it was found out that they had network changes in their sites and this was causing the issue.','null'),(1005,'Yunus Ozturk','AS-GW','2015-04-10','150410-525892','Geisinger Health System','Problem Description:\r\n====================\n\r\nCustomer reported that during the peak traffic, callers could not join a conference on another blade. They would receive music on hold indefinitely waiting for the chair person to arrive. If two people called into meetme and hit two separate blades, two conferences with the same id could be created with the chair pins. \n\r\nSolution:\r\n==========\n\r\n- Accessed the MAS Servers and checked the date and timezone configurations\r\n- Noticed that date of the MAS3/MAS4 Servers were 1 hour ahead of MAS1/MAS2 Servers. If the MAS Servers are not synchronized with the NTP Server and if there is 15 seconds time difference between the date of the MAS Servers, we can possibly encounter this kind of refer problems on the Meetme scenarios.\r\n- We have synchronized the MAS3/MAS4 Servers with the NTP Server by following actions;\n\r\n- Login the MAS Server with root\r\n- Change directory to /usr/sbin/ --> cd /usr/sbin/\r\n- Run the script --> service ntpd stop\r\n- Run the script --> ./ntpdate \r\n- Run the script --> service ntpd start\n\r\n- The date of the MAS3/MAS4 Servers were synchronized after applying the steps above. However, we have then noticed that, the Timezone configurations of the MAS3/MAS4 Servers were also different from the MAS1/MAS2 Servers. MAS1/MAS2 Servers were set as \"EST\" but MAS3/MAS4 Servers were set as \"EDT\". It appears that this has caused date difference between the MAS1/MAS2 Servers and MAS3/MAS4 Servers. \r\n- We have decided the set the Timezone of the MAS3/MAS3 Servers as \"EST\" to have the same configurations with MAS1/MAS2 Servers.\r\n- We have used the \"tzConfig.pl\" script on MAS3/MAS4 Servers and set the Timezone as \"EST\". Afterward, the servers have been rebooted. \r\n- Furthermore, we have decided to rebuild the cluster configurations between all 4 MAS Servers since on MAS2 (Secondary MAS) Cluster Status screen, we were not able to see the Standard MAS Servers. We expect to see all 4 MAS Servers on the Cluster Status screen. Even the cluster configurations between the 4 MAS Servers are correct, Cluster Status screen did not show up the MAS3/MAS4 Servers on MAS2 for some reason. This should not impact the Callp as it is just used for monitoring the cluster status. However, to be on the safe side, we have decided to rebuild the clustering again. \r\n- We have dropped the replication between 4 MAS Servers and set all them as \"Primary\" Role. Additionally, we have removed the existing \"Replication Account\" configurations.\r\n- Rebuilt the cluster again with a new \"Replication Account\" and restarted the MAS Services on 4 MAS Servers. This has fixed the \"Cluster Status\" screen issue on MAS2. We were able to see all 4 MAS Servers on \"Cluster Status\" screen. \r\n- Customer performed test calls by hitting all the MAS Servers and they were all successful.','null'),(1006,'Burak Biyik','AS-GW','2015-04-07','150407-525286','Geisinger Health System','Donnell Williamson paged me once again to report a CRITICAL alarm on MAS4 which did not clear with application restart/reboot. Here is the event logs regarding alarm;\n\r\n=============================================== \r\nCritical 2015-04-07 00:55:15 Info Alarm Activated: A component has exited too many times (Id: 380) \r\n14600 Critical 2015-04-07 00:55:15 Info Alarm Activated: A component has terminated abnormally (Id: 378) \r\n16012 Error 2015-04-07 00:55:15 Info The number of attempts to start component [cstore] within the allowe... \r\n16010 Error 2015-04-07 00:55:15 Info Component [cstore] PID [19764] has terminated with exit status [6] [... \r\n============================================== \n\r\nIt seems that CStore database of MAS4 is corrupted/crashed somehow. Then I realized a misconfiguration that would lead this kind of data corruption scenarios. MAS servers on site are installed with following services;\n\r\nMAS1-> adhoc,annc,moh,meetme  (PRIMARY)\r\nMAS2-> adhoc,annc,moh,meetme  (SECONDARY)\r\nMAS3-> meetme  (STANDARD)\r\nMAS4-> meetme  (STANDARD)\n\r\nHaving a cluster members which have different set of services is not a supported way since all service configurations/files are being synchronized between them so that a service mismatch might lead data loss/corruption.\n\r\nI reinstalled MAS4 without preserving data to sync correct data from PRIMARY. After reinstallation, all MAS components were without any alarm.\n\r\nThen, customer reported a meetme bridge issue. Once a bridge is established over one MAS after entering access code, new bridge was opening instead of joining existing one with the same access code. This was showing that MAS instances were not talking to each other and cluster structure was not healthy to keep all members informed about their status.\n\r\nThen I followed below action plan;\n\r\n1.Setting all MAS instances as primary by removing their replication account (Cluster Configuration -> Server Designation)\r\n2.Putting them into a cluster by configuring a new replication account \r\n3.Restarting PRIMARY MAS\n\r\nCustomer confirmed that all tests calls passed.\n\r\nI informed SWD to inform customer about current misconfiguration which is not supported. I am not sure who they were consulted but this needs to be corrected.\n\r\nA possible solution might be expanding MAS3&4 license with adhoc,annc,moh and reinstalling them with these services.This activity can be performed by GPS later on if customer asks.\n\r\nAgreed & dropped the call.','null'),(1007,'Burak Biyik','AS-GW','2015-04-07','150407-525274','Geisinger Health System','I have been paged by Donnell Williamson from SWD team for the following MAS upgrade path;\n\r\n16.0.0.802 --> 16.0.0.840\n\r\nThere are total of four MAS servers that are the member of the same cluster. SWD wanted to take backups from each MAS as pre-upgrade step but it failed for MAS2.\n\r\nFirst way to take it was from MAS EM GUI (Tools -> Backup and Restore -> Backup Tasks). When we RUN configured backup set, history logs didn\'t show any confirmation about backup task even if we changed refresh interval several times. There is also no backup appearing under /var/mcp/ma/MAS/platdata/EAM/Backups/.\n\n\r\nThen we tried to take it via cli after running following script;\n\n\r\n[root@MAS2 ~]# backuprestore -b /var/mcp/ma/MAS/platdata/EAM/Backups/Pre_840Upgradeconfig_MAS2.geisinger.edu__2015_04_06_8_41_19g\r\nzip -t config \r\n   ----------------------------------------------------\r\n               MAS Backup and Restore\n\r\n   ----------------------------------------------------\n\r\nStarting Data Backup...\r\n...\r\n...\r\nExporting repository resource /ReportParts/VxmlOM_es_US.properties\r\nExporting repository resource /ReportParts/VxmlOM.jrxml\r\nExporting repository resource /ReportParts/VxmlOM.properties\r\nExporting repository resource /ReportParts/VxmlRecOM.jrxml  \n\n\r\nHowever, the script was hung at the latest line everytime and never completed.\n\r\nWe tried MAS application restart as well as server reboot and retried the steps above. It failed again.\n\r\nConsidering that this is MAS MR Upgrade (which asks you to Preserve data to keep all configuration) and MAS2 is SECONDARY in the cluster (which we would install and sync data from PRIMARY in worst case), I recommended SWD to continue MAS MR Upgrade.\n\r\nI can still see that it is not possible to take backup from MAS2 even after upgrade to 16.0.0.840.\n\r\nThis will be investigated with linked case.\n\r\nAgreed and dropped the call.','null'),(1008,'Oktay Esgul','AS-GW','2015-04-02','150401-524754','LIME','John Kishner from ER paged me in order to report one way speech path problem for CICM calls at Lime. There were several outages during last 10 days on this site, customer and engineering team was working with high priority for permanent resolution.\n\r\nThe reason that they paged me to clarify CICM call path and if there is any misconfiguration on BCP side,\r\nsince with previos network configuration CICM calls was being handled by GWC5 adn BCP[7100], however with new configuration, they were trying handle CICM calls with GWC2 and GWC6 with BCP7200 which all IPs are totally different than BCP7100 which were used for CICM before.\n\r\nNote :For the last week outage, they changed routes on network in order to handle and route incoming BCP request tO NET2 IP of the BCP instead of NET1 . Normally this behaviour should be handled by GWC, but due to this site is running on quite old and EOL release level (MCP9.1) ))\n\n\r\nWhen I logined to VM , only MCP gui was launched which BCP7200 configured, I asked John to open other MCP gui that BCP7100 are configured which they did not aware in order to\r\ncompared both BCP config and  clarify and proved there is not any particular configuration for CICM on BCP side.The only thing we do, adding GWCs  which are inserting BCP to call.\r\nGWC5 was added on BCP7100 while GWC2&GWC6 are added to BCP7200 as expected.\n\r\nI also provide CICM and GWC teams which ip adresses are being used for both BCP7100 and BCP7200 to help them to network investigation. Then , they realized CICM can not access BCP7200s \r\nwhich seems to be root cause of outage.\n\r\nIt looked like network routing issue since they dont have proper routing between cicm IPs (172.x.x.x) and bcp7200 net1 (65.48.193.36). \n\r\nThat is why , we decided to disassociate the CICM from GWC2 and GWC6  and then the issue solved.\n\r\nThe call path : A2 lines  calling pri pbx / pstn. Call flow A2 -> CS2K -> SIP -> C20 -> G^ - PBX which means portal insertion is being\r\n made on c20 side .\n\r\nAfter all these changes, they perform several tests for almost 1 hour for each call scenerio,and all calls were good.They will continue testing  in the morning of their local time.Since the\r\nall calls OK we dropped the call.','null'),(1009,'Yunus Ozturk','AS-GW','2015-03-27','150327-523980','C&W Lime','Problem Description:\r\n=====================\n\r\nCustomer contacted GB to report that they had a 1 way audio call problem with apx 50% of the calls coming from the Pegwell CS2K going to PRI PBX customers that had been rolled from the CS2K / PVG to the new C20 / G6 in Windsor Lodge. GB integration investigated and traces indicated that all calls with 1 way audio were hitting BCP blade 4. GB shut blade 4 down to force all calls off blade 4 and all calls test calls started failing - appearing to go to dead air. GB brought blade 4 back up but it would not joined the cluster (stayed +1). GB shut down blade 1 to try and put blade 4 back in the cluster but blade 4 remained +1. All test calls continued to fail - dead air. Traces indicated that the calls were getting a rel msg from the CS2K prior to even attempting to insert BCP. \n\r\nProblem started when T1s to PRI PBXs rolled from CS2K - PVG over to C20 - G6\n\r\nActions Taken:\r\n===============\n\r\n- Removed the new added BCP Blade4 completely from the system.\r\n- Deleted the BCP Blade4 configurations on the MCP GUI and disassociated the Blade4 from the GWCs on the CMT GUI.\r\n- Corrected the Port assignments of the BCP Blades as they are were assigned with 5600 ports per blade. Corrected them as 1850 ports per blade.\r\n- Restarted the all the BCP blades within the cluster for the changes to take affect. Afterwards, wrong Cluster alarms cleared on the BCP Blades.\r\n- Restarted the SESM instances as there were connection alarms to the BCP Blades. Then, alarms cleared.\n\r\nTest Call still failed (Dead Air)\n\r\n- Based on the investigations, we noticed the following issue on the failing calls;\n\r\n0x02b72b0a/0x00031561\r\n10.226.53.36:64982    <*-> 65.48.193.36:44486\r\n                                                      0x02b72b0b/0x00032461\r\n                           65.48.193.36:44320    <-->     10.1.87.233:1638\n\r\nConnection 0x02b72b0a:\r\n        Flags: 0x00031561\r\n        Portal Address: 65.48.193.36:44486\r\n        Client Address: 10.226.53.36:64982\r\n        Created by call server: 192.168.32.168:3904\r\n        Call ID: 00011512\r\n        Event Request ID: 8310122C70BAE4C8000033F200011512\r\n        RX Packets: 0\r\n        Prev RX Packets: 0\r\n        TX Packets: 2428\r\n        Prev TX Packets: 0\r\n        RX Octets: 0\r\n        RX Octet Rollover: 0\r\n        TX Octets: 194240\r\n        TX Octet Rollover: 0\r\n        Dropped Packets: 0\r\n        Packets Lost: 0\r\n        Interarrival Jitter: 0\r\n        Transmission Delay: 0\n\r\n- BCP did not get the RTP packets from the orig side (calls through G6). TX packets were ok but there were no incoming RX packets. \r\n- Customer told us that G6 should communicate with the NET2 (private) leg of the BCP as G6 calls were originated from private side. \r\n- Told customer that BCP assigns the NET1/NET2 IP/Port as a request of the Call Server (SESM or GWC)\r\n- On this scenario, the G6 originated calls were inserted by the DPL GWC based on the investigations. The other calls were working fine as NET2 leg of the BCP was inserted correctly for those calls. Just G6 originated calls have this problem. \n\r\nThe logs below show that the DPL GWC inserted the NET1 leg of the BCP for the problematic scenario instead of NET2,\n\r\n190:07:05:24.31[T:5]Sending msg --> GW IP[192.168.33.5] GW PORT[3904]\r\nCRCX 53145 $/$/$ MPCP 3.2\r\nC:020110C6\r\nM: Sendrecv\r\nL:MP/vpn:NET1, MP/ept:RTP   <----- Here is the NET1 request\r\nX:8200C62E4087B1B8000031F2020110C6\r\nR:MP/idl(N), MP/lc(N)\n\r\n190:07:05:24.31[T:6]Received msg <-- GW IP[192.168.33.5] GW PORT[3904]\r\n200 53145 OK\r\nI:00145ebe8eb4_11df6878\r\nZ:11df6878/0/RTP\r\nv=0\r\nc=IN IP4 65.48.193.37\r\nm=50876\n\r\n- Since NET2 leg should be inserted for this scenario, we have checked the Net2_Routable_Networks configurations on the BCP side and confirmed that the required route was added there (For 10.226.53.32 Network with 255.255.255.240 Subnet)\n\r\n- if GWC does not insert the NET2 leg of the BCP, this scenario will never work even the required Net2_Routable_Networks configurations exist on the BCP. \n\r\n- Advised the customer to add a static route on the Router side if the GWC will not be able to insert the NET2 of the BCP. NET1 side network of BCP 65.48.193.36/37/38 should communicate with the 10.226.53.36 client network. So, there should be a static routing between these 2 networks on the customer site. This is actually an engineering issue on the customer\'s network. \n\r\n- Customer had concerns about the Business customers that have been rolled from the CS2K to the C20. We discussed the issue with LIME and a decision was made to roll the T1s back but to have GPS continue investigating the issue. At this point, the traffic was routed to fix the issue on G6 originated calls. In the meantime, GPS continued investigation. \n\r\n- We have noticed that the working calls (where the NET2 side is inserted) were using the ISUP Trunks and they are able to insert the NET2 leg of the BCP. However, the failing G6 calls were using DPT Trunks. GWC GPS recommended to set the Inter_Domain setting of the DPT Trunk to \"N\" which might help to insert the NET2 side of the BCP. \n\r\n- Therefore, asked the customer to create a separate DPT Trunk just for the G6 originated calls to set the Inter_Domain configuration to \"N\" as we could not modify the existing DPT Trunk not to impact the other calls going through this trunk. \n\r\n- Customer added a separate DPT Trunk with Inter_Domain=N configuration and assigned a test DN for further tests. Translations Team involved here to complete the required configurations of this trunk. However, test calls did not work again with this configuration.\n\r\n- After some investigations and based on the following information, it was noticed that DPT Trunks are not able to insert the NET2 leg on customer\'s existing release. Customer is running on SN09 Release which is very old.\n\r\nIn CVM11 there will be the ability to mark DPT trunks as being natted (meaning the clients are natted), and that will be the only way to ensure that IP discovery is enabled on the BCP for SIP calls. The support of IP address auto-discovery for natted SIP gateway clients in CVM11 is provided by activity A00014389, titled \"Inter-Carrier SIP Trunking: Support for Border Control Point Insertion. With this activity, the INTER_DOMAIN field for DPT tuples in table TRKOPTS is changed to a field named LOCATION with the following attributes: INTRA_DOMAIN, PRIVATE, PUBLIC, NATTED or UNKNOWN.\n\r\n- Since customer is running on SN09 Release, the Inter_Domain feature will not work. \r\n- At this point GWC GPS colleagues involved (Mike Green / Norman Caron) and they started investigation for the behaviour of the GWC side. They will try to find a way to have the GWC sending the NET2 request to the BCP.\n\r\nInvestigation on GWC side continues..','null'),(1010,'Yunus Ozturk','AS-GW','2015-03-17','150317-522129','Vodafone (New Zealand)','Problem Description:\r\n--------------------\n\r\nWhen performing upgrade from 14.1.8 to 14.1.10 the BCP blade 6 failed to deploy. And the respective Wizard step failed. \n\r\nAttempted to deploy manually but that didn\'t do anything: \n\r\nPNB006.0: Maintenance State: None -> Deploying \r\nPNB006.0: Maintenance State: Deploying -> None \r\nPNB006.0: Maintenance State: None -> Deploying \r\nPNB006.0: Maintenance State: Deploying -> None \r\nPNB006.0: Maintenance State: None -> Deploying \r\nPNB006.0: Maintenance State: Deploying -> None \r\nPNB006.0: Maintenance State: None -> Deploying \r\nPNB006.0: Maintenance State: Deploying -> None \n\r\nThe upgrade is stuck \n\r\nSolution:\r\n---------\n\r\n- Accessed the problematic BCP blade with ssh connection \r\n- Removed the MCP_12.0 / MCP_14.0 and MCP_14.1 folders under /var/mcp/run/ directory \r\n- Restarted NED (neinit restart) \r\n- Rebooted the blade \r\n- Deployed and Started the blade on MCP GUI','null'),(1011,'Oktay Esgul','AS-GW','2015-03-05','150305-520441','GENBAND - GTAC','Tony Pittman from SWD team paged me in order to report snmp failure issue for MAS server after they performed MAS MR upgrade.\n\r\nHe said that all MAS services are servicing properyly but MAS was grey out on MCP GUI. While we are discussing the issue, he said that this is  lab, so that I suggested him to open a case and dispatch to our queue,then dropped the call.','null'),(1012,'Senem Gultekin','AS-GW','2015-02-05','150205-516082','UnityMedia','Problem Description:\n\r\nER paged Gateway pager for a BCP upgrade failure at Unitymedia.\n\r\nBCP was in a loop, getting activating-> active-> unavailable\n\r\nUpgrade path 14.1.0.12 to 14.1.8.0.\n\n\r\nSolution:\r\n-	Accessed to the site over ERs PC.\r\n-	Checked the work logs for bcp1, I saw the following error;\r\njava.lang.UnsatisfiedLinkError: /opt/mcp/mediaportal/lib/libHalHeartBeater.so: libACE.so.6.1.0: cannot open shared object file: No such file or directory\r\n-	Once this error was prompted BCP tries to come back up again, but it fails for every attempt.\r\n-	Checked the platform level for BCP1 server\n\r\n[root@bcp7200server0 ntsysadm]# mcpRelease.pl\n\r\n        *** MCP Platform Release ***\n\r\nSystem Type:     mcp_bcp_linux_ple3\r\nRelease Level:   14.1.10 (via patching)\r\nHardware Env:    Intel-TIGH2U\n\r\n-	Platform was patched newly. Ive checked it from 14.1.8 MR ReleaseNotes and it showed it \n\r\nBCP7200 Platform installer 	mcp_bcp_linux_ple3-14.1.10.iso \r\nBCP7200 Platform Patches 	mcp_bcp_linux_ple3-14.1.10.patches.r-1.ext.iso \r\nMCP Platform Release Level 	14.1.10 \n\r\n-	However this is not correct, checked it from the Loaldlineup and NPV, the platform level for 14.1.8 level should be 14.1.11.\r\n14.1.18 MR platform levels;\r\nMCP-Platform-Release-Level>\n14.1.12\n14.1.13\n14.1.11\n14.1.2\n\r\n-	The reason for this was that the upgrade was performed manually not with wizard. \r\n-	System Manager Server was also in older platform level (14.1.10). Where it should have been 14.1.13 for ple2.\n\r\n[root@sslem00 ~]# mcpRelease.pl\n\r\n        *** MCP Platform Release ***\n\r\nSystem Type:     mcp_core_linux_ple2\r\nRelease Level:   14.1.10 (via install)\r\nHardware Env:    Intel-TIGH2U\n\r\n-	ER confirmed that site was upgrade manually 1 year ago, only for SM, SESMs and PROVs. They didnt upgrade BCPs.\r\n-	So when we look at the whole system, the releases are all mixed up! This is why we dont support manually upgrade.\r\n-	Ive deployed 14.1.0.12 load for BCP1 to recover the current situation. It came up successfully and customer reported that not issues seen.\n\r\nNext Action:\n\r\n-	Since Unitymedia platform levels are not aligned, customer needs to bring them to the official 14.1.8 MRs level.\r\n-	Perform platform upgrade with patchPlatform.pl script on ple2 (SMs,SESMs) and ple3(BCPs) servers.\r\n-	Platform levels should come to the following;\r\n-	\r\no	SMs and SESMs(HT Langley) = 14.1.10 ple2 -> 14.1.13 ple2\r\no	SESMs (CC3310) = 14.0.9 ple1 -> 14.1.12\r\no	BCPs (HT Langley) = 14.1.10 -> 14.1.11\n\r\nAfter platform upgrades are complete for the BCPs, the BCP application can be deployed from the MCP GUI with 14.1.8.0 load.','null'),(1013,'Yunus Ozturk','AS-GW','2015-01-30','150130-515183','Global Tel*Link GTL','Problem Description:\n\r\nER paged out GW GPS with the request of Callp GPS and reported that all incoming PSTN calls were failing to ring telephone. \n\r\nSolution:\n\r\nAt the time of we were paged out, Callp GPS was already working on the issue and they have asked us to check the Branding Service configurations on MAS as they were thinking that the cause of the issue was getting 404 SIP response messages from the MAS1 and no response from the MAS2. \n\r\nCallp GPS found out that the customer is not using the Branding Service on MAS and due to invalid Branding Service configurations on MAS1, it was responding with 404 Not Found messages since the required Branding wav. files did not exist on MAS1. When MAS1 responded back with 404 Not Found, SESM routed the calls to MAS2 but this time did not get any response from MAS2. When we have checked the MAS2, we noticed that it was not working properly. Ping was working but SSH access and GUI access was not working. This was also causing SNMP agent communications alarms on the MCP GUI. \n\r\nSince customer was not using the Branding Service, even after Branding Service was disabled for customer\'s domain, all incoming PSTN calls started working again.\n\r\nRCA case was opened up for Callp GPS Team to see why SESM failed the calls when it could not get a response from the MAS.\n\r\nAdditionally asked the customer to open up a separate case to follow up the issue on MAS2 as it requires a detailed investigation to see why MAS2 is not working properly.','null'),(1014,'Senem Gultekin','AS-GW','2014-12-29','141209-507856','Bermuda Telephone Company','Problem Description:\n\r\nER paged for a BCP04 server not booting up problem at Bermuda Telephone. BCPs are running on BCT chassis, Release is 17.0.7.13.\n\r\nThere are 4 BCPs in total, 3+1 clustering. \n\r\nSolution:\n\r\n-	Accessed to the problematic BCT blade from console and saw the stuck part with Kernel Panic I/O errors for sda disk.\r\n-	Rebooted few times but the result was the same.\r\n-	Since its hardware/platform related issue gave the following actions to be performed. In order if one steps doesnt work, next step needs to be performed;\r\n1-Reseat blade\r\n2-    Swap disks\r\n3-    Re-install the BCP04 blade\r\n4-    Replace the disk/server\n\r\n-	ER reported that there is no one on site and they will perform it later on.\r\n-	Agreed with ER and ended the pager call.','null'),(1015,'Yunus Ozturk','AS-GW','2014-12-04','141203-506988','Axtel','Problem Description:\n\r\nAxtel contacted GENBAND reporting they were receiving customer complaints about one way voice on SIP calls. During initial investigation, customer was receiving more and more complaints of 1 way or speech path and SIP calls failing to complete. This outage affected 50% of the sites EXPERIUS SIP Lines.\n\r\nActions Taken:\n\r\n- Customer has switched the ERS units from 8600-0 to 8600-1 - Did not help.\r\n- The BCP Servers have been rebooted - Did not help\r\n- The BCP blades have been re-seated - Did not help\r\n- Since SSH connection from the EM/SESM Servers to BCP Servers were failing, checked the ssh configurations on the BCP/SESM/EM Servers, IBM BCT MM Module and ESM units. The SSH configurations were ok.\r\n- Checked the ESM units. The communication to the ESM units was so slow. The ESM 1 GUI was not working. ESM 1 is being used for the eth0 interfaces of the BCP Servers and ESM 2 is being used for the eth1 interfaces of the BCP Servers.\r\n- Checked network interfaces of the BCP Servers. All of them were using their eth0 interfaces where they are connected to ESM 1. \r\n- Switched the eth interface from eth0 to eth1 manually on one of the BCP Servers and then SSH started work. Then, we thought that the problem is on the ESM 1 since the eth0 interfaces are connected to ESM 1. Additionally, ESM 1 GUI was not working.\r\n- Powered off / powered on the ESM 1 unit on the IBM BCT MM Module and the eth0 interfaces and SSH connection started work through the eth0 interfaces.\r\n- Customer still had the 1 way voice path issues even we fixed the eth0 interfaces and the SSH connection. Therefore, we decided to reboot the BCP servers again\r\n- Rebooted all the BCP servers again and the voice path issue was fixed. It did seem that SESM was not able to initiate the calls properly while the eth0 interfaces were not working. \r\n- Restarting the ESM 1 unit has fixed the eth0 interfaces and rebooting the BCP servers ran the service network restart on the background (refreshed the network interfaces of the servers) and caused the SESM to be able to initiate the calls to BCP servers properly.\n\r\nAfter performing these actions, the test calls were successful.','null'),(1016,'Yunus Ozturk','AS-GW','2014-12-04','141203-506988','Axtel','Problem Description:\n\r\nAxtel contacted GENBAND reporting they were receiving customer complaints about one way voice on SIP calls. During initial investigation, customer was receiving more and more complaints of 1 way or speech path and SIP calls failing to complete. This outage affected 50% of the sites EXPERIUS SIP Lines.\n\r\nActions Taken:\n\r\n- Customer has switched the ERS units from 8600-0 to 8600-1 - Did not help.\r\n- The BCP Servers have been rebooted - Did not help\r\n- The BCP blades have been re-seated - Did not help\r\n- Since SSH connection from the EM/SESM Servers to BCP Servers were failing, checked the ssh configurations on the BCP/SESM/EM Servers, IBM BCT MM Module and ESM units. The SSH configurations were ok.\r\n- Checked the ESM units. The communication to the ESM units was so slow. The ESM 1 GUI was not working. ESM 1 is being used for the eth0 interfaces of the BCP Servers and ESM 2 is being used for the eth1 interfaces of the BCP Servers.\r\n- Checked network interfaces of the BCP Servers. All of them were using their eth0 interfaces where they are connected to ESM 1. \r\n- Switched the eth interface from eth0 to eth1 manually on one of the BCP Servers and then SSH started work. Then, we thought that the problem is on the ESM 1 since the eth0 interfaces are connected to ESM 1. Additionally, ESM 1 GUI was not working.\r\n- Powered off / powered on the ESM 1 unit on the IBM BCT MM Module and the eth0 interfaces and SSH connection started work through the eth0 interfaces.\r\n- Customer still had the 1 way voice path issues even we fixed the eth0 interfaces and the SSH connection. Therefore, we decided to reboot the BCP servers again\r\n- Rebooted all the BCP servers again and the voice path issue was fixed. It did seem that SESM was not able to initiate the calls properly while the eth0 interfaces were not working. \r\n- Restarting the ESM 1 unit has fixed the eth0 interfaces and rebooting the BCP servers ran the service network restart on the background (refreshed the network interfaces of the servers) and caused the SESM to be able to initiate the calls to BCP servers properly.\n\r\nAfter performing these actions, the test calls were successful.','null'),(1017,'Oktay Esgul','AS-GW','2014-11-12','141112-503052','Windstream','Keith Marshall from SWD paged me in order to report BCP issue at post installation steps.\n\r\nUpgrade Path \n\r\nFrom : 14.0.12\r\nTo: 17.0.7\n\r\nKeith said that they have patched BCP platform manuelly due to one of known issue at pre-upgrade step. After all servers patched, 3 of 4 BCP blade had become up and active properly while the second instance of first cluster hung at configured state during deploying with new load.\n\r\n1.I have connected site and checked the MCP Gui for alarms and current status of the BCP blade.\r\n2.Killed the BCP instance from MCP gui.\r\n3.Then , ssh to problematic blade . \r\n4.Firstly , I have run \"neinit restart \" then cleared the /var/mcp/run directory of the server.\r\n5.Before redeploy the instance,I have change BCP instance load to 17.0.7 and redeploy it.\r\n6.Deployment completed successfully. \r\n7.I have started the deployed instance and it became active without any issue and all alarms are gone .\n\r\nWe aggreed with Keith problem is over, then I dropped the call.','null'),(1018,'Seren Batmaz','AS-GW','2014-11-02','141102-501640','Axtel','Problem Description:\n\r\nER paged me for problem on two BCP instances which occurred after upgrading the platform levels to MCP 14.1.10 MR. The BCP instances were not able to become Active.\r\nUpgrade path was from 14.1.0.12 to 14.1.10.0\n\r\nSolution:\n\r\nI have connected to the site and chechked the system. In MCP GUI, I saw that none of the instances, not even SM nor DB, was upgraded to MCP 14.1.10.0. So, I told customer that BCPs may had upgraded after or during Upgrade Wizard had been ran to upgrade to MCP 14.1.10.0. The customer told that they were not using Upgrade Wizard and they had also upgrade platforms of EMServer1, EMServer2, SESMServer1 and SESMServer2 manually.\r\nSo, I told them that manual upgrade was not supported and they should had used Upgrade Wizard. Hence, I suggested to rollback platform levels of upgraded 6 servers (2 EMServers, 2 SESMServers and 2 BCPs) and use Upgrade Wizard to upgrade the system.\n\r\nTo be able to rollback platform levels, I ran \"patchPlatform -rs\" script for 6 servers. After this operation was completed, all instances including SMs, SESMs and BCPs became up and running properly.\n\r\nFinally, I suggested the customer to contact with their GB account to arrange a SWD engineer to run the upgrade properly. As soon as we agreed, I left the call.','null'),(1019,'Senem Gultekin','AS-GW','2014-10-22','141022-499757','Axtel','Problem Description:\n\r\nSWD pager for MAS GUI launch issue at Axtel for 2 MAS servers.\n\r\nThese MAS servers are running on 6.1.0.417 MAS (MCP 12.0) level and they only have Announcements on it.\n\r\nIn total there are 4 MAS servers. Other 2 MAS were upgraded yesterday.\n\r\nSolution:\n\r\n-	Accessed to the site and both MAS GUIs were not launching 172.28.134.76 and 172.28.134.77.\r\n-	Since this is old MAS, its running on windows, and there were no free space left on the server. \r\n-	It seems like the logs files have been filled up all the space.\r\n-	SWD was not able to get MAS backup to continue with the upgrade (6.1.0.417 to 14.0.0.299).\r\n-	Resintalled MAS Platform and Applications back with 6.1 MAS, however this time received java related issue due to windows corruption.\r\n-	Since the main obligation is to perform 6.1.0.417 to 14.0.0.299 upgrade, suggested SWD to continue with the upgrade without taking backup.\r\n-	Customer doesnt have any customized announcements, therefore backup is not really needed at his point.\r\n-	Gave the following action to SWD. \r\n-	There are only two manual actions needs to be done once the upgrade is completed.\r\n1-	Add SESM service addressed to MAS GUI.\r\nLogin MAS GUI once the upgrade is completed. \r\nGo to System Configuration  SIP  Nodes and Routes\r\nAdd 3 SESM service addresses both to Trusted Nodes and Routes.\r\nSESM Service IPs: \r\n189.210.92.6\r\n189.210.92.19\r\n189.210.92.22\n\r\n2-	Configure MAS clustering \r\nLogin to MAS GUI once the upgrade is completed.\r\nGo to Cluster Configuration  Server Designation\n\r\nMake 172.28.134.76 primary MAS, and 172.28.134.77 secondary MAS. \n\r\n-	SWD completed upgrade of 172.28.134.76, however customer wanted to continue 172.28.134.77 in the next maint window and same actions will be performed for the second MAS.\r\n-	Agreed with customer and SWD.','null'),(1020,'Senem Gultekin','AS-GW','2014-10-21','141021-499575','Axtel','Problem Description:\n\r\nSWD had server access problem during MAS upgrade from 6.1.0.417 to 14.0.0.299.\n\r\n14.0 MAS platform was installed with linux ( 14.0.26 Linux OS) and after the installation, SWD was not able to ssh nor ping the server.\n\r\nSolution:\n\r\n	Accessed to the site and all MAS servers (old releases) were pingable, except the new upgraded (newly installed) one.\r\n	We were only able to access to MAS server via console (Management Module)\r\n	Checked the physical IP and gateway IP. They were correct.\r\n	Checked /admin/userinfo.txt file and realized the VLAN was set as 0.\r\n	Requested the specbook from SWD to check the MAS VLANs. And the specbook had the MAS VLAN as 30.\r\n	Ran reconfigure.pl on the problematic MAS server. Set the VLAN as 30.\r\n	After the reboot, MAS server(172.28.134.74) was pingable and reachable.\r\nAlso during reconfigure.pl script asked the mtceGateway and mtceMachineIP. If you insert VLAN to the configuration these IPs are requested by the system.  Made them fake IPs (doesnt have an impact)\r\nmtceGateway=1.1.1.1\r\nmtceMachineIP=1.1.1.1\r\n	For the other MAS installation SWD will use again VLAN 30.\r\n	Agreed with SWD and ended the call.','null'),(1021,'Seren Batmaz','AS-GW','2014-10-18','141017-499134','Unity Media','Problem Description:\n\r\nER paged me for a Bussiness Critical alarm raised on one of BCP instances. The alarm description was as below:\n\r\nAlarmName: HA Layer Invalid Cluster Configuration \r\nTimeStamp: Fri Oct 17 21:38:09 CEST 2014 \r\nFaultNumber: 806 \r\nShortFamilyName: RTPB \r\nLongFamilyName: RTPBLADE \r\nSeverity: CRITICAL \r\nProbableCause: underlying resource unavailable \r\nDescription: Cluster is in a 1+0 configuration with 0 node(s) shutting down and \r\nshould be in a 2+1 configuration \r\nCorrective Action: Ensure all nodes within the cluster are operational. If not, \r\nmay need to restart the cluster nodes. \n\r\nSolution:\n\r\nI suggested to restart the problematic BCP instance from MCP GUI first. After ER did that, the alarm persisted. So, I suggested to restart other BCP instances in same cluster. As soon as ER completed that action, the alarm was cleared. Hence, we agreed to finish the call.','null'),(1022,'Oktay Esgul','AS-GW','2014-09-25','140925-495514','AT&T','Dean Gilbert from ER paged me to report that secondary GMS guest servers on both 2 host server are down  . There are 4 MS on site running on 2 genius IA-RMS servers, 2  for each.\n\r\n===============================\r\nAT&T Live Site\r\nGMS Version: 1.4 \r\n================================\n\r\nCustomer had rebooted the host servers due to maintenance purpose 2 days ago  but could not realized for a long time that some of GMS were greyed out on IEMS.(Secondary MS guests were totally unreachable OAM,CALLP,MEDIA ips were down)\n\r\nDean provided site access and then I connected the first IA RMS server to check the secondary MS on this server. \r\n===========================================================================\r\nNote: Genius platform is quite different than redhat platform that we are familiar,so you should follow up below path to login the server.\n\r\n1.Ssh to server as normal\r\n2.Login with mtc/mtc default username/passwd login credential\r\n3.You will be at cli when the first logined,you need to run \"sh\" to jump to shell.\r\n4.Then switch root user as normal ==> su - (li69nux is default root passwd)\r\n===========================================================================\n\r\nAfterward , logined successfully I jump to cli to check vm status.\n\r\nFirstly, tried to /lock/unload/load/unlock the VM server via below commands.\n\r\ncli>aim service-unit show all\n\r\nsg-name                    su  adm-state  opr-state  rdy-state       jam-state\r\n-------------------------  --  ---------  ---------  --------------  ---------\r\nDHCP                       0   unlocked   enabled    in-service      unjammed\r\nDsIpMgr                    0   unlocked   enabled    in-service      unjammed\r\nGAPS                       0   unlocked   enabled    in-service      unjammed\r\nGENBANDMS-VM               0   unlocked   enabled    in-service      unjammed\r\nGENBANDMS-VM2              0   unlocked   enabled    in-service      unjammed\r\nIPADDR                     0   unlocked   enabled    in-service      unjammed\r\nPKI                        0   unlocked   enabled    in-service      unjammed\r\nPMCentral                  0   unlocked   enabled    in-service      unjammed\r\nSolidWd                    0   unlocked   enabled    in-service      unjammed\r\nUSMMGR                     0   unlocked   enabled    in-service      unjammed\r\nVRDS_FILTER_RMS            0   unlocked   enabled    in-service      unjammed\r\nVRDS_NS_RMS                0   unlocked   enabled    in-service      unjammed\r\noamFmServer                0   unlocked   enabled    in-service      unjammed\r\nsecpdm                     0   unlocked   enabled    in-service      unjammed\r\nshelfmtc_shelf0            0   unlocked   enabled    in-service      unjammed\r\nstorageService             0   unlocked   enabled    in-service      unjammed\r\nsysmtc                     0   unlocked   enabled    in-service      unjammed\n\n\r\ncli>aim service-unit lock GENBANDMS-VM2 0 f\n\r\ncli>aim service-unit unload GENBANDMS-VM2 0\n\r\ncli>aim service-unit load GENBANDMS-VM2 0\n\r\ncli>aim service-unit unlock GENBANDMS-VM2 0\n\r\n==========================================================================\r\nI tried to ping the secondary VM after unlock completed,it failed .\n\r\n==========================================================================\n\r\nThen, tried to telnet to problematic virtual machine, but it refused.\n\r\nNote: Telnet port for VM servers can be listed with \"ps -ef |grep qemu\" as below.\n\r\nroot@typhoon-base-unit0:/root> ps -ef | grep qemu \n\r\nroot 4646 1 28 01:35 ? 00:39:11 /usr/local/bin/qemu-system-x86_64 -name GENBANDMS-VM-0 -smp 14 -cdrom /opt/vm/data/dvd_mcpcore0170_genbandMS.iso -drive file=/dev/mapper/ntvg-GENBANDMS_VM_0,cache=none,if=virtio -m 28000 -device pci-assign,host=04:10.2 -device pci-assign,host=04:10.3 -boot c -L /usr/local/share/qemu -cpu host -smp 14,sockets=1,cores=7,threads=2 -smbios type=1,manufacturer=Intel,product=S2600CO-KVM \n\r\nroot 9640 1 37 03:45 ? 00:02:32 /usr/local/bin/qemu-system-x86_64 -name GENBANDMS-VM2-0 -smp 14 -cdrom /opt/vm/data/dvd_mcpcore0170_genbandMS.iso -drive file=/dev/mapper/ntvg-GENBANDMS_VM2_0,cache=none,if=virtio -m 28000 -device pci-assign,host=04:10.6 -device pci-assign,host=04:10.7 -boot c -L /usr/local/share/qemu -cpu host -smp 14,sockets=1,cores=7,threads=2 -smbios type=1,manufacturer=Intel,product=S2600-KVM \r\n==========================================================================\r\nTelnet was not working for the second guest server for the first attempt, so I tried to modify telnet port via below command to see that this is a port problem.\n\r\ncli> vm-instance modify GENBANDMS-VM2 0 option \"-smbios type=1,manufacturer=Intel,product=S2600CO-KVM -cpu host -smp 14,sockets=1,cores=7,threads=2 -serial telnet:localhost:2700,server,nowait\n\r\n=============================================================================\r\n==> I was able to telnet if I try it asa unlock the guest server and this helped me to check the boot up logs of VM, It was booting up properly but then it s getting stuck with below message which means guest can not initialize properly.\n\r\n[/sbin/fsck.ext3 (1) -- /] fsck.ext3 -a /dev/VolGroup00/LogVol00\r\n/dev/VolGroup00/LogVol00: clean, 42221/34701312 files, 1493786/34693120 blocks\r\n[/sbin/fsck.ext2 (1) -- /tmp] fsck.ext2 -a /dev/VolGroup00/LogVol02\r\n/dev/VolGroup00/LogVol02 was not cleanly unmounted, check forced.\r\n/dev/VolGroup00/LogVol02: 13/6338368 files (7.7% non-contiguous), 209746/6332416 blocks\r\nConnection closed by foreign host.\n\r\n=============================================================================\r\nWe thought, to start guest server with Interactive mode while booting after Unlock , but could not succeed due to network speed that cause delay to hit \"I\" to start guest with Interactive mode.\r\n============================================================================\r\n==>I tried several times /lock/unload/load/unlock ,then somehow secondary VM became active properly and reachable .\n\r\n==> On the other hand, when the secondary one activated ,the first one became ofline-outofservice state somehow.\n\r\n=============================================================================\n\r\n==> We focused on this might be because of MAC address conflict on the server ,so tried to check  MAC setting via below command. All MAC configuration was correct based on GENIUS MS IM document.\n\n\r\ncli>vm-instance show all d\n\r\nsu-name                          = GENBANDMS-VM\r\nsu-number                        = 0\r\nframe                            = 0\r\nserver                           = 1\r\nsubslot                          = 0\r\nnic1-data                        = if=eth2;model=pci-assign;vf=virtfn0;pci=0000\r\n                                   :04:10.2;dev=8086:1520;id=0;MAC=02:A0:B0:11:\r\n                                   D0:01\r\nnic2-data                        = if=eth3;model=pci-assign;vf=virtfn0;pci=0000\r\n                                   :04:10.3;dev=8086:1520;id=0;MAC=02:A0:B0:11:\r\n                                   D0:02\r\nnic3-data                        = if=none;none\r\nnic4-data                        = if=none;none\r\nnic5-data                        = if=none;none\r\nnic6-data                        = if=none;none\r\nnic7-data                        = if=none;none\r\nnic8-data                        = if=none;none\r\nstatus                           = installed\n\r\nsu-name                          = GENBANDMS-VM2\r\nsu-number                        = 0\r\nframe                            = 0\r\nserver                           = 1\r\nsubslot                          = 0\r\nnic1-data                        = if=eth2;model=pci-assign;vf=virtfn1;pci=0000\r\n                                   :04:10.6;dev=8086:1520;id=1;MAC=02:A0:B0:11:\r\n                                   D0:11\r\nnic2-data                        = if=eth3;model=pci-assign;vf=virtfn1;pci=0000\r\n                                   :04:10.7;dev=8086:1520;id=1;MAC=02:A0:B0:11:\r\n                                   D0:12\r\nnic3-data                        = if=none;none\r\nnic4-data                        = if=none;none\r\nnic5-data                        = if=none;none\r\nnic6-data                        = if=none;none\r\nnic7-data                        = if=none;none\r\nnic8-data                        = if=none;none\r\nstatus                           = installed\n\r\n=============================================================================\n\r\n==> Current status was we could not make both VM activated on the same host server,so as a last option we rebooted the server as per customer approval.\n\n\r\nWhen the host server boot up after reboot, both VM were ofline- outofservice.\n\r\nI tried /lock/unload/load/unlock for each guest again,then both guest servers on the same host became active and reachable.\n\r\n============================================================================\n\r\nDean refreshed VM servers on IEMS and both guest server on primary host became green as expected.\n\r\n=========================================================================\r\nWe performed same procedure for the second host server and started the 2 vm on secondary host properly as well.\n\r\n======================================================================\n\r\nIn conclusion, all 4 MS VMs became reachable and functional and customer can monitor all of them from IEMS properly.\n\r\nWe discussed which logs should be collected for RCA investigation, then dropped the call.\n\r\nThanks','null'),(1023,'Burak Biyik','AS-GW','2014-09-05','140905-491833','Ohio State University','Chris Henwood reported that following the upgrade from MAS_6.1.417 to MAS_6.1.472, there was a critical alarm in MAS GUI causing MAS not to function properly. Here is alarm details;\n\r\n----------------------------------------------------------------------------\r\nId: 379 Severity: Critical Component: MAS Start/Restart Manager \n\r\nDescription: A component could not be startedDescription: A component could not be started Description: A component could not be started \r\nProbable Cause: The .exe file corresponding to the component could not be found in the MAS binaries folder, or the process-creation system call failedProbable Cause: The .exe file corresponding to the component could not be found in the MAS binaries folder, or the process-creation system call failed \r\n-------------------------------------------------------------------------------\n\r\n- First of all, this MAS application is installed on Windows OS. So, all user interfaces and installation files are a bit different from what is used for linux platform. (exe files are used here)\n\r\n- I opened MAS GUI by clicking \"Media Application Server Console\" icon on desktop.\n\r\n- I navigated to System Status-> Advanced-> Component Staus . This panel provides detailed information about main components of MAS. I saw that \"MAS Reporting Agent\" failed to start and was not functioning properly.\n\n\r\n- Since these components are installed with MAS platform, I suggested following steps to recover the system;\n\r\n    - Set MAS to Locked state\n\r\n    - Uninstall MAS Applications (Windows Start, Control Panel, Add/Remove Programs) --> clicked preserve data\n\r\n    - Uninstall MAS Platform (Windows Start, Control Panel, Add/Remove Programs)   --> clicked preserve data\n\r\n    - Install Windows 2003 hotfixes (specific for Windows MAS,e.g win2003_hotfix_.exe )\n\r\n    - Install MAS Platform\n\r\n    - Install MAS Applications (Adhoc, Annc)\n\r\n- Alarm were not presented on MAS GUI anymore. We were agreed to drop the call.','null'),(1024,'Burak Biyik','AS-GW','2014-09-02','140902-490938','Cable & Wireless (CALA)','Thomas Godwin paged me again that this time they have \"Loss of Communication\" alarms on CMT GUI while adding media proxies to GWCs.\n\r\nIt seemed that adding OAM VLAN to newly added BCP blades solved \"deploy\" problem but it looked like these BCPs are still not talking to GWCs.\n\r\n- Checked CallP, NET1, NET2 VLANs if they are added to ESM as configured in MCP GUI\n\r\n- Realized that these VLANs were missing for INT3 and INT5 ports on ESM(for blade3 and blade5). Added those to each ESM.\n\r\n- Performed a warm swact to associated GWCs\n\r\n- Alarms were cleared.\n\r\n- I dropped the call.','null'),(1025,'Burak Biyik','AS-GW','2014-09-02','140902-490938','Cable & Wireless (CALA)','Thomas Godwin from ER paged me to report an issue about BCP blade extension. Customer was adding two BCP 7200 blades to existing cluster but they were not able to deploy them after OS installation.\n\r\n- Since SM is responsible for deploying other NEs, I checked the connectivity between SM and BCPs. None of the BCP IPs (OAM,CALLP, NET1,NET2) were not pingable.\n\r\n- I checked assigned VLANs from MCP GUI (Network Data and Mtc -> Media Portal Data -> VLANs) and verified that there were seperate VLANs for each BCP networks\n\r\n- Customer was running MCP 9.1. I used admin/portal.cfg file instead of admin/userinfo.txt to check \"Netmask, gateway, VLAN\" info of the BCP servers. I realized that no OAM VLAN was assigned to these BCPs.\n\r\n- I run PortalConfig.pl (now it is reip.pl ) with Vlan parameter (since it is old platform release)\n\r\ne.g. PortalConfig.pl Vlan\n\r\n- I added OAM VLAN IDs as configured in MCP GUI.\n\r\n- I also added same VLANs on ESM. OAM VLAN was added to INT3 and INT5 ports of ESM (these BCPs were blade3 and 5)\n\r\n- After these operations, I was able to ping other NEs from BCP as well as deploy it. \n\r\n- Since the BCPs were successfully deployed, I dropped the call.','null'),(1026,'Yunus Ozturk','AS-GW','2014-09-01','140901-490853','Singtel Optus Pty Ltd','David Giomio paged the GPS and requested assistance on the MAS 14.0 to MAS 16.0 upgrade process. \n\r\nProblem Description:\r\n---------------------\n\r\n- Unable to restore the backup data after upgrading to MAS 16.0\r\n- Needs assistance on uninstallation of wrong installed the applications and re-installation of the announcement service only. \r\n- Invalid license key\n\r\nSolutions:=\r\n------------\n\r\n- On MAS 14.0 to MAS 16.0 upgrades, the backups have to be restored with the following scripts. These scripts should be run under /var/mcp/ma/MAS/bin/ directory.\n\r\nmasupgrade _{hostname}_{date}.zip\r\nmasupgrade _{hostname}_{date}.zip \n\r\nCustomer tried to restore the backups on the MAS EM GUI which is wrong (This is documented on the upgrade documents)\n\r\n- For the second item, customer installed the adhoc, moh and annc applications by mistake. We have uninstalled the existing applications with preserving the data and re-intall only the annc application. \n\r\n* Uninstalled the MAS Applications with the script under /var/mcp/admin/ directory. During the uninstallation, preserved the config data not to lose any data.\r\n* Installed the annc application only with the script under /var/mcp/media/ directory. During the installation, selected the Customized option to be able to install the annc application only. \n\r\n- For the third item, customer copied/pasted the wrong part of the license to the MAS EM GUI. Therefore, they did receive Invalid License alarm. Asked the customer to copy/paste part under \"LICENSE BEGINS BELOW THIS LINE\" on the license key. \n\r\nEven after the actions above, the upgrade completed.','null'),(1027,'Yunus Ozturk','AS-GW','2014-08-22','140822-489770','Unitymedia','ER contacted GPS to report a Management Module GUI access issue which has been started after the upgrade of Managemenr Module Firmware. Firmware was applied to the first MM (1) and restarted. The Managment Module was no longer reachable by HTPP or Ping. \n\r\nDuring the call, GPS informed the ER that this is actually not a pager issue as there was no service impact on the BCP blades even the MM GUI was not accessible. The MM GUI is just used for the administration/management of the IBM BCT Chassis and the blades on it. ER informed GPS that customer sent an engineer to the site to recover this issue and they requested assistance from GPS.\n\r\nTo recover this issue, the actions below have been taken;\n\r\n- Asked the customer to re-seat the MM Modules one by one. However, this did not fix the issue\r\n- Asked the customer plug-out both MM modules at the same time. Then, plug-in the Primary MM Module only and check if the GUI access is available. Afterwards, plug-in the Secondary MM Module. \r\n- If these actions do not work and since customer was not able to ping the MM Modules, we thought that there might be a IP/Network communication problem.\r\n- So, we have recommended the customer to \"reset\" the MM Module. This will set the IP configurations of the MM modules to the default factory settings and require fresh IP configurations with the help of KVM console access to the MM Modules. \r\n- Provided the required documentation and explanations to the customer to make these configurations.\r\n- Then customer has noticed that IP Addresses of the MM Modules have somehow changed back to their older IP addresses which have been changed last year.\r\n- After correcting the IP addresses again, the issue was resolved.','null'),(1028,'Yunus Ozturk','AS-GW','2014-08-22','140822-489781','SingTel (OPTUS)','ER contacted GPS to report that all of their BCP blades were down in their Sunshine office along with their SIP PBX trunk groups.\n\r\nThe customer found that there was a route missing in one of their CSLAN 8600\'s (8602) and they bounced the BGP link between the 8602 ERS8600 and their POP ERS8600 to restore the route. \n\r\nOnce the route was restored, the Admin state of the 11 BCP blades were at Offline status and 1 of their BCP blades was not reachable (no IP connectivity)\n\r\nTo recover the BCP blades that were at Offline status, the actions below have been taken;\n\r\n- Accessed MCP GUI\r\n- Killed the instances of the BCP blades\r\n- Started the instances of the BCP blades\n\r\nTo recover the BCP blade that was not reachable, the actions below have been taken;\n\r\n- Asked the customer to re-seat the non-reachable BCP blade and the IP connectivity came up back after reseating it\r\n- Killed/Started that BCP blade as well\n\r\nAfter all the BCP blades were up and running, we dropped the call..\r\n-','null'),(1029,'Yunus Ozturk','AS-GW','2014-08-22','140822-489781','SingTel (OPTUS)','ER contacted GPS to report that all of their BCP blades were down in their Sunshine office along with their SIP PBX trunk groups.\n\r\nThe customer found that there was a route missing in one of their CSLAN 8600\'s (8602) and they bounced the BGP link between the 8602 ERS8600 and their POP ERS8600 to restore the route. \n\r\nOnce the route was restored, the Admin state of the 11 BCP blades were at Offline status and 1 of their BCP blades was not reachable (no IP connectivity)\n\r\nTo recover the BCP blades that were at Offline status, the actions below have been taken;\n\r\n- Accessed MCP GUI\r\n- Killed the instances of the BCP blades\r\n- Started the instances of the BCP blades\n\r\nTo recover the BCP blade that was not reachable, the actions below have been taken;\n\r\n- Asked the customer to re-seat the non-reachable BCP blade and the IP connectivity came up back after reseating it\r\n- Killed/Started that BCP blade as well\n\r\nAfter all the BCP blades were up and running, we dropped the call..\r\n-','null'),(1030,'Seren Batmaz','AS-GW','2014-08-17','140815-488496','OneConnect','Problem Description:\n\r\nChris Wadden paged A2 GW pager for the problem occurred during preupgrade steps of MAS upgrade. Two voice mail MAS\' were unable to take backup. There had been an SFDC case(140621-478831) and Jira Issue for this problem and it was fixed in release that the customer already had.\n\r\nSolution:\n\r\nI connected to the site and reached customer\'s EM GUI, when I tried to run \'full backup\' which was created by customer with all of the backups(including System Configuration, Application Content and Network Data), it failed immediately. After that, I created separate backup tasks for each of them. By that way, I was able to take System Configuration and Application Content backup. However, I wasn\'t able to take Network Data backup.\r\nIn the meantime, Chris was insisting on calling Yunus who was the case owner from OAM and GW GPS team. When I said it was 5am in Turkey, the problem occurred during preupgrade steps, normally GPS shouldnt had been paged for preupgrade steps and it had no service impact, Chris left the call.\r\nAfter a couple hours Yunus connected to the site and investigated the case. After a few attempts, he was able to take the backups after creating more space on voice mail MAS servers. The issue will be investigated by design to indicate the root cause and work on the fix again.','null'),(1031,'Seren Batmaz','AS-GW','2014-08-14','140814-487840','Yadtel','Problem Description:\n\r\nSWD paged A2 GW GPS pager due to the problem occured after platform upgrade of a MAS Server. Server became unreachable after its platform upgraded to 14.1.13 ple2 platform level and rebooted itself. SWD had already tried hard reboot on MAS blade.\n\r\nSolution:\n\r\nI requested direct console access to the server to be able to see the failure occured during boot up and connected to it through IBM BCT Management Module.\r\nWhen I check the console, I saw that the server were not booting up due to file system errors. The error message requested to manually scan the file system.\r\nHence, I ran \"fsck\" for each file system to repair them. Just after all file systems were repaired, I restarted the server and it successfully became up and running. \r\nFinally, I requested SWD and the customer to check if they can access to MAS EM GUI. As soon as they reported it was working properly, I left the call.','null'),(1032,'Burak Biyik','AS-GW','2014-08-12','140812-487231','Yadkin Valley Telephone','Chris Henwood from SWD paged me to report the inability to stop/start \"mysql.server\" service on MAS server. Because this service should be running properly, it was blocking MAS upgrade (from 16.0.0.774 to 16.0.0.802). Here the errors I get when trying to check status and start \"mysql.server\" service.\n\r\n--------------------------------------------------------------------------------------------------\r\n# service mysql.server status \r\nERROR! MySQL is not running, but lock exists \r\n# service mysql.server start \r\nStarting MySQL.. ERROR! Manager of pid-file quit without updating file. \r\n---------------------------------------------------------------------------------------------------\n\r\n- First I tried to stop mysql service first and then start. It failed with the same error.\n\r\n- Then I looked at the similar cases on salesforce to have an idea but could not find the same errors.\n\r\n- Made a quick search on the web. Checked if there is any difference in mysql configuration files (/var/mcp/ma/MAS/MySql/my.cnf) between problematic and working MAS server cause this problem could be because of conflicting entries in \"my.cnf\" file. There was no difference between them.\n\r\n- I rebooted the server and made \"mysql\" service running without any issues for awhile. Then I realized I was not able to connect to GUI although \"jboss\" service was running. So, the same errors came back again. When I was able to open EM GUI at intervals, it was not possible to get any data from database after I click something on navigation panel.\n\r\n- Due to unstable conditions of \"mysql\" service, I decided to reinstall existing MAS release. Followed steps below;\n\r\n --> uninstalling MAS applications and platform respectively (files are located under var/mcp/admin for MAS16)\n\r\n --> installing MAS platform and applications (only adhoc) (installer files are located under /var/mcp/media)\n\r\n --> Adding SIP trusted nodes/routes from MAS EM GUI\n\r\n --> Applying license key \n\r\n- \"mysql.server\" service was running successfully and customer was able to launch EM GUI without any issues.\n\r\n- Agreed to end the call and continue MAS upgrade.','null'),(1033,'Oktay Esgul','AS-GW','2014-06-18','140615-477833','Cable & Wireless (CALA)','Rodney Neese from ER paged me again on Wednesday morning to report that no media  for sip to sip calls and Cicm calls as well . After that time Yunus,Seren and I worked nonstop on this Lime outage till Friday night.\n\r\nMCP Load : MCP_9.1\n\r\n===========================================================\r\nFirst Day:June 18th\n\r\nAll day we worked with Er and ERS team to recover BCPs when they were down,since they were becoming down almost every hour.We joined call conferences with customer and support team to answer their question during the day.\n\r\nDuring this time period, we did not have any root password since somehow customer did not know the passwords,so only recovery option was rebooting the BCP blades if we could access to Management Module(MM was sometimes getting down due to network issues)\n\n\r\n============================================================\r\nSecond Day:June 19th\n\r\nSame situation had continued for the second day.\n\r\nDuring second day,we recovered servers password and this helped us to provide detailed network data to ERS and network support teams.\n\r\nAfter getting root passwords and able to run mptool -l command on BCP  server ,we realized that for bcp7200s even  primary instance seemed offline on MCP gui,it was the active BCP and call were routed by primary BCP.\n\r\nIn Addition to mptool output,we provided below data to network team to help their investigation.\n\r\nTherefor,genband sent some network folks to site to  expedite the recovery progress.\n\r\n[root@blade11 sbin]# netstat -rv\r\nKernel IP routing table\r\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\r\n192.168.3.11    *               255.255.255.255 UH       40 0          0 eth1\r\n200.50.91.87    *               255.255.255.255 UH       40 0          0 eth0\r\n192.168.39.64   *               255.255.255.240 U        40 0          0 eth1\r\n200.50.91.80    *               255.255.255.240 U        40 0          0 eth0\r\n192.168.3.0     *               255.255.255.0   U        40 0          0 eth1\r\n127.0.0.0       *               255.0.0.0       U        40 0          0 lo\r\ndefault         200.50.91.81    0.0.0.0         UG       40 0          0 eth0\n\r\n[root@blade11 sbin]# route -e\r\nKernel IP routing table\r\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\r\n192.168.3.11    *               255.255.255.255 UH       40 0          0 eth1\r\n200.50.91.87    *               255.255.255.255 UH       40 0          0 eth0\r\n192.168.39.64   *               255.255.255.240 U        40 0          0 eth1\r\n200.50.91.80    *               255.255.255.240 U        40 0          0 eth0\r\n192.168.3.0     *               255.255.255.0   U        40 0          0 eth1\r\n127.0.0.0       *               255.0.0.0       U        40 0          0 lo\r\ndefault         200.50.91.81    0.0.0.0         UG       40 0          0 eth0\n\r\nFurthermore,we provided  how to recover procedure when BCPs down to ER to avoid new pager calls .On the other hand, they paged me if they observe any strange behaviour on the system to get explanation of the behaviour to provide customer.During the day they paged me several times.\n\n\r\nERS and network teams had continued the investigation during the day and onsite team had arrived to site and started working.\n\r\nBased on the investigation,rca was arp tables for BCPs server on ERS were  being removed somehow ,so BCPs were coming down due to network connection lost and reboot was recovering this situation.\r\n=====================================================================\r\nThird Day: June 20th\n\r\nActive investigation continued during the day with network teams to figure out why arp table is getting empty.Therefor,to avoid loosing network connection due to arp tables, newly created perl script that send auto ping to ERS provided to onsite team and applied for all BCP server.\n\r\nWe joined conference calls and provided updates and answered all BCP related question.\n\r\nAlso, we asked customer to swact SM instances since BCP state were not stable at MCP gui even they were active.After swact,BCP state were stable at MCP gui,this was reported another network issue to network team..\n\r\nFurthermore,during the day onsite team realized that this ARP problem was not only for BCP but also for GWC as well.So,everybody aggreed that this is ERS problem since it somehow  clears arp tables.ERS ,GWC and onsite team worked on the issue.\n\r\n================================================================ \r\n4th day: June 21th\n\r\nER had recovered the BCPs with provided procedure,but if they ran into any problem they paged to get support .\n\r\nAfter applying the script that send auto ping to keep arp tables,we did not observe the issue for 12 hours,but then problem reoccured once..\n\r\nSince all focus of investigation was on ERS and BCP perspective of view there were nothing further to investigate and also since we provided required recovery procedure to ERs,I dropped the call.\n\r\nThen,keeped continue to monitor the system for awhile.\n\r\nIn conclusion,problem still persists and active investigation on ERS and Network side is ongoing,ERs are applying the procedure if BCPs become down..','null'),(1034,'Oktay Esgul','AS-GW','2014-06-21','140621-478831','OneConnect','Peter Maloney from SWD paged me to report that during MAS upgrade process at OneConnect,upgrade of 4th MAS that the secondary of UComm cluster failed with following error ..\n\r\n==============================================================================\r\nUninstall Error\r\n---------------\n\r\nRemoval of the GENBAND Media Application Server Platform has failed. Found\r\nrunning app.\n\r\nAll installed applications must first be removed.\n\r\nYour system has not been modified. \n\r\n==========================================================================\n\r\nMCP Load : MCP_17.0.3.13\r\nMas Ple2: 17.0.11\n\r\nMas Upgrade Path :\r\nFrom :MAS 16.0.0.802\r\nTo:  MAS 16.0.0.821\n\r\n=========================================================================\n\r\nPeter reported that he has not encountered any problem during first 3 MAS upgrades.\n\r\nHe provided teamviewer then I logined  and checked the system .\n\r\nChecked the history of root,and verified that Uninstall_Mas_Applications had been run via root user .(Checked the history since I found out that another similar case and rca showed that uninstallation performed via ntsysadm,and this issue fixed by ds in earlier release then OneConnect had allready)\n\r\nRun ps -eaf |grep java to check running java processes on the server and killed all running  java processes,then retried and failed again.\n\n\r\nChecked the sfdc solution for similar issues and found that 140205-453967 is similar issue and resolved with document updates,then checked the documents but the solution s for if the system has custom announcements even OneConnect does not have CustomAnnouncements folder in their system.\n\r\nThen rebooted the server just in case if there is undesired running process,it failed again.\n\r\nSince MW is getting end,we decided to reinstalled old version application , after reinstallation, it took almost 30 minutes data resync between primary and secondary MAS.\n\r\nWhen the resycn  is completed ,customer made several test during 30 minutes and verified that system is stable and everything working fine.\n\r\nBased on this statement  on MAS fault management document  \"All servers within a cluster must ultimately be on the same release version. However, other MAS clusters do not needto be on the same software release version and can be upgraded independently.\" \n\r\nDifferent application levels are undesired,but  at currect configuration  while primary ucomm MAS is at 16.0.0.821 the second one is at 16.0.0.803 to by pass E2 condition.\n\r\nIn conclusion,all tests passed even this undesired configuration and customer approved to move the investigation forward with design on the first busineess day.Afterward aggrement I dropped the call.','null'),(1035,'Senem Gultekin','AS-GW','2014-06-13','140613-477630','Liberty Global','Problem Description:\n\r\nAfter site engineer performed BCM and BIOS firmware upgrades to BCP0-1, the instance didnt come up.\r\nRelease: MCP-12.0.12.5\n\n\r\nSolution:\n\r\n-	Accessed to the site and checked the problematic BCP0-1, it was getting stuck during deploying action.\r\n-	Tried to ssh the BCP0-1 but it was not working, and it was not pingable at all.\r\n-	After few reboots the result was the same.\r\n-	The is console connection to the BCP0-1 server, but there is no network output for this server. Therefore it cannot communicate with any of SM, SESM or other BCPs.\r\n-	Both eth0 and eth1 seemed up. Eth0 was active one. Brought eth0 down to see if its an interface problem. At that point eth1 was active. And the server became pingable.\r\neth0 is active -> no network, no ping\r\neth1 is active -> network connection, pingable\n\r\nHere is the interface for this BCP0-1 server\r\n------------------------------------------------------------------------------------------\r\n[root@nl-ams02-ssw01-BCP0-1 bonding]# cat bond0\r\nEthernet Channel Bonding Driver: v3.2.4 (January 28, 2008)\n\r\nBonding Mode: fault-tolerance (active-backup)\r\nPrimary Slave: None\r\nCurrently Active Slave: eth1\r\nMII Status: up\r\nMII Polling Interval: 100\r\nUp Delay: 0\r\nDown Delay: 0\n\r\nSlave Interface: eth0\r\nMII Status: up\r\nLink Failure Count: 1\r\nPermanent HW addr: 00:14:5e:3e:47:3c\n\r\nSlave Interface: eth1\r\nMII Status: up\r\nLink Failure Count: 0\r\nPermanent HW addr: 00:14:5e:3e:47:3d\r\n------------------------------------------------------------------------------------------\r\n-	Since when eth1 is active everything is working properly, it could be physical, and suggested the network engineers to investigate the issue.\r\n-	There are no call issue or BCP instance failures after the eth switch.\n\r\nNext Action:\r\n-	Continue with the next BCP firmware upgrades. Before upgrading one of the BCPs perform the following actions to verify that both interfaces are working properly before the firmware upgrade.\n\r\n1-	First cat the current interface;\r\n#cd /proc/net/bonding/\r\n#cat bond0\r\nSave the output.\n\r\n2-	Restart the interfaces. Check the output, whichever interface is active, bring that down with the following command;\r\n#ifconfig ethX down     	-> this will bring the other ethY up.\r\n#ifconfig ethX up0	-> this will bring ethX as backup interface.\n\r\n3-	Save the interface output.\r\n#cd /proc/net/bonding/\r\n#cat bond0\r\nSave the output.\n\r\n4-	Ping the server and see if its pingable with the other interface. This will verify that both interfaces work before the firmware upgrade.\r\n5-	Perform the firmware upgrade, and monitor if the server is reachable. If not perform the interface up and down from the console.','null'),(1036,'Cigdem Vural','AS-GW','2014-06-18','140616-477982      ','WARWICK VALLEY TEL','That is an ongoing issue from yesterday.\nCustomer has problems with ERS and got hacked, after that they have all BCPs down and could not reach BCT GUI.\n\nMCP Load is MCP 10.1.x.x\n\nSite has 4 BCPs and 3 of them were up but one was down.\nWhen we checked for the down BCP, we saw that from SM the routing is failing at IP 10.1.26.3 which is ERS 8600-1 IP. That route is OK for the working BCPs.\n\nFor a working BCP:\n\n[sysadmin@WRWKMASSLM2 /]$ traceroute 10.1.27.10\ntraceroute to 10.1.27.10 (10.1.27.10), 30 hops max, 38 byte packets\n 1  10.1.26.3 (10.1.26.3)  7.668 ms  13.951 ms  14.669 ms\n 2  10.1.27.10 (10.1.27.10)  0.342 ms  0.159 ms  0.130 ms\n\nFor a down BCP:\n\n[sysadmin@WRWKMASSLM2 /]$ traceroute 10.1.27.11\ntraceroute to 10.1.27.11 (10.1.27.11), 30 hops max, 38 byte packets\n 1  10.1.26.3 (10.1.26.3)  8.083 ms  13.812 ms  14.554 ms\n 2  **\n\nSo told ER that ERS is blocking the BCP IP so from MCP GUI, we are not able to reach to that BCP to make it active.\n\nFor BCT GUI, we requested them to pull off standby MM so check for the modules one by one. When he pulled off the standby module, no change. and then he pulled the active one and insert only standby and we could reach to BCT GUI.\n\nSo active MM was bad and need to be replaced.\n\nAt MCP GUI, first 4 blades were OK and stable but the other 4 baldes were not stable. They were failing with these states ==> Init Failed & Comm Error\nThen asked customer to reseat these blades and power on physically. If they are not seen as On at MCP GUI push the reset button on blade. Then restart from the BCT GUI. After all these actions all blades became stable at GUI.\n\nER bounced the ports at ERS for the BCP3 which was down and it became active.\nThen other 3 BCP became down, ER again bounced them at ERS and they become available too.\n\nThen checked for MAS servers and they were all OK.\n\nSo explained the BCP issues that ERS is blocking them and SM-BCP communication is lost. ERS GPS told the customer engineering has problems and network is not secure.\n\nCustomer network then has some issues with GWC communication. We did not have any action left so dropped off.\n\nSo as A2 team we provide action as to replace the broken BCT MM','p2'),(1037,'Oktay Esgul','AS-GW','2014-06-18','140615-477833','Cable & Wireless (CALA)','Tom Draper paged me to report that CICM gps suspecting BCP cause CICM call failures with  following erros respond at tapi traces.\n\r\n************** ITA Exception Msg ***************\r\n000:00:34:42.95 5341 ,L--,K00   31> CB: ITA Exception msg:   Current State: SlaveWaitItaUpdate Context: 1\r\n000:00:34:42.95 5341 ,L--,K00   31> CB: Enter State: SlaveFailed Context: 1\r\n000:00:34:42.95 5341 ,L--,K00   31> CB: CSM ConnFail: Byte1: 0 Byte2: B6\r\n000:00:34:42.95 5341 ,L--,K00   31> CB: CSM ConnFail: Byte3: 40 Byte4: 16\r\n000:00:34:42.95 5341 ,L--,K00   33> CSM: Hdl CB MT: GCSM_CB_FAILURE Mode: cbAlloc Ntg: ntg_lfntg\r\n000:00:34:42.95 5341 ,L--,K00   33> CSM: Rep **NTG NOT FOUND**: Ret: 141  Mode: cbFailCon Ntg: ntg_lfntg\r\n000:00:34:42.95 5341 ,L--,K00   33> CSM: Ev: cbFailure : cbAlloc -> cbFailCon Ntg: ntg_\r\narmansilahli: we are having ITA exception\r\narmansilahli: might have a MP connection problem \r\narmansilahli: or BCP\r\narmansilahli: ok seems MP`s are pingable from GWC side\r\narmansilahli: but we have the failure that GWC cannot insert the MP..\r\narmansilahli: \r\n!/1 [192.168.32.25]:2944 P=33233{C=1558{A=te/0860{M{L{v=0\r\no=cicm 847387 847387 IN IP4 0.0.0.0\r\ns=-\r\nc=IN IP4 172.19.211.50\n\n\r\nI have logined back to system and even BCPs were up we were getting this failure.Gwc and CICM gps tried to restarted GWC but it did not fix the problem.\n\r\nDuring investigation we realized that CICM is using different BCPs than we work for the E1 issue..And finally customer report that They have 2 extra BCPs very old version BCP7100 running on their second MCP gui that CICM trying to use.\n\r\nThen,they proved MCP gui details , I logined and verified those extra BCPs were down.\n\r\nThen, I connected the blades and run neStop for both blades and redeployed them.Then became active properly.Customer ran basic call tests ,while some of them has only one way speech path,some of them nearly %50 has successfull voice path.\n\n\r\nWe tried to run mportal - l to check rtp packages over BCP,but the command did not work since the BCPs are very old.\n\r\nWe tried cold swact for gwc ,but it did not fix the problem.\n\r\nEven BCPs are very old version,BCP point of view everythin seemed working properly . \n\r\nER checked ERS setups and figured out another ERS problem,he started investigating the issue and customer report cpu utilization is high again since they removed all rules those added during all day investigation.\n\r\nEr and ERS team is working again to solve ERS issues,since BCPs are ok ,I aggreed with ER and dropped the call.\n\r\nThanks','null'),(1038,'Oktay Esgul','AS-GW','2014-06-17','140615-477833','Cable & Wireless (CALA)','Marc Zattiero had paged Ozgur since SIP Calls failing  ,when he first tried to access the MCP gui,he had got USERID failure .So,Marc paged OAM gps to reset the password for MCP .\n\r\nMCP LOAD LEVEL : MCP_9.0\n\n\r\nSenem and Seren handled this pager since I was working on Walwick E1 pager,they checked the server disk usage via df -k and figured out that var/mcp/ is full .Afterward clearing  unused data from server and restarting SM instance ,USERID problem was resolved.\n\n\r\nThen,they checked and verified that BCPs are down and ERS team involved again since this ERS problem has allready reported and ERS team working on this issue allready.Ers team added missing rules from ERS gui .When the all rules are set properly BCPs became active again.But, like the other ERS issue on Walwick ERS team realized that CPU usage is coming %100 sometimes, so they checked and verified there is attack to ERS on this site as well.So, they started blocking unknown IPs from firewall.ERS team was suspecting BCP communication and during investigation on ERS,BCP blades became unavailable  again and they worked on this issue while blocking external attack to ERS.\n\r\nEven server ips could be pingable,BCPs were down  and we were back E1 condition again.We asked customer to provide username/passwd to connect to server to check the BCPs since I could do nothing from MCP GUI due to ned issues.But,somehow we could not find someone who knows the login credential of the servers.\n\r\nBCP1 was active but not acting properly and BCP2 was at Configured status.Media Resource unavailable alarms raised at SESMs  and all calls were failing.I joined customer bridge .\n\r\nThey tried to make terminal connection to bypass username/passwd issue ,they connected server and provide me access details but When i checked the servers,realized that the server we connected is not the correct one.\n\r\nMeanwhile we were trying to access to server with Yunus,we just tried to deploy server again and somehow we could deploy BCPs properly at last attempt,then start the BCPs and they became active.Media resources alarms cleared.\n\r\nWe asked ERS team to check if they did any config changes during investigation that can recover network issues for bct blades, but we could not find what triggered this network recovery.\n\r\nCustomer made basic call tests and reported that all calls passing except CICM calls,so they paged CICM GPS to get support.\n\r\nOn the other hand ,Yunus and I checked MCP gui alarms and observe MAS alarms ,then connect each MAS(Windows Mas) and restarted them,alarms cleared.\n\r\nThe only alarms left on MCP gui was DB replication alarms,we discussed with ER and decided to follow up this replication alarms via a follow up case since all ER and customer engineers focused on CICM outage.\n\r\nEventually, all OAM ang GATEWAY component are working properly recently.\n\r\nERS and CICM gps teams working to recover ERS and CICM .\n\r\nAfter aggreement with ER,we dropped to conference.\n\r\nThanks everyone for cooperation..','null'),(1039,'Oktay Esgul','AS-GW','2014-06-17','140616-477982','WARWICK','Jeff Brennan from ER paged me to report BCP blades are down at WARWICK system after ERS (8600)card replacement and reconfiguration.\n\r\nMCP Level : MCP_10.1\n\r\nI logined the system and verified that all BCPs and MASs are down those all running on same BCT chassis.\n\r\nI tried to login MM module but we could not since network slowness.Then, ERS team checked the ERS and changed some configuration ,MM start to list bct blades.Afterward Blade1&Blade2 listed on MM,I powered on the blades and  BCP1&BCP2 became active (E1 condition was ended).Even BCP1&2 were activated, BCP3&4 and all MAS were still down.\n\r\nDuring the day, ERS team has investigated the issue and realized that there is DDOS attack to ERS that triggers cpu utialization threshold icreasing to %90-100.Customer blocked the ip addresses .Afterward ips blocked,cpu usages decrease to normal level but during they ERSs were under attack.Meanwhile ERS gps was investigating configuration BCP4 became accessable and active again.\n\r\nSince E1 condition is resolved for BCP (MAS still down) and we had another E1 pager regarding same 8600 problem from LIME ,ERS team stopped working on Warwick system. After aggrement to investigate issue further by ERS Gps tomorrow, I dropped call since all team has aggreed this is network issue and nothing further investigation on gateway side.\n\r\nThanks','null'),(1040,'Cigdem Vural','AS-GW','2014-06-11','140611-477307','Paltel','Gary called by telling customer has an E1 and BCPs has the problem. No calls are successfull.\n\r\nThey have moved the BCPs to another place and calls failed then they moved them to original location but problem was not cleared.\n\r\nCall from PVG to Sipline working\r\nSipline to PVG is not working\n\r\nAll BCPs were UP and running. Checked with CallP engineers and it seems problem was related only one domain:ramallahcs2k.paltel.ps\n\r\nWhen we checked the media routability group at Prov GUI it was set for none group for this domain. We set it as ramallah and after that tests are passed. E1 ended.','null'),(1041,'Oktay Esgul','AS-GW','2014-04-12','140412-467780','OneConnect','Kyle Mawst paged me to investigate the MAS EM GUI can not be accessible after server replacement by site engineer,even they tried to reboot the server.\n\r\nI accessed the system and check the problematic Mas Server (HT Langley,MCP_17.0.11,Media Application Server - v.16.0) , the primary MAS EM is working but  secondary one could not be accessed .Login the MCP gui and check the MAS status from gui and alarms,then login the server and check the network.I ran service network restart to ensure all networks setups up and running for server ,then check the Jboss service is running properly and run \"service Jboss restart\" .Then, I asked the ER ,both of primary and secondary server are in same network or different network,he asked the site engineer and we realized that servers are pointed to different switch in network,so that we moved the problematic server to other network that primary is working properly and verified  gui is accessible in other network.After this,site engineer checked the second network switch that we had problem and reconfigured it.After that when we moved the server to original network ,problem is resolved and EM is accessiable and then I dropped the call.','null'),(1042,'Yunus Ozturk','AS-GW','2014-02-26','140226-457173','Vodafone New Zealand Ltd','Problem Description:\n\r\nUpgrading A2 from 14.1.0.10 to 14.1.8. Half of the systemn has been upgraded and active. Upgrade Wizard started upgrading the second half of A2. It applied Linux patches to BCP blade # 5, # 6 (they are in cluster), EM Server2 and SESM server 2. Then it rebooted them. \n\r\nBCP blade #6 failed to come up after reboot. It stuck at POST waiting on MM and complaining about MM timeout. \n\r\nSolution:\n\r\n- Rebooted the problemactic BCP Blade but this did not fix the issue\r\n- Asked the customer to re-seat (plug out/plug in) the blade on the IBM BCT Chassis. This fixed the issue and the connection came back','null'),(1043,'Senem Gultekin','AS-GW','2014-02-07','140207-454233','Timico','Problem Description:\n\r\nSWD was having problems during MAS rollback at Timico;\n\r\nMAS s/w -  16.0.0.802  -> 16.0.0.707 \r\nOS -  14.1.13 -> 14.1.9 \n\r\nHe was following document 630-01217-02 06.02.\r\nstep 8 Install the previous release MAS platform\n\r\nFailure is;\n\r\n0;root@MAS-ANNC-T-01:/var/mcp/media[root@MAS-ANNC-T-01 media]# ./MAS_Platform_Installer_16.0.0.707_2011.09.08.bin\r\nPreparing to install...\r\nExtracting the installation resources from the installer archive...\r\nConfiguring the installer for this system\'s environment...\n\r\nLaunching installer...\n\r\n===============================================================================\r\nMAS_Platform                                     (created with InstallAnywhere)\r\n-------------------------------------------------------------------------------\n\r\nPreparing CONSOLE Mode Installation...\n\n\n\n\r\n===============================================================================\r\nInstallation Cancelled\r\n----------------------\n\r\nAttempt to install the same version. Please cancel the installation\n\r\nPRESS  TO ACCEPT THE FO\n\n\r\nSolution:\n\r\n-	Checked the document steps one by one, which was done by different customers several times there wasn\'t any odd step.\r\n-	Accessed to the site and checked the file permissions and all MAS versions they seemed normal.\r\n-	While working with SWD we realized that there was an existing mas.prop under /etc directory as following;\r\n0;root@MAS-ANNC-T-01:/etc[root@MAS-ANNC-T-01 etc]# cat mas.properties \r\nmas=16.0.0.707\r\ninst_dir=/var/mcp\r\ninstalled=yes\r\nmysql=/var/mcp/ma/MAS/platdata/MySQL/\r\nannc_app=16.0.0.707\r\nmoh_app=16.0.0.707\r\ninstalled_apps=16.0.0.707\r\n0;root@MAS-ANNC-T-01\r\n-	This file was already was stating that MAS is 16.0.0.707 and MAS installer was failing.\r\n-	Edwin renamed the file as mas.properties.org,  the installer was not able to see any current version and it was success.\r\n-	Edwin continued with Platform and Applications installer for MAS.\r\n-	After all the steps were completed, customer performed test calls. Most of them successful, but some scenarios were failing. However they are not sure if those scenarios were working before the upgrade.\r\n-	They will open a separate case for the failed scenario.\r\n-	After the MAS rollback was completed our job was done. \r\n-	Left the conf call.','null'),(1044,'Senem Gultekin (NETAS External)','AS-GW','2013-12-23','131221-447522','Unitymedia','Problem Description:\n\r\nER paged me for a MAS issue seen at Unitymedia. One of the MAS servers were replaced with another existing MAS, however there were alarms on the system and seemed like the replaces MAS not functional.\n\r\nCustomer is running on 12.0.12.5 MR\r\nMAS version is 6.1.0.463\n\r\nSolution:\n\r\n-	Accessed to the site and checked the alarms both on MCP GUI and MAS GUI.\r\nPool Unavailable (ContentStore)\r\n-	When we checked from the replaced MAS GUI, even tough the MAS physical IP was 80.69.109.212, the Server Address in Element Status was 80.69.109.91.\r\n-	80.69.109.91 IP is for another MAS which does not belong to this system. Once there was a problem with the current MAS(80.69.109.212), the other MAS was put(80.69.109.91)\r\n-	Since 80.69.109.91 was still there MASs were not seeing eachother.\r\n-	Accessed to MAS as remote desktop (80.69.109.212) changed it from the following  config;\r\nD:\\Program Files\\Nortel\\MAS\\common\\Nortel MAS Console.msc\r\nNortel MAS Console -> Configuration -> IP Interfaces ->  IP Interface Assignment For The Signaling, Media, Cluster Traffic Classes\r\nModified 80.69.109.91 as 80.69.109.212. \r\n-	Saved, configured 80.69.109.212 MAS as secondary from MCP GUI, Element Status.\r\n-	Restarted the MAS.\r\n-	After the restart, Alarms were cleared. However calls were not coming to this MAS, license key was not correct. Site Engineer and ER sent MAC addresses to KRS team to generate the new license key.\r\n-       After the new license key was applied everything was back in normal.','null'),(1045,'Yunus Ozturk (NETAS External)','AS-GW','2013-12-06','131122-443309','Bermuda Telephone Company','Problem Description:\n\r\nER paged and informed us that the customer has an E2 situation on of the ESM units. The problematic ESM units was rebooting itself for each 4 minutes. When the customer disconnected the connection cables of this ESM unit on the ERS8600, the ESM reboots stopped. They have requested assistance to replace this problematic ESM unit. \n\r\nActions Taken to fix the problem:\n\r\n- Before replacing the problematic ESM unit, we have tried to get the config data of the working ESM unit and put it to the non-working ESM unit. However, this did not change the situtaion. When the customer connected the connection cables to the ERS8600, the ESM unit started to reboot itself again.\r\n- We have provided the required steps to the customer to replace the problematic unit. \r\n- When the hardware replacement is completed, we have dumped the image file of the working ESM unit to non-working ESM unit since the new replaced ESM unit had a different image version.  \r\n- Then, we tried to put the config data of the working ESM unit to this new replaced ESM unit. However, It somehow did not work and we got an error message \"Invalid Configuration file\".\r\n- Since we could not put the config file to the new ESM units, we configured the new ESM unit manually from scratch by comparing with the working ESM unit configuration. \r\n- When the configuration is completed, the issue is resolved and the ESM unit became stable.','null'),(1046,'Senem Gultekin (NETAS External)','AS-GW','2013-11-22','131122-443294','Bell Aliant','Problem Description:\n\r\nAfter upgrading MAS to 16.0.0.802 Release, Bell was not able to see Load Balancing option in the MCP GUI.\n\r\nTheir lab was in the same load but they were seeing Load Balancing there.\n\r\nSolution:\r\n-	I was on my way to work so it took around 50 min to get back touch with SWD.\r\n-	Accessed to the site checked MAS and Prov configurations. \r\n-	Realized that Load Balancing is not configurable after MAS16 Release. It\'s design intent.\r\n-	Talked with design and found out that to show Load Balancing on MAS GUI, we have to update the table from the MAS database.\r\n-	Performed the following steps on both MAS servers.\n\r\nLogin to MAS server and drop to the mysql screen first;\n\r\n1-	Login to the server as root user and run mysql\r\n#mysql -u root\n\r\n2-	To update the tree ran the following query to change show_in_nav_tree parameter.\r\nUPDATE eam.nav_task SET show_in_nav_tree=\'1\' WHERE task_id =\'e90fb020-d0f4-11dd-bb98-0002a5d5c51b\';\n\r\n3-	After the update logout and login to MAS. No need for a restart.\r\n4-	Customer was able to see Load Balancing on MAS GUI and performed tests for 30 min, all success.\n\r\n-	Agreed with SWD and pager call ended.','null'),(1047,'Seren Batmaz','AS-GW','2013-11-21','131121-443087','Axtel','Problem Description:\n\r\nER paged A2 OAM pager regarding calls were progressing on BCP after ERS8600 upgrade similar with the case 131018-436404.\n\r\nSolution:\n\r\nI recommended rebooting BCP blades first. After rebooting the BCPs, issue has been resolved.\n\r\nI asked ER to call A2 GW pager for next BCP related issues and left the call.','null'),(1048,'Joyce Lyon','AS-GW','2013-10-28','130710-414278','Singtel Optus Pty Ltd','At the top of busy hour (3 & 4 pm), customer experienced poor voice quality.  Customer had action plan to capture packet capture on BCP, but needed clarification on execution and other assistance.  Captured data, but unsure if data is sufficient.  Capture could only be brief as running the tcpdump causes the BCP to go into overload within 30 to 45 seconds and also causes call failures.  Still waiting for the logs to be shipped over for analysis.','null'),(1049,'Senem Gultekin (NETAS External)','AS-GW','2013-10-04','131004-432702','Claro Codetel (Dominican Republic)','Problem Description:\n\r\nSWD paged for a MAS upgrade issue.\r\nUpgrade path: MAS 14.0.0.299 to 16.0.0.798 (CVM14 to CVM16) \n\r\nHe has followed the normal procedure, however the following issue occurred during installing MAS 16.0.0.798;\n\r\n===============================================================================\r\nInstall Failed\r\n--------------\n\r\nThe installation of MAS_Platform is complete, but some errors occurred during \r\nthe install.\n\r\nGENBAND Media Application Server Platform Installation failed due to the \r\nfollowing:\n\r\nERROR - sperr.txt found after running dbscripts.\n\r\nPlease see the installation log for details.\n\r\nPRESS  TO EXIT THE INSTALLER: \r\n0;root@MASMEL001:/var/mcp/media[root@MASMEL001 media]# cat /var/mcp/ma/MAS/ScriptProcessor/sperr.txt\r\nERROR 1136 (21S01) at line 42 in file: \'./SqlScripts/DBLoadEAMTaskNav.sql\': Column count doesn\'t match value count at row 1\r\n0;root@MASMEL001:/var/mcp/media[root@MASMEL001 media]#\n\r\nSolution:\r\n-	Checked error and searched similar issues. Found out that this issue is already fixed (AAK-30768) in the new release which is MAS 16.0.0.802. This fix inside of 14.1.9 MAS only MR.\r\n-	Suggested SWD to upgrade to 16.0.0.802, but this load was in V status and he preferred to rollback to 14.0.0.299\r\n-	He will upgrade it once its in R status. Which will be ASAP.\r\n-	Agreed and ended the call.','null'),(1050,'Senem Gultekin (NETAS External)','AS-GW','2013-09-13','130909-427663','Cypress Communications','ER paged me for a IPCM issue seen at Cypress. After reinstallation of the IPCM-2 server the IPCM instance was not coming up.\n\r\nCustomer is running in a really old load which is MCP 10.2.1.19\n\r\nWork log:\r\n[Fri Sep 13 05:55:32 2013] Arg[15] = -classpath ../jars/delta_mcp.jar:../jars/mcp.jar:../jars/jhall.jar:../jars/3rdParty.jar:../jars/bcprov-jdk14-124.jar\r\n[Fri Sep 13 05:55:32 2013] Arg[16] = -DpropFile=../data/neprops.txt\r\n[Fri Sep 13 05:55:32 2013] Arg[17] = com.nortelnetworks.ims.mw.system.MCPBootstrapper\n\r\nSolution:\r\n-	I\'ve accessed to the site and checked IPCM-2 instance. \n\r\nThe server is running on 10.1.5 where it should be 10.1.14 just as SMs and IPCM-1. \n\n\r\n-	It seems like after the installation the platform patch was not performed. They have to apply the 10.1.14 patch to IPCM2 server(64.190.127.21). \n\r\nThey will follow the document for the platform (OS) patch procedure. And try deploy/start again to the IPCM-2 instance; \n\r\n[root@atl-56m-mcs-ipcm0 root]$ mcpRelease.pl \n\r\n*** MCP Platform Release *** \n\r\nSystem Type: mcp_core_solaris \r\nRelease Level: 10.1.14 \n\n\r\n[root@atl-56m-mcs-ipcm1 root]$ mcpRelease.pl \n\r\n*** MCP Platform Release *** \n\r\nSystem Type: mcp_core_solaris \r\nRelease Level: 10.1.5\n\r\nThey will proceed with the platform patch upgrade.','null'),(1051,'Senem Gultekin (NETAS External)','AS-GW','2013-09-10','130905-426804','Bermuda Telephone','Problem Description:\n\r\nSWD paged me for an MAS upgrade question. They had a failure with the MAS upgrade procedure a week ago, and they were starting the upgrade to the secondary MAS, but wanted to get an advice from GPS for the procedure.\n\r\nMAS upgrade path: CVM16/CVM17 (16.0.0.778/16.0.0.790)\n\r\nSolution:\n\r\n-	Suggested him to continue with the documented procedure. Which is applying the OS patch, uninstalling MAS application, uninstalling MAS platform, installing MAS platform, installing MAS applications\r\n-	They have performed the MAS upgrade successfully for the second MAS, however had a java failure when they applied the same steps for 3rd MAS. During uninstalling MAS platform;\n\r\n*****************************************************************\r\n ./Uninstall_MAS_Platform \r\n \n\r\nat com.zerog.ia.installer.util.VariableManager.a(DashoA10*..) \r\nThis Application has Unexpectedly Quit: Invocation of this Java Application has caused an InvocationTargetException. This application will now exit. (LAX) \r\n*****************************************************************\n\r\n-	Suggested them to rollback the OS first and uninstall MAS application and MAS platform, after that continue with OS patch and other steps.\r\n-	We may have to change these steps in the upgrade document, will discuss it first with design.\r\n-	Ended the call.','null'),(1052,'Seren Batmaz','AS-GW','2013-09-05','130905-426804','Bermuda Telephone Company','Problem Description:\n\r\nSWD paged me due to a problem while running MAS upgrade from 16.0.0.778 to 16.0.0.790. An error occured while uninstalling MAS platform. ./Uninstall_MAS_Platform script was run to do that. \n\n\r\nSolution:\n\r\nI double checked the procedure in NN10440-450 document and made sure that the previous steps had been run correctly. \r\nI checked the logs under /var/mcp/os/log and realized that \"InvocationTargetException\" had been throwed while running the script. This exception probably had been caused by some missing files in OS of the MAS.\r\nHence, I recommended to reinstall the OS of the server with 16.0.0.778 load and restore MAS backup on that. So that, the upgrade was going to be able to run without any problem.\n\r\nAlso, I suggested the a part of rollback procedure, which is about reinstallation of MAS Server, from NN10440-450 document. \r\nKeith applied the procedure and made the MAS up and running on 16.0.0.778 load.\r\nThe customer ran some tests to see whether MAS server had been installed correctly and eest Scenarios were successfull.\n\r\nKeith told me that MAS upgrade will be attempted again in next MW.\r\nSo, we dropped the call.','null'),(1053,'Seren Batmaz','AS-GW','2013-09-03','130902-424824','Paltel','Problem Description:\n\r\nER paged me and told me that three BCPs of the customer were not able to come up and active, after upgrading them. In work logs, there was following error:\n\r\njava.lang.UnsatisfiedLinkError: /opt/mcp/mediaportal/lib/libHalHeartBeater.so: \r\nlibACE.so.5.3.1: cannot open shared object file: No such file or directory \r\nat java.lang.ClassLoader$NativeLibrary.load(Native Method) \r\nat java.lang.ClassLoader.loadLibrary0(Unknown Source) \r\nat java.lang.ClassLoader.loadLibrary(Unknown Source) \r\nat java.lang.Runtime.loadLibrary0(Unknown Source) \r\nat java.lang.System.loadLibrary(Unknown Source) \r\n........................\n\r\nER told me that the upgrae path was 14.0.13->14.0.19 and they had run manual upgrade on BCPs. The A2 core load was 14.0.9.11.\n\r\nSolution:\n\r\nI connected to site and check the platform levels of BCPs. Platform patches on all BCPs were applied successfully:\n\r\n[ntappadm@RMLABCP0-2 work]$ mcpRelease.pl\n\r\n                           *** MCP Platform Release ***\n\r\n                   System Type:     mcp_bcp_linux_ple3\r\n                   Release Level:   14.0.19 (via patching)\r\n                   Hardware Env:    IBM-HS20\n\r\nI knew that 14.0.19 ple3 load belonged to 14.0.16 MR. For platform levels of that MR, A2 Core load should have been running at least 14.0.16.0 whereas the customer\'s core load was 14.0.9.11.\n\r\nSo, I recommended the following operations:\n\r\n-Rollback the platform levels of upgraded servers:\r\n    patchplatform.pl -rs\n\r\n-Start the BCPs\n\r\n-Patch platform of BCPs to 14.0.18 ple3 which is the latest applicable MR of 14.0.9.11 core load.\n\r\nAfter that ER told me that the customer has upgraded the platform levels of SESM servers. So, I suggested to rollback SESM servers as well and use Upgrade Wizard to run the upgrade. I also reminded that manual upgrade was not supported.\r\nI also suggested them to follow the upgrade document and review the release notes before starting the upgrade.\n\r\nAs soon as the customer agrees, I droped the call.','null'),(1054,'Joyce Lyon','AS-GW','2013-08-06','130805-419380','Alphawest Services P/L (Carr)','BCPs failed to completely recover after customer site experienced a power outage. All BCP\'s in the cluster returned as active causing a cluster error and no RTP on every 3rd call. All failures were determined to be caused by the cluster failing to initialize.\n\r\nAfter attempts to gracefully recover the BCP cluster failed, we power cycled the blades from the IBM BCT from which none recovered.  We killed all process, removed all deployed load folders, then redeployed and started each BCP.  All BCPs came up and cluster came up in 2+1 configuration.  After traffic tests passed, we were released from the call.','null'),(1055,'Joyce Lyon','AS-GW','2013-08-05','130805-419380','Alphawest Services P/L (Carr)','BCPs failed to completely recover after customer site experienced a power outage.  All BCP\'s in the cluster returned as active causing a cluster error and no RTP on every 3rd call.  All failures were determined to be caused by the cluster failing to initialize.\n\r\nNo actions could be taken on the cluster as incident occured during customer\'s normal busy hour. One BCP was left down to stop the call failures and a plan was provided to reinitialize cluster during the next maintenance window.','null'),(1056,'Seren Batmaz','AS-GW','2013-07-31','130730-418449','','Problem Description:\n\r\nDave paged me for dropping problem on Meetme bridges in Genband Site and asked me to contact with Douglas Holladay for the details.\n\r\nI talked with Douglas. The MAS was running on 16.0.0.764. The error message was as below:\n\r\n5200	Warning	2013-07-30 15:18:25	Info	PlayResponse status is not OK. \r\n7022	Warning	2013-07-30 15:18:25	Info	Request Queue Full Request queue is full. Details: error instance (a...\n\n\r\nIt seemed that it was a capacity issue. Hence, I asked Douglas to change some configurations on MAS:\n\n\r\nApplications -> MeetMe Multimedia Conferencing -> Multimedia -> Allow Video Capability -> Un-select\n\r\nSystem Configuration -> Engineering Parameters -> CPU -> Number Of Media Processing Units -> Verified parameter was set to 1500\n\r\nSystem Configuration -> Engineering Parameters -> Session and Thread Resource Pools -> Max Allocated Sessions -> Set as 4096 , this was set to 1500\n\r\nSystem Configuration -> Engineering Parameters -> MX Settings -> MX Maximum Sessions -> Set as 1500, this was set to 256\n\n\r\nTo enable the changes, MAS needs to be restarted. However, since there were active bridges which ER and GPS were using, Douglas could not restart MAS. He will do it as soon ad there is no active bridge.\r\nAdditionally, I highly recommended to upgrade the MAS to 16.0.0.774 which is the latest MAS load.\n\r\nThen, we agreed to drop the call.','null'),(1057,'Mehmet Salim Demir (NETAS External)','AS-GW','2013-07-22','130724-417357','Arrow S3','Chris had paged me about MAS platform installation failed. He connected MAS server over Remote desktop connection and remove MAS application and MAS platform(6.1.397) in order. But once he tried to install new MAS Platform(6.1.472) he encountered that the installer was not executing properly. Then, I confirmed with him that the older MAS application and platform were removed properly or not. He said they had been removed in order (first application then MAS Platform). Then, I suggested to change his exe file and re-try the installation. Then, he informed me that installation was finished successfully after he changed the exe file (New MAS Platform)','null'),(1058,'Senem Gultekin (NETAS External)','AS-GW','2013-06-14','130417-396307','Cleveland Clinic','Problem Description:\n\r\nChuck paged for an ring back tone issueseen at Cleveland Clinic. Customer is running on A2 CVM15.\r\nThe case was already opened as BC (130417-396307)and they requested me to investigate new traces.\n\r\nSolution:\r\n-	Asked them if there is an current outage. Answer was no.\r\n-	Explained Chuck that this can be investigated in work hours, but he told me that its an urgent issue and will at least needs to be investigated on Saturday.\r\n-	Arranged a call on Saturday morning their time, and 3pm on our time.\r\n-	Agreed with Chuck and ended the call to be worked on Saturday.','null'),(1059,'Yunus Ozturk (NETAS External)','AS-GW','2013-04-08','130408-394736','AAPT','Problem Description:\r\n---------------------------\n\r\nER called and reported that customer had voice path issues with the calls when they have touched the BCP 7100 blades. Customer had both BCP 7100 and BCP 7200 blades. When the calls have touched the BCP 7200 blades, they did not have a problem. However, when the calls have touched the BCP 7100 blades, they did encounter voice path problems.\n\r\nActions Taken:\r\n-------------------\n\r\n-Access the site and checked the configuration of the BCP 7100 blades. The configurations were fine to go.\r\n-Advised the customer to restart all the BCP 7100 blades but it did not work. Customer continued the same problem after restarting the blades. \r\n-Advised the customer to stop 2 of 3 BCP 7100 blades and keep just 1 BCP 7100 blade up at a time.\r\n-Asked the customer to perform test calls and at that time, we have collected mptool data on the corresponding BCP 7100 blade. \r\n-mptool data showed that the RTP media streams were fine. We have seen 2-way RX/TX packets on the blade. Additionally, we could not find out any packet loss, jitter and delay on the data. Even though the RTP streams were fine on the mptool data, customer continued to have the same voice path problem. \r\n-Asked the customer to capture wireshark network trace on the corresponding BCP 7100 blade. Customer has configured port mirroring on the router and captured the required wireshark trace from the problematic blade.\r\n-When we have checked the wireshark trace, we have seen that the RTP packets coming from the PBX side are out of sequnce with ratio %36.7. However, the outgoing packets from BCP to PBX are out of sequence with ratio % 5.9. So that, we have informed the customer that they need to find out why the packets from the PBX side are coming out of sequnce. BCP did seem that it tried to fix the out of sequence packets and decreased the ratio from %36.7 to %5.7. However, since the packets were always coming out of sequence from the PBX side, BCP could not do further on the coming packets.\r\n-We have informed the customer with the explanation above. However, customer has informed us that the out of sequence packets were just because of the port mirroring configuration on the router and it was a known issue on the customer site that they had problems before. \r\n-We have informed the customer that we can only comment on the logs/data that we have and the logs were pointing that the problem was not on the BCP blade.\r\n-Requested the network path of the call from the customer and they have informed us that the original failing calls were made to the BCP using SST trunk vctsspeaknsw2 and they failed all the time. Then, we have advised the customer to use another trunk for the same scenario. Afterwards, customer made a couple of calls using a test SST trunk sccts and they all passed. \r\n-So, the problem was related with the configuration of the SST trunk that they have been using so far. After they have used another SST trunk with different configurations, the calls started work fine.\r\n-Customer informed that they will be engaging their IP folks to see what may be wrong on the configuration of the SST trunk and we left the call at that time.','null'),(1060,'Joyce Lyon','AS-GW','2013-04-05','130405-394525','Alphawest Services P/L (Carr)','ER called with SESM overload and MAS license exhaustion issue.  MAS was suspected of causing the SESM overload condition.\n\r\nCustomer had N+1 cluster using the Announcement service.\r\nThe logical entity for the cluster was configured with the weighted selection algorithm and routes to each blade at 50%.  The secondary blade, however, was first in the list of routes.\r\nLicense exhaustion alarms were being seen on the secondary (.30, blade 8) MAS only.\n\r\nBefore calling ER, the customer stopped and restarted secondary mas, but license alarms persisted.  Customer left the secondary MAS stopped and made the route to the primary MAS 100%.\n\r\nLogs showed that primary was having issues and that the secondary was attempting to handle the bulk of the load. Calls made to verify this confirmed that the primary MAS was failing.  To recover the system, we rebooted secondary MAS which came into service. After permissions were granted, we rebooted the primary MAS, put back the 50/50 weighted routes and made the primary first in the LE route list.\n\r\nAfter this, system was performing normally again.\n\r\nFollow-up cases are being opened for both the MAS resource issue and SESM overload issue.','null'),(1061,'Senem Gultekin (NETAS External)','AS-GW','2013-02-03','130130-383600','Puerto Rico Telephone','Problem Description:\n\r\nER paged gateway pager for an Call issue on Puerto Rico Telephone. They were having one way speech problem on long distance calls and complaining that the issue may be related to BCPs. Customer is running on 12.0.6.9 Release and according to the customer they didnt do any changes on the system.\n\r\nSolution:\n\r\n-	When I joined to the conference there was Norm from GWC GPS and Sabri from CALLP GPS.\r\n-	Asked customer about the history of the issue, they explained that they were facing this issue for few days and the wanted to get fixed until Monday.\r\n-	Customer was explaining that they didnt do any changes on the system.\r\n-	Requested site access, they provided site access info but none of them worked for me and Sabri.\r\n-	Listed the required logs for us to gather by them. These were mptool l from all BCP servers, and SESM traces for call processing. \r\n-	Customer was reproducing the issue.\r\n-	Norm was continuing on the investigation with his site access, and he realized that the packages were not even passing over PVC (GWC side) to A2. It seemed there was a  routing or firewall issue. \r\n-	Explained ER that this issue has been seen on site for few days and normally we shouldnt be kept this long on a pager call for a network issue on a Saturday night until Sunday morning. \r\n-	If PVC is not able to even ping any BCP servers this is not an issue that  gateway, callp or oam gps needs to work on it.\r\n-	Requested from the customer to work on this issue with the site network engineer and left the conf.','null'),(1062,'Yunus Ozturk (NETAS External)','AS-GW','2012-12-20','121220-376995','Alphawest Services P/L (Carr)','Problem Description:\r\n---------------------\n\r\nFrom looking at the Event Logs for each MAS server customer has seen the following type of logs on MAS Servers BUWDMAS0007TA, BUWDMAS0008TA and UTMOMAS0001TA: \n\r\nMajor - Alarm Activated: License Threshold Reached (Id: 312) \r\nWarning - License pool threshold reached. License pool [annc::sess] with 40 total licenses has reached its alarm threshold. \n\r\nBut looking at the Event Logs for the MAS Server UTMOMAS0002TA, they get a different story. There is a continuous stream of Error logs of the kind: \n\r\nError - Session Event Timeout. Status reception timeout 0 expired for session 2921. Terminating session. \n\r\nProblem Resolution:\r\n-------------------\n\r\nWe have been informed that when only 1 MAS Server is allowed and after performing a few test calls, there is no Session Timeout logs observed on the site. However, we assume that customer does not see the alarms as soon as performing a few test calls. Most probably, they will see the alarms after a period of time when they have high call traffic on the site. \n\r\nWe have discussed the issue with Callp GPS Team and they have informed us that they are still investigating the SESM side. They are suspicious about some signaling on the logs and they will provide the details soon. \n\r\nSince there is no MAS alarms on the site after performing the test calls, we do not expect to see the reason within the existing logs. Most probably, when the customer observes the memory overload alarms on the SESM, they will also see the session time event alarms on the MAS side since the problem on the SESM side seems to trigger the alarms on the MAS side. \n\r\nAdditionally, the signaling seems weird. On many of the calls, MAS does not receive ACK or BYE message from the SESM even though MAS sends out the 200 OK message to the SESM. We guess, these calls with no ACK and BYE message from the SESM causes the hung sessions and alarms on the MAS. \n\r\nCustomer has also some calls with ACK and BYE message. So that, it seems that there is a problem with the signaling on some of the call types. We need to wait for the response of the Callp GPS Team to see why SESM does not send ACK and BYE message to the MAS and causing the hung sessions. \n\r\nSince MAS does not receive ACK and BYE messages from the SESM and since there are hung sessions on the MAS due to this signaling problem, License Exhaustion and Session Event Timeout alarms are expected for this situation. After 40 calls, these alarms will appear on the MAS if the hang sessions cannot be terminated fine since the capacity of the MAS license file of the customer is 20 for G.711 codec and 20 for G.729 codec. \n\r\nWe are still working on this issue with the Callp GPS Team.','null'),(1063,'Yunus Ozturk (NETAS External)','AS-GW','2012-12-19','121219-376827','Razorline','Problem Statement:\r\n-------------------\n\r\nDuring MAS 16.0.0.707 to MAS 16.0.0.769 MR Upgrade, the MAS data is not preserved. However, it is already required to preserve the data during the MR upgrades. Since the SWD did not preserve the data, they did encounter a problem. However, they did take the appropriate backups before the upgrade\n\r\nThe problem lies in the fact that there is no dropping of sync between the two units during the process, and as such, their supposedly preserved data was corrupted.\n\r\nProblem Resolution:\r\n-------------------\n\r\n- Accessed the site and check the MAS Servers\' current situations\r\n- Noticed that MAS Server 1 is upgraded but MAS Server 2 is not.\r\n- Logged into the MAS Server 2 which was not upgraded and noticed that the data was corrupted on the MAS Server 2 since there was a server designation configuration between the MAS Server 1 and MAS Server 2. Since the data on the MAS Server 1 is lost, it affected the MAS Server 2 due to designation between them. We recommend to drop the replication between the MAS Servers before the upgrades.\r\n- Dropped the server designation between the MAS Server 1 and MAS Server 2.\r\n- Restore the old load backup files to the MAS Server 2 to clear out the corrupted data. Since the MAS Server 2 is not upgraded, we were able to restore the backup files to this node to save the data on this node. However, we cannot restore the backup files to the MAS Server 1 since it is already upgraded. The restore procedure only works for the MAS 16.0.0.X to MAS 16.0.0.X upgrades. If you are upgrading from MAS 16.0.0.X to MAS 16.0.0.Y, then you need to preserve the config data during the upgrade.\r\n- Built the server designation again. Configured the MAS Server 2 (not upgraded)as Primary and MAS Server 1 (upgraded)as Secondary.\r\n- Data Synchronization has been performed between the MAS Server 1 and MAS Server 2. The good data on the MAS Server 2 (Primary) has been synchronized with the MAS Server 1 (Secondary)\r\n- Finally, changed the server designation again as the MAS Server 1 as Primary and MAS Server 2 as Secondary.\n\r\nThe issue is fixed after the performing the steps above and the MAS data has been restored successfully. SWD will continue to upgrade the MAS Server 2 by preserving the config data during the upgrade.','null'),(1064,'Senem Gultekin (NETAS External)','AS-GW','2012-11-14','121114-371184','Suddenlink','Problem Description: \n\r\nSWD paged me for an MAS installation issue seen at Suddenlink. They were trying to install MAS for 14.0.9 MR, but customer was not able see the boot screen once the server was rebooted. This was happening on both MAS servers. Servers are HTLangley\n\r\nSolution: \n\r\n-	Requested from the customer to restart the server. Same result.\r\n-	Requested to perform power cycle. Same result\r\n-	Asked SWD to check the CDs, he was saying that the customer inserted the installer CD.\r\n-	They were using A2EJ0140 Version: 18. This ordercode goes with 2 CDs. The first one is the platform.  installer and the second one is the platform patch. Asked the customer to check his CDs again and be sure that they were using the installer one. Which the CD label is as;  A2 8.0 (14.0.9) Software@Core and MAS Linux Installer\r\n-	After a while customer realized that they were using the Platform Patch CD, not the Platform Installer CD.\r\n-	Once the customer inserted the installer they were able to see the boot prompt on the screen. They were able to start the installation.\r\n-	Left the conference.','null'),(1065,'Adem Aydin (NETAS External)','AS-GW','2012-11-07','121107-370076','Trinidad & Tobago Limited (TSTT)','Mark paged gateway GPS pager and reporting one of customer BCP blades was down. The BCP Blade-1 failed to recover after GENBAND SoftWare Delivery rebooted BCP Blade during patch application. \r\nconnected to the site and started to Investigation problem and it showed that the Blade Center Chassis Management Module reporting communication problems with all the blades. Re-seat of StandBy then Active Management Module allowed the BCP Blade-1 to be re-started which recovered the BCP blade.\r\ncustomer confirmed BCP status and left from pager call.','null'),(1066,'Joyce Lyon','AS-GW','2012-10-29','121029-368329','Corporacion Telemic CA','BCP RTP blades down (BCP7100 in SAM16 chassis), unable to communicate to either the NET1 or NET2 addresses on Blade 6 running on MCP 10.1 load.  Customer indicated no hardware, provisioning , or other changed of any kind made to system.\n\r\nBCP in domain B completely down.  Determined hardware issue.\n\r\nBCP in domain A reconfigured (host card could not ping media card), rolled hardware (media card), cleaned up java out of memory issue, cleaned spool files on BCP, cleaned files on SM, cleaned up BCP instances, rebooted BCP to clean memory, deployed BCP, started BCP, BCP failed to come up (from SM perspective).  After, several attempts, found that BCP instance was actually active though the SM did not display it (similar issue 120828-357242).  Customer tested calls and all calls were successful.\n\r\nProvided maintenance window activity for correcting SM.  Hardware info provided for Domain B card(s).','null'),(1067,'Joyce Lyon','AS-GW','2012-10-24','TBD','Suddenlink','Connection to customer site dropped during platform install step of MAS 16 upgrade.  Upon reestablishment of connection SWD attempted to restart the platform install, but this failed.\n\r\nPaged to find out if there was a way to uninstall the platform without using the wizard.  Provided non-MAS wizard backdoor uninstall, but it was unsuccessful.\n\r\nConnected to site to determine issue and found uninstall was failing because library file overwritten with zero bytes:\r\nError occurred during initialization of VM\r\nUnable to load native library: /var/mcp/jdk/lib/i386/libjava.so: file too short\n\r\nGrabbed file from another server, but ran into other corruptions during uninstall.  To avoid leaving any corruption, asked SWD to just reinstall Linux, reapply Linux patch, then install the MAS Platform cleanly.\n\r\nLinux install in progress now.  SWD page again if additional assistance needed.','null'),(1068,'Adem Aydin (NETAS External)','AS-GW','2012-10-11','121011-365224','BT PLC (Manchester)','Camilla paged  me and informed one of the BCP blade generate post error code 162 during patch upgrade.\n\r\nI confirmed that the Continue option should be selected. When selecting Continue, the following messages were displayed: \n\r\nThe configuration settings are invalid. Select one of the following: \r\n- Automatically reconfigure the system and continue. \r\n- Continue with the corrupted values \r\n- Exit Setup. \n\r\nthen recommend GSD to proceed with Exit setup. Reboot sequence was complete, testing was successful.\r\nAlso checked IBM post error code and IBM suggest to lad BIOS default settings.','null'),(1069,'Yunus Ozturk (NETAS External)','AS-GW','2012-09-27','120927-362741','Suddenlink (Cebridge Connections Inc)','Problem Description:\r\n----------------------\n\r\nDuring a CVM13 to CVM15 MAS Upgrade, SWD Team was not able to access the MAS EM GUI after the MAS Platform Upgrade.\n\r\nActions Taken:\r\n---------------\n\r\n- Installed the Ple2 Linux Platform on the HT Langley Server\r\n- Installed the MAS Platform \r\n- Verified that MAS EM GUI does not work. We got the Server is too busy response from the server\r\n- Rebooted the server. However, it did not work\r\n- Verified that JBOSS service is working fine\r\n- Re-installed the MAS Platform again but it did not work\r\n- Checked the Release notes and verified that MAS Platform level and Linux Platform level is fine\r\n- Checked the ping status of the server and verified that everything is fine. There is no packet loss on the ping packets\r\n- Can access the MAS blade successfully via ssh connection. Only GUI is not working\r\n- Thought that this might be related with the customer network or firewall. The firewall might block the IP or port of the MAS EM GUI\r\n- Customer checked their firewall and enabled the IP range / port to allow us to be able to connect to the MAS GUI. Once they enabled the IP Range / Port, the MAS GUI is no longer timed out and the issue is fixed','null'),(1070,'Joyce Lyon','AS-GW','2012-09-17','120918-360966','BT TELECOMUNICACIONES S A','BT Spain upgraded A2 from MCP 10 to MCP 12 with the exception of a 5+1 BCP cluster which remained on MCP 10. The state of the BCP was configured (undeployed), though the customer did not say what happened to get it in this state.  When attempting to deploy, it failed and NED told us\n\r\n[Tue Sep 18 2012 03:30:59.404563] Transferring file /var/mcp/run/MCP_12.0/SM_0/work/neprops_1347939056278 from 62.7.41.69:2100 to /var/mcp/run/MCP_10.3/RTP3-1_0/data/neprops.txt.nedReceiving...\r\n[Tue Sep 18 2012 03:30:59.498695] Error! Received error reply from ftp server: 530 Login incorrect.\r\n[Tue Sep 18 2012 03:30:59.507354] Error! File transfer failed while attempting to transfer: /var/mcp/run/MCP_10.3/RTP3-1_0/data/neprops.txt.nedReceiving: 530 Login incorrect.\r\n[Tue Sep 18 2012 03:30:59.540041] Client :62.7.41.69:54366 closed connection.\n\r\nThe BCP failed to deploy because the SM/DB was on MCP 12 while the BCP was on MCP 10 with different user/passwords.  Added nortel user to allow BCP to be brought up on MCP 10.  Started manual deployment, but did not complete.  Customer requested suspension of work since upgrades are scheduled to be completed in the next day or two.  Additionally, only spare is down so no bandwidth issues.','null'),(1071,'Adem Aydin (NETAS External)','AS-GW','2012-08-29','120829-357365','wind telecom','Thomas from ER team called me and reported oneway audio problem on Wind Telecom.\r\nafter that customer provided remote desktop connection from site and checked all BCP configuration but there is nothing wrong with the configuration. I have started to collect mptool output to check incoming and outgoing RTP stream and when i investigate the output i realized this issue only occurs one types of call (PSTN to SIP) and other calls working as expected. Asked customer to learn any changes on the network. Customer informed before the problem occured they made firewall upgrade and configuration chnages on customer network. explained BCP working mechanism and stopped CLuster B RTP blades to prevent oneway audio temporarly and I suggested to check customer firewall configuration to fix the problem. they will fix their firewall problem and will be reactive Cluster B RTP blade.','null'),(1072,'Joyce Lyon','AS-GW','2012-08-22','120822-355847','Suddenlink (Cebridge Connections Inc)','SWD paged during attempted upgrade of MAS from CVM13 (Windows OS) to CVM15 (Linux) with complaint that customer was not able to install Linux OS to MAS HT Langley server.\n\r\nHad customer go to BIOS level of server. Found that multi-core processing was not enabled which prevented installation.  Enabled multi-core processing then stepped through entire BIOS setup and Linux config with customer.  Once customer was comfortable, the call was dropped.','null'),(1073,'Joyce Lyon','AS-GW','2012-08-22','120822-355847','Suddenlink (Cebridge Connections Inc)','SWD paged during attempted upgrade of MAS from CVM13 (Windows OS) to CVM15 (Linux) with complaint that customer was not able to communicate to MAS HT Langley server via terminal server.  Customer stated that different BAUD rate settings had been attempted at the terminal server but none worked.\n\r\nHad customer go to BIOS level of server.  Verified that both serial ports were enabled. Verified console was redirected to port B (com2), BAUD rate was set to 9600, terminal type was set to VT100 and legacy OS redirection was disabled.\n\r\nSince pre-Linux (on the Langley, serial port is not usable until after the OS is loaded), had customer enable legacy OS redirection to immediately see console output then modify terminal console settings to match BIOS settings.  Upon reboot, customer was able to view output on terminal server.','null'),(1074,'Adem Aydin (NETAS External)','AS-GW','2012-08-16','120816-354829','VTR Global Com SA','Eric from SWD called me and informed When trying to access the MAS BCT MM for the windows to linux MAS installation the BCT MM was responding very slowly or not at all and was for the most part unusable. \r\nrequested to reseat the blade and MM module then it started to work as expected.','null'),(1075,'Senem Gultekin (NETAS External)','AS-GW','2012-08-01','120731-351587','BlackBridge','Problem Description:\n\r\nSWD paged me for a MAS upgrade procedure concern . He was going to start MAS Upgrade from MAS_16.0.0.707 (MR_14.1.0) to MAS_16.0.0.764 (MR_14.1.2), but realized some odd steps in the 630-01217-01 03.04 document. \r\nCustomer is BlackBridge.\n\r\nSolution:\n\r\n-	Checked 630-01217-01 03.04 document, all related documents for MAS16 and release notes. There were mistakes in the documents such as path of the scripts and title of the procedure.\r\n-	The uninstall script path has been changed from /var/mcp/ to /var/mcp/admin in 8.0SP1. Also for MAS 16 uninstall and install sciprts are valid only MAS Application itself, no other application.\r\n-	SWD didnt want to the upgrade without double checking with design.\r\n-	Ended the call, next day needed information has been provided and MAS_16.0.0.764 upgrade has been performed successfully on site.\r\n- Documentation CR\'s will be opened to fix the problematic steps in the document for MAS upgrade.','null'),(1076,'Adem Aydin (NETAS External)','AS-GW','2012-07-20','120720-349036','Avaya','ER called GPS reporting that MAS02 had many critical alarms after attempting to upgrade from 6.1.0.417 to 6.1.0.472. Several attempts to reboot and power-cycle the server failed to resolve the critical alarms. \r\nTo resolve the issue, followed below steps.\r\n1. Fresh windows install on MAS02 \r\n2. Installed MAS 6.1.397 load \r\n3. Restored MAS 6.1.397 backup files \r\n4. Installed 12.0.7 hot fixes \r\n5. Reboot the server \r\n6. Uninstall 6.1.397 meetme and platform \r\n7. Reboot the server \r\n8. Install MAS 6.1.472 load \r\n9. Download and install new license key \n\r\nAfter that all alarms cleared on MAS02.','null'),(1077,'Adem Aydin (NETAS External)','AS-GW','2012-07-18','120718-348498','IHUB','Mark from ER called me and informed IHUB having installation problem on MOH MAS servers during site upgrade and they have only 1 MOH server on the system\n\r\nMark provided bomgar access and here are the steps taken to restore the node, \r\n- Stop MAS Application Services via EM. \r\n- Stop SQL services via the Window services \r\n- UnInstall newly upgraded release without preserving the data. \r\n- Verify nortel services stopped properly \r\n- Install original software MAS Platform, necessary applications. \r\n- Restore backups previously taken from old software \r\n- Verify nortel & SQL services stopped properly \r\n- UnInstall the old software, while preserving the data \r\n- Re-install the new software upgrade.\n\r\nAfter node was recovered, some of the system configuration data could not be restored from the backups and had to be configured manually, this included the license keys. Located license key file on the node and applied, this cleared the alarms, and node back in service. \n\r\nWhile attempted to make some test calls, Marc noted that the Music on Hold was not functioning properly. After enabling the debug logs on the system, it appears that the license key not correct: \r\n\"Error: Licesne pool failure. The failed license command was [moh:sess]\" \n\r\nGPS verified that the license did not contain any resources for MOH services. \n\r\nER(Mark)Discussed issue with Marc, he was going to consult internally to locate the proper license keys.','null'),(1078,'Yunus Ozturk (NETAS External)','AS-GW','2012-07-12','120711-347524','UPC Broadband','Problem Description:\r\n====================\n\r\nGenband Integration Team contacted ER to report that multiple BCP 7200 blades in BCT 0 and 2 had failed to stop and start during a SESM extension (adding SESM 8). The MAINT status on the blades were hung at \"STOPPING\" state and failed to shut down. Number of the blades had actually stopped but some the blades remained in a STOPPING state. Customer also stated that there are several alarms on the BCP blades.\n\r\nInvestigation:\r\n===============\n\r\nLogged into the site and noticed that there were critical alarms on all the blades in BCT 0 and 2 indicating Cluster Configuration does NOT match other nodes in the cluster and HA Layer Invalid Cluster Configuration.\n\r\nAlarmName: RTP Media Portal Configuration/Initialization Error\r\nFaultNumber: 804\r\nShortFamilyName: RTPB\r\nLongFamilyName: RTPBLADE\r\nSeverity: CRITICAL\r\nProbableCause: configuration or customization error\r\nDescription: An error occurred during initialization. Configuration Checksum\r\nError. Cluster Configuration does NOT match other nodes in the cluster. The RTP\r\nMedia Portal is NOT operational.\r\nCorrective Action: Please contact your next level of technical support.\n\r\n---------------------------------------------------------\n\r\nAlarmName: HA Layer Invalid Cluster Configuration\r\nFaultNumber: 806\r\nShortFamilyName: RTPB\r\nLongFamilyName: RTPBLADE\r\nSeverity: CRITICAL\r\nProbableCause: underlying resource unavailable\r\nDescription: Cluster is in a 4+0 configuration with 0 node(s) shutting down and\r\nshould be in a 7+1 configuration\r\nCorrective Action: Ensure all nodes within the cluster are operational. If not,\r\nmay need to restart the cluster nodes.\n\r\nAdditionally, checked the instance state of problematic BCP blades and noticed that Maintenance state of the blades change from STOPPING to NONE but they can not  switch to the OFFLINE status.\n\r\nSolution:\r\n==========\n\r\nSince the BCP blades cannot be stopped, thought that there were active calls on the blades. When there is an active call on the BCP blade, it is not possible to stop the instance until the call is ended on the blade. Therefore, performed the Kill operation to end the call and recover the blade. As soon as the blade is killed, the Maintenance state of the blade changed to OFFLINE status. Performed the same operation on all the BCP blades that encounter this problem and the blades were successfully started without any issue.\n\r\nAlthough the blades became up, the alarms mentioned above still existed on the system. Since a new SESM extension has been performed on the site, all the BCP blades within the same cluster should be stopped/killed at the same time, then should be started one by one to prevent the incorrect data from being transferred between the blades. So, based on this information, stopped/killed all the BCP blades on the same cluster and started them one by one. Then the alarms are cleared and the issue is fixed. \n\r\nRoot Cause:\r\n===========\n\r\n- When there is an active call on the BCP blade, it is not possible to stop the instance until the call is ended on the blade. Therefore, the blade should be killed to end the call.\n\r\n- When there is new node is added to the system (ie. SESM, GWC, etc), in order to prevent the incorrect data from being transferred between the blades, all the BCP blades within the same cluster should be stopped/killed at the same time, then should be started one by one. This will also prevent the potential alarms that might raise after addition of the new node to the system.','null'),(1079,'Adem Aydin (NETAS External)','AS-GW','2012-07-05','120704-346169','Ventelo','Jeff called me and informed customer was changed ESM0 and when they powered up the ESM0 they started to get call failures on BCPs.\r\nConnected to the site and reconfigured ESM0 but internal port shown as offline on the ESM0. After that we started to check customer network and finally found a fault on Customer cisco router fiber GIGE interface. I suggested to customer to change the interface. customer changed interface and ESM0 started to work as expected.','null'),(1080,'Joyce Lyon','AS-GW','2012-06-21','120621-342790','CenturyLink','Customer was attempting to upgrade from MCP_10 to MCP_12.  BCPs blades would not deploy.\n\r\nUpon logging onto BCP blade 2, I found rogue instances, which I removed.  When the deploy was attempted again, the following error was seen from NED:\n\r\n  Client connection accepted from :10.57.193.138:51825.\r\n  Error! Could not get user id ntappsw from password file.\r\n  Client :10.57.193.138:51825 closed connection.\r\n  Error! Instance not configured.\n\r\nAt this point, it was evident that the OS had not been upgraded.  To verify further, the password file viewed which showed nortel and sysadmin users present.\n\r\n  nortel:x:500:500:Nortel:/home/nortel:/bin/bash\r\n  sysadmin:x:501:501:Sysadmin:/home/sysadmin:/bin/bash\n\r\nIn the MCP_12 LINUX OS, the users are changed from sysadmin, nortel, etc. to ntsysadm, ntappadm, etc.\n\r\nCustomer will continue upgrade in next maintenance window starting with the OS upgrade.\n\r\nWent over documentation with customer to show the missed steps.\n\r\nHad customer make calls to ensure remaining BCPs on old load were working without issues.\n\r\nCustomer was satisfied, and we dropped the call.\n\r\nNote that neither MCP nor BCT remote control could not be viewed due to JAVA setup at site.','null'),(1081,'Senem Gultekin (NETAS External)','AS-GW','2012-06-16','120530-338449','Timico','Problem Description:\n\r\nGPS paged gateway pager for fax over T.38 issue seen at Timico live site after the A2 7.0 SP1 12.0.12.2 -> A2 8.0 BRC 14.0.9.7 upgrade. GPS already worked on the issue, but customer requested more support to fix the issue. Since they were assuming that it was related to BCP we had to work on it.\n\r\nSolution:\n\r\n-	Tried to gather more information about the issue. Calls were not failing, only fax was failing.\r\n-	Package path is; BCP to ESM, ESM to cisco and cisco to MG32K. Since the packages were going out from BCP properly requested from the customer to collect logs from cisco and MG32K parts.\r\n-	Worked with Adem Aydin, and he investigated  BCP logs detailed.\r\n-	According to signaling audio seem good and BCP mptool output shows both way packet transmission but collected wireshark traces bcp_trace.pcap does not cover all incoming and outgoing RTP packets. It only show outgoing packets from BCP to both side.\r\n-	We were not able to find any UDP or T38 packets from BCP logs.  Suggested customer to collect logs of incoming and outgoing BCP traces via port mirroring method on customer router or ERS switch. \r\n-	Mustafa Sonmez (NTS) was on call and waited for a while for customer response. They told that they will collect them later on.\r\n-	Left the call.','null'),(1082,'Yunus Ozturk (NETAS External)','AS-GW','2012-05-30','120530-338376','BRITISH TELECOM PLC','Customer (BT)was performing an ESM upgrade yesterday. When they could not\r\ntelnet to the ESM2 card, they replaced the ESM2 card and upgraded it. When\r\nthey applied the config and save on ESM 2, it brought down the chassis. Following this event they were unable to get the 5 configured blades back up in the MCP GUI.\n\r\nThomas Godwin (ER) has paged me and explained the situation. I have accessed the site performed the actions below;\n\r\n- Accessed the ESM1 and ESM2 units and pulled the \"tsdmp\" logs\r\n- Noticed that there were some differences between the configurations of ESM1 and ESM2\r\n- Since the ESM2 was the new replaced unit,decided to pull the ESM1 configuration and put it into the new replaced ESM2 unit\r\n- After applying the ESM1 unit configuration to ESM2 unit, there is nothing changed on the problem. Thought that there was another problem on the site\r\n- Accessed the System Manager and tried to ping and traceroute to the BCP Blades\r\n- Since pings are failed and traceroute is failed on the ESM IP address, suspected that there was VLAN inconsistency problem between the ESM units and BCP Blades\r\n- Tried to access the BCP Blades to check the VLAN configuration which should be configured under /admin/userinfo.txt file. However, due to a java issue on the site, I could not access the blades via IBM BCT MM GUI. Additionally, I was not able to ssh to the blades as well\r\n- Since the MTCE window has ended at that time, informed the ER that this issue needs furhter investigation\r\n- During day time, I have accessed the site with the assistance of GTS Engineer\r\n- While I was checking the configuration, noticed that customer has applied the wrong ESM configuration backup to the ESM2 unit after the ESM upgrade. \r\n- I also thought that there was also a problem on the ESM1 unit configuration. Because, I have applied the ESM1 config to the ESM2 unit, but nothing has changed.\r\n- Then noticed that customer has performed some VLAN related corrections on the ESM1 unit but they have rebooted the ESM1 unit without saving the corrected configuration. Since they did not save the new configuration, the new VLAN corrections have gone after the reboot.\r\n- Since the new VLAN corrections have gone, they have taken ESM1 configuration back up with the old configuration and applied that old configuration back up to the ESM2 unit as well.\r\n- Due to the wrong configuration backups applied to the both ESM units, the 5 configured blades did not come up on the MCP GUI since there was a VLAN incosistency between the old ESM unit configuration and the current BCP Blade configuration\r\n- Also advised the customer to compare the current VLAN configurations of the BCP Blades and the new corrected VLAN configuration of the ESM units.\r\n- Then the VLAN configurations have been corrected on the ESM configuration files and they have been applied to the ESM units again\r\n- After applying the correct configuration files, the issue is resolved and the 5 configured blades came up on the MCP GUI again.','null'),(1083,'Joyce Lyon','AS-GW','2012-05-09','120510-335246','Avaya Inc','Customer called to get assistance configuring ESM after restoring default configuration.  No backup was taken prior to the restore, but found 2-month old configuration backup and had specbook.  While getting the data, customer noticed they were in a network outage (likely due to loop created in taking ESM to default).  Used backup to restore configuration and verified by comparing port configuration to specbook.  Network recovered after ESM bay 1 was restored.  Continued with bay 2 while customer tested.  Customer continued to have instability with phone registration and receiving calls.  Customer noticed that ACME SBC swact\'ed a few times while the network was down.  After swact\'ing SBC, phones became stable and calls were successfully completed.','null'),(1084,'Joyce Lyon','AS-GW','2012-05-09','TBD','Axtel','SWD had 3 BCP clusters to upgrade tonight in 3 different locations.  In 2 of the locations, the have the incorrect platform CD.  Confirmed that the upgrade could continue with the one cluster and the others to follow in separate maintenance window.\n\r\nSWD also inquired about HWER 707 alarm:\r\nAlarmName: Raid Status Check Script Does Not Exist \r\nTimeStamp: Mon Jun 20 17:31:59 BST 2011 \r\nFaultNumber: 707 \r\nShortFamilyName: HWER \r\nLongFamilyName: HWER \r\nSeverity: MAJOR \r\nProbableCause: information missing \r\nDescription: RAID status check script is not in the system. \r\nCorrective Action: You need to install the latest official platform patch. Verify that the SNMP Profile\r\nconfiguration for this server is correct and update it as necessary.\n\r\nVerified script existed.  Provided steps to clear alarms.','null'),(1085,'Adem Aydin (NETAS External)','AS-GW','2012-04-30','120429-330624','GENBAND','Rob from ER team called me from Gateway pager and informed customer having upgrade issue on all 4 Announcement MAS blades and 1 MeetMe blade.\n\r\nProblem Description:\r\nMAS installation get stucked (2 hours) during upgrade process, after that all MAS blades generates error related with database\n\r\nResolution:\r\nConnected to the site and checked database size from all announcement blades and database size is to big (6GB-14GB) because of that SQL script does not work as expected during installation.\r\nRemoved old MAS folders and sql files from server but this time installation failed because of SQL release. then decided to install MAS 5.0 to fix the sql issue on all MAS servers. MAS 5.0 fixed the problem as expected after that followed upgrade procedure to upgrade MAS blade into MAS 6.1.417 release and restored customer backup files to servers.\n\r\nStephane Simon proceed the upgrade from 6.1.417 to 6.1.463 release without having any problems and closed the pager call.','null'),(1086,'Yunus Ozturk (NETAS External)','AS-GW','2012-04-25','120402-325920','Ziggo','SWD Team called and informed me that while upgrading the MAS Blades from CVM13 to CVM15, they had some questions regarding to apply the custom announcement files. Customer wants to use the previous modified files that they already has on the previous blade 5 for annc. \n\r\nI have provided the following information to the SWD Team regarding this issue;\n\r\nIf you know the specific modified files exactly, you can use them again on the new MAS. You need to back up the tones/announcement files which were modified before the MAS upgrade. After completing the upgrade, you can put the previous modified files back into the new MAS. \n\r\nHowever, if the specific modified files are not exactly known, we do not recommend to copy all the files back into the new MAS under /var/mcp/ma/MAS/platdata/Announcements/annc/default/us/en/l16 since these files are modified on the new MAS. They will have the same names but their content might be changed on the new MAS. Therefore, some incompatibilities might occur between the new MAS and old MAS files. \n\r\nSo, if you would like to use the same previous MAS modified files on the new MAS, you need to be sure which files were exactly modified on the old MAS.\n\r\nWhen you upgrade to MAS16, you will have the default announcement files under /var/mcp/ma/MAS/platdata/Announcements/annc/default/us/en/l16 folder. They are the new default announcement files that come with the new MAS16 load. So, if you copy/paste all the previous announcement files that come from the old MAS load, the new default announcement files will be overwritten on the new MAS16 load. The new and old announcement files will have the same names but their content might be changed on the new MAS. Therefore, some incompatibilities might occur between the new MAS and old MAS files. Thats why we recommend to copy/paste only the modified announcement files. \n\r\nIf the customer would strictly like to use all the previous announcement files, you can copy of all the previous l16 files to the /var/mcp/ma/MAS/platdata/Announcements/annc/default/us/en/l16 folder.However, this is customers own risk.','null'),(1087,'Yunus Ozturk (NETAS External)','AS-GW','2012-04-24','120402-325920','Ziggo','SWD Team called and asked that how they can get the licence keys onto the MAS blades at Ziggo. They cant cut and paste from their desktop to the site as its on Remote Control and does not record the copy command.\n\r\nI have advised the following action plan as a workaround solution;\n\r\n	Copy the content of the license key into a notepad.txt file and put that notepad.txt file  to your USB stick\r\n	Plug in the USB stick to the IBM BCT Chassis USB drive\r\n	Mount the USB stick and copy the notepad.txt file from the USB stick to a directory on the Linux Server\r\n	Open the content of the notepad.txt file with cat command and try to copy/paste to the MAS EM GUI\r\n	Since the notepad.txt file and MAS EM GUI will be on the same system, this time copy/paste can work.\n\r\nSWD Team was able to managed to get the license key on to site','null'),(1088,'Yunus Ozturk (NETAS External)','AS-GW','2012-04-23','120402-325920','Ziggo','Meraz Aziz from SWD Team has called and asked me to review and confirm the procedure that was prepared by the SWD Team for the Ziggo MAS CVM15 upgrade.\n\r\nThe required procedure that was needed to be reviewed was as below;\n\r\n-	Preparations (data collection and backups etc.) as normal per NN10440-450.\r\n-	Also make a backup of the tones directory, as the customer has modified tones. The customer will have to indicate which files have been changed. This is mentioned also in the NN10440-450 preparation procedures.\n\r\n-	Insert USB drive into the USB slot in the Media Tray of the BC-T.\r\n-	USB removable drive will automatically appear in the windows drive list.\r\n-	Copy the backups made (2 files per blade) to the USB drive.\r\n-	Copy the saved tones files to the USB drive.\n\r\n-	Upgrade the MAS blade to linux as per normal procedure.\r\n-	Install Linux patches if needed as per normal procedure.\n\r\n-	In the unix environment, use the command fdisk l to determine which drive is the USB drive. Normally this is listed as /dev/sdb1. (Insert USB stick if needed, of course).\r\n-	Mount the USB drive mount /dev/sdb1 /mnt/cdrom.\r\n-	Copy the desktop installer and the backups from the USB drive to a directory on the MAS blade.\r\n-	Unmount the USB drive umount /mnt/cdrom.\r\n-	Unzip the desktop installer and mount the ISO file mount o loop  /mnt/cdrom.\r\n-	Run the desktop installer as per bulletin.\r\n-	Unmount the installer umount /mnt/cdrom.\n\r\n-	Continue the upgrade as per normal procedure, using the backup files which are now stored on the MAS blade. When instructed to start the EM, refer to the bulletin for instructions.\n\r\nI have reviewed and confirmed the procedure above. Furthermore, advised some further clarifications regarding the bulletin.','null'),(1089,'Yunus Ozturk (NETAS External)','AS-GW','2012-04-19','120419-328944','Shaw Cable Systems','SWD Team called and informed me that while performing a MAS 16 upgrade, he has noticed that KRS license keys are not for CVM16 load. They are for CVM14. \n\r\nSWD has asked that if he can continue with the MAS 16 upgrade or not by using the CVM14 KRS license keys. I have informed him that if he is uprading from MAS 14 to MAS 16, then he needs to have the license keys for MAS 16. The license keys for MAS 14 does not work on the new upgrade. Customer should have the new license keys from the KRS Team for MAS 16. \n\r\nSWD Team has also informed me that customer has 2 adhoc MAS blades and one of them has the correct KRS license key but the other one does not. Customer will be able to get the new KRS license key within 1 - 2 days. SWD has asked that if the customer can continue to work with only 1 adhoc MAS blade while waiting for the new KRS license key for the other adhoc MAS blade. I have confirmed that if the customer does not have high adhoc conference call traffic, they can continue with 1 adhoc MAS blade for 1 - 2 days.','null'),(1090,'Yunus Ozturk (NETAS External)','AS-GW','2012-04-19','120419-328944','Shaw Cable Systems','SWD Team called and informed me that while performing a MAS 16 upgrade, he has been following up the latest upgrade document (NN10440-450 10.09). However, he has noticed that there is a missing section on the \"Restoring the MAS data backups\" part of the document.\n\r\nOn page 1655, there is a section named as \"Restoring the MAS data backups\". At the end of this section, there should be another section named as \"Migrating the MAS data backups\". Within this section, restoring of the MAS configuration backups should be explained. \n\r\nOn the previous versions of this document, the required section exists. However, somehow on the latest version (NN10440-450 10.09), this sections does not occur. \n\r\nSWD Team asked me that if he needs to perform the masupgrade script to restore the configuration backup after the MAS upgrade since this information is missing on the latest upgrade document. However, this information exists on the previous versions of the upgrade document. \n\r\nSo, we will need the upgrade document to be updated.\n\r\nThe required steps that need to be added to this latest document are as below;\n\r\nMigrating the MAS data backups\n\r\nUse this procedure to migrate the configuration and server data from backup files saved earlier to the MAS.\n\r\nPrerequisites\n\r\n The backup files should already be stored on the /var/mcp/upgrade_bkups directory on the MAS server.\r\n You have the IP address of the MAS Server.\r\n You know the password for the ntsysadm user ID and root user ID.\n\r\nProcedure Steps\n\r\nStep Action\n\r\nAt your workstation\n\r\n1 Establish a connection to the MAS server through SSH using the server\'s IP address, and log on using the ntsysadm user ID and password.\n\r\nssh -l ntsysadm \n\r\nFrom the MAS server\n\r\n2 Change to the root user.\n\r\nsu - root\n\r\n3 Change the directory.\n\r\ncd /var/mcp/upgrade_bkups\n\r\n4 Migrate the configuration backup.\n\r\nmasupgrade {hostname}__{date}.zip\n\r\nEnter y to stop all MAS services when prompted.\n\r\nThe time to complete the Service Data upgrade depends on the amount of service data in the backup file. It can take a significant amount of time.\n\r\nRepeat this step for each remaining backups:\n\r\n Service Data _{hostname}{date}.zip\r\n Network Data, if Quantum security was enabled :{hostname}{date}.zip\r\n Billing Data (optional) : Billing_{hostname}_{date}.zip\n\r\n5 Verify JBOSS has restarted.\n\r\nservice jboss status\n\r\n6 If it reports that it is stopped, then start it manually.\n\r\nservice jboss start\n\r\nThe MAS EM is inaccessible for a small amount of time after the jboss service has been started again.\n\r\n7 You have completed this procedure.\n\r\nIf you were directed to this procedure from a high-level procedure, continue as follows:\n\r\n if you opened a new window to access this procedure, close the new window to return to the original location.\r\n if you did not open a new window to access this procedure, click the Previous View button in the Acrobat Reader window until you return to your original location.\n\r\n-------------------------------------------END-------------------------------------------\n\r\nAdditionally, the name of the \"Restoring the MAS data backups\" section needs to be renamed as \"Transferring backup files to the MAS\". After this section, \"Migrating the MAS data backups\" section above needs to be added.','null'),(1091,'Yunus Ozturk (NETAS External)','AS-GW','2012-04-17','120417-328479','Yadkin Valley','SWD called and informed me that he has performed a BMC firmware upgrade on Blade 5(MAS1). After completing 100% of the upgrade, the status bar stated that the update had failed. SWD has checked the current firmware status after the upgrade and noticed that the BMC firmware has been upgraded even though this failure message on the status bar. \n\r\nI have informed the SWD that this issue has been seen before and the reason of this problem is, during this upgrade process, some versions of the Web Browsers (Internet Explorer, Firefox, etc) prints out this failure even though the the upgrade is completed without an issue.\n\r\nI have asked the SWD to reboot the MM GUI and after the reboot is completed, SWD was able to see that BMC firmware is updated successfully. So, this failure message is a false message due the version of the Web Browser.','null'),(1092,'Joyce Lyon','AS-GW','2012-04-09','120410-327252','Optus-Alphawest Services P/L (Carr)','MAS Annc calls failing due to license issue.  Decoded licenses and pull licenses from KRS to ensure customer had correct key, which they did.  Had site send me snap-shots of MAS Resource OMs and found that all licenses were being utilized and pool was being exhausted.  Had customer check active sessions to ensure sessions were not stuck.  Found some sessions over 10 minutes old.  Had customer release these sessions.  In a short time, we were back to the max and the pool was exhausted again.  Collected logs for follow-up case and received permission to restart MAS servers.  Note that when traffic was blocked for the ensuing MAS restart, licenses were being returned to the license pool.  Customer ramped  traffic up slowly.  After reaching highest traffic level expected for the time of day, MAS still performed correctly.  Collected additional logs after the restart.  Customer was satisfied and ended the call.','null'),(1093,'Adem Aydin (NETAS External)','AS-GW','2012-03-28','120327-324672','Alphawest Services P/L (Carr)','Jeff Brennan from ER called gateway pager and informed ATO upgraded their BCP firmware into MCS12.0.12 release, but now they have voice path issue on BCP blades,\r\nProblem solution.\r\nDavid Giomi provide mptool output via email. checked provided trace and mptool output and found there are no TX and RX packets on BCP blades. \r\nChecked BCP release and saw it is MCS 10.3 but firmwares are for MCS 12 release and suggest to upgrade BCP release in maintenance window. they performed BCP upgrades then every call works as expected.','null'),(1094,'Joyce Lyon','AS-GW','2012-03-13','120313-321791','Optus','Customer unable to make 3-way conference calls using MAS.  All parties drop when conference (join) is initiated.  Customer has 2 Ahoc Conferencing clusters, each having 3 servers.  Set secondary and standard servers to pending lock to test service on primary only.  Found that on every test call, a vid license was being requested, but there were no vid licenses purchased.  MAS rejected each of these requests indicating resource issues.  When I checked the Adhoc parameters, I found that \"Video Capability Negotiation\" was enabled.  Once disabled on both primary servers, customer was able to make conference calls successfully.  Brought secondary and standard servers back up and made additional test calls which were also successful.','null'),(1095,'Senem Gultekin (NETAS External)','AS-GW','2012-03-08','120308-320514','Avaya','Problem Description:\n\r\nIve received pager call from ER for Adhoc issue after MAS 5.0.643 upgrade at Avaya. Customer was performing test call  from 919-563-2008, this line calls 919-246-9259, they then try to conference 336-438-4200 on and when join is selected both ends of the calls drop.\n\r\nSolution: \n\r\n-	Accessed to the site and checked the MAS blade configuration for Adhoc.\r\n-	Everything seemed normal\r\n-	Requested from the customer to test with other lines and it worked.\r\n-	Customer agreed that issue was specific for a line and other ones are working.\r\n-	Ended the call.','null'),(1096,'Senem Gultekin (NETAS External)','AS-GW','2012-03-08','120308-320514','Avaya','Problem Description:\n\r\nIve received pager call from ER for MAS upgrade issue seen at customer in 10.3 Release. There are 8 MAS blades at the site and 2 of them were facing problem during MAS installation process. Blade 4 and Blade 6 are the problematic blades. The error is as following;\n\r\n\"Error while running DB scripts UpgPlatDB.tcl\"\n\r\nSolution: \n\r\n-	Accessed to the site and checked the MAS blades.\n\r\n-	It seems that during uninstallation the database for MAS has been corrupted. \n\r\n-	Performed solution to install MAS properly to the blade. \n\r\n-	Delete Nortel directory at Registry Editor in Windows. Following action performed at site for Blade4 and Blade6;\n\r\nHKEY_LOCAL_MACHINE->SOFTWARE->Nortel->MAS was deleted from Registry Editor \r\nRestarted MAS blade. \r\nMAS platform re-installed successfully. \r\nLicense key has been applied. \r\nBoth MASs are up and running on 5.0.643 now.\n\r\n-	Agreed with customer and ended the call.','null'),(1097,'Senem Gultekin (NETAS External)','AS-GW','2012-03-06','120306-320102','Unitymedia','Problem Description:\n\r\nSWD paged for a MAS upgrade issue seen at Unitymedia live site. Release is 6.1.0.463.\r\nWhen installing MAS SWD faced lack of disk space in drive C. \n\r\nSolution: \n\r\n  According to the upgrade document  NN10440-450_07.60 MAS upgrade prerequisites pg 1608 indicates that the Minimum disk partition size requirement is 3 GB free space. SWD already had 3.11 GB space but it was still failing due to C disk size.\r\n Requested from the SWD to clear out 1 GB more for C disk.\r\n Once she had 4 GB free space she was able install MAS.\r\n Ended the call.','null'),(1098,'Senem Gultekin (NETAS External)','AS-GW','2012-03-05','120306-320101','Unitymedia','Problem Description:\n\r\nSWD paged for a MAS upgrade issue seen at Unitymedia live site.  MAS backup fails prior to upgrade with the following error;\n\r\nBackup failed: See Event Viwer logs for details\r\nError backup tool, DB dump failed.\r\nInsuficient system resources to complete the requested service\n\r\nAlso SWD was not able to get proper response from ipconfig. \n\r\nSolution: \n\r\n	Accessed to the site and checked the MAS blade over the Management Module. It seemed it was not responding.\r\n	Requested to restart the problematic MAS blade.\r\n	Customer wanted to wait until midnight. \r\n	After restart SWD was able to get output from ipconfig and she was able to get the backups properly.\r\n	Ended the call.','null'),(1099,'Adem Aydin (NETAS External)','AS-GW','2012-03-02','120302-319635','unitymedia','ER contacted with me and explained the issue.\r\nto resolve the issue I suggest to Restart Blade System Mgmt Processor but blade still would not discover. \r\nthen suggest to restart management module and blade was discovered. \r\nSWD then powered up and started Blade1 without issue. Currently, Blade1 is loaded and runnig active on CVM13 while Blades 2 and 3 remain on CVM11. Blades 2 and 3 will be upgraded at a later date.','null'),(1100,'Tugrul Timorci (NETAS External)','AS-GW','2012-02-16','120217-316348','Cable Onda','Donnell Called me for upgrade wizard issue. \n\r\nDuring the Upgrading Primary Network Element Inatances using the MCP Upgrade wizard, the tool never seems to complete even though everything thing states it completed successfully just continue to sit on the same screen. \r\nI logged in the site and checked the system. Primary DB upgrade was performed successfully but the instances did not upgrade to new load. I Upgrade the instances manually, I saved and exited from Upgrade wizard. I started the wizard on Debug mode and I passed this screen with force next option. \n\r\nI saved and exited wizard again and I opened wizard normally. After this step wizard went successfully. After agreement I dropped the call','null'),(1101,'Yunus Ozturk (NETAS External)','AS-GW','2012-02-23','120223-317281','Hargray Communications Inc.','Tony Pitmann from SWD Team has called and informed that after MAS MR upgrade and Linux OS Patch Installation, MAS Blade did not come up after reboot.\n\r\nCustomer does not have the blade in IBM BCT MM Chassis. SWD had console access to the blade via Terminal Server. \n\r\nSWD indicated that during the reboot, they do not see anything on the screen. It becomes stuck at that point. GPS advised the SWD to powercycle the blade again but the result was same. I thought that since we cannot see any error messages during the reboot, it might be related to the hardware. Therefore, I asked the SWD to replace the blade with the spare one. At that point, we noticed that there was a CD in the CD-ROM of the problematic blade. Asked the customer to remove the CD from the CD-ROM and perform another reboot again.\n\r\nAfter removing the CD and rebooting the server, it came up successfully without any issue. Checked the Load and Patch levels of the blade and everything was fine. \n\r\nSo, the problem was the CD in the CD-ROM.','null'),(1102,'Joyce Lyon','AS-GW','2012-02-17','TBD','MONTERREY NGN','ConfMP MSLINK alarm after installing hot fixes on two MAS blades\n\r\nCustomer on MAS 5.0.643 prepping for MAS 6.1.463.  Alarm frequently seen in 5.0 and actually addressed in the load to which customer is upgrading.  Restarted MAS application a couple times on each server to reestablish connection to SC.','null'),(1103,'Joyce Lyon','AS-GW','2012-02-14','120214-315699','ETB Columbia','Customer complaining that all calls going over PVG have one-way audio.  Since BCP had comms issues in the site earlier and because all calls were inserting portal, BCP was the suspected culprit.  Worked with ER and customer, testing, collecting and analyzing data, examining configuration, etc.  Based on examination, issue appeared to be routing problem.  Finally able to get calls working properly again after customer removed a static route which was added to address PVG-GWC communication issues.  Left the call with ER trying to figure out how to appropriately address the PVG-GWC comms issue.','null'),(1104,'Joyce Lyon','AS-GW','2012-02-14','120214-315699','ETB Columbia','Started as a BC, but quickly became an E1.\n\r\nBCP\'s down after changing subnet mask to add an additional blade.\n\r\nCustomer had 7 BCP blades in its BCT chassis configured with a subnet MAS of 255.255.255.240.  Customer added BCP blade in slot 8 and changed subnet mask to 255.255.255.224 on all blades and the router. After changing all but 2 blades, it was discovered that all BCP\'s, where the MASK was changed, were down.  Customer reverted all changes but BCP\'s remained down.\n\r\nFound that reconfigure.pl script, used to modify MASKs, has a flaw.  It does not ask for VLAN id.  It automatically assigns zero.  Modified the userinfo directly to correct the VLAN config on each BCP.  After this, we were able to clean up, deploy and start each BCP.','null'),(1105,'Joyce Lyon','AS-GW','2012-02-14','120214-315675','Avaya Inc.','Customer called with Invalid License Key alarm after performing MAS MR from 6.1.397 to 6.1.463.\n\r\nFound that License Service was stopped and could not be enabled.  Installed 12.0.7.0 Hotfixes.  Service started but MAS component still down and server eventually stopped.  Uninstalled and reinstalled MAS services and platform on top of the Hotfixes.  License service remained running and MAS component remained up and working, however, Invalid License Key alarm returned.\n\r\nDecoded license key and it looked fine, but still pulled key from KRS and sent to customer.  After applying key the alarm remained.  Checked the network interfaces and noticed that ipconfig /all returned MASTeam MAC and other NIC MACs.  Disabled all interfaces except MASTeam interfaces.  Invalid License Key alarm cleared.','null'),(1106,'Adem Aydin (NETAS External)','AS-GW','2012-01-27','120127-312886','AVAYA','Robert called me and informed all BCP\'s in customer networks are down after upgrade. Customer were able to provide site access after 2 hour.\n\r\nResolution\r\nAfter connecting to the site ı have checked all BCP\'s configuration and settings, I found SM and SESM server\'s are not able to ping all BCP blades. but when I check BCP network I realized all BCP\'s are able to ping gateway IP address then we engaged ERS GPS team to the call they checked all routings VLAN and Arp tables on ERS switch but they could not find any problem on ERS.\n\r\nER provide customer spec book then ı started to check all network configuration and compare with BCP\'s settings and I found a incorrect Subnet had been entered at some time in the past , I have corrected the subnet to match the spec book and all service was restored.','null'),(1107,'Yunus Ozturk (NETAS External)','AS-GW','2012-02-04','120204-314201','Avaya Inc.','Customer was attempting to upgrade a HT Langley MAS server from 6.1.3.93 to MR 12.0.11 but the PSFTP script was failing to ftp the backups between the servers.\n\r\nER has sent me the upgrade procedure. Customer has stopped on page 166 step 14.\n\r\nEstablish a transfer session to the secondary EMS. \r\npsftp  \n\r\nI have reviewed the procedure and found out why PSFTP was not working. Customer was attempting to FTP from MAS1 to MAS2 and MAS2 to MAS1. Procedure is to FTP from MAS1 to secondary EM Server and MAS2 to secondary EM Server. \n\r\nCustomer has verified that this works and then I left the call accordingly.','null'),(1108,'Joyce Lyon','AS-GW','2012-01-19','120117-310906','Axtel','Customer still experiencing one-way speech path on BCP2 of BCP cluster, MTY2BCP2.  Customer claimed that when a PBX user originated a call to a terminator via BCP2 of cluster MTY2BCP2, the call failed with one-way audio, but if that call went over BCP1 of the same cluster it worked fine. \n\r\nWhen we began to make calls, we found that the success paths were not on BCP of the same cluster and that the failure on via BCP2 was no speech path. \r\nPBX user -> BCP2 (172.25.130.9) -> end user --> no speech path \r\nPBX user -> BCP1 (201.158.135.36) of some other cluster -> end user --> worked \r\nCalls were not hitting BCP1 (172.25.130.8) of this cluster. \n\r\nHaving found this, we changed our debugging focus from within the cluster to the cluster itself. That is, we made the reasonable assumption that failures on BCP2 would also be seen on BCP1 of the cluster. \n\r\nAfter making multiple calls, we found that the Network, in particular the NAT, needed to be investigated. No issues were found with the BCP. \n\r\nThe MPTOOL showed that the BCP was not receiving any packets from the PBX user. After seeing this we immediately speculated that the PBX user here was behind a NAT which was confirmed by the customer. With the client behind the NAT, BCP could not do port discovery to determine where to forward packet. This is determined with/after it has received packets from the client. The tool also confirm no speech path. \n\r\nThe PCAP trace simply confirmed the data provided by MPTOOL showing no packets from the client behind the NAT. \n\r\nNext steps \r\nFind the network issue. We have asked the customer to look at the NAT and its routes to see where the packets are being blocked.','null'),(1109,'Joyce Lyon','AS-GW','2012-01-18','120117-310906','Axtel','Customer complained about one-way speech on one member of BCP cluster.  While examining data, entire cluster went down with no communication. Found data mismatches between ESM\'s and data mismatches between ESM\'s and specbook.  Corrected ESM and re-established communications.  BCP cluster back up.\n\r\nCustomer back testing but experiencing voice quality issues.  Found that when BCPs were recovered, BCP3, which was standby, was now active.  BCP3 was made standby again and testing commended again.\n\r\nWaiting on customer.','null'),(1110,'Joyce Lyon','AS-GW','2012-01-19','120119-311427','Optus','Customer could not reach MAS followng upgrade to CVM 14.  Customer was unaware that in CVM 14, MAS moved to the LINUX OS and that with this move Remote Desktop was no longer used. Provided new details and assisted customer in accessing MAS.  Assisted customer DTMF configuration.','null'),(1111,'Adem Aydin (NETAS External)','AS-GW','2012-01-09','120109-308681','Timico','Bill Price from ER called me and informed Customer has identified a potential fraud call being perpertrated against MCS5200 by what apperas to be mobile subscriber and told me Ozan from Callp team working on this to find the cause.\r\nI have joinde the call and start to investigate MAS logs and I found some  users have custom greating messages and key1, key2 refer system to make fraud call. and informed customer how they can create custom greating message on system.\r\nAlso ı explained to create this customer greating they have to know end user pin for UCOM and now they are trying to find a way to change UCOM pin on system or to disable * option on MAS blade.\r\nJoyce from MAS desgn find a way to disable greating options on MAS but it will disable this options for all user not only problematic user.','null'),(1112,'Adem Aydin (NETAS External)','AS-GW','2012-01-02','120102-307649','Rcable','Bill called me and joined the call to work with customer ı have check Management module of BCT chassis. and ı saw there is only one cooling devices installed to BCT chassis and then customer installed other cleaned cooling devices one by one but every time MM generate an alarm for other 3 cooling devices. and also when they check air circulation behind the cooling devices there was no air.\r\nI have performed MM test also restart MM but it did not clear the alarms and I suggest to order new cooling devices to customer and after getting approval from customer droped from call.','null'),(1113,'Adem Aydin (NETAS External)','AS-GW','2012-01-02','120102-307649','Rcable','Bill from ER called me for the same problem but this time he informed me customer does not have any spare cooling devices and they wants to know how to resolve this issue. I explained how BCT works and explained why they need that cooling devices. Also Bill told me customer is going to clean cooling devices to test it again and he will call me during business hours and ı dropped from the call.','null'),(1114,'Adem Aydin (NETAS External)','AS-GW','2012-01-02','120102-307649','Rcable','Goktug called me again and informed they reseat the cooling devices but there were no air circulation behind the 3rd and 4th cooling devices, but still not able to power on MAS blades. I suggest to replace cooling devices with the spare ones and drooped out from the call.','null'),(1115,'Adem Aydin (NETAS External)','AS-GW','2012-01-02','120102-307649','Rcable','Goktug called me and informed customer not able to power on MAS blade from BCT MM module becuase of 3rd and 4th cooling devices error and i suggest to reseat those cooling devices then try to power on MAS blades.','null'),(1116,'Adem Aydin (NETAS External)','AS-GW','2012-01-04','111220-306637','Bannerhealth','Er called me and informed customer configured their logical entity wight as 0% but MAS blades still receives INVITE messages for meetme service. \r\nworked with David Mcnutt and connect to customer PROV and checked logical entity for meetme service and i realized selection algorithm selected as round robin. ı have changed this as weighted avarage and removed second MAS blade from list.\r\nCustomer performed test call and they confirmed MAS 2 not get any INVITE for meetme service.','null'),(1117,'Yunus Ozturk (NETAS External)','AS-GW','2011-12-30','111220-306637','Avaya Inc.','Customer paged A2 GW GPS and requested assistance on the dead air issue exists on the MeetMe conference bridge, There is a report of dialing into MeetMe Bridge and receiving no audio or what the customer reports as \"Dead air\". \n\r\nCustomer states that when a CICM line user tries to access the Meetme conference bridge, he can not hear any RTP media and after 4 - 5 tries, MAS disconnects that CICM line user automatically. The Meeme bridge admin can see the that CICM line user when he accesses the bridge. However, the CICM line user is disconnected from the bridge automatically without hearing any voice. So the problem is on the CICM line user side. \n\r\nFor this issue, we have checked the MAS SIP logs. As per the logs, an INVITE message comes to the MAS for the CICM line user to access the Meetme bridge, then within 18 seconds, a BYE message comes to MAS, then MAS disconnects this user as expected due this BYE message coming from the CICM line side. This CICM line user tries to access the Meetme bridge 4 times and everytime he is connected from the Meetme bridge with a BYE message. The point is BYE message is not sent from the MAS, in fact it comes from the CICM line user. Somehow, the system sends this BYE message and causes this CICM user to be disconnected from the Meetme bridge. \n\r\nPlease see the BYE signalling messages for these 4 attempts below;\n\r\n1. \r\n(30 09:55:38.517) ENG[018:A]  Incoming SIP Message: BYE sip:meetme@10.77.253.25:5060 \r\nBYE sip:meetme@10.77.253.25:5060;nt_service=meetme SIP/2.0 \r\nFrom: ;tag=-45026-1b6f75e-5ab930d9-1b6f75e \r\nTo: ;tag=7cda0a8-19fd4d0a-13c4-50017-105f17-4dfd5c9-105f17 \r\nCall-ID: c28a5ef843aed20a112d6657e23a792884b605@10.65.227.178 \n\r\n2. \r\n(30 09:56:00.111) ENG[098:A]  Incoming SIP Message: BYE sip:meetme@10.77.253.25:5060 \r\nBYE sip:meetme@10.77.253.25:5060;nt_service=meetme SIP/2.0 \r\nFrom: ;tag=-45026-1b6f773-3bef60b1-1b6f773 \r\nTo: ;tag=7ce0288-19fd4d0a-13c4-50017-105f2d-73a350c6-105f2d \r\nCall-ID: bdcf5ef843fad39a112d6668a3778a33af0b5a@10.65.227.178 \n\r\n3. \r\n(30 09:56:18.423) ENG[087:A]  Incoming SIP Message: BYE sip:meetme@10.77.253.25:5060 \r\nBYE sip:meetme@10.77.253.25:5060;nt_service=meetme SIP/2.0 \r\nFrom: ;tag=-45026-1b6f786-219fef8a-1b6f786 \r\nTo: ;tag=7cde5d8-19fd4d0a-13c4-50017-105f40-6912cce2-105f40 \r\nCall-ID: 7d646ef8430d5e6f12d669fd5f6daf08bfd0e@10.65.227.178 \n\r\n4. \r\n(30 09:56:37.033) ENG[007:A]  Incoming SIP Message: BYE sip:meetme@10.77.253.25:5060 \r\nBYE sip:meetme@10.77.253.25:5060;nt_service=meetme SIP/2.0 \r\nFrom: ;tag=1643857509 \r\nTo: \"meetme\";tag=7ce5898-19fd4d0a-13c4-50017-105f63-17d65d34-105f63 \r\nCall-ID: 201d6ef843c0c07a112d66abf4bb5b54f75367@10.65.227.178 \n\r\nSince the end user is somehow disconnected from the Meetme bridge, he thinks that he hears nothing. This is the dead air issue that the customer is mentioning about. \n\r\nAdditionally, as per the SESM SIP logs that Ramey has sent, it seems that SESM proxies the INVITE messages coming from the end user to the MAS to access the Meetme bridge. However, after a period of time, SESM sends a BYE message to the MAS again and causes the end user to be disconnected from the Meetme bridge. \n\r\nSo, MAS is behaving as expected here. Since a BYE messages comes to it, it disconnects the user as usual. The problem here is why the CICM line side sends this BYE message to the MAS. This is the point that needs to be investigated.','null'),(1118,'Yunus Ozturk (NETAS External)','AS-GW','2011-12-29','111227-307277','Cypress Communications','Following a voicemail outage yesterday, the customer reconfigured the Dallas CS2K Firewall Services Module to move the RTP traffic flow out from behind the firewall to alleviate the load and unnecessary packet inspection of the RTP traffic. \n\r\nFollowing this procedure the customer started experiencing a new problem. When a line calls a CICM line that is in Converged Desktop mode and the call forwards to voice mail, there is no RTP in either direction. Calls to CICM lines that are not in converged mode that forward to voice mail are OK.\n\r\nIn unconverged mode the customer answers the call with soft client; CICM line has AIN triggers; no RTP when calling voice mail. In converged mode, CICM hard client rings and also receives screen pop on pc client when they receive a call; when call forwards to voice mail, no RTP either direction. CICM line also has AIN triggers. \n\r\nWhen the AIN triggers are removed from the CICM line, and a call forwards \r\nto voice mail, RTP is present both direction. Everything is OK when the CICM line does not have the AIN triggers.\n\r\nDuring the trouble shooting we performed a trace route on each BCP blade \r\n(10.0.2.5 and 10.0.2.6) to the voice mail public IP address 216.246.202.74/75 \r\nand the traceroute failed. Next, the customer performed a change on the firewall they altered last night to allow all traffic and now the traceroute is successful. Also performed ping on each BCP blade (10.0.2.5 and 10.0.2.6) to the voice mail public IP address 216.246.202.74/75 and they were also successful. However, the call to voice mail still has no RTP either direction.\n\r\nChecked the mptool -l and mptool -q outputs on the blades while the customer was performing test calls and noticed the following;\n\r\n0x013a35ef/0x00002569\r\n???                   <--> 64.190.127.132:49668\r\n                                                      0x013a35f0/0x00002469\r\n                           64.190.127.132:47950  <-->   76.182.241.57:50018\n\n\r\n[root@atl-56m-bct0-3 sysadmin]# mptool -q 0x013a35ef\n\r\nConnection 0x013a35ef:\r\n        Flags: 0x0000256b\r\n        Portal Address: 64.190.127.132:49668\r\n        Client Address: 0.0.0.0:0\r\n        Created by call server: 64.190.127.6:21016\r\n        Call ID: 2820443\r\n        Event Request ID:\r\n        RX Packets: 0\r\n        Prev RX Packets: 0\r\n        TX Packets: 0\r\n        Prev TX Packets: 0\r\n        RX Octets: 0\r\n        RX Octet Rollover: 0\r\n        TX Octets: 0\r\n        TX Octet Rollover: 0\r\n        Dropped Packets: 1110\r\n        Packets Lost: 0\r\n        Interarrival Jitter: 0\r\n        Transmission Delay: 0\n\r\n        RTCP:\r\n                RX Packets: 0\r\n                TX Packets: 0\r\n                Dropped Packets: 5\n\r\n[root@atl-56m-bct0-3 sysadmin]# mptool -q 0x013a35f0\n\r\nConnection 0x013a35f0:\r\n        Flags: 0x0000246b\r\n        Portal Address: 64.190.127.132:47950\r\n        Client Address: 76.182.241.57:50018\r\n        Created by call server: 64.190.127.6:21016\r\n        Call ID: 2820449\r\n        Event Request ID:\r\n        RX Packets: 2144\r\n        Prev RX Packets: 0\r\n        TX Packets: 0\r\n        Prev TX Packets: 0\r\n        RX Octets: 0\r\n        RX Octet Rollover: 0\r\n        TX Octets: 0\r\n        TX Octet Rollover: 0\r\n        Dropped Packets: 0\r\n        Packets Lost: 0\r\n        Interarrival Jitter: 0\r\n        Transmission Delay: 0\n\r\n        RTCP:\r\n                RX Packets: 9\r\n                TX Packets: 0\r\n                Dropped Packets: 0\n\r\nAs per the mptool -l output, VM side has no IP address and Port. It shows it as ???. It means that the call server has told the Portal that the client is obscured (unknown). In this situation, the Portal cannot forward packets to such a client. It seems that packets come from PC client to BCP but since the VM side is obscured, BCP can not forward the packet to the VM side. In other words, BCP cannot forward the traffic since the destination IP (VM ip)is seen as 0.0.0.0. (unknown / obscured). Since this destiantion is obscured, BCP is dropping all the packets passing through the VM side. We confirmed that this issue seems to be Network configuration. \n\r\nIn the mean time, since this information comes from SESM to BCP, Callp GPS has also looked into the SESM logs for the failing scenario and confirmed that the signalling is Ok and SESM should insert the portal without any issue. Normally, before sending this information to BCP, SESM first contacts the destination side. Then forwards this information to BCP. So, it seems that SESM received the obscured IP information from the VM side, then forwards this information to BCP. Then BCP did not forward the packets to the destination side. Somehow, destination side generates an unknown IP/Port information due to a configuration issue within the network.\n\r\nAt this point, ERS 8600 GPS was paged. Customer has taken an action the day before and moved the layer-3 stuff regarding the RTP to a Cisco Router which is directly connected to ERS units and outside of the Firewall. So, when looking into the ERS ARP table, ERS GPS cannot see any BCP IP addresses since layer-2 VLANs have been configured for these devices (vlan-720 and vlan-730). Then ERS GPS checked the HDB table for these VLANs which will be used when the packets are being forwarded:\n\r\n8600a:5# show vlan info fdb-entry 720\r\n================================================================================\r\nVlan Fdb\r\n================================================================================\r\nVLAN MAC QOS SMLT \r\nID STATUS ADDRESS INTERFACE MONITOR LEVEL REMOTE\r\n--------------------------------------------------------------------------------\r\n720 self 00:00:5e:00:01:5c Port-cpp false 1 false \r\n720 learned 00:03:ba:04:7b:d4 MLT2 false 1 false \r\n720 learned 00:15:40:ac:c2:03 MLT2 false 1 false \r\n720 self 00:16:ca:0b:a2:03 Port-cpp false 1 false \n\r\nIt is figured out that all the MAC addresses for BCP units have been learned over MLT 2 which is actually an SMLT on 1/16 ports of each 8600 unit and is connected to Storm units. When customer was informed that ERS learns the MAC addresses of BCP from a different port, customer stated that they encountered this before and this is an indication of a switching loop. First action that we took is to shut down a port on ESM side to eliminate the loop. After that customer also made a fail over to Cisco Firewall. Immediately after this action, the voice mails appeared on the Voice Mail server. Then customer made a couple of tests and all the system was working fine as well as the RTP traffic. \n\r\nThen we dropped from the call with an agreement.','null'),(1119,'Damla Sanligencler ( NETAS External )','AS-GW','2011-12-17','111116-298290','Avaya','Because that I wasn\'t available, Adem connected to the site. There was MAS reporting alarm. Adem asked if they installed the hotfixes. GTS said yes but they couldn\'t. So Adem installed the 12.0.7 hotfixes which is consisting of 2 parts. He continued installing without restarting OS. So after installing the hotfixes and restarting OS, he installed the newly generated license and all alarms cleared. So the call ended after testing MeetMe is working.','null'),(1120,'Damla Sanligencler ( NETAS External )','AS-GW','2011-12-18','111217-306153','Avaya','I worked with Lee Taylor from Avaya GNTS. They reported that after the upgrade, Announcemnts/Treatments/Tones are not working. I connected to the site and checked the provisioning settings. I saw that Announcments logical entity is using only MAS2. MAS1 has a weight of 0. So I changed it to 1 and the weight for both MAS1 and MAS2 became 50%. So I wanted them to test and everything worked. They told me to go ahead with the upgrade of MAS2.','null'),(1121,'Damla Sanligencler ( NETAS External )','AS-GW','2011-12-14','111214-305271','Thunderbay','Munir paged me again while doing the rollback operation. He was trying to install MAS 5.0 again when he had a DB script error and couldn\'t complete the installation. We removed the Nortel folder and restarted again and tried to install MAS again. But this didn\'t solve the issue. So we cleared the registery from the Nortel contents and restarted the server again. And this time MAS platform could be installed. And then MAS Adhoc Service is installed. Then I restored the backups from the old system. And it\'s running without any alarms. Now they need to find RAM and Windows Server 2003 installer for upgrading to MAS 6.1.','null'),(1122,'Damla Sanligencler ( NETAS External )','AS-GW','2011-12-12','111116-298290','Avaya','Christopher paged me but, then I contacted with Lee Taylor who is in Avaya ER and I got in touch 5 weeks ago about this issue. After I instructed to reinstall 5 weeks ago, they had chance to do so. But they reported that they have again the plicd.exe error and the MAS licensing error. plicd.exe refers to MAS license Server and it cannot be started somehow. As Adem adviced me, I first instructed them to copy the plicd.exe from the working MAS2 and paste to MAS1. but this didn\'t solve the issue. So I instructed them to reinstall the OS. I waited for Craig Kjell (customer) to install Windows Server 2003. He installed the network drivers, too and gave me permission to remote access. I then installed MAS 6.1.397 platform. And plicd.exe error was gone. But their old license was not working. I checked the license with the decoder and everything seemed to be ok including the MAC Addresses. But I couldn\'t find out why license seems to be invalid. I told them to continue tomorrow and I finished the call. The problem is not solved yet.','null'),(1123,'Damla Sanligencler ( NETAS External )','AS-GW','2011-12-14','TBD','Thunderbay','SWD told me while upgrading to MAS 5.0 to MAS6.1, they have errors about RAM and MySQL. He said that the server has 1.5GB RAM and MySQL services cannot be started. I told him to add RAM to 2GB as required and upgrade the OS to Windows Server 2003 from Windows Server 2000. And then continue the upgrade process. He agreed and call ended.','null'),(1124,'Joyce Lyon','AS-GW','2011-12-07','111207-303713','Compania Dominicana de Telefonos C por A','During firmware upgrade for ESM in Bay 2, the ESM stopped responding. The upgrade was a two-step process where it was first upgraded to 1.2.5.1 then to 1.5.1. Paul indicated that the upgrade to 1.2.5.1 was successful. After the 1.5.1 images were uploaded and the restart performed for the second step, the ESM did not come back. \n\r\nFrom the I/O Module Task level of the BCT, we see that the power on self-test (POST) fails. The failure code returned was 00 which is a critical error in base internal functions. \n\r\nAttempted to follow IBM procedure to recover, but no success. \n\r\nReplaced ESM with onsite spare which restored ESM communications for Bay 2. Provided and stepped through upgrade procedure.  Upgrade completed successfully.','null'),(1125,'Adem Aydin (NETAS External)','AS-GW','2011-12-01','111130-302056','Optus','Roberto called me from gateway pager and informed about the customer issue during BCT blade firmware upgrade. \r\nProblem Description:\r\nAfter firmware upgrade one of blade went down and now customer not able to reach that blade via remote control.\r\nSolution:\r\nHe provide a few action to customer and I confirmed that action before sending it to the customer.\n\r\nCustomer applied GTS action and successfully replaced old blade with new one.','null'),(1126,'Yunus Ozturk (NETAS External)','AS-GW','2011-11-23','111123-299305','Unitymedia NRW GmbH','Accessed the site and checked the ipsec configuration. Since there was no ipsec configured on the site, BCP Blade 1 and System Manager could not communicate each other and this caused to prevent the deploying operation of the BCP Blade. Went through the ipsec configuration document. Performed the ipsec configuration steps step by step on the site. When the configuration is completed, BCP blade and System Manager started communicate each other again.. Then, the blade was successfully deployed and started. Customer performed test calls and everything was Ok.','null'),(1127,'Yunus Ozturk (NETAS External)','AS-GW','2011-11-25','111125-299652','Singtel Optus Pty Ltd','Optus is having a problem at preparation for upcoming A2 upgrade scheduled next Monday.\n\r\nAs part of this, the new linux patches need to be loaded onto the blade, but the old images need to be deleted first as part of the procedure.\n\r\nThis is the procedure..\n\r\nAt the login session to the server\r\n3 Change to root user.\r\nsu - root\r\n4 Remove the older patch loads.\r\nrm rf /var/mcp/os/install/images/*\n\r\nunfortunately the above command got truncated and resulted i the following   rm rf /var/mcp\n\r\nthis has now deleted all the files and directories from /var/mcp/\n\r\nThey have tried a undeploy/deploy. this re-added the run and spool directories. They have rebooted it and done another undeploy and re-deploy.  This has not worked and the directories are still missing.\n\r\nAs per GPS recommendation, after another undeploy and deploy on the blade, it is now up and running. Customer asked if they really need directories under /var/mcp/.\n\r\nDuring the upgrade, the directories under /var/mcp/ will be required since some upgrade and installation scripts exist under these directories. So, if the customer will be upgrading the blade via patching, we recommend them to copy/paste all the directories under /var/mcp/ from the good blade to the bad blade. However, if the customer will be upgrading the blade via fresh install, they do not need to perform this action since during the fresh install, these directories will be created again. The recommendation of GPS is to install the Linux OS (fresh install) again on this problematic blade before the upgrade since there might occur minor/unexpected problems if they do not perform a fresh install.\n\r\nIf the customer will prefer to perform the upgrade via patching instead of fresh install, they should be careful on the permissions while copying/pasting the directories under /var/mcp/ from the good blade to bad blade. As you know, some directories are created with ntappadm , ntsysadm or root permissions. So, they should copy the directories with these permissions. For instance, if a directory was created with the ntsysadm user permission on the good blade, then same directory needs to be created again with ntsysadm user permission on the bad blade as well.','null'),(1128,'Yunus Ozturk (NETAS External)','AS-GW','2011-11-24','111124-299553','Unitymedia NRW GmbH','During the upgrade of the third blade on a BCP cluster (2+1), no administration config data was found on the system. The installation screen did not display the \"Use Administrative data found on the disc\" option. The options displayed were \n\r\n1) Manual install \r\n2) Re-Install with platform data stored on remote server \r\n3) Re-Install with platform data stored on USB drive \n\r\nBefore chosing any option, blade was powered on/off, GSD confirmed that /admin data was still present. According to upgrade NTP, this can happen when either no administrative data was found or data is not usable. \n\r\nGSD then tried to use option 2 to re-install with platform data stored on remote server. Transfer of the backup was successful, but the following error was seen when restoring the backup: \n\r\n\"Error: the given file does not contain valid data for this installation\"\n\r\nGPS recommended to install the blade manually and reconfigure IPsec.\n\r\nAccessed the site and checked the ipsec configuration. Since there was no ipsec configured on the site, BCP Blade 3 and System Manager could not communicate each other and this caused to prevent the deploying operation of the BCP Blade. Assisted SWD Engineer during the call and went through the ipsec configuration document. Explained the ipsec configuration steps to the SWD step by step. When the configuration is completed, BCP blade and System Manager started communicate each other again.. Then, the blade was successfully deployed and started. Customer performed test calls and everything was Ok. As per the agreement, left the call..','null'),(1129,'Damla Sanligencler ( NETAS External )','AS-GW','2011-11-16','111116-298290','Avaya','Problem: MAS unit 0 unreachable during s/w upgrade 6.1.397->6.1.463\r\nSolution: I directly communicated with Lee Tylor from Avaya ER. One of the technicians on the site went to the servers and used the KVM. Because the servers are not BC-T, and they\'re IBM X550, we couldn\'t do remote KVM. MAS1 windows OS was working but it couldn\'t be remotely accessed. We suspected from the network settings and wanted to sync the network settings with MAS2. While doing reboot they realized that flash disk is still plugged and server was trying to reboot from the USB disk. So it is unplugged and MAS1 was started to be remotely accessed.\r\nAfter this, MAS1 was not working properly. There were alarms about license. So we tried to uninstall and reinstall MAS platform and applications. But it took too long than we expected. And at the same time, we made MeetMe and AdHoc working on MAS2 without redundancy.\r\nInspite of reinstalling the MAS platform and applications to MAS1, it still gave the same license errors. And after rebooting MAS1, we saw that plicd.exe, ivrmp.exe and confmp.exe gave errors in the startup. And also one of the alarms on MAS1 was \"one or more services cannot be started\". so we checked the services on MAS1 from administrative tools and we found out that Nortel License Server is not started. We tried to start it but it couldn\'t get started. And it\'s process name was plicd.exe.\r\nFor contribution, we think that this is an OS problem. And OS should be reinstalled to MAS1 and than hotfixes 12.0.7 should be applied and lastly MAS 6.1.463 should be installed with it\'s applications. I told Lee Tylor to do these steps properly without touching the working MAS2. If MAS1 is upgraded without any problems, then MAS2 should be upgraded, too.\r\nAfter this advice, call ended.','null'),(1130,'Yunus Ozturk (NETAS External)','AS-GW','2011-10-31','111030-295039      ','Warwick Valley Telephone','- ER called for a problem on the BCP and MAS blades. After a power outage occurred on the site, BCP and MAS blades did not come up again. \n- Accessed the site and checked the MCP Console and IBM BCT chasis. \n- Noticed that all the BCP and MAS blades seem at down status on the MCP Console. \n- Tried to kill, undeploy, deploy, stop and start operations on the BCP instances but they did not work. \n- Restarted the BCP servers and MM Module via IBM BCT GUI but again the blades seem down on the MCP GUI. \n- Then noticed that the servers were actually up but there was a connection problem between the SM and BCP blades. \n- Tried to ping the SM from BCP blades but it did not work. Also tried to ping BCP blades from SM but we were again unsuccessful. \n- Then performed a traceroute command and noticed that the packets did not pass through the ESM. At that point, we checked the ESM configuration and found out that there was no VLAN configuration defined on the ESM1 for the BCP and MAS blades. It seems that previously ESM configuration has been performed on the ESM1 but somehow the configuration has not been saved after the completing the configuration. Then after an outage occurred on the site, the configuration was lost since it was not saved before. \n- So, requested the specbook from the customer and configured the VLANs and ports for the BCP and MAS blades. \n- Also performed VLAN taggging and PVID tagging operations on the ports. \n- After saving the configuration, the connection between the SM and BCP blades came up again. Then we were able to deploy and start the BCP blade instances successfully. \n- Customer has performed some test calls to check if there is any problem on the blades and told us that everything is ok. \n- Then agreed with the customer and left the bridge.','null'),(1131,'Adem Aydin (NETAS External)','AS-GW','2011-10-31','111016-291521','AAPT Limited','I have joined the bridge with engineering team and explained why BCP is inserting it NET1 address instead of NET2 address also explain how to add NET2 routable Network address from MCP GUI to fix the issue','null'),(1132,'Adem Aydin (NETAS External)','AS-GW','2011-10-28','111016-291521      ','AAPT Limited','Roberto called me and informed customer has a voice path issue and they are wondering about why BCP is inserting public IP address(NET1) instead of NET2(private) IP address to the call.\n\nI have checked MPtool output and ı found incoming connectin address does not match with Media Portal NET2 routable networks. this is why BCP inserting NET1 instead of NET2. I explained it to the customer contacts and explained how to add a new NET2 routable networks.\n\nAfter that dropped from bridge also they adjust a bridge with customer 2 days later.','null'),(1133,'Joyce Lyon','AS-GW','2011-10-20','111019-292505      ','Wind Telecom','BCP Blade 2 bouncing up and down.  Case came in yesterday, however, customer increased severity to E2 after discovering that calls established while blade up are lost when blade goes down ...\n\nFound that BCP was restarting due to HAL repeatedly dying and restarting. Killed and restarted HAL (basically redeploying BCP).  This caused the bouncing to cease but created an alarm due to HAL connect failure.  Cleaned up BCP and HAL instances and restarted blade. After this HAL connected and BCP came up.  Monitored the site for 30 minutes to ensure BCP remained up.','null'),(1134,'Joyce Lyon','AS-GW','2011-10-18','110928-286736','INFORMATION SYSTEMS JET','Customer has 4 BCPs in BCT chassis.  Two BCPs stuck in configured state.  Attempted to restart blades and redeploy but this failed.  When attempting to login to BCPs, found that passwords were unknown to customer.  Employed password recovery procedure to reset the passwords, but this initially failed because the load version was 10.1.5.  Found procedure for this load and reset the passwords.  Once access reestablished, removed rogue processes on BCP and deployed loads successfully.','null'),(1135,'Adem Aydin (NETAS External)','AS-GW','2011-10-06','111006-288243      ','Axtel','John Kisner from ER paged me and informed Axtel has an outage and they are having one way voice path with the SSL lines and BCP sending so many huge packets into the OLT servers.\n\nI have connected to the site and checked All 7 BCP blades logs and I could not find any unusual logs then I have checked mptool -q outputs to see Rx and Tx traffic on BCP and I saw BCP receives so many packets from one of the leg of call and transmit the packets to customer OLT servers. John provide this info to the customer but this time they changed the problem definition and they told BCP is sending UDP packet to an unkown IP address on their network.\nThey provide me a couple of IP addresses and I checked all pool outputs to find those IP addresses on BCP\'s but ı could not find any of them.\nI made below explanation to customer.\n\" BCP blades gets connection IP addresses from signaling and it cannot send the packets to an unknown IP address\" and I told there is nothing wrong on BCP\'s\nCustomer changed their problem again.\nthey informed us they have 16 OLT server on their network. All 16 servers communicating with 7 blades and 9 of OLT servers are having this problem but other 7 OLT servers do not have.\n\nAfter that they unblocked all BCP blades IP address from one of OLT servers and it started to work without having any problem. \n\nEr talked with customer and informed BCP\'s do not have any problem. \nWe dropped from bridge','null'),(1136,'Damla Sanligencler ( NETAS External )','AS-GW','2011-09-28','110928-286708      ','NTT','SWD was upgrading NTT\'s lab from CVM 14 to CVM 16. He started to upgrade MAS then he face an error while running the mcpExtractContent.pl script. The error was including these: \"/var/mcp/extract/mnt/manifest System return strings: >>Verification Failure<< Signature verification failed for \"/var/mcp/extract/mnt/manifest\" \"\nI connected to the site via VNC and saw the error. But I couldn\'t name the problem. I called Huseyin for help. He suspected from the platform level. The blade\'s platform level was at 14.0.15. This means SWD hasn\'t done the latest MR upgrade before upgrading to CVM 16. First we tried to extract the tar.gz file manually, without any script. But after extracting, the patch.iso file returned with no applicable patches result. This was not expected. So we researched the patch contents and found out that before running that patch to MCP14.1, the platform patches should have been applied to MCP14.0.18. So we manually gave SWD MCP 14.0.18 patch and ended the call. Later on, SWD returned us with a positive result.The root cause of this problem is, this signature verification is added to this mcpExtractContent.pl script in MCP 14.0.21 patch. So if the previous platform level is under this load, the extract script gives error. We resolved the issue manually but, the real solution should be applying the latest MR of MCP 14.0 before upgrading to MCP 14.1.','null'),(1137,'Adem Aydin (NETAS External)','AS-GW','2011-09-14','110914-281261      ','Virgin Media','Edwin paged me and informed me customer is having a problem while getting meetme info IM and unable to record meetme bridge.\nI have tolked with customer NOC to get site access but they informed me i am not in the access list of that site then Umaa provide me and access info. After that i have connected customer MAS blades and configured settings correctly. and this resolve the issue on customer site.\nAlso Customer raised another 4 case during outage. and i fixed all issues on MAS blades.','null'),(1138,'Huseyin Yavuzturk (NETAS External)','AS-GW','2011-08-25','110825-277837','Telrad','Haydee called GPS as the end users cannot able to make ad hoc conferencing on sip lines at CVM13. GPS checked the MAS configuration and the prov configuration if there is any configuration missmatch found on server, but everything seems fine. From pcap file it seems the SESM replies as 406 not enough port messages. We checked the MAS sip signalling and found out that there is no sip invite messages recieved by MAS. We request SESM signalling logs but customer did able to collect it and request to drop the pager call. We countinue the investigation on the case.','null'),(1139,'Damla Sanligencler ( NETAS External )','AS-GW','2011-08-24','110824-277447      ','North-State Telephone Company','Problem Description: After upgrading 2 blades on the site, the 3rd blade when restarted to load linux was extremely slow on the boot, it was not get to boot from the dvd. On the remote console screen, it looked like it was printing out at about 300 baud. It seemed to just keep slowly searching through the load devices, but never picking up the cdrom drive. SWD tried to change the blade with a spare one, but it gave memory error because it had 1.7G, but MAS needs 2G.\n\nActions Taken: I connected to the site with the given access info. And I saw the slow booting. It seems to be a HW problem. So I adviced SWD to replace the blade. He said there is no spare blade except the one lack of memory. So I adviced him to go on the upgrade procedure with the 4th blade and after it is completed and up, go on with the 3rd blade which will be newly come as spare. He should wait for the new 3rd blade before going on the upgrade. So until the 3rd blade comes, the 4th blade will be working without redundancy.','null'),(1140,'Huseyin Yavuzturk (NETAS External)','AS-GW','2011-08-18','110818-276272','Corporacion Telemic CA','When the ER called to the GPS the both BCP7100 MCP 10.1 was not able to take the calls after the upgrade from 9.1. We login to the site and checked the MCP gui alarms and sw releases.On mcp gui the media blades have communication alarms with the hosts cards. We checked the platform loads and releases that the platfrom patches had not done yet.  We stopped/undeployed the first RTP and reboot it. This actions recovered the communication problem between the host and media blade on the first RTP portal.When, we deployed/start the Media portal and it started to take the calls. On the other hand, the same actions to the other RTP it did not work, additionally we unseated/seat the card but also this action failed to recover the media blade. We suggested to SWD to patch the platform since may be one of the platform patches can have a fix targeting this issue.\r\nAfter the patch/reboot the Host card the communication provided and /deploy/start the RTP portal without any issue.','null'),(1141,'Adem Aydin (NETAS External)','AS-GW','2011-07-27','110726-236584','Telecom Liechtenstein AG','Meraz Called me from gateway pager and informed MAS blade generate an alarm \"A component could not be started\" after patch upgrade. after that Meraz provide me VNC connection and connect to the MAS blade checked alarms and component status on MAS blade then I realized 12.0.7 hot fixes for MAS was missing on the blade. to resolve the issue I provide the fix to Meraz and after applied the fix the problem was gone and MAS blades starts to work without no alarm','null'),(1142,'Huseyin Yavuzturk (NETAS External)','AS-GW','2011-07-18','110718-235042','Alphawest Services P/L (Carr)','The BCP blades are working without redundacny and the Management module cannot be remote access. First we made the management module accessable from network and reconfigure it by restoring the backup. But at that time the ESM modules external ports turned off and the blades are dropped from the network. We made the ESM module enable and the blades back into the network. When I look to the logs it seems the blades are active but cannot reach DB so I have to stop/start them in order to make them reachable to DB and it works, then we started the second BCP blade and it was standby as expected by that way we provide service to BCPs with redundancy and we do the same thing to the cluster 1 and it also resolved the cluster1 issue. THe left thing is that putting the MM 2 module to the chasis and it also worked without issue as the synchronization of the MM with primary one in an hour. I left the bridge.','null'),(1143,'Huseyin Yavuzturk (NETAS External)','AS-GW','2011-07-18','110718-235042','Alphawest Services P/L (Carr)','15/07/2011 the site power outage and 18/07/2011 customer realized that the BCP incoming and outcoming calls are failing and they observed that the BCP cannot be power on found on the same chasis. We tried to power on/off and reseat the Blades but the power command cannot be reached to the blades and it always gives init_failed error. I recomend to replace the MM module but the while the MM module replacing the IBM chasis connectivity lost and not able to reach it via remotely. On site engineer connected to the Chasis via KVM(locally)and we releazid that the MM is on default settings but the BCP blades became reachable and we boot them to make them up. On the other hand, we were not able to back them all into service via MCP gui because of HAL layer problem but since the customer has 2 clusters as working 1+1 we made the only one blade up in each cluster to provide 100% service without redundancy. Customer request us to left the site without redundancy as there is no service impact. We negotiated with cutomer to make MW to fix the redundancy issue and Management Module issue.','null'),(1144,'Adem Aydin (NETAS External)','AS-GW','2011-06-24','110623-229338','one connect','Kyle paged me and discussed the process about replacement of HT langley server. Customer is going to replace one of his UCOM MAS server and they have to be sure about what they need to do during replacemnt.','null'),(1145,'Adem Aydin (NETAS External)','AS-GW','2011-06-22','110621-229066','Century Link','Kenneth paged ME today about the issue. \r\nI advised them to reinstall the MAS server. He paged me again when he finished reinstall OS and informed he can able to access MAS blade \n\r\nI have connected to the site and try to run reconfigure.pl script on MAS blade and asked if VLAN is used or not. And Kenneth said that it is not used, so I configured in that way. But it didn\'t work again. Then I provide new OS installer to Ken and we installed blade with that installer but it did not resolve the issue\n\r\nSo I asked again if there is a VLAN and Ken found out that a VLAN is used. After that, we installed the server again with a VLAN configuration. But when we configure VLAN on MAS blade we needs second VLAN ID as maintenance VLAN so \r\nI gave a dummy VLAN id and an IP address for that. After the last installation we could ping the MAS server successfully. \r\nBut we need the real maintenance VLAN id and IP address from that VLAN. Now we\'re waiting for this information...\r\nBut we need the real maintainance VLAN id and IP addresse from that VLAN. Now we\'re waiting for this information...','null'),(1146,'Adem Aydin (NETAS External)','AS-GW','2011-06-23','110623-229338','One Connect','Kyle Paged me and informed one of Ucom MAS HT langley server console port does not work, and customer not able to access MAS server also they tried to use same console cable on other Ht langley server and it works as expected. \n\r\nHard reboot did not resolve the issue then I suggest to replace HT langley server on site.\n\r\nCustomer will replace the server ASAP but right now customer does not have any spare HT langley server onsite. \n\r\nThey will serve UCOM service from redundant UCOM MAS HT langley server.','null'),(1147,'Ayse Ozer (NETAS External)','AS-GW','2011-06-20','110617-228139','GET','Problem / Executive Summary :\n\r\nGPS has been provided the action to apply platform patches on BCP after fresh install on MCP GUI there was \"trusted certification expiry\" alarms. After customer finished the application of patches with instructions of GPS he couldn\'t deploy the software patch on BCP.\n\r\nActions Taken / Resolution:\r\nRedeploy deploy BCP. Start over to apply the software patch bundle 10.3.2.13 for the rest of the blades. All alarms cleared and the BCPs left up and running. Customer confirmed and I dropped from the call.','null'),(1148,'Ayse Ozer (NETAS External)','AS-GW','2011-06-17','110617-228096','R-Cable','Problem / Executive Summary :\n\r\nWeb collaboration File transfer not working after CVM13 MAS upgrade.\n\n\r\nActions Taken / Resolution:\n\r\nWe have changed regedit configuration for webcollab by following below procedure. \r\nPlease follow the instructions below; \r\ni) Connect to the WebCollab Server Blade. \r\nii) Click Start/Run and type regedit. \r\niii) For OfficeUser, ensure the following registry entry value is set to \"OfficePassword!\" \r\nKey = HKEY_LOCAL_MACHINE\\SOFTWARE\\WebDialogs\\ConvertManager\\Users\\OfficeUser1 \r\nName = \"P\" \r\niv) For FtpUser, ensure the following registry entry value is set to \"FtpPassword!\" \r\nKey = HKEY_LOCAL_MACHINE\\SOFTWARE\\WebDialogs\\ConvertManager\\Users\\FtpUser \r\nName = \"P\" \r\nAlso let me remind additional information for webcollab to. Web collab server is using 2 IP address one for app sharing one collaboration but now this server has only one IP address \"213.60.205.10\". and they can get different problem while they are using webcollab feature.','null'),(1149,'Ayse Ozer (NETAS External)','AS-GW','2011-06-16','110616-227860','R-Cable','Problem / Executive Summary :\n\r\nMAS blades to be upgraded on today\'s maitenance window were: \r\n213.60.205.10 --> Blade 8 (WEB COLABORATION SERVER, WEBInterpoint) \r\n213.60.205.11 --> Blade 7 (MOH) \r\n213.60.205.12 --> Blade 6 (Web collaborate, Meetme) \r\n213.60.205.13 --> Blade 5 (Web collaborate, Meetme) \n\r\nBlade 8 was the first blade to be upgraded 213.60.205.10. \r\nBlade 8 had a MAS platform already installed (MAS Server Console). \r\nAccording to upgrade procedure, MAS services then MAS platform were uninstalled. \r\nWhen installing MAS platform 6.1, failure was returned indicating port 80 was already in use by IIS Manager. \r\nGSD stopped Default web site on IIS Manager (Web site using port 80), MAS platform was installed successfully. \r\nWeb collaboration Server was then installed. \r\nAlthough installation was successful, critical alarm is active on blades 5 and 6 indicating \"Connection to Collaboration Server Down\". \r\nCustomer confirmed Meetme functionality was working, but indicated they could not verify Web collaboration functionality prior to the upgrade and although this was not affecting their customers, they intend to provide this functionality in the future. \r\nGSD needs GPS support indicating what needs to be changed on MAS blade 8 to ensure active alarms are cleared (these were not active prior to upgrade) and customer can work on Web collaboration testing scenarios. \n\n\r\nActions Taken / Resolution:\n\r\nFirstly, WEB COLLABORATION is not a service which Genband supports and the installation of this service should not be supported in the future. GPS informed SWD with this info and asked them to inform customer with the info. \r\nSWD wants to continue with the resolution and GPS involved by providing the best effort on an unsopported MAS service. MAS collaboration SERVER was running MAS platform despite the platform should not be there so GPS asked SWD to remove the platform on MAS collaboration server. It was also installed on 5.0 MAS collab server by wrong. GPS asked them to remove the web collab and re-install it again. Before than that GPS checked the all config on Web Collab Server and meetme blade.\n\r\n1. uninstall mas platform\r\n2. reboot\r\n3. check out the alarms \r\n-- alarms disseappeared but blade was stayed down so action continued\r\n4. if alarms persists uninstall web collab server \r\n5. reinstall it again\r\n6. reboot\n\r\nWeb collab server became up and metme conference was successful without the file sharing. Since the communication alarms gone and customer will be checking ftp server, I dropped from conference.','null'),(1150,'Ayse Ozer (NETAS External)','AS-GW','2011-06-16','110616-227851','R-Cable','Problem / Executive Summary :\n\r\nSWD runs provtool script on MAS version 6.1.463 and got the provtool moh_upload_media concellodesada.es sada D:\\GRAL_CENTREX.wav error.\n\n\r\nActions Taken / Resolution:\n\r\nOn 6.1 MAS there was no way to provision MOH through provtool. The way is to provision it through MAS EM. I sent instructions to complete the provisioning for the MOH. From media management -> provision media should be added the .wav file with the content namespace as MusicOnHold. Customer was able to provision the MOH file successfully after that.','null'),(1151,'Ayse Ozer (NETAS External)','AS-GW','2011-06-17','110617-228142','LIMA MMP','Problem / Executive Summary :\n\r\nSWD reported that during the install of the new MAS Adhoc Conferencing platform install (6.1.463), during the check the install would fail due to a hardware FAIL, after checking the upgrade log, it was notice that the MAS does not have the required amount of RAM to complete the upgrade, which is minimum of 2GB (recommended 4GB). \r\nHere is a copy of the Failed message: \r\nHARDWARE failure \r\nMemory FAIL Size 1023 The system has only 1023 MB of RAM memory, when the minumum requirement is 2 GB.\n\n\r\nActions Taken / Resolution:\n\r\nBCT blades on 5.0.643 to 6.1.463 doesn\'t supported minumum than 2GB memory and SWD received the error because of the prerequirement isn\'t applied. The blade had only 1 GB memory. So SWD asked preocedure to roll back to 5.0.643 platform.\r\nI told him to remove the current platform and after a reboot install the 5.0 platform and then ad-hoc service on MAS blade. To be able to successfully roll back to MAS he also needed to restore the service and config backup to MAS through MAS console. After a reboot the MAS was back in service and we dropped from conference.','null'),(1152,'Ayse Ozer (NETAS External)','AS-GW','2011-06-17','110617-228139','GET','Problem / Executive Summary :\n\r\nGPS has been recommended customer to replace the blade. After blade replacement BCP blade status seems down after replacement. \n\r\nActions Taken / Resolution:\n\r\nGPS checked out the config on BCP blade but customer never added the previous config on BCP blade and the platform level has been stayed on 9.0 instead of 10.1. Reinstalled the BCP platform and configured BCP with the previous back up. BCP blade came up and running status but since the platform patches didn\'t applied there was trusted certification expiry alarms on BCP blade slot 3. We replaced the internal and external certification files but it didn\'t resolved the issue. We asked customer to apply platform patches. They told me it will be done on Monday so I dropped from the call.','null'),(1153,'Ayse Ozer (NETAS External)','AS-GW','2011-06-17','110617-228090','GET','Problem / Executive Summary :\n\r\nSWD reported that he was applying patch bundle 10.3.2.13 to the SSL elements. He had successfully patched all elements except the BCP blades and proceded with the BCP patching. Blades 2 and 1 completed successfully. Blade 3 failed patching and also said it failed rollback in the MCP gui. BCP blade stayed in unavailable state.\n\r\nActions Taken / Resolution:\n\r\nAfter SWD reported the problem we had access issues to ssh BCP blade and there was access issues to be able to troubleshoot the problem. I rebooted the bladeto be able to apply the password recovery solution on 10.3 load through GRUB screen and it failed at filesystem checked and got stuck in maintenance mode and also didn\'t start with the kernel panic and also filesystem check errors. GPS recommended to replace the blade and re-install the BCP with the backup config.','null'),(1154,'Ayse Ozer (NETAS External)','AS-GW','2011-06-16','110615-227679','Videotron','Problem / Executive Summary :\n\r\nSWD reported that MAS EM cannot be restarted from web interface after MAS 6.1 upgrade. The button was greyed out and GPS found out Nortel MAS service got stuck on Windows services. Also noticed SWD didn\'t delete the old Nortel folder from disk D before installing MAS 6.1.0.463 load as a prerequisite. \n\r\nActions Taken / Resolution:\n\r\nUninstalled MAS 6.1 platform and services from blade 1, 2 and 4. Before installing the loads again, I followed upgrade procedure step by step and removed the old Nortel folder from disk after that installed platform and services correctly and it works as expected. On the blade 3 SWD forgot to uncheck the preserve config data tab during MAS 5.0 uninstall and this caused database problem when deleting Nortel folder from disk. To fix this problem I followed the steps below. \r\n1. Installaed MAS 5.0.629 load again. \r\n2. Restart windows OS \r\n3. Uninstalled MAS platform without preserving configuration and service data 4. Restart Windows OS \r\n5. Installed MAS 6.1 palatform and service successfuly and it works as expected. \n\r\nThen SWD confirmed all MAS blades are operating successfully and we dropped from bridge.','null'),(1155,'Huseyin Yavuzturk (NETAS External)','AS-GW','2011-06-08','110608-226558','Shaw Cablesystem','While trying to restore the config and data on MAS blade 2, I received the following error: \n\r\n/var/mcp/ma/MAS/bin/masupgrade MAS_blade2_preCVM150_nortel-2bludvy \r\n9__2011_06_07_7_54_54.zip \r\n/var/mcp/ma/MAS/bin/masupgrade: error while loading shared libraries: libcpemerson.so: cannot open shared object file: No such file or directory \n\r\nThere also appears to be a problem with the procedure that it tells you to use just the masupgrade command, but I had to give it the path to the masupgrade command for all blades: \r\n/var/mcp/ma/MAS/bin/masupgrade \n\r\nthe both server designated to each other to make the config same and then remove the designation and the issue resolved. Thanks to Adem Aydin for his support.','null'),(1156,'Adem Aydin (NETAS External)','AS-GW','2011-03-30','110330-213554','UPC','Problem Description: \r\n===================== \r\nER reported that the customer upgraded their system from MCP 10.3.2.4 to 10.3.2.10 release. After that all BCP blades generates below alarm.\r\nAlarmName: RTP Media Portal Configuration/Initialization Error\r\nTimeStamp: Wed Mar 30 04:30:59 EEST 2011\r\nFaultNumber: 804\r\nShortFamilyName: RTPB\r\nLongFamilyName: RTPBLADE\r\nSeverity: CRITICAL\r\nProbableCause: configuration or customization error\r\nDescription: An error occurred during initialization. The RTP Media Portal Cluster Data is misconfigured. Please review cluster config data.\r\nCluster Call Servers - too many datafilled. (1-20). The RTP Media Portal is NOT operational.\r\nCorrective Action: Please contact your next level of technical support.\n\r\nResolution: \r\n============ \r\nFirst of all, I requested site connection info and MCP GUI address then I have checked all GWC datafill from RTP cluster settings when I compared the GWCs for BCP cluster from specbook, according to specbook Cluster1 has only 8 GWCs, Cluster 2 has only 4 GWCs and Cluster 3 has only 18 GWCs but on the MCP GUI all GWC assigned to all BCP clusters.\r\nI discussed this issue with ER and he also said specbook does not have the correct information then he checked provisioning setting and found the assigned GWCs then we add those GWCs to the BCP clusters and all alarms is gone.\n\r\nCustomer performed call test then I drop from the bridge.','null'),(1157,'Ayse Ozer (NETAS External)','AS-GW','2011-03-27','110326-213049','Cypress Commmunications','Problem Description:\r\n=====================\r\nThe BCP blade didn\'t come up after powercycle and gave error as kernel panic, customer re-called ER to make sure what the new action will be according to this error. The prompt screen was not coming previously and this error was newly appeared after powercycle.\n\n\r\nResolution:\r\n============\r\nGPS connected to customer site via VNC and see the error which shows sector error on disk and also a kernel panic on screen after restart and this addresses the problem as it is the bad H/W. \r\nThe error text was shown on blade as below.\r\n\"Failing the same thing kernel panic - not syncing: Attempted to kill init.\"\n\r\nGPS recommended to replace the BCP blade with a spare. Customer told there is a MAS blade and asked to use this blade by installing the BCP on it. GPS told them the ESM/ERS config and VLAN settings will be changed so it will cause an outage, so asked to find a BCP blade. Customer found a spare BCP blade and started to reinstall the blade. GPS helped to config the network settings. Customer deployed the BCP (1+1 cluster) 4 blades in total. GPS monitored the BCP blade through MCP GUI, no alarms appeared and test calls were successful, dropped from the call with agreement.','null'),(1158,'Ayse Ozer (NETAS External)','AS-GW','2011-03-26','110326-213049','Cypress Commmunications','Problem Description: \r\n===================== \r\nER reported that the customer was trying to connect one of the BCP blades with root user and realized the root password doesn\'t work. To apply a recovery procedure to change the root password from GRUB screen customer did a restart and after that the prompt never appeared and got stuck on a screen which shows the MAC addresses. \n\r\nResolution: \r\n============ \r\nFirst of all, I requested the recovery password procedure which customer is trying to apply, this workaround has been applied on releases before CVM13 and it was not applicable. Also the customer just restarted the blade to see the GRUB screen but the GRUB screen also never appeared as expected. I did a couple restart on this BCP but it didn\'t help. I asked customer to powercycle (hard reset)the blade but there was no technician on site also the installation CD was not inserted on tray to re-install. I provided them an action plan below to do at site on maintenance hours. \n\r\n1.Powercycle the blade 4. \r\n2.Reseat blade 4. \r\n3.Insert the BCP installation CD into tray and then try to install by following the installation procedure. (Send the procedure) \r\n4.If below steps dont work it probably shows us the hardware is problematic. So make sure a spare BCP blade ready on site. \n\r\nCustomer would talk to a techinian to apply the actions on site in maintenance hours and would give a call to ER if given actions won\'t help. I dropped from the call with agreement.','null'),(1159,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2019-05-15','190515-257655','KANDY','After SpiDR migration in KANDY China, customer reported that subcriber registrations fail. SpiDR CallP team were working on the issue and identified that there is no any entry in database. \n\r\nI connected to DB and checked accounts table and confirmed that all entries were removed somehow. That was the reason for why subscription of users failed.\n\r\nmysql> select * from account;\r\nEmpty set (0.00 sec)\n\r\nIn order to recover system database from a backup file, I checked the lastly taken backup lists and observed that customer take regular DB backups everyday at 6:30 A.M via cron job.\n\r\n[root@ainfospdr1-1 bin]# find / -name *.sql.gz\r\n/opt/backups/mysql/fulldb.190513T064537.sql.gz\r\n/opt/backups/mysql/fulldb.190515T064537.sql.gz\r\n/opt/backups/mysql/fulldb.190514T064537.sql.gz\r\n/opt/backups/restore/ainfospdr1-db-2-fulldb.sql.gz\n\r\nSince there was a taken backup from morning, I restored empty DB with lastly taken backup as follows:\n\r\n1. Connect to the active Admin tier as root via SSH.\r\nssh -l root \r\n2. Stop all services on the system.\r\nrunon --all \"service wae stop\"\r\n3. Stop all DB instance services on the system.\r\nrunon --db \"service wae stop db\"\r\n4. Restore the DB backup of the system.\r\n/opt/wae/bin/restoredb fulldb.190513T064537.sql.gz\r\n5. Activate the DB.\r\nservice wae activatedb\r\n6. After the restored DB becomes active, start the standby DB.\r\nrunon --sdb \'service wae start\r\n7. Restore the backup of system.conf.\r\nputon  \"/root/config_backups/system_.conf\" \"/opt/wae/etc/system.conf\" Where  is the tier ID\r\n8. Restore the backup of wae.conf.\r\nputon  \"/root/config_backups/wae_.conf\" \"/opt/wae/etc/wae.conf\"\r\n9. Repeat Step 7 and Step 8 for all tiers in the system.\n\r\nAfter I completed step 6, I could verify that account table data entries had been restored successfully. I was able to monitor account records in accounts table. \n\r\nWhile I was performing step7 and step8 (distribution of wae and system conf files to other tiers), I faced the following collision errors on admin1.\n\r\n[root@ainfospdr1-1 host_1]# puton 1 \"/root/config_backups/host_1/system_1.conf\" \"/opt/wae/etc/system.conf\"\r\nMay 15 18:19:39 puton FATAL: SQL and  WAE_OAM _NET config collision. 172.24.189 - 172.24.185\n\r\nIt was blocking to take any action on admin1. As a resolution, we decided to take a copy of system and wae conf files from admin2 and apply on adm1. In this way, we were able to fix inconsistencies between SQL- WAE_OAM _NET and SQL_GATEWAY- WAE_OAM_GATEWAY. \n\r\nAfter conf files recovery actions, both admin tiers were working fine. Due to the fact that DB recovery action includes stopping all tiers in step2, all other tiers were in down position. As a last action, we run on service wae start on tier 3,5,6,7,9. \n\r\nAfter DB recovery had been completed and all tiers started to work as expected, customer performed registration and other tests and reported that everything works fine again.\n\r\nIn order to understand the reason for why DB resident config data was lost and caused this kind of an outage, we collected all required logs from all tiers. RCA will be provided on case 190515-257655','null'),(1160,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2019-03-20','190320-178316','INTELECOM','Upgrade Path --> 9.3.2 to 9.5.0\n\r\nProblem Description\r\n====================\r\nDuring the upgrade of SPIDR spidr-1, Host 5 failed to start:\n\r\nActions taken\r\n=================\n\r\nI have been paged by Donnell Williamson from SWD and he reported that host 5 failed to start \n\r\nWhen I checked the wae-status output I noticed that turn5 was STO. I tried to \"service wae start\" to recover this tier but it failed.\n\r\nAfter that, I started to investigate the logs but they did not help. After I consulted the Serhat from the design team, and we started to investigate the problem together.\n\r\nWhen we checked the rpm packages of this tier under /opt/wae_repo/current/wae directory we noticed that \"wae-webrtcBroker-user-mode-media-9.5.0.ax69-1.noarch.rpm\" was missing in there. We connected to admin1 tier and sent the related rpm package to the host5 via \"puton\" command. After that we connected to host5 again and executed the below commands respectively;\n\r\n#yum install wae-webrtcBroker-user-mode-media-9.5.0.ax69-1.noarch.rpm\r\n#wae-assetup --rebuild \r\n#service wae restart\n\r\nAfter performing the above action, the problem has been solved and broker tier returned its healthy state.\n\r\nAny Follow-up Case\r\n===================\r\n190320-178316','null'),(1161,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2019-03-20','190319-178296','INTELECOM','Upgrade Path --> 9.3.2 to 9.5.0\n\r\nProblem Description\r\n====================\r\nUpgrade has failed due to \"wae FATAL: WAE_TURNSERVER_LISTENING_GATEWAY is not defined\"  \n\r\nActions taken\r\n=================\n\r\nI have been paged by Donnell Williamson from SWD and he reported that upgrade has failed due to \"wae FATAL: WAE_TURNSERVER_LISTENING_GATEWAY is not defined\" error on host7.\n\r\nI jumped the problematic Turn tier (host7) and described the below parameters on its system.conf file.\n\r\n#WAE_TURNSERVER_LISTENING_GATEWAY\r\n#WAE_TURNSERVER_RELAY_GATEWAY\n\r\nAbove parameters should be matched with its DEFAULT_GATEWAY parameter on wae.conf file. \n\r\nI performed the same actions for host8.\n\r\nAfter I edited conf files, I execute the \"service wae restart\" command on host7 and then the problem has been resolved.\n\r\nAny Follow-up Case\r\n===================\r\n190319-178296','null'),(1162,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2019-03-20','190319-178296','INTELECOM','Upgrade Path --> 9.3.2 to 9.5.0\n\r\nProblem Description\r\n====================\r\nUpgrade has failed due to \"wae FATAL: EXT_OAM_GATEWAY is not defined\" error on Broker tier. \n\r\nActions taken\r\n=================\n\r\nI have been paged by Donnell Williamson from SWD and he reported that upgrade has failed due to \"wae FATAL: EXT_OAM_GATEWAY is not defined\" error on host5.\n\r\nI jumped the problematic broker tier (host5) and described the EXT_OAM_GATEWAY and DEFAULT_GATEWAY parameters on its system.conf, and also I copied the DEFAULT_GATEWAY parameter to its wae.conf file. I copied these parameters to system.conf and wae.conf files of the admin tier as well. \n\r\nI performed the same actions for host6.\n\r\nAfter I edited conf files, I execute the \"service wae restart\" command on host5 and then the problem has been resolved.\n\r\nAny Follow-up Case\r\n===================\r\n190319-178296','null'),(1163,'Omer KIRCALI','','2019-03-11','190310-176751      ','IPC SYSTEMS','Customer Load(If an upgrade problem upgrade path): 9.3.2.au44\n\r\nProblem Description:\r\n==============================\r\nNeed a procedure for changing an unknown admin user password.\n\r\nActions Taken:\r\n====================\r\nBob Johnson paged me about asking a procedure for changing root password of admin tier. He stated that they do not know the admin root password and they cannot recover it with below procedure;\n\r\n1.ssh to admin tier IP with root user.\r\n2.Change the directory as /opt/wae/bin/\r\n3.Run the script \" ./checkPasswd.pl\" to see which users have the alarm\r\n4.Run \"passwd \" (like admin etc) and enter new password\n\n\r\nExample:\r\n[root@gpsspidr-1 ~]# cd /opt/wae/bin/\r\n[root@gpsspidr-1 bin]# ./checkPasswd.pl\r\nThe password of >>admin<< is expired.\r\n[root@gpsspidr-1 bin]# passwd admin\r\nChanging password for user admin.\r\nNew password:\r\nRetype new password:\r\npasswd: all authentication tokens updated successfully. \n\r\nInitially , I informed him that we are not support that kind of problem with pager. Even thought this is the case , I shared a procedure \"SPiDR_Admin_Tier_Root_Password_Reset.docx \" with him. It also attached to the case. Since a reboot needed for performing procedure , they will done it off shit. I dropped from call after shared the procedure.\n\r\nAny followup case:\r\n=======================\r\nNone','null'),(1164,'Buket Nazmiye KUCUKER (NETAS External)','','2019-03-11','190310-176751      ','IPC SYSTEMS','Customer Load:9.3.2.au44\r\nProblem Description\r\n====================\r\nAdmin and Root Passwords Expiring on multiple site\n\r\nActions taken\r\n=================\r\nThomas Godwin from Ribbon ER reported that IPC systems have Passwords Expiring alarms on their New York and London systems. \r\nI have told ER that we dont give pager support for Major alarms if there is no outage. \r\nThomas have replied that The guy that called into ER was demanding that this be worked now and ER have opened a BC case.\r\nI\'ve asked site connection but ER didnt connect the site before calling. \r\nI told Thomas that they have a case for this alarm now and it doesnt affect the system so  i should not be paged. \r\nHe told me that he doesnt know anything about SPIDR so i offered him to give the procedure to customer.\r\nCustomer support team had been told not to do any procedure. \r\nWhile he was trying to get the Webex access, he convinced the customer support team to wait till morning in their time(8am EST). \r\nCustomer wants to call back to ER in the morning and also to have the procedure for later expiring alarms. \r\nI have prepared procedure for ER and IPC systems. \r\nER confirmed that he will make the procedure with Bob Johnson so GPS help will not be needed. \r\nYou can reach the procedure and other steps with following case ID 190310-176751.\n\n\r\nAny Follow-up Case\r\n===================\r\nNone','null'),(1165,'Bill Picardi','KandyLink/SPiDR-OAM','2019-02-11','190211-172535','IPC SYSTEMS','The customer reports many tiers are down and unreachable from wae-status output.  The tier down/unreachable status is a result of network lag, and the command timing out.  They report that outages and issues experienced with their hosting provider over the weekend had included massive latency.\r\n  The host2 tier has admin2 stopped and db2 failed.  The admin2 software and database was corrupted, requiring \"wae-assetup --rebuild\" and restoredb to restore services.  The cause of this is due to the vendor\'s VMware host issues (outage?) and the network latency issues.  We performed status checks of the network with the ping command, both from the workstation and from tier to tier, and network latency and timeouts were seen.\r\nDue to network latency and timeouts, the health checks believe that the peer is down and will randomly failover the database due to this condition, and raise alarms.  This caused a replication issue, resulting in one of the database tiers to bounce up and down continuously.  Another DB restore and service restart was necessary to recover from this.\r\nThe customer was advised to review and correct the networking issues, and we can check SPiDR status again once that is resolved.','null'),(1166,'Buket Nazmiye KUCUKER (NETAS External)','KandyLink/SPiDR-OAM','2019-01-08','190108-167585','IPC SYSTEMS','Rodney Neese from ER called OAM SPIDR GPS that IPC system had DB connection Failure alarm in their system. He helped me to connect the site and i run \"wae-status\" all tiers were working stable. I wanted them to open SPIDR GUI but IPC DIDNT KNOW how to OPEN SPIDR GUI!! I told Redney that we have a case for this alarm and it doesnt affect the system. After Redney told it to customer, IPC DROPPED THE CALL. The duplicate case was 181227-166665. I told them we are still investigating the issue and we should continue with case. I have checked the case and seen that the case owner already gave the update and IPC didnt Reply. \n\r\nPS:IPC AGAİN CALLED PAGER FOR AN EXISTING CASE WHICH GPS  WAITING FOR AN UPDATE FROM THEM BUT NO REPLY.','null'),(1167,'Buket Nazmiye KUCUKER (NETAS External)','KandyLink/SPiDR-OAM','2019-01-08','190108-167573','IPC SYSTEMS','Rodney Neese from ER paged OAM SPIDR GPS pager. TURN tier on Host7 and Host8 was Stopped. He called before providing the site access information. When i runned wae-status i have seen that Host7 and Host8 was stopped. I performed the restart and start first by using \"service wae restart\" and \"service wae start\". I received error while starting it. Then i run the command \"service wae diag\" and i took errors about the Certificate and Licence Key was missing. I have change the directory /etc/certs and run \"ls-larth\" \r\nThere were neither Licence Key nor Certificate in there. I told the customer that i will deploy the files one by one. I run the commands below to deploy the all files :\r\nwae-certsetup --rebuild \r\nhost7 \r\n\"service wae stop\" \r\n\"wae-assetup --rebuild\" \r\n\"wae-fwsetup --rebuild\" \r\n\"service iptables restart\" \r\n\"service wae start\" \r\nhost8 \r\n\"service wae stop\" \r\n\"wae-assetup --rebuild\" \r\n\"wae-fwsetup --rebuild\" \r\n\"service iptables restart\" \r\n\"service wae start\" \n\r\nAfter this the system turned stable. \n\r\nThe follow up case was open:190108-167579','null'),(1168,'Buket Nazmiye KUCUKER (NETAS External)','KandyLink/SPiDR-OAM','2019-01-07','190107-167383','IPC SYSTEMS','ER paged OAM SPIDR GPS pager about the issue that we gave the actions before. The ER paged us when the customer called them back after reboot. The procedure of i gave was not proceed. ER didnt connect the site before calling. After he connected he sent me the GTS VM. When i have connected to site, i wanted to run \"wae-status\". I asked to take the control , they could not give the control after this, IPC dropped the call because of they have another BC problem and they said to ER that they will call them back. We had to close the session after we have connected. I gave the actions plan again to Tom and said that if it doesnt work call us back. \r\nIPC called back in a few hours. Er saw that the procedure worked and could tar the logs under /var/log on Host4. \n\r\nThe follow up case was opened with 190107-167492. \r\nThe procedure that should be followed :\n\r\nAfter reboot :\n\r\n1. Run \"wae-status\" on Host1.\r\n2.If all is running you can collect the logs and attach them to the case. The procedure to follow is :\n\r\nhost4 \r\ntar cpvzf /opt/logcollection-app_pres4.tar.gz /var/log/ \r\nexit\r\ngetfrom 4 /opt/logcollection-app_pres4.tar.gz /opt\r\nrunon 4 \"rm /opt/logcollection-app_pres4.tar.gz\"\n\r\n3. If Host4 is not running (STO or FAI) then please proceed the steps below:\n\r\nhost4\r\nservice wae restart\r\nexit\r\nwae-status\n\r\n4. If the Host4 is still not running; page to OAM GPS Pager . \n\r\nThey runned wae-status and saw that all tiers are running. They could collect the tar file successfully and attach them to the case.','null'),(1169,'Buket Nazmiye KUCUKER (NETAS External)','KandyLink/SPiDR-OAM','2019-01-07','190107-167383','IPC SYSTEMS','Thomas Goldwin from Ribbon ER reported that IPC system had DB Connection Failure alarm on their systems. After fixing the problem, they realised that they couldnt take logs from host4. I have connected the site with Thomas and run wae-status. All hosts and tiers were up. After i tried to connect the Host4 and when try to do the procedure  i received the same problem:\n\r\n[root@ny5con-4 ~]# tar cpvzf /opt/logcollection-app_pres4.tar.gz /var/log/ \r\ntar: Removing leading `/\' from member names \r\ntar (child): /opt/logcollection-app_pres4.tar.gz: Cannot open: Read-only file system \r\ntar (child): Error is not recoverable: exiting now \r\n/var/log/ \r\n/var/log/messages-20181226.gz \r\n/var/log/cron-20181229.gz\n\n\r\nI have checked the files permissions under the directory /var/log and /opt. But the files were read-write. After this i run \"service wae restart\" and i received the all files Read-Only error. \n\r\nDue to this failure, i saw that the problem is on the disks on VM. This problem is caused by the third party service hosting the virtual machines, in which their storage array/SAN used by the virtual machines faulted.  This placed the operating system to place the SPiDR virtual machines\' disk drives into a read only state, recoverable only by a power-cycle of the virtual machine. \n\r\nI recommended the Power cycling to Host4 VM. After power cycling(reboot) ER will proceed the steps below:\n\r\n1. Run \"wae-status\" on Host1.\r\n2.If all is running you can collect the logs and attach them to the case. The procedure to follow is :\n\r\nhost4 \r\ntar cpvzf /opt/logcollection-app_pres4.tar.gz /var/log/ \r\nexit\r\ngetfrom 4 /opt/logcollection-app_pres4.tar.gz /opt\r\nrunon 4 \"rm /opt/logcollection-app_pres4.tar.gz\"\n\r\n3. If Host4 is not running (STO or FAI) then please proceed the steps below:\n\r\nhost4\r\nservice wae restart\r\nexit\r\nwae-status\n\r\n4. If the Host4 is still not running; page to OAM GPS Pager .','null'),(1170,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2019-01-04','190104-167277','IPC SYSTEMS','Gary Norwood paged me out and reported that TURN2 is down in IPC\'s NYC SPiDR site. IPC clasically was created a BC case and directly called ER for receiving a case update in 5 minutes.\n\r\nIn order to connect to problematic customer site, I have joined GTS VM session of Gary but nothing was ready for starting to work on the issue. (Site access, connected server connections, IPs, etc.)\n\r\nI have waited approximately 15-20 mins but nothing changed. IPC was not still providing site access via WebEx. \n\r\nSince we don\'t have anything to work on the reported issue even though waiting for a long time, I put a case comment on BC case. Basically, what should be done is starting the host8 and collecting a few turn2-related logs. I have shared what should be performed in detail on case notes. I hope that they will be able to perform the provided simple action plan.\n\r\nBasically the following actions will be taken on host8 by customer (hopefully):\n\r\n1- Connect to admin1 as root \r\n2- Run \"wae-status\" and make sure that TURN2 (Host8) is in STOPPED/down status.\r\n3- After confirming that TURN2 is down, please jump from admin1 to host8 by running \"host8\"\r\n4- Run \"service wae start\" \r\n5- You can re-check the status of problematic TURN tier by running \"wae-status\" on admin1.\r\n6- If recommended action plan brings the TURN2 into UP status again, please compress everything located under /var/log/ on host8 via tar and attach the tarred file into case files for further investigation.\r\n7- Additionally, you can collect TURN trace logs by taking the below steps:\n\r\na- ssh to host1(admin1) and jump to host8 with below command;\r\n>host8\r\nb- go to /tmp/ directory on host8;\r\n>cd /tmp/\r\nc- run below command on host8 under /tmp/ directory;\r\n>tar -zcvf gpsHost8Logs.tar.gz /var/log/wae/waeas/turn_81/*\r\nd- run below command to check if \"gpsHost8Logs.tar.gz\" is exist under /tmp/ directory or not.\r\n> ls -lrt\r\ne- type \"exit\" to turn back to admin1;\r\nf- run below command on admin1 to transfer the tar.gz file from host8 to /tmp/ directory of the admin1;\r\n>getfrom 8 /tmp/gpsHost8Logs.tar.gz /tmp/\r\ng- run below command to check if \"gpsHost8Logs.tar.gz\" is exist under /tmp/ directory or not.\r\n> ls -lrt\r\nh - As a final action, transfer the \"gpsHost8Logs.tar.gz\" to your local pc and attach to case.\n\r\nSince we worked on similar issues in the past, we have created design JIRAs and design investigation will be ongoing next week again. I will be providing the latest updates on case notes regularly again.','null'),(1171,'Mustafa YUKSEK (NETAS External)','KandyLink/SPiDR-OAM','2018-12-24','181224-166396','IPC SYSTEMS','Tom Draper paged to me about DB_CONNECTION_FAILURE critical alarm on NY5 nodes that issue belonged to IPC SYSTEMS which is on 9.3.2 release.\r\nWhen I connect the site with webex I checked the DB states, as you can see below;\n\r\n[root@ny5inet-1 ~]# wae-status \r\nHost VERSION ADMIN DHCP DNS REPO APP PROXY PRES DB NTPD TURN BROKER \r\n1 9.3.2.au65 run OFF run run - - - ACT run - - \r\n2 9.3.2.au65 run OFF run run - - - STA run - - \r\n3 9.3.2.au65 - - - - run run run - run - - \r\n4 9.3.2.au65 - - - - run run run - run - - \r\n5 9.3.2.au65 - - - - - - - - run - run \r\n6 9.3.2.au65 - - - - - - - - run - run \r\n7 9.3.2.au65 - - - - - - - - run run - \n\r\nand output of \"service wae diag\" showed to us DB\'s are working as ACT/STA. In this respect, I asked for confirmation if there is any service impact or not, after a half hour later they responded that there wasn\'t any service impact. For this reason, I recommended to the customer that we should work on this issue as a case work.\r\nThey opened 181224-166403 case and attached related logs then I dropped from the pager.','null'),(1172,'Omer KIRCALI','KandyLink/SPiDR-OAM','2018-12-18','181218-165616','IPC SYSTEMS','Customer Load(If an upgrade problem upgrade path): 9.3.2.au65\n\r\nProblem Description:\r\n==============================\r\nBob Johnson paged me about calls failing on Host3 and host 4. The service wae diag output is below;\n\r\nservice wae diag  \r\nntpd :running: pid=2370  \r\nserver: OK  \r\nrsyslog :running: pid=7968  \r\nslb :running: pid=24826  \r\neSIP: OK  \r\niSIP: OK  \r\napp :running: pid=25787  \r\nSIP: no udp port 5180  \r\nApp servlets: OK  \r\nMPCP: no udp port 3904  \r\npres :running: pid=26845  \r\nClient Servlet: OK  \r\nClient Notifications: OK\n\r\nActions Taken:\r\n====================\r\nI informed bob that the issue is a callp issue and he should contact with callp team. I connected the site and checked the states of the tiers which was normal;\n\r\n[root@ld4con-1 ~]# wae-status \r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER \r\n 1   9.3.2.au65        run   OFF  run run  -   -     -    STA run  -    - \r\n 2   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    - \r\n 3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n 8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n[root@ld4con-1 ~]#  \n\r\nAfter informing bob, I dropped from call. Investigation is continue on callp side.\n\r\nAny followup case:\r\n=======================\r\nNone','null'),(1173,'Omer KIRCALI','KandyLink/SPiDR-OAM','2018-12-17','181216-165376','IPC SYSTEMS','Customer Load(If an upgrade problem upgrade path):9.3.2.au65\n\r\nProblem Description:\r\n==============================\r\nTom draper paged me about registration problem on spider ny5con-2 site. He stated that customer thinks , disabling dbhealthcheck script on HOST2 may caused the issue.\n\r\nActions Taken:\r\n====================\r\nI connected the site and check the status of the tiers which was fine;\n\r\nny5con-2 ~]# wae-status \r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER \r\n 1   9.3.2.au65        run   OFF  run run  -   -     -    STA run  -    - \r\n 2   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    - \r\n 3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n 8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n[root@ny5con-2 ~]#  \n\r\nI inform customer that disabling dbhealthcheck script on HOST2 is not root casue of the callp issue and they should page callp team for registration problem. After inform ER and customer I dropped from call.\n\r\nAny followup case:\r\n=======================\r\nNone','null'),(1174,'Omer KIRCALI','KandyLink/SPiDR-OAM','2018-12-17','181217-165397','IPC SYSTEMS','Customer Load(If an upgrade problem upgrade path):9.3.2.au65\n\r\nProblem Description:\r\n==============================\r\nMark Zattiero contact me about critical alarms on NY4 nodes ipcrtcadm01nye and ipcrtcadm02nye: WAE STR1=DB2 not running.\n\r\nService: Genband Traps\r\nHost: ipcrtcadm01nye\r\nAddress: 10.108.52.138\r\nState: CRITICAL\r\nInfo:\r\nThis notification indicates that an alarm of Critical severity enterprises.5584.3.1 2018-12-16T12:44:33.414-05:00 NODE_NOT_ALIVE Gauge32: 2 Gauge32: 163 DB-2 NODE_NOT_ALIVE domain=jboss.management.local,system=NodeMonitor,entity=NodeAlarm,name=DB-2,J2EEApplication=wae-base-adm.ear,EJBModule=model-ejb.jar Gauge32: 2 Gauge32: 13 Gauge32: 13 / mib-2.118.1.2.2.1.10 ():enterprises.5584.3.1 mib-2.118.1.2.2.1.2 ():2018-12-16T12:44:33.414-05:00 mib-2.118.1.2.2.1.11 ():NODE_NOT_ALIVE enterprises.5584.2.1.1.1.1.1.1.1 ():Gauge32: 2 enterprises.5584.2.1.1.1.1.1.1.2 ():Gauge32: 163 enterprises.5584.2.1.1.1.1.1.1.3 ():DB-2 enterprises.5584.2.1.1.1.1.1.1.4 ():NODE_NOT_ALIVE enterprises.5584.2.1.1.1.1.1.1.5 ():domain=jboss.management.local,system=NodeMonitor,entity=NodeAlarm,name=DB-2,J2EEApplication=wae-base-adm.ear,EJBModule=model-ejb.jar enterprises.5584.2.1.1.1.1.1.1.6 ():Gauge32: 2 enterprises.5584.2.1.1.1.1.1.1.7 ():Gauge32: 13 enterprises.5584.2.1.1.1.1.1.1.8 ():Gauge32: 13 enterprises.5584.2.1.1.1.1.1.1.11 ():\r\nDate/Time: 2018-12-16 12:44:44\n\n\r\nThe wae-status output is ;\n\r\n[root@ny5con-2 wae]# wae-status\r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER\r\n1   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    -\r\n2   9.3.2.au65        run   OFF  run run  -   -     -    DEG run  -    -\r\n3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    -\r\n4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    -\r\n5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run\r\n6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run\r\n7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  -\r\n8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  -\r\n[root@ny5con-2 wae]#\n\n\r\nDb was on DEG stage.\n\r\nActions Taken:\r\n====================\n\r\nAfter performing Investigation with DB realted logs (dbhealthcheck etc..) , I have worked with DS team and the first action performed is disabling the crontab of dbhealthcheck on host 2 and run the below scripts on host2.dbhealthcheck script should not be exist on Host2;\n\r\nroot@ny5con-2 wae]# crontab -l\r\n5 * * * * /usr/sbin/hwclock --systohc --localtime > /tmp/cron.hwclock.out 2>&1\r\n# * * * * * /usr/bin/flock -n /tmp/fcj.lockfile /opt/wae/sbin/dbhealthcheck -minutely\r\n05 2 * * * /opt/wae/sbin/getconfig --update > /tmp/getconfig.log 2>&1\n\r\nrunon --db \"service wae stop db\"\r\nrunon --db \"service wae kill\"\r\nrunon --db \"wae-hbsetup --rebuild\"\r\nrunon --db1 \"service wae activatedb\"\r\nrunon --db1 \"service wae start db\"\r\nrunon --db2 \"service wae start db\"\r\nrunon --db \"service wae start\"\n\r\nHowever the problem remained same;\n\r\n[root@ny5con-2 wae]# wae-status\r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER\r\n1   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    -\r\n2   9.3.2.au65        run   OFF  run run  -   -     -    run -    -\r\n3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    -\r\n4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    -\r\n5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run\r\n6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run\r\n10.108.153.17 - unable to connect\r\n10.108.153.17 - trying to reconnect\r\n7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  -\r\n8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  -\r\n[root@ny5con-2 wae]# wae-status\r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER\r\n1   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    -\r\n2   9.3.2.au65        run   OFF  run run  -   -     -    DEG run  -    -\r\n3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    -\r\n4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    -\r\n5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run\r\n6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run\r\n7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  -\r\n8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  -\r\n[root@ny5con-2 wae]#\n\n\r\nAfter that I have run waesetup --rebuild;reboot; on host2 but the problem was remained the same;\n\r\nVERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER \r\n 1   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    - \r\n 2   9.3.2.au65        run   OFF  run run  -   -     -    DEG run  -    - \r\n 3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n 8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n[root@ny5con-2 ~]#  \n\r\nAfter that DS engineer provided the below action and It resolved the issue;\n\r\nOn host2;\r\n# echo \"standby\" > /data/dbstate\r\n# service wae stop db\r\n# service wae start db\n\r\n[root@ny5con-2 ~]# wae-status \r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER \r\n 1   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    - \r\n 2   9.3.2.au65        run   OFF  run run  -   -     -    STA run  -    - \r\n 3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n 8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n[root@ny5con-2 ~]#  \n\r\nPlease note that disabling the dbhealthcheck script crontab is a part of this solution! UnbeatMgr is an application on SPIDER that recognize the state of the DB instances as ACT/STD via the scripts under /data/dbstate. It was corrupted due to the dbhealthcheck script was enabled on host2 and upbeat was on unknown stage and we could not see the stage of the DB on Host2 as can be seen on dbhealthcheck log;\n\r\n2018-12-16 13:17:02 dbhealthcheck Database is live 20181216131703. DB1 state is active and DB2 state is \r\n2018-12-16 13:18:03 dbhealthcheck Database is live 20181216131804. DB1 state is active and DB2 state is \r\n2018-12-16 13:19:02 dbhealthcheck Database is live 20181216131903. DB1 state is active and DB2 state is \r\n2018-12-16 13:20:03 dbhealthcheck Database is live 20181216132004. DB1 state is active and DB2 state is \r\n2018-12-16 13:21:03 dbhealthcheck Database is live 20181216132104. DB1 state is active and DB2 state is \r\n2018-12-16 13:22:02 dbhealthcheck Database is live 20181216132203. DB1 state is active and DB2 state is \r\n2018-12-16 13:23:03 dbhealthcheck Database is live 20181216132304. DB1 state is active and DB2 state is \r\n2018-12-16 13:24:02 dbhealthcheck Database is live 20181216132403. DB1 state is active and DB2 state is \r\n2018-12-16 13:25:03 dbhealthcheck Database is live 20181216132504. DB1 state is active and DB2 state is \n\r\nWe manually edited this file as standby and the condition is become stable. \n\r\nLastly I tried to check replication btw primary and secondary DB by adding a external provider named as DUMMY. I dumped it from either Primary DB or Secondary DB and saw that the replication between two DB is healty;\n\r\nFrom first host1;\n\r\nmysql> use ny5con; \r\nDatabase changed \r\nmysql> select id,name from infrastructureservice; \r\n+-------------+---------------+ \r\n| id          | name          | \r\n+-------------+---------------+ \r\n|       32768 | 10.108.54.126 | \r\n|       32769 | 10.108.54.254 | \r\n|  1310818304 | 10.108.56.254 | \r\n| -1509425152 | 10.108.58.126 | \r\n| -1509425151 | 10.108.59.254 | \r\n| -1509425150 | 10.108.60.254 | \r\n|   163905536 | 10.108.61.126 | \r\n| -1509425149 | 10.108.62.126 | \r\n| -1673396224 | 10.108.64.126 | \r\n| -1673396222 | 10.108.65.126 | \r\n| -1673396223 | 10.108.81.254 | \r\n| -1673330688 | 10.108.82.126 | \r\n| -1673363456 | 10.108.82.254 | \r\n| -1673330687 | 10.108.83.126 | \r\n| -1673297920 | 10.108.84.126 | \r\n|  -854032384 | DUMMY         | \r\n|           1 | genband       | \r\n+-------------+---------------+ \r\n17 rows in set (0.00 sec)  \n\r\nFrom Host2;\n\r\nmysql> use ny5con; \r\nDatabase changed \r\nmysql> select id,name from infrastructureservice; \r\n+-------------+---------------+ \r\n| id          | name          | \r\n+-------------+---------------+ \r\n|       32768 | 10.108.54.126 | \r\n|       32769 | 10.108.54.254 | \r\n|  1310818304 | 10.108.56.254 | \r\n| -1509425152 | 10.108.58.126 | \r\n| -1509425151 | 10.108.59.254 | \r\n| -1509425150 | 10.108.60.254 | \r\n|   163905536 | 10.108.61.126 | \r\n| -1509425149 | 10.108.62.126 | \r\n| -1673396224 | 10.108.64.126 | \r\n| -1673396222 | 10.108.65.126 | \r\n| -1673396223 | 10.108.81.254 | \r\n| -1673330688 | 10.108.82.126 | \r\n| -1673363456 | 10.108.82.254 | \r\n| -1673330687 | 10.108.83.126 | \r\n| -1673297920 | 10.108.84.126 | \r\n|  -854032384 | DUMMY         | \r\n|           1 | genband       | \r\n+-------------+---------------+ \r\n17 rows in set (0.00 sec) \n\r\nmysql>  \n\n\r\nAny followup case:\r\n=======================\n\r\n181216-165376 Follow-Up:Genband Traps, WAE status on ipcrtcadm01nyeand and ipcrtcadm02nye','null'),(1175,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-12-17','TBD','IPC SYSTEMS','I have been paged by Bob Johnson and he stated that they were unable to connect to host8. Here is the wae-status output;\n\r\n[root@ny5inet-1 ~]# wae-status \r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER \r\n 1   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    - \r\n 2   9.3.2.au65        run   OFF  run run  -   -     -    STA run  -    - \r\n 3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n10.108.154.18 - unable to connect \r\n10.108.154.18 - trying to reconnect \r\n10.108.154.18 - trying to reconnect \n\r\nI connected to the admin tier and tried to jump to host8 but I got the error below;\n\r\n[root@gpsspidr-1 ~]# host8\r\nssh: connect to host x.x.x.x port 22: No route to host\n\r\nSince the host8 was not reachable, I recommended rebooting to VM. \n\r\nAfter reboot, the problem has been solved.\n\r\n[root@ny5inet-1 ~]# wae-status \r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER \r\n 1   9.3.2.au65        run   OFF  run run  -   -     -    ACT run  -    - \r\n 2   9.3.2.au65        run   OFF  run run  -   -     -    STA run  -    - \r\n 3   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 4   9.3.2.au65        -     -    -   -    run run   run  -   run  -    - \r\n 5   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 6   9.3.2.au65        -     -    -   -    -   -     -    -   run  -    run \r\n 7   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  - \r\n 8   9.3.2.au65        -     -    -   -    -   -     -    -   run  run  -','null'),(1176,'Mustafa YUKSEK (NETAS External)','KandyLink/SPiDR-OAM','2018-12-16','181216-165376','IPC SYSTEMS','Bob Johson paged out to me about the IPC System which is on 4.3.2 release the output of the \"wae-status\" script on Host1 was showing DB of Host2 was degraded and there is a critical alarm on Host1 and Host2 about NODE_NOT_ALIVE and they faced these problems after having a network issue.\n\r\nThen I tried to connect the site and nearly one hour we had waited for preparıng the Webex. \r\nAfter connecting the VM, we established an ssh connection to the Admin1 and Admin2 checked \"wae-status\" and \"service wae diag\" for checking the current state of the tiers and I saw Primary was Active, \r\nbut the Secondary degraded, we faced ER mentioned the problem. \r\nIn this respect, for setting the state of the DB from degraded to standby, connected to the Admin2 and I\'ve run ;\n\r\n\"service wae start db\"  \n\r\nthen when I checked again the state of the secondary DB I saw the DB passed run mode and still \"service wae diag\" script show us ; \n\r\nLocal node status =DEGRADED \r\nreason=\"dbcheck failed:  \n\r\nThen we wanted logs from the customer which are under the /var/log directory and it has taken a long time, during this time we\'ve run \n\r\n\"service wae stop db\" \r\n\"service wae start db\" \n\r\non host2 however, the results are same for the other state checking scripts. \n\r\nAfter all of these actions, we\'ve run below scripts respectively for restart all services of the Host2.; \n\r\n\"service wae stop\" \r\n\"service wae start\"  \n\r\nHowever, the result didn\'t change. \n\r\nFinally, the customer attached to the logs and we said \"For applying another action procedure we need design team\'s confirmation(will start in 7 Hours) becasue we apply this procedure when DB is failed not the deg state. Also, we need  investigate the logs deeply and it will take a long time so we should continue in our office hour\", so they accepted the request and we will contınue working on this issue with design team.','null'),(1177,'Buket Nazmiye KUCUKER (NETAS External)','KandyLink/SPiDR-OAM','2018-10-24','181024-157625','IPC SYSTEMS','Mark(ER) paged the pager and said that the customer have seen Node_Not_Alive critical alarm on their system for APP-3 node.  ER (Mark Zattiero)provide me the site access.I connected the admin1 tier via ssh and run the wae-status.I  saw that all tiers are running fine(App-3 was also running ) So, There were no problem in their tiers. After this, i connected the SPIDR GUI to see the all alarms showing there. As we saw in that list, there is no node_no_alive in their systems for now. On the other words, there was no alarm on the app tier. I have looked at the alarm list under the directory (/var/log/wae/waes/adm11/alarms ). Therefore i have seen NODe_Not_Alive alarm on the lists , it means that some problem was appeared and gone.\r\nAccording to this, because there is no current problem in their system, We opened a new case for investigate further.','null'),(1178,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-10-26','181026-158147','IPC SYSTEMS','I have been paged by Gary Norwood from ER and he stated that TURN1 tier was down. I connected to site and run below command on host7 to recover it;\n\r\n#service wae start\n\r\nAfter running the above command, the problem has been resolved.','null'),(1179,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-10-25','181025-157946','IPC SYSTEMS','Upgrade Path: 9.1.3 to 9.3.2\n\r\nI have been paged by Donnell Williamson from SWD and he reported that upgrade could not start due to the ADMIN 1 being down.\n\r\nWhen upgrade script started, there were some problems about disk space and it seems that \"reboot\" and  \"waesetup --rebuild\" script have been applied to the admin1. Once the \"waesetup --rebuild\" script has been applied, admin1 was trying to get new packets depends on the new platform load of the admin1 and failed to start because upgrade has not been completed successfully and new packages have not been installed properly. \n\r\nWe have reverted back to the platform levels of the admin1 to the old load with below commands;\n\r\n#rm -rf /opt/wae_repo/current\r\n#ln -s /opt/wae_repo/9.1.3 /opt/wae_repo/current\r\n#rpm Uvh /opt/wae_repo/current/wae/wae-platform-9.1.3.ap78.noarch force\r\n#wae-setup --rebuild \r\n#service wae start adm\n\r\nAfter applying the above procedure, admin1 tier became up and running as we expected.\n\r\n[root@hk2con-1 ~]# wae-status\r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER\r\n1   9.1.3.ap78        run   OFF  run run  -   -     -    STA run  -    -\r\n2   9.1.3.ap78        run   OFF  run run  -   -     -    ACT run  -    -\r\n3   9.1.3.ap78        -     -    -   -    run run   run  -   run  -    -\r\n4   9.1.3.ap78        -     -    -   -    run run   run  -   run  -    -\r\n5   9.1.3.ap78        -     -    -   -    -   -     -    -   run  -    run\r\n6   9.1.3.ap78        -     -    -   -    -   -     -    -   run  -    run\r\n7   9.1.3.ap78        -     -    -   -    -   -     -    -   run  run  -\r\n8   9.1.3.ap78        -     -    -   -    -   -     -    -   run  run  -','null'),(1180,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-10-26','181025-157954','IPC SYSTEMS','Upgrade Path: 9.1.3 to 9.3.2\n\r\nI have been paged by Donnell Williamson and he reported that there was an error after running validating database schema script;\n\r\n[root@hk2inet-1 ~]# cat /var/log/upgrade_logs/upgrade_log* | grep \"Validating Database Schema\" -A 1 \r\nOct 26 00:23:45 wae-upgrade Validating Database Schema... \r\nDatabase validation schema not created : 2013: Lost connection to MySQL server during query \n\r\nWhen the validation script started, there was a connection loss to the MySQL server for a short time. Because of this reason, scripts had failed.\n\r\nWe connected to the site and run the validation script manually, the script has been completed without any error.\n\r\n[root@hk2inet-1 ~]# wae-upgrade --validatedbschema\r\nupgrade previous phase: none done\r\nno upgrade in progress\r\nOct 26 20:44:48 wae-upgrade Validating Database Schema...\r\nOct 26 20:44:49 wae-upgrade The schema is valid\n\r\nSince this issue has been resolved, the customer was able to proceed with the patch application.','null'),(1181,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-09-21','180921-151969','TBAYTEL','I have been paged by Donnell Williamson from SWD. He reported that upgrade script ran but it looks like it only upgraded Tiers 1,2,4,6 & 8 but did not upgrade 3, 5 & 7. When we connected the site we noticed that upgrade script skipped the odd tiers because of the midpoint flag which is left from the previous upgrade. Normally this flag should have been deleted after upgrade automatically but somehow it could not be deleted. When the script started, it supposes the first half upgrade has been completed and then it skipped the odd tiers because of this flag. We forced the upgrade script to rollback and re-upgrade from the scratch. After performing this, upgrade completed successfully.','null'),(1182,'Omer KIRCALI','KandyLink/SPiDR-OAM','2018-09-08','180908-149934','IPC SYSTEMS','Hello,\n\r\nFROM/TO LOAD: SPiDR 4.3.2 + patch 7 (9.3.2.au61) / 4.3.2 + patch 87 (9.3.2.au65) \n\r\nCustomer:IPC SYSTEMS\n\r\nDonnell Williamson paged me about the health check error on the app1 tier during the patching.\n\r\n[root@inetchi-1 patch]# runon 3 \"service wae diag\" \r\n10.144.37.13: \r\nntpd     :running: pid=2346 \r\n  server: OK \r\nrsyslog  :running: pid=2156 \r\nslb      :running: pid=3717 \r\n  eSIP: OK \r\n  iSIP: OK \r\napp      :running: pid=4646 \r\n  SIP: no udp port 5180 \r\n  App servlets: no tcp port 8180 \r\n  MPCP: no udp port 3904 \r\npres     :running: pid=7533 \r\n  Client Servlet: OK \r\n  Client Notifications: OK \r\n[root@inetchi-1 patch]#  \n\r\nHe also state the the restart and reboot was tried and no success.\n\r\nAfter connecting the site , between host 3 and 4 such the heartbeat was stuck.\n\r\nAfter stopping and starting the app1 and app2 (on host3,host4) with \"service wae stop\" , \"service wae start\" at the sae time and they come up fully and syncronize with its peer.\n\r\nAfter the health check was completed and the patching was completed I dropped from the call.\n\r\nPATCH STATUS :: \r\n--------------- \n\r\nPATCH_NAME              NUMBER   VERSION   STATUS     STAT_TIME \r\nSPiDR_9.3.2.au61_P_7    7        9.3.2     APPLIED    2018-06-21 22:52:55 \r\nSPiDR_9.3.2.au65_P_8    8        9.3.2     APPLIED    2018-09-07 21:10:17  \n\r\nA case (180908-149934) has been assigned to me in order to investigate why health check failed on the app1 tier while the patch application.','null'),(1183,'Buket Nazmiye KUCUKER (NETAS External)','KandyLink/SPiDR-OAM','2018-09-12','180911-150333','IPC SYSTEMS','Bob Johnson paged and told that it was shown db_not_sync alarm in SPIDR, we connected the site and run wae-status. We saw that db works fine. to check the database services run properly, we run service wae diag db . That looks also fine. On the other hand, we connect to databases one by one and run select count(*) from alias for both.  After running this command for both database, both of them gave us the same values . After that, we ensured that both databases are sync and work properly , we dropped to call.','null'),(1184,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-09-05','180830-148753','Genband US LLC','We have worked with the design team to recover admin1 db which has failed state. \n\r\nThe \'wvm82-back\' user was encountering the \"access denied\" error during the backup process. Also there was an error like \"mysqldump: Couldn\'t execute \'show events\': Access denied for user \'wvm82-back\'@\'10.82.5.12\' to database \'wvm82_adm\' (1044)\". These two errors were causing the backup failure and\r\nwhen the backup fails the standby DB cannot take its role. \n\r\nWhen we checked the waeconfig.py, \"mysql_back_password\" was \'My5qZBack\' instead of  \'My5q!Back\'. When we corrected this password, the first problem has been resolved. \n\r\nAfter that we added the EVENT privilege for the \'wvm82-back\' user on db2 for the second problem and then we were able to take backup.\n\r\nAfter performing above actions, database returned to its correct state.','null'),(1185,'Cigdem Vural','KandyLink/SPiDR-OAM','2018-09-01','180901-148927','KBS EMEA','ER paged SPiDR OAM for bot IM and IP2IP failure on GAAP tool.\r\nSPiDR is on 4.4 load. And at the same time there was an AS 12.1 upgrade on site.\n\r\nThat was the message on GAAP tool:\r\nOrig Call Term - Voice only - Turn Default\r\nOrig: SPiDR Subscribe Failed\r\n	Response Code: 403:Forbidden\n\r\nIssue seems related with CallP side but before that I wanted to be sure.\r\nAll SPiDR tiers were up and running.\r\nSuspected for F5 configuration since there is an As upgrade as well cause that happened sometimes before.\r\nStephen-KBS team joined and told he made the configuration on F5 and issue cleared.\r\nSo it was a missing step on KBS side after the AS 12.1 upgrade as a post step and no issue on SPiDR or AS caused that outage. \n\r\nThen we dropped off.','null'),(1186,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2018-08-14','180814-145770','Kandy Business Solutions','Donnell Williamson from SWD paged me out to report that Nuvia SPiDR Upgrade stucked while upgrading the RPM packages. \n\r\nI have connected the site and run wae-status to check the current status of the tiers.\n\r\nTier 5 was not reachable as follows:\n\r\n[root@nuvspidr-2 ~]# wae-status\r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD TURN BROKER \r\n 1   9.5.1.ax82        STO   run  run run  -   -     -    STA run  -    -      \r\n 2   9.4.0.ax21        run   run  run run  -   -     -    ACT run  -    -      \r\n 3   9.5.1.ax82        -     -    -   -    STO STO   STO  -   run  -    -      \r\n 4   9.4.0.ax21        -     -    -   -    run run   run  -   run  -    -      \r\n10.101.1.15 - unable to connect\r\n10.101.1.15 - trying to reconnect\r\n10.101.1.15 - trying to reconnect\r\n6   9.4.0.ax21        -     -    -   -    -   -     -    -   run  -    run    \r\n 7   9.4.0.ax21        -     -    -   -    -   -     -    -   run  STO  -      \r\n 8   9.4.0.ax21        -     -    -   -    -   -     -    -   run  run  \n\n\r\nI have tried to reboot broker VM several times but we could not bring this guest back due to the following errors:\n\r\nPlease append a correct \"root=\" boot option\r\nKernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)\n\r\nSince I don\'t know the site status, I have asked whether it\'s a production system or not but SWD was not aware of the site status. He checked his e-mails and confirmed that it\'s a lab site.\n\r\nDue to the fact that it\'s not a live site and lab upgrades are not supported in scope of pager support, I have recommended to continue to work on the issue during our daytime.\n\r\nI\'ve agreed with SWD and dropped from the site. Investigation will be ongoing during GPS daytime.','null'),(1187,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-08-08','180808-145106','IPC SYSTEMS','I have been paged by Robert Starling from ER. He stated that there was a Spidr issue in IPC Communications. They were unable to login to their 4 servers. In order to understand that is a server problem or authentication problem we connected the site and run the \"wae-status and\" \"service wae diag\" commands and confirmed that all tiers and their services were up and running. Also, Robert indicated that when the customer clicks on the Unigy 360 application on their PC, they put in the user id and password and it just hangs. Since this issue was not OAM specific, we suggested that Callp GPS should be paged and dropped the call.','null'),(1188,'Bill Picardi','KandyLink/SPiDR-OAM','2018-07-11','180711-140547','IPC: Unigy 360','Customer reported to ER that subscriber logins are failing (registrations and subscriptions).  \r\nFor System verification testing, a host was power-cycled (hard shutdown) to test failover recovery of the application service onto the secondary units.  When the power was restored, the database did not come online and was cycling through Active, Go-Standby, Failed, and db out of sync alarms were reported.  The customer stopped and restarted the admin/db tier hosts and restarted the db services to recover Active/Standby DB status.  After this, the subscriber logins failed.\n\r\n    We checked wae status and wae diag for each tier, and for the presence of the SQL service IP address.  All these appeared to be valid.  We logged into the wae-admin GUI to check for subscriber devices, SPiDR configurations, and to run account trace.  We found the database data was erased, and the External Providers, Client Adapters, and Subscriber Accounts data were missing.\n\r\n    We stopped all services, and restored the SPiDR database from the most recent backup to admin2/db2.  The tier db2 was activated, and the admin tier services started, so we could verify the config data within the wae-admin GUI.  Once verified, the db1 tier was started, and allowed to become standby, then all services restarted.  After this, clients could register and test calls succeeded.  \n\r\nThis is related to dbcheck, dbstatemon and cron scripts and of JIRA ABE-21118 and ABE-21135','null'),(1189,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2018-07-04','180704-139738','AGNI SYSTEMS LIMITED','David Berry from ER paged me out and reported that system is completely down but he was not able to provide any other detail about the customer problem. \n\r\nWhen I asked the product name used by customer, ER was not sure and just said that it may be related to SPiDR. \n\r\nIn order to understand the problem details, I pinged Dave Berry via Trillian and requested the opened E1 case ID. In this respect, it\'s identified that this problem is not associated with the SPiDR and related product is C3 Gateway controller and G9 media gateway. \n\r\nAfter identifying the correct product name, ER contacted to G9 GPS and worked with them together for further investigation.','null'),(1190,'Feridun Bircan SUBASI (NETAS External)','KandyLink/SPiDR-OAM','2018-07-04','180704-139572','HONG KONG BROADBAND NETWORK LTD.','I have been paged by David Giomi and he stated that system reverted back to the old certificate after applying the patch. After connecting and checking the site, we noticed that the issue here was caused by the customer naming the .crt and .key files incorrectly turn1 & turn2. According to \"certificate installation and usage\" document, if any TURN tier should use different certificate file than the one for all TURN tiers, then certificate file for that TURN tier should be renamed as turn.crt. In this respect, we have performed the below procedure;\n\r\nFrom admin1\r\ncat turn7.server.crt turn7.intermediate.crt > turn7.crt\r\ncp turn7.private.key turn7.key\r\nchmod 644 turn*\r\nsftp root@10.32.88.17\r\ncd /etc/cert\r\nput turn*\r\nquit\r\nfrom admin1 login to host7 ([root@hkbnkndy01-1 cert]#host7\r\nservice wae stop\r\nwae-assetup --rebuild\r\nservice wae start\n\r\nAlso, we have performed the same procedure for turn8;\n\r\nFrom admin1\r\ncat turn8.server.crt turn8.intermediate.crt > turn8.crt\r\ncp turn8.private.key turn8.key\r\nchmod 644 turn*\r\nsftp root@10.32.88.18\r\ncd /etc/cert\r\nput turn*\r\nquit\r\nfrom admin1 login to host8 ([root@hkbnkndy01-1 cert]#host8\r\nservice wae stop\r\nwae-assetup --rebuild\r\nservice wae start\n\r\nThe problem has been resolved after performed this procedure.','null'),(1191,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2018-04-13','180412-126376','IPC SYSTEMS','Tim Warren from ER paged me out to report that IPC called ER to receive a case update about newly created business critical case. (180412-126376)\n\r\nFirst of all, I\'ve asked that whether is there any VM connection which is connected to customer system or not but there was no any site access connection. Apart from this, I wanted to know current problem details and ER just referred case ID: 180412-126376\n\r\nThen, I\'ve started to review case comments.. Accordingly, even though the fact that there is no so much detail in case notes, customer was stating that when they check wae status, STR1=NTPD7 not running and recovering on its own..\n\r\nI\'ve taken ownership of this case and put a case comment..\n\r\nIn this respect, I\'ve requested to share wae-status output and told them how they can restart ntpd service at problematic tier.. \n\r\n\"Connect to admin tier and run the below commands at problematic tier..\n\r\nservice ntpd stop\r\nntpd -qg\r\nservice ntpd start\"\n\r\nAlso I\'ve stated that active calls shouldn\'t be affected by NTPD status, so there shouldn\'t be any service impact of this situation. So, I noticed that we can help them better for all cases which has no any service impact within our business hours and provided next case update as tomorrow... \n\r\nIn the light of the provided action plan, investigation will be ongoing after receiving an update from customer...','null'),(1192,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2018-03-24','180321-123064','IPC SYSTEMS','Brent Combs from ER paged me out to report that IPC Systems want to receive an update about the created BC case 2 days ago. (180321-123064)\n\r\nCustomer was unable to launch the SPiDR GUI of Admin1 in Chicago datacenter.. I\'ve called back the customer and started to work on the issue.\n\r\nI\'ve connected the site via Cisco Webex remote control.. We were able to launch the SPiDR GUI of Admin2 in Chicago data center. However, couldn\'t reach the Admin1 GUI. Also customer was able to launch the both admin1 and admin2 SPiDR GUI from New York data center. I\'ve checked the current status of all SPiDR tiers and confirmed that SPiDR is fully functional. In this respect, I\'ve stated that it looks like there is a network problem in the Chicago data center.\n\r\nIf SPiDR was really problematic, we were not be able to launch the GUI from anywhere..\n\r\nWhen I wanted to connect the Chicago site for trying to launch admin1 GUI again, customer has stated that there is a network maintenance in Chicago and it will be completed in five hours.. For this reason, we had no site access to Chicago..\n\r\nAfter I told the customer what I\'ve wrote above, I\'ve agreed with customer to continue to work on the issue as a case working on Monday and dropped from the bridge...','null'),(1193,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2018-03-23','180321-123064','IPC SYSTEMS','Scott Roland from ER paged me out to report that admin1 spidr gui is not reachable in IPC Chicago data center.\n\r\nScott was paged me at 6 A.M in our timezone. Since there is an already opened BC case of this issue, I\'ve offered to work with case owner during our business hours and keep the customer updated. For this reason, I\'ve asked that whether can customer wait approximately 2 hours more or not. Then, Scott has stated that customer is agree to wait for our business hour.\n\r\nCase investigation will be ongoing during GPS office hours...','null'),(1194,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2018-02-12','180212-569505','IPC SYSTEMS','Tom Draper from ER paged me out to report that one of the admin tiers have 100% utilization for /var/log and inodes were full (100%) for /var directory.\n\r\nActions Taken\r\n----------------\n\r\nI connected the site and confirmed that /var/log and /var inodes were full in admin2 tier of Chicago site and hotstandby DB is down.\n\r\nI tried to remove the files under /var/spool/postfix/maildrop directory to clear the inode usage of the /var/ directory but server was reacting too slow reaction against our commands. Since we can\'t run anything due to the slowness issue, we have suggested to reboot host2. Then, on-site engineer rebooted the host2 and we could continue to run the required commands. \n\r\nIn addition to this, we have manually deleted some of the high sized maillog files under /var/log directory in order to clear the disk usage for /var/log directory.\n\r\nThen, since it\'s a known issue for IPC and there is a workaround which is applied to all other sites of customer, we have performed the same workaround for this site. Accordingly, the problem was related to dbhealthcheck script. Cronjob list has not redirected dbhealthcheck output to a folder. This cronjob was as follows:\n\r\n* * * * * /usr/bin/flock -n /tmp/fcj.lockfile /opt/wae/sbin/dbhealthcheck --minutely\n\r\nWe have modified the cronjob as follows:\n\r\n* * * * * /usr/bin/flock -n /tmp/fcj.lockfile /opt/wae/sbin/dbhealthcheck --minutely > /tmp/dbhealthcheck.cron.log 2>&1\n\r\nAfter clearing inodes and disk space issues and applying the workaround, issue is resolved.\n\r\nApart from this, after host2 reboot, repo for admin2 was seen as stopped. I\'ve tried to run service wae stop/start but it was still looking down.\n\r\nAfter running the following command on host2, repo has started to run and this issue also has been resolved: wae-adminsetup --repo\n\r\nRequired logs have been collected by ER and he will attach these logs into the follow-up cases. Due to the fact that it\'s a known issue, GPS will continue to track follow-up with design team.','null'),(1195,'Yunus Ozturk','KandyLink/SPiDR-OAM','2018-02-10','180209-569350','IPC SYSTEMS','Problem Description:\r\n=====================\n\r\nER paged out GPS and reported that one of Admin Tiers have 100% utilization on for /var/log/ directory and inode usage of the /var/ directory was again at %100 utilization.\n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and verified that /var/log/ was showing %100 for df -h output and /var/ was showing %100 for df -i output.\n\r\n- We have manually removed the files under /var/spool/postfix/maildrop directory to clear the inode usage of the /var/ directory.. Additionally, we have manually removed some of the high sized maillog files under /var/log directory to clear the disk usage of the /var/log/ directory.\n\r\n- Afterwards, as per Design Team\'s recommendation, we have applied a workaround solution to prevent this issue to be happened again in future..\n\r\nThe problem has relationship with dbhealthcheck script. Cronjob list has not redirected dbhealthcheck output to a folder. The existing dbhealthcheck cronjob was as follows;\n\r\n* * * * * /usr/bin/flock -n /tmp/fcj.lockfile /opt/wae/sbin/dbhealthcheck --minutely\n\r\nWe have modified this cronjob as follows;\n\r\n* * * * * /usr/bin/flock -n /tmp/fcj.lockfile /opt/wae/sbin/dbhealthcheck --minutely > /tmp/dbhealthcheck.cron.log 2>&1\n\r\nCustomer has informed us that the same cronjob modifications have also been applied on all sites. Due to the inode/disk space issue on this node, they were not able to apply this solution just to this node.. \n\r\nAfter clearing inode and disk space issues and applying the workaround solution, the issue is resolved.','null'),(1196,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2018-02-06','180206-567228','IPC Systems','Rodney Neese from ER paged me to report that 4 different sites of IPC Systems were down with the same symptom and impact at the same time. \n\r\nTheir both SPiDR Databases were down for each and every site. \n\r\nWhen I connected the site, both databases of each site were down and the following warning was returning while trying to start database: No space left on space.\n\r\nAt this point, I suspected that inodes might be full and checked them with \"df -i\" command. As I guessed, usage of /var folder was at %100 level. In order to understand which files occupy the largest IUsage, I\'ve run du -sh * and seen that /var/spool/postfix/maildrop directory occupies the largest part of /var directory. In this way, we have deleted them since there is no any impact on the system. Apart from this, var/log directory was using %100 usage and we have deleted some unnecessary old maillogs. Then these usages have been decreased from %100 to %10-20 levels. We have completed these operations in each site of IPC one by one. In other words, inodes were full in all sites of customer and we have cleared them by deleting /var/spool/postfix/maildrop directory.\n\r\nAfter performing the above operations, started to execute the following commands on each site one by one:\n\r\nrunon --db \"service wae stop db\"\r\nrunon 1 2 \"service wae kill\"\r\nrunon --db \"wae-hbsetup --rebuild\"\r\nrunon --admin1 \"service wae activatedb\"\r\nrunon --db \"service wae start db\"\r\nrunon 1 2 \"service wae start\" \n\r\nSo, these operations worked to recover all problematic sites. At this point, we strongly suspect a network problem/outage/maintenance activity took place as it happened at 4 different sites at the same time. \n\r\nRequested logs will be collected by ER and attached into the created follow-up cases. Then he will dispatch the created follow-up cases to SPiDR OAM queue for further investigation.','null'),(1197,'Burak Biyik','KandyLink/SPiDR-OAM','2017-12-25','TBD','Immmr GmbH','Kandy NOC team gave heads up to chat room as GAAP tool indicated service outage at IMMMR. This was a known issue that both App tiers were problematic and somehow they could NOT get SIP port as well as TCP port for App servlets. \n\r\n[root@imspdr1-4 ~]# service wae diag\r\nntpd :running: pid=2143\r\nserver: OK\r\nrsyslog :running: pid=94816\r\nslb :running: pid=94662\r\neSIP: OK\r\niSIP: OK\r\napp :running: pid=125090\r\nSIP: no udp port 5180\r\nApp servlets: no tcp port 8180\r\nMPCP: OK\r\npres :running: pid=94834\r\nClient Servlet: OK\r\nClient Notifications: OK\r\n[root@imspdr1-4 ~]#\n\r\nThe problem was resolved by rebooting the App tiers as a known workaround. Since the problem has also impacted other Kandy sites, we asked SPiDR Design to be engaged and assist.\n\r\nBased on the initial analysis, it was found that some pres process remains open for a long time, driving up system CPU utilization (kern) to 100%. This seems to cause the app to lose its listeners.\n\r\nKandy Dev-Ops and Support will work together to discuss various methods to detect the condition and automatically recover it after collecting any relevant data. \n\r\nAgreed and dropped the call.','null'),(1198,'Burak Biyik','KandyLink/SPiDR-OAM','2017-12-23','171223-658109','IPC Systems','The server memory utilization was high in both Admin tiers running SPiDR 4.1.3 MR, so Nagios raised the relevant CRITICAL alarm. The alarm was also available in SPiDR GUI.\n\r\nActions Taken:\n\r\n1.	Since there was some free memory on both host1 and host2 based on free m, GPS did suspect application memory utilization so captured current memory utilization with gcutil. The values in the output were normal.\n\r\n2.	GPS captured heap dump (which took more than half an hour) in case it was the application related memory issue. This action was taken before restarting wae services in order not lose data.\n\r\n3.	Restarting wae services did not help clearing memory alarms so the next action was rebooting both host1 and host2 one by one without having a service impact.\n\r\n4.	After rebooting host1, wae services did not come up properly since inode utilization was 100% for /var directory. Inodes were cleared under /var directory so that wae services were up properly. These actions took some time.\n\r\n5.	Memory alarm was cleared on Nagios for host1 and the same actions were taken for host2\n\r\n6.	There were few issues while bringing up the database on host2. It was failing to switch to STANDBY mode. After running few maintenance scripts, this problem was resolved too.\n\r\n7.	Nagios was alarm free for SPiDR tiers.\n\r\nActions to be taken:\n\r\n1.	Retrieve heap dump under /opt directory on host1 and attach to the case.\r\n2.	Archive /var/log directory and attach to the case for both hosts for further investigation.\n\r\nAgreed and dropped the call.','null'),(1199,'Yunus Ozturk','KandyLink/SPiDR-OAM','2017-09-25','170924-647832','Genband Kandy','Problem Description :\r\n=====================\n\r\nSPiDR CallP GPS paged out us to report a problem on SPiDR DB instances. One of the DB instance was stuck at \"Failed\" state and could not get into \"Standby\" status\n\r\nActions Taken:\r\n===============\n\r\nApplied the following steps on the problematic DB instance;\n\r\nservice wae stop db\r\nservice wae kill\r\nwae-hbsetup --rebuild\r\nservice wae activatedb\r\nservice wae start db\r\nservice wae start\n\r\nThese actions did not recover the DB instance..\n\r\nThen we decided to apply the same steps on both DB instances;\n\r\nrunon --db \"service wae stop db\"\r\nrunon 1 2 \"service wae kill\"\r\nrunon --db \"wae-hbsetup --rebuild\"\r\nrunon --admin1 \"service wae activatedb\"\r\nrunon --db \"service wae start db\"\r\nrunon 1 2 \"service wae start\" \n\r\nThese steps also did not fix the issue..\n\r\nThen we have rebooted the problematic DB instance and the problem is resolved.\n\r\nCallP GPS also informed us that there was another problem on the App2 Tier (host4) and it could not retrieve the MPCP port 3904 hence LB was pointing out it as failed. \n\r\nTo fix this issue, we recommended to reinstall the host4 from scratch with the following script;\n\r\nwaesetup --rebuild\r\nreboot\n\r\nThe issue is resolved after rebuild process','null'),(1200,'Burak Biyik','KandyLink/SPiDR-OAM','2017-09-23','170923-647804','IPC Systems','IPC Systems performed some changes on SPiDR system.conf file to configure SPiDR to sent SNMP traps to Nagios. The changes were performed based on GENBAND 630-01832-01.07.02_SPiDR-4.1_Fault-Management.pdf.\n\r\nAfter running few setup commands instructed by the document as part of this procedure, all tiers were down in \"wae-status\"  on Admin1.\n\r\nCustomer accepted that they accidently replace WAE_OAM_NET parameter in the system.conf file with an IP address. Since tiers are communicating through WAE_OAM network, rebuilding system with this config file broke the system functionality.\n\r\nCustomer had reverted their mistake and re-run setup commands to recover the situation, yet it did not help. This was the point that OAM GPS was engaged.\n\r\nI run \"wae-net\" to see if each network is assigned an IP and noticed that Admin1 is running without any IP on WAE_OAM network. It was not the case for Admin2, so \"wae-status\" command showed all tiers as accessible.\n\r\nSince the problem was isolated to Admin1 only, I let IPC run following commands in given order;\n\r\n$ service wae stop\r\n$ service network stop\r\n$ wae-adminsetup --dns\r\n$ wae-netsetup --rebuild\r\n$ service network start\r\n$ wae-fwsetup --rebuild\r\n$ service iptables restart\r\n$ wae-assetup -rebuild\r\n$ service wae start\n\r\nAdmin was assigned an IP from WAE_OAM after running recovery commands. All tiers were accessible and databases were in ACT-STA status as expected. Customer performed test calls too.\n\r\nIPC asked GENBAND to assist/perform this kind of system-wide changes not to cause such outages, yet I told that this should be discussed first with Design and Management Team.\n\r\nIPC was happy with the status of the system and let GENBAND drop the call.','null'),(1201,'Burak Biyik','KandyLink/SPiDR-OAM','2017-09-21','TBD','Kandy US','Ken reported that there is an outage in Kandy US and none of the clients were able to register. After some investagion with Sabri from CallP GPS, they found following exception in SPiDR APP logs:\n\r\n#################################################################################\r\n...\r\n2017-09-21 02:10:30,894 DEBUG [org.apache.axis.ConfigurationException] Exception: \r\norg.apache.axis.ConfigurationException: サービス名ServiceUserServiceは利用できません / [en]-(No service named ServiceUserService is available)\r\norg.apache.axis.ConfigurationException: サービス名ServiceUserServiceは利用できません / [en]-(No service named ServiceUserService is available)\r\n                at org.apache.axis.configuration.FileProvider.getService(FileProvider.java:233)\r\n                at org.apache.axis.AxisEngine.getService(AxisEngine.java:311)\r\n                at org.apache.axis.MessageContext.setTargetService(MessageContext.java:756)\r\n                at org.apache.axis.client.Call.invoke(Call.java:2690)\r\n                at org.apache.axis.client.Call.invoke(Call.java:2443\r\n...\r\n#################################################################################\n\r\nThe PROV instances were stopped and started once at a time, yet this did not help.\n\r\nWhile trying to understand the root cause, the issue ended after reverting back some manual changes in log4j.xml, that was done for investigation of another issue.\n\r\nAfter everything was back to normal, we agreed to end the call.','null'),(1202,'Burak Biyik','KandyLink/SPiDR-OAM','2017-09-19','170919-647293','IPC Systems','I was paged by ER to report IPC cronical problem that both SPiDR db is down.\n\r\nThis was a known issue with the following recovery procedure on-hand. Customer asked GENBAND to apply the procedure immediately.\n\r\nrunon --db \"service wae stop db\"\r\nrunon 1 2 \"service wae kill\"\r\nrunon --db \"wae-hbsetup --rebuild\"\r\nrunon --admin1 \"service wae activatedb\"\r\nrunon --db \"service wae start db\"\r\nrunon 1 2 \"service wae start\" \n\r\nThe first two attempts failed to make both DBs up since we found that mysql was not running on host2. We noticed the following failure when we tried to access to mysql.\n\r\n# mysql\r\nERROR 2002 (HY000): Cant connect to local MySQL server through socket /var/lib/mysql/mysql.sock (2)\n\r\n#/etc/init.d/mysql status\r\nERROR! MySQL is not running, but lock exists\n\r\nWhen we tried to start mysql service, then we ended up with following error.\n\r\n#service mysql start\r\nStarting MySQL.The server quit without updating PID file (/[FAILED]mysqld/mysqld.pid)\"\"\n\r\nThe only way to restore mysql.pid file was rebuilding mysql database for host2. The following commands were run respectively;\n\r\n1. # wae-dbsetup --rebuild\r\n2. # service wae start db\n\r\nBoth Admin tiers and databases were up after taking those actions, yet it took longer than normal as system response to our commands was too slow. CPU utilization and load average values were showing high utilization, especially for host2.\n\r\nCustomer was also agreed that this part of the problem might be caused by underlying disk issue. Customer reported a latency issue with their SAN server. They will be planning to move the database to another storage server in parallel.\n\r\nGPS collected the logs in order to identify the root cause of the initial problem.\n\r\nAgreed and dropped the call.','null'),(1203,'Oktay ESGUL','KandyLink/SPiDR-OAM','2017-09-12','170912-646537','IPC','Tom Draper paged me out to report IPC Spidr E1 outage.\n\r\nThis is a known issue on this site which is being actively investigated via design team involvement.\n\r\nWhile we were working to access to site, customer had applied the recovery action on their own and when we connected ,site was recovered.\n\r\nIn order to use during investigation I have collected below traces and dropped the call.\n\r\n1.Ssh to admin1 tier.  \r\n2.Run \"wae-status \"  \r\n3.Run \"wae-net\"  \r\n4. Run \"netstat -an\"  \r\n5. Run \"ip addr show\"  \n\r\n6.Run \"host2\" to jump admin2 tier.  \r\n7.Run \"wae-net\"  \r\n8.Run \"netstat -an\"  \r\n9. Run \"ip addr show\"  \r\n10. Run \"host1\" to go back to admin1 tier .   \n\r\n11. /var/log/wae directory logs from admin tiers\n\r\nInvestigation will be continued according to new logs.','null'),(1204,'Cigdem Vural','KandyLink/SPiDR-OAM','2017-08-16','170816-643783','IPC Systems','IPC NYC SPiDR load: 9.1.3\n\r\nIPC paged by stating one of their DB is at DEG state.\r\nI connected and check for the logs but there was no point letting us why DB is down.\n\r\nThe only alarm was related with one DB is down.\r\nI run the following on DEG DB host:\r\n#service wae stop db\r\n#service wae start db\n\r\nAfter that DB got the STA state and stable.\r\nBut the strange thing here on this site is DB floating IP was not seen on \"wae-net\" output. Normally ACT dB is assigned that IP and it is seen on wae-net output.\n\r\nSince E2 condition is cleared and customer told everything is working fine, we closed the pager.\n\r\nBut I requested someone from site to provide access for us to check that DB floating IP with our Design team during work hours. Robert Baurkot-customer told he will be around to provide access and work with us.\n\r\nWe dropped the call.','null'),(1205,'Emre OVA (NETAS External)','KandyLink/SPiDR-OAM','2017-08-12','170812-643274','IPC Systems','Gary Norwood (ER) paged me to report that both databases down. I\'ve connected the site via customer Webex directory. First of all, I checked the status of DBs by running wae-status command and it was seen that both DBs were down. Customer indicated that it\'s the first time when the both DBs are down. \n\r\nIn parallel, I checked the related previous cases and found that it\'s a faced issue before and there is an applied procedure in order to recover this situation again. Accordingly, I found the following procedure:\n\r\nrunon --db \"service wae stop db\"\r\nrunon 1 2 \"service wae kill\"\r\nrunon --db \"wae-hbsetup --rebuild\"\r\nrunon --admin1 \"service wae activatedb\"\r\nrunon --db \"service wae start db\"\r\nrunon 1 2 \"service wae start\" \n\r\nI informed the customer to apply the above procedure since the same issue has occured before and solved by applying this procedure but customer didn\'t want to perform this operation before receiving an explanation about the root cause. In this regards, I joined the bridge (+18775664408) and started to talk with the customer.\n\r\nI said that it\'s a known issue and there is an already opened case and it\'s under design investigation. (170628-637568). They said that only one DB was getting down until this time, it\'s the first time when the both DB\'s are down. In order to understand the root cause I asked that whether any special operation has taken or not during the last 1-2 days and they said that a maintenance window has executed yesterday about network configurations. Then I stated that it might be triggered the issue and it should be investigated as case investigation after the log collection has made.\n\r\nIn this regards, after receiving approval from customer I applied the procedure about deactivating and activating db and both DBs came back again.\n\r\nThen I dropped from the bridge.\n\r\nIn order to continue root cause analysis, /var/log folders from both admin1 and admin2 have collected. A follow-up case will be opened and assigned to PS Spidr OAM queue.','null'),(1206,'Yunus Ozturk','KandyLink/SPiDR-OAM','2017-08-04','170804-642331','CenturyLink','Problem Description:\r\n=====================\n\r\nCustomer contacted ER to report that they performed the MOP from SPiDR GPS to add new routes to their Broker Servers (ref case 170718-640076) but the routes were not added after applying the corresponding MOP.\n\r\nCustomer Release : SPiDR 3.1.2\n\r\nBasically, MOP includes the following steps to update the WAE_BROKER_PRI_MEDIA_ROUTES for Broker Tiers;\n\r\n- Add necessary route parameters to the system.conf file, used by all SPiDR tiers\n\r\nWAE_BROKER_PRI_MEDIA_ROUTES=\"prdtmf1 prdtmf2\" \r\nWAE_BROKER_PRI_MEDIA_ROUTE_prdtmf1=\'10.74.6.0/24 via 10.74.32.1\' \r\nWAE_BROKER_PRI_MEDIA_ROUTE_prdtmf2=\'10.74.7.0/24 via 10.74.32.1\' \n\r\n- Push the system.conf to all tiers, then deploy the new configuration with commands\n\r\nwaeconfig validate \r\nservice wae stop \r\nwae-netsetup --rebuild \r\nservice network restart \r\nwae-fwsetup --rebuild \r\nservice iptables restart \r\nservice wae start \n\r\n- Use wae-net command to verify network interfaces and routes\n\r\nwae-net \n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and checked/verified the new routes were not added after applying the MOP.\r\n- After making some further investigation, we noticed that the \"route-eth1\" file under /etc/sysconfig/network-scripts/ directory was not updated after running the \"wae-netsetup --rebuild\" script. This script should actually copy/paste the pre-configured routes under /etc/wae/system.conf or /etc/wae/wae.conf files into /etc/sysconfig/network-scripts/route-eth1 file.\r\n- We noticed that even customer ran the corresponding script (wae-netsetup --rebuild) after adding the new routes under /etc/wae/system.conf file, the \"route-eth1\" file was not updated for some reason. \r\n- We noticed that \"route-eth1\" / \"route-eth2\" / \"ifcfg-eth2\" files under /etc/sysconfig/network-scripts/ directory were not updated due to a special configuration on customer system under /etc/wae/wae.conf file.\n\r\nIt seems that customer intentionally enabled the following parameter on the wae.conf file and that prevents these files to be updated after running the \"wae-netsetup --rebuild\" script\n\r\nPRESERVE_NETCONF=\"ifcfg-eth2 route-eth1 route-eth2\"\n\r\nWith this configuration enabled, the MOP does not work as it is preventing to update the corresponding files. So, the provided MOP is partially incorrect if the \"PRESERVE_NETCONF\" parameter is enabled. \n\r\nCustomer also informed us that they also want to keep the existing routes previously configured under /etc/sysconfig/network-scripts/route-eth1 file. If the MOP worked fine, then they would lose all their existing routes. \n\r\nThe official way to configure the routes should be to add them under /etc/wae/system.conf or /etc/wae/wae.conf files and then run the above scripts to activate them. So, the steps in the MOP would work if customer did not have the \"PRESERVE_NETCONF\" parameter enabled.\n\r\nSo, with current configuration, customer needs to use the following steps to update the routes\n\r\n- Add the new routes into the /etc/sysconfig/network-scripts/route-eth1 file by keeping the existing routes as well\r\n- Manually modify the route-eth1 file by adding the following routes at the end of the existing routes.\n\r\n10.74.6.0/24 via 10.74.32.1 dev eth1\r\n10.74.7.0/24 via 10.74.32.1 dev eth1\n\r\n- And run \"service network restart\" script to activate the new routes. \n\r\nIf customer did not use the \"PRESERVE_NETCONF\" parameter and did not preserve the \"route-eth1\" file, then they would add all the existing routes within the /etc/sysconfig/network-scripts/route-eth1 file into /etc/wae/system.conf file and run the corresponding scripts.\n\r\nHowever, this is not the current situation on this site as we are thinking that this \"PRESERVE_NETCONF\" parameter was intentionally enabled in the past.\n\r\nIf customer would like to keep this parameter enabled, then the only way to add the new routes is to update the /etc/sysconfig/network-scripts/route-eth1 file manually and run the \"service network restart\" script.\n\r\nCustomer also informed us that they were able to apply the steps in the provided MOP successfully on their lab as they do not have the \"PRESERVE_NETCONF\" parameter enabled on the lab. So, that was the reason the MOP worked fine on the lab but not worked on the live site. \n\r\nWe have provided the required steps to customer and dropped the call. They have informed us that they would like to try the new steps on the lab first.','null'),(1207,'Yunus Ozturk','KandyLink/SPiDR-OAM','2017-08-03','170802-642141','GENBAND Kandy','Problem Description:\r\n====================\n\r\nGENBAND NOC contacted ER to report that Kandy IP to IP and IP to Video calls were failing. NOC requested that ER engage Spidr GPS Teams. ER setup a bridge and engaged SPiDR OAM & SPiDR CallP GPS teams.\n\r\nActions Taken:\r\n===============\n\r\nAs per the request of ER Team, we have joined the bridge. The issue was not actually related to SPiDR OAM side. \n\r\nSpidr CallP GPS logged into site and could not find any problems. After some time, the IP to IP and IP to Video calls started to work. Spidr OAM/CallP GPS Teams did not perform any actions to restore the service\n\r\nER was also not aware of any actions taken to restore the service. The system recovered itself automatically or someone from Kandy GPS fixed the problem.','null'),(1208,'Oktay ESGUL','KandyLink/SPiDR-OAM','2017-07-24','170724-640826','IPC','Gerry Dean paged me out  to report intermittent client registration failures at IPC.\n\r\nSpidr Load: SPiDR 9.1.3.ap78\n\r\nI have take a look at the case description and informed them to page out Spidr CallP team since the issue is callp related.\n\r\nCallP team engaged and they continued the investigation.\n\r\nThank you','null'),(1209,'Burak Biyik','KandyLink/SPiDR-OAM','2017-07-18','170718-639867','IPC Con','ER reported a speech path issue for the customer \"IPC\" running SPiDR 9.1.3.ap78. The SPiDR nodes were running on VMWare ESX host as virtual machines. The problem was pointing Broker nodes as per customer initial investigation.\n\r\nBased on the packet capture from nearest firewall, broker was sending ICMP \"Port Unreachable\" messages for the port allocated by TURN for the media path.\n\r\nBasic health-check was performed and logs were not indicating any error/failure. After capturing tcpdump on Broker itself, we did not see any incoming/outgoing ICMP message when customer performed WebRTC test calls.\n\r\nCustomer then send UDP traffic with a test tool over Broker and provided the MAC address sending the ICMP UnReachable messages.\n\r\nFrom the \"wae-net\" output, we noticed that this MAC is different than Broker PublicMedia (eth2) interface. The MAC address was in VmWare range (00:50:56:00:00:00-00:50:56:3F:FF:FF), so it was likely to be another virtual machine on ESX.\n\r\nWe made another test by running following perl script. This was sending a UDP packet from localhost to the pubMedia interface (eth2) over the UDP port 40444(in the range of 40.000-41.000 to hit the firewall)\n\r\n#perl -e \'use IO::Socket::INET;$socket = new IO::Socket::INET (PeerHost => \"127.0.0.1\", PeerPort => \"40444\", Proto => \"udp\",);$size = $socket->send(\"Burak\");print \"sent $size bytes\\n\";shutdown($socket, 1);$socket->close();\n\r\nWe verified that port 40444 is also unreachable.\n\r\nCustomer then found that unknown MAC address belongs to their SBC. After logging in SBC, it was found that a duplicate broker IP was configured for a new service. \n\r\nCustomer fixed duplicate IP issue and the problem was resolved. Agreed and dropped the call.\n\r\nThanks Ken for his assistance.','null'),(1210,'Yunus Ozturk','KandyLink/SPiDR-OAM','2017-07-14','170714-639421','Bell Aliant','Problem Description:\r\n====================\n\r\nSWD paged out GPS for an issue while applying the SPiDR patch (SPiDR_9.3.2.au40_P_1)\n\r\nFROM/TO LOAD: 9.3.2.au36 / SPiDR_9.3.2.au40_P_1\n\r\nAfter running the ./patchManager.sh command got the following error: \r\n============================================================= \r\n[root@spiderbb-1 patch]# ./patchManager.sh \r\n: No such file or directoryï»¿#!/bin/bash \r\n: command not foundline 2: \r\n: command not foundline 24: \r\n: command not foundline 27: \r\n\'/patchManager.sh: line 28: syntax error near unexpected token ` \r\n\'/patchManager.sh: line 28: `initialize() \r\n[root@spiderbb-1 patch]# \n\r\nActions Taken:\r\n==============\n\r\nWe have been informed that the original patch application script file has been modified manually for an existing issue and the new version of the patch application script file has been provided to SWD Team to be applied on the customer site. \n\r\nThe patch application script file has been manually modified with the following commands;\n\r\ncat /tmp/patchManager.sh.abe-18599 > /opt/wae_repo/patch/patchManager.sh\n\r\nHowever, it seems that this script (patchManager.sh) was not re-compiled again after the modification.\n\r\nWe have executed the following command and re-compiled the script;\n\r\n\"dos2unix patchManager.sh\"\n\r\nAfter running the command above, \"./patchManager.sh\" worked fine and we were able to apply the patch successfully.','null'),(1211,'Cigdem Vural','KandyLink/SPiDR-OAM','2017-07-08','170708-638723','Telstra Belong','E1 outage started at 10:15AM on Telstra Belong site\r\nTelstra Belong SpiDR load is  3.1.2\n\r\nI have been involved at 07:00PM with DB is slow issue\r\nWhen I checked for \"wae-status\" it was too slow to respond.\r\nDB was active at Admin tier1 and \"df -h\" output was showing %85 for /opt directory\r\n/opt directory was changing around \"%90 to %40\" for the aCT dB.\r\n/opt/data/mysql directory has lots of temporary tablespace files and slow_queries.log was growing fast as well. \n\r\n-rw-rw---- 1 mysql mysql 2.9G Jul  8 09:57 #sql_6d2c_6.MYD\r\n-rw-rw---- 1 mysql mysql 4.6G Jul  8 09:57 #sql_6d2c_5.MYD\r\n-rw-rw---- 1 mysql mysql 152M Jul  8 09:57 #sql_6d2c_1.MYD\r\n-rw-rw---- 1 mysql mysql  15G Jul  8 09:57 #sql_6d2c_12.MYD\r\n-rw-rw---- 1 mysql mysql 3.1G Jul  8 09:57 #sql_6d2c_10.MYD\r\n-rw-rw---- 1 mysql mysql 8.0G Jul  8 09:57 #sql_6d2c_0.MYD\r\n-rw-rw---- 1 mysql mysql 8.1M Jul  8 09:57 mysql-bin.000007\r\ndrwx------ 8 mysql root  4.0K Jul  8 09:57 .\r\n-rw-rw---- 1 mysql mysql 907M Jul  8 09:57 #sql_6d2c_8.MYD\r\n-rw-rw---- 1 mysql mysql 4.7G Jul  8 09:57 #sql_6d2c_4.MYD\r\n-rw-rw---- 1 mysql mysql 441M Jul  8 09:57 #sql_6d2c_3.MYD\r\n-rw-rw---- 1 mysql mysql 1.3G Jul  8 09:57 #sql_6d2c_14.MYD\n\r\nOn error.logs for mysql the below line was written and the file \"#sql_6d2c_9.MYI\" was under /opt/data/mysql directory. It was at 72GB big when /opt reached to %90 level :\n\r\n-rw-rw---- 1 mysql mysql  72G Jul  8 10:29 #sql_6d2c_14.MYD\n\r\nFrom log files:\n\r\n****170708 10:12:26 [ERROR] /usr/sbin/mysqld: Incorrect key file for table \'/data/mysql/#sql_6d2c_9.MYI\'; try to repair it\r\n****170708 10:12:26 [ERROR] Got an error from unknown thread, /export/home/pb2/build/sb_0-16513225-1442593410.12/rpm/BUILD/mysqlcom-pro-5.5.46/mysqlcom-pro-5.5.46/storage/myisam/mi_write.c:226\n\r\nI checked for the expired devices via DB. Total device count was ~3700 and expired device count was ~3550.\r\nI deleted all expired devices and stale registrations from DB and the swacted the DB to admin tier 2.\n\r\nDelete expired devices\r\n1. step: Take current time (say x)\r\n#select unix_timestamp(now())*1000 from dual;\n\r\n2. step: show number of expired devices, just for information (replace x with the actual value)\r\n#select count(*) from device where expiry < x;\n\r\n3. step: delete expired devices\r\n#delete from device where expiry < x;\n\r\n4. step: Delete stale registrations\r\n#delete from registrations where alias_id in ( select af.alias_id from aliasfunction af inner join enduserpreference eup on af.enduserpreference_id = eup.id inner join enduser eu on eup.enduser_id = eu.id inner join account a on eu.account_id = a.id where a.id not in (select account_id from device));\n\n\r\nAfter swacting the DB, DB slowness issue seems resolved and all queries were OK, there is no temporary tablespaces seen on ACT DB server.\n\r\nBut registration was still not working and error was changed. So we requested Stephen to change the IMS setting back as they changed in first hours of the outage.\r\nAfter that DB slowness issue came back and temporary tablespaces being produced again.\n\r\nWhen I checked for user\'s device count there were some users which has 40 devices which should not be on Telstra system. Cause Telstra has the Max device count setting as 5. At SPiDR Admin GUI, it was set as 100 so that was the problem. Even we set the audit interval, the max device count should not be more than 10 in a 3.x system.\n\r\nWe stopped both App tiers. Changed the max device count number as 5.\r\nCleaned up the database by deleting all records on devices and registrations tables. IF we cleanup all records from these tables the users need to be re-regostered again.\n\r\nDB queries to delete devices and registrations tables:\r\n#delete from devices;\r\n#delete from registrations;\n\r\nThen started app tiers one by one and asked for test registartion.\r\nRegistration was succesfull. All tests passed and outage restored.\n\r\nThen dropped from the call.','null'),(1212,'Kemal AYDEMIR (NETAS External)','KandyLink/SPiDR-OAM','2017-06-08','170608-634596 ','NetFortris','Chris paged me for the take details on the case 170608-634596 since the upgrade is scheduled for 8-Jun 21:00 EDT and cannot move forward without resolved this case.She asked me if we have root password recovery procedure on SPIDR and I have told her we do aware any procedure to recover the root password of admin tiers.We have asked for assistance from Design and keep her in mail chain.Then dropped the call.','null'),(1213,'Yunus Ozturk','KandyLink/SPiDR-OAM','2017-05-26','170526-632953','IPC Systems','Problem Description:\r\n=====================\n\r\nER paged out GPS for the following issue on customer system;\n\r\nDB shows status as FAIL in our NYC Production Deployment. Both Admin Tiers are showing the DB as down..\n\r\nRelease : SPiDR 4.1.3\n\r\nActions Taken:\r\n===============\n\r\n- Accessed the site and noticed both DB components were at down status and they were reporting the following errors while trying to start up.\n\r\nwae is_db_reachable() output = /usr/bin/mysqladmin: connect to server at \'SQL_Service_IP\' failed\r\nwae waiting for db access.\r\nwae is_db_reachable() output = /usr/bin/mysqladmin: connect to server at \'SQL_Service_IP\' failed\r\nwae waiting for db access.\r\nwae is_db_reachable() output = /usr/bin/mysqladmin: connect to server at \'SQL_Service_IP\' failed\r\nwae waiting for db access.\r\nwae is_db_reachable() output = /usr/bin/mysqladmin: connect to server at \'SQL_Service_IP\' failed\r\nwae waiting for db access.\r\nwae is_db_reachable() output = /usr/bin/mysqladmin: connect to server at \'SQL_Service_IP\' failed\r\nwae waiting for db access.\r\nwae is_db_reachable() output = /usr/bin/mysqladmin: connect to server at \'SQL_Service_IP\' failed\r\nwae waiting for db access.\r\nadm server timeout waiting for DB\n\r\n- We have tried to stop/start the DB and restart the wae services but they both did not work.. \r\n- We were able to recover the system after applying the following steps;\n\r\nrunon --db \"service wae stop db\"\r\nrunon 1 2 \"service wae kill\"\r\nrunon --db \"wae-hbsetup --rebuild\"\r\nrunon --admin1 \"service wae activatedb\"\r\nrunon --db \"service wae start db\"\r\nrunon 1 2 \"service wae start\"\n\r\nFor some reason, both DB nodes could not connect to the SQL Service IP and they could not switch to Active and Standby status.. \n\r\nAfter recovering the system, customer performed test calls and they passed.\n\r\nWe will ask for some additional logs for RCA.','null'),(1214,'Oktay ESGUL','KandyLink/SPiDR-OAM','2017-04-26','170426-628773','Nuvia KBS-EMEA','Kandy NOC team reported that subscribe failures. \n\r\nInitial investigation performed by Spidr CallP GPS and they figured out that the app tiers are not responsive.\n\r\n[root@emeaspdr1-1 ~]# wae-status\r\nHost VERSION ADMIN DHCP DNS REPO APP PROXY PRES DB NTPD TURN BROKER\r\n1 9.3.0.at86 run run run run - - - ACT run - -\r\n2 9.3.0.at86 run run run run - - - STA run - -\r\n10.27.1.13 - unable to connect\r\n/opt/wae/etc/functions: fork: Cannot allocate memory\r\n/opt/wae/etc/functions: line 942: % 2: syntax error: operand expected (error token is \"% 2\")\r\n/opt/wae/etc/functions: fork: Cannot allocate memory\r\n/opt/wae/etc/functions: line 942: % 2: syntax error: operand expected (error token is \"% 2\")\r\n/etc/wae/config: fork: Cannot allocate memory\r\nApr 26 08:53:30 wae FATAL: this_host_num must be one of the existing hostnums: \"\"\r\n10.27.1.14 - error 7\r\n5 9.3.0.at86 - - - - - - - - run - run\r\n6 9.3.0.at86 - - - - - - - - run - run\r\n7 9.3.0.at86 - - - - - - - - run run -\r\n8 9.3.0.at86 - - - - - - - - run run -\n\n\r\nIn order to recover the app tiers, they wanted to restart app tiers yet since ssh connection is also down for they could not do it.\n\r\nLater on, they requested from OAM team to reboot the app guest servers.\n\r\nIn order to proceed, connected host servers and rebooted the guest one by one.\n\r\nOnce the servers boot up succesfully, application is recovered and all call tests passed succesfully.\n\r\nThanks','null'),(1215,'Kemal AYDEMIR (NETAS External)','KandyLink/SPiDR-OAM','2016-12-24','161224-611684','Kandy JP','Problem Description:\n\r\nI have been paged by ER in order to report that registrations are failing at Kandy JP.\n\r\nBefore I involve to the pager call, CallP GPS was trying to recovering the outage for host4 (app2) and after restarted the services on this tier, below error is appeared.\n\r\nDec 24 18:03:43 wae waiting for db access. \r\nDec 24 18:04:05 wae is_db_reachable() output = /usr/bin/mysqladmin: connect to \r\nserver at \'52.69.185.9\' failed \n\n\r\nActions Taken:\r\n-Executed the wae-status command and seen that DB1 is ACT ,DB2 is STA.\r\n-Checked the IP-52.69.185.9 and it was the primary db SQL_NET IP.\r\n-Tried ping from the other spidr tiers to this IP. Pings were unsuccessful.(Except from WAE_OAM adm1(52.69.188.9) to SQL_NET primary db IP(52.69.185.9))\r\n-Tried to connect and execute a simple command on the primary db and it was successfull.\r\n-Investigated the db and server logs on adm1 tier a while.There were a bunch of exception but not enough to indicate the root cause.\r\n-Rebooted the VM for DB2(STA) at first and checked the connectivity to db1 IP.Pings were still failing.\r\n-Rebooted the VM for DB1(ACT) and made the DB2 as ACT.After reboot,SQL_NET primary db IP was reachable from the other tiers.After this action registrations were successful on the site.\r\n-In order to check the access issue while the DB1 is ACT state, made a SWACT again.Pings and registrations were still successful.\r\n-Requested to create a follow-up case from ER for this db connectivity issue and after that I have collected the logs for RCA.\n\r\nI will investigate the logs detailed and try to figure out what was the trigger point for db connectivity issue.','null'),(1216,'Burak Biyik','KandyLink/SPiDR-OAM','2016-11-02','161102-604835','GENBAND UCC','Following the upgrade of the APP/PRES tiers, SWD reported that \"slb\" service were failed to start for both host3/host4.\n\r\n##########################\r\n10.82.1.13: \r\nntpd :running: pid=32487 \r\nserver: OK \r\nrsyslog :running: pid=31036 \r\nslb :failed \r\napp :running: pid=9075 \r\nSIP: no udp port 5180 \r\nApp servlets: OK \r\nMPCP: no udp port 3904 \r\npres :running: pid=10723 \r\nClient Servlet: OK \r\nClient Notifications: OK \r\n###########################\n\r\nThe tiers recovered after re-execution of \"wae service restart\"\n\r\nAgreed and dropped the call.','null'),(1217,'Burak Biyik','KandyLink/SPiDR-OAM','2016-11-02','161102-604819','GENBAND UCC','SWD paged me again to report a different failure for the same upgrade.\n\r\nAfter upgrading the ADMIN1 tier to the new load (9.2.0. ar54), \"wae-status\" was showing ADMIN1 in FAILED state after starting wae services with \"service wae start\"\n\r\n=================================================================================================================== \r\n[root@wvm82-1 wae]# wae-status \r\nHost VERSION ADMIN DHCP DNS REPO APP PROXY PRES DB NTPD TURN BROKER \r\n1 9.2.0.ar54 FAI OFF run run - - - STA run - - \r\n2 9.1.2.ap68 run OFF run run - - - ACT run - - - \r\n3 9.1.2.ap68 - - - - run run run - run - - - \r\n4 9.1.2.ap68 - - - - run run run - run - - - \r\n......\r\n================================================================================================================== \n\n\r\nadm service was failing to start based on \"service wae diag\" output:\n\r\n================================================================================================================== \r\n[root@wvm82-1 wae]# service wae diag \r\n.....\r\npeer node status=ACTIVE \r\nreason=\"\" \r\naction=\"\" \r\nrsyslog :running: pid=3028 \r\nadm :failed \r\n================================================================================================================== \n\r\nLooking at the logs under /var/log/wae/waeas/, the following sql execution error was found.\n\r\n----------------------------------------------------------------------\r\n1:32:12,732 INFO  [JBossASKernel] Installing bean(com.genband.wae:service=data-model,type=wae-base) into kernel\r\n01:32:12,734 INFO  [EJBContainer] STARTED EJB: com.genband.wae.model.startup.ModelStartup ejbName: ModelStartup\r\n01:32:12,734 INFO  [JndiSessionRegistrarBase] Binding the following Entries in Global JNDI:\r\n01:32:12,747 INFO  [ModelStartup] Verifying Data Model DB connectivity ...\r\n01:32:12,856 INFO  [ModelStartup] TrackedUserCacheStartup is starting.\r\n01:32:12,880 WARN  [JDBCExceptionReporter] SQL Error: 1146, SQLState: 42S02\r\n01:32:12,880 ERROR [JDBCExceptionReporter] Table \'wvm82.tracked_user\' doesn\'t exist\r\n01:32:12,880 ERROR [ModelFacadeBean] retrieveAllTrackedUserList org.hibernate.exception.SQLGrammarException: could not execute query\r\n01:32:12,883 WARN  [ModelStartup] Data Model DB connectivity not acceptable.\r\n01:32:12,883 ERROR [STDERR] com.genband.wae.model.ModelException: javax.persistence.PersistenceException: org.hibernate.exception.SQLGrammarException: could not execute query\r\n-----------------------------------------------------------------------\n\r\nSo, Admin1 (running new load) was not able to talk to ACTIVE DB (old load), then it went down.\n\r\nWe were agreed to start Admin tiers following to db upgrade, which worked both admin tiers.SWD could contine the upgrade of the rest of the tiers.\n\r\nThe discussion of this connection problem will be tracked under 161102-604819.','null'),(1218,'Burak Biyik','KandyLink/SPiDR-OAM','2016-11-02','161102-604811','GENBAND UCC','SWD reported a failure during the upgrade of Admin1-tier for the path: 4.1 MR2 (9.1.2.ap68) -> 4.2 (9.2.0.ar54)  \n\r\nThe execution of the repository update command \"yum -y upgrade\" was producing the following error:\n\r\n###############################\r\nTransaction couldn\'t start:\r\ninstalling package kernel-2.6.32-642.6.1.el6.x86_64 needs 24MB on the /boot filesystem\r\n###############################\n\r\nApparently, /boot directory had the old kernel files (2.6.32-573) while trying to install the last update (2.6.32-642.6.1).\n\r\nEven though the installed kernels in the system were as follows, somehow version \"2.6.32-573\" files were still under /boot\n\r\n[root@wvm82-1 ~]# rpm -qa | grep kernel\r\nkernel-2.6.32-642.3.1.el6.x86_64\r\nkernel-2.6.32-642.6.1.el6.x86_64\r\nkernel-2.6.32-642.1.1.el6.x86_64\n\r\nI moved old kernel files to another directory to free up disk space and then SWD could continue upgrading Admin1. \n\r\nThe existence of the old kernel files will be tracked (not to block next upgrades) with the jira linked to the 161102-604811.','null'),(1219,'Oktay Esgul','KandyLink/SPiDR-OAM','2016-09-07','160907-597013','Kandy','Gerry paged me to report registration failures at Kandy EU. \n\r\nWhile I was connecting site, I have informed that Ken had already involved and figured out that registration failing due to db corruption.\n\r\nThere is an outstanding upgrade activity on this site and  all upgrade files still located at admin servers. Apparently, due to these files disk capacity was overloaded and cause db errors which had triggered registration failures. \n\r\nAfter cleaned up the corresponding upgrade files, system became stable.','null'),(1220,'Senem Gultekin','KandyLink/SPiDR-OAM','2016-08-15','160815-593572','NIFTY Corporation','Problem Description:\n\r\nER paged OAM SPiDR GPS for a registration issue on Kandy Nifty(Japan) site. CALLP SPiDR GPS was already involved.\r\nSPiDR version: 3.1.0 (8.5.0.al84)\n\r\nSolution:\n\r\n	Discussed the issue with CALLP SPiDR and registration worked on gencom web after password change from PA.\r\n	However customer was still having problems for some specific users.\r\n	Even though user was on A2 database, I was not able to see it on SPiDR database. Which is normal because user will appear on SPiDR database after registration.\r\n	Later on it was detected that issue was seen on new android devices only. Problem was in KOL 1.3, Only Android currently working against KOL 1.3.\r\n	After KOL investigation fix was prepared and deployed by Arkady.','null'),(1221,'Burak Biyik','KandyLink/SPiDR-OAM','2016-08-02','160802-591925','Kandy-Japan','It has been reported that devices are not cleaned up even they are expired on Kandy Japan Site (3.1.1 MR- 8.5.0.al84). We verified that some users were not able to log in to GencomWeb client.\n\r\nActions taken\r\n-------------\n\r\n-> Connect to the database and checked the number of expired devices from db (around 9500 device)\n\r\n-> Checked if audit was running. Apparently, audit was running but not fast enough to clear high number of expired devices. The default is clearing 600 devices per hour.\n\r\n-> The number of expired devices were dropping as queried from db, yet we deleted all expired devices to take immediate outage recovery action.\n\r\n-> Verified that problematic users can login to web client and outage was over\n\r\n-> GPS will monitor the site within the next hours to make sure that expired devices are not increasing oddly. If this is the situation, we will edit audit script to delete 6000 devices per hour.\n\r\n-> We will also set \"Max non-Provisioned Devices per Subscriber\" field to lower values (less than 10) from SPiDR GUI. As experienced earlier on UCC site, having this value at 100 (current config) in 3.1.X MR is locking db after a while in case of registration failures for users with so many devices. This is fixed in 4.1.1 MR.\n\r\n-> Agreed and dropped the call.','null'),(1222,'Oktay Esgul','KandyLink/SPiDR-OAM','2016-07-17','160717-589809','Genband','Sabri paged me to out to report that calls over TURN of Kandy FR were failing due to certificate errors.\n\r\nWe checked the existing certs and ensured that they are expired.\n\r\n[root@mspdrdt1-1 cert]# openssl x509 -in turn.crt -noout -dates\r\nnotBefore=Jul 14 22:57:12 2015 GMT\r\nnotAfter=Jul 16 11:03:52 2016 GMT\n\n\r\nThen, waited for new certs to be supplied for a while.\n\r\nOnce the certificates are shared, I have applied below procedure to apply new certs.\n\r\n1.Renamed the new certs as turn.crt and turn.key.\n\r\n2.Transfered the new certs to /etc/cert directory of admin1 after backing up old ones.\n\r\n3.Run below commands at admin1.\n\r\n    ==>runon --turn \"service wae stop\"\r\n    ==>runon --turn \"wae-assetup --rebuild\"\r\n    ==>runon --turn \"wae-fwsetup --rebuild\"\r\n    ==>runon --turn \"service iptables restart\"\r\n    ==>runon --turn \"service wae start\"\n\r\nOnce the procedure is completed,I have verified new certificates as seen below.\n\r\n[root@mspdrdt1-1 cert]# openssl x509 -in turn.crt -noout -dates\r\nnotBefore=Jun  9 00:00:00 2016 GMT\r\nnotAfter=Aug  8 23:59:59 2019 GMT\n\r\nAfterward ,Meron perform tests call and verified all tests are OKEY we ended the call.\n\r\nThank you','null'),(1223,'Oktay Esgul','KandyLink/SPiDR-OAM','2016-07-16','160716-589792','Genband Kandy','Gary paged me out in order to sip registration had been failing due to certificate issues since CallP side requested assistence  for this procedure.\n\r\nNuri has updated me in regards the issue and informed me that Kandy team is working on the apply certificate at kandy communicator  since it is possible trigger point of the issue.In the meantime, he shared the certificates those sent to him by Phil.\n\r\nWhile we were discussing, Gary informed that once the certificate applied at Kandy Communicator, system became stable and all looks fine and registration passing.\n\r\nAfterward getting final approval that everything fine,I have dropped the call.\n\r\nThank you','null'),(1224,'Yunus Ozturk','KandyLink/SPiDR-OAM','2016-06-24','160624-586803','Telstra Belong','Problem Description:\r\n=====================\n\r\nRoberto Garcia from CALA GTS called this in for Telstra Belong and reported that customer was unable to make outbound calls.\n\r\nActions Taken:\r\n===============\n\r\nIt was determined that for some reason the communication to both host5 / host6 (Broker1 / Broker2) was lost after restarting the API Axle component on Kandy side.\n\r\nSince we were not able to access the host5 / host6 through Admin Tiers, we have decided to reboot these 2 Guest servers through AWS and recovered the communication back again. \n\r\nThere was a known issue which was getting the Broker Tiers down and was fixed with SPiDR 4.1.1 MR. To be able to verify if this was the same issue, we have checked the Broker logs but we did not see the expected \"kernel: BUG: soft lockup - CPU#6 stuck for 67s! [java:4214]\" errors which was the indication of the problem that was already fixed.\n\r\nSo this was a different problem and we asked for a RCA case to investigate this further with Design Team. Required logs will be collected and attached to the RCA Case : 160624-586830','null'),(1225,'Kemal AYDEMIR (NETAS External)','KandyLink/SPiDR-OAM','2016-06-16','160616-585549','NUVIA UCC','I have been paged by Rodney due to report that cannot make any calls via SPiDR and Smart Office(OMNI) on the NUVIA UCC site.\n\r\nThe site was preparing to the upgrade (4.1.1 MR).While SWD was following the pre-steps on the upgrade document, reported that wae-status command on the admin tiers stayed hang. After 10 minutes passed roughly , outage has started at 22:07 EST.\n\r\nI have connected to the system and made health checks.\r\n-CPU&Memory utilization was OK.\r\n-service wae diag command was giving the output that DB and admin tiers are running successfully.\r\n-wae-status command was hanging on the admin tiers.\r\n-Checked the wae.log and system.log and DB related logs on the admin tiers and there was no issue on the logs I have seen.\n\r\nI have restarted the wae services on the admin1 and rebooted the admin1 VM in order to switch active DB but this action did not help to solve the issue.\r\nAfter this,Phil reported that broker2 is down for a couple of days and broker1 is also having the same issue now.  \n\r\nThen we concentrated on the broker issue and found that connection to broker tiers are hanging from admin tiers. In addition to that, we are not able to execute any commands(hangs) after we logged into brokers via console connection.\r\nI have requested from Phil to reboot these guests.\r\nBrokers has been rebooted and then we were able to make calls.\n\r\nI have requested from ER to create a Follow-up case for this issue. After we have collected the logs of broker tiers to find RCA here, then dropped from call.','null'),(1226,'Burak Biyik','KandyLink/SPiDR-OAM','2016-06-07','160607-584157','Kandy-Japan','Failing registrations were reported from Kandy Japan site (8.5.0.al84) for Nifty accounts. While performing some checks on Spidr, ER told that the issue was resolved. Further discussion with involved people, the RCA was provided as below:\n\r\nThere was a new domain created by designer using mediator (jp-mediator.kandy.io) named \"ncell.fring.com\". This creation let Mediator to modify existing External Provider configuration. Mediator changed External Provider IPs to point incoming registrations to Stating Setup instead of Tokyo Setup.\n\r\nTo resolve this issue, External provider had to be updated manually on SPiDR to point to correct IPs.\n\r\nMediator Design will work on the fix not to cause such outage again.','null'),(1227,'Burak Biyik','KandyLink/SPiDR-OAM','2016-06-06','160606-583991 ','SPiDR UCC','I was paged by ER to report an outage in SpiDR UCC system (running 9.0.0.ao36). Smart Office client was not responding at all and db slowness was found to be initial cause.\n\r\nLooking at the registered users from database, there were two problematic users found with a high number of registered devices (87 and 88 devices). Those devices were cleared from database and the system started to be stabilized after that.In order to put the system into a healthy state as soon as possible, we swacted db instances. Also, we changed the allowed number of devices per user as 5 (instead of 100 which was set in that system). The default number of devices is 5.\n\r\nThis is the DB problem that was seen in Telstra before. It occurs if a user has lots of devices (>15-20) and the SIP registration of the user fails. There is a problematic query that runs in case registration failures of which cost is increasing by the order of [number of  devices of that user]^3.  This query is fixed at 4.1.1 MR.','null'),(1228,'Bill Picardi','KandyLink/SPiDR-OAM','2016-05-31','160531-583123','Telstra Belong','ER paged out to SPiDR OAM GPS for calls with no audio from android 4G devices (with mobile- application) to other devices or PSTN.  Joined the conference bridge to assist.  Previously a reset of the transcoder corrected this, and they attempted another reset, but did not resolve the issue.  Checked the status of the nodes and service tiers.  Made team aware that host6 and host8 were not available, and host 4 had SQL deadlock issues and \"too many open files\" issue.  No restart or maintenance was performed, due to busy hour, and pending customer approvals.  The CallP team and NETAS was notified, and asked to join in the investigation.  \r\n  Discovery revealed this issue is related to problems between the Mobile-SDK client and the broker tier, and is part of an ongoing case investigation, 160530-583052.  After more discussions, I was asked to report any anomalies  seen upon the servers to the team, and it was not necessary for OAM GPS to remain on the call.','null'),(1229,'Bill Picardi','KandyLink/SPiDR-OAM','2016-05-16','160516-581319','Telstra Belong','Paged to assist in outage of Telstra-Belong Kandy.  A configuration change was performed on the SPiDR failed.  The change attempt was to correct redundancy and routing, but the change was reverted, as it disrupted functionality.  I logged in to check the status of the hosts and services. After changing the configurations back, host8 did not come online.  The login to the host and a status check displayed:\r\n    Commands are not allowed until after the reboot and rebuild is complete.\r\n    wae FATAL: rebuild in progress: status\n\r\n    The \"waesetup --rebuild\" did not complete and the services would not start.  The steps of the rebuild were manually processed, and configuration was manually corrected.  Startup scripts were reset on the host, then the host was restarted.   \r\n    Host7 and host8 are TURN servers, and it was mentioned that these are not utilized at this site. I remained on the call to check service status for the tier until support was not needed, then dropped the call.','null'),(1230,'Bill Picardi','KandyLink/SPiDR-OAM','2016-05-16','160516-581319','Telstra Belong','Paged to assist in the outage of Telstra-Belong/Kandy.  All calls were failing, and no requests were reaching the SPiDR. Joined the bridge and asked to review the SPiDR ADMIN tiers and check the database status for any problems or possible connection issues.  No problem was found and the service status was functional.  I remained on the call to monitor status, and dropped when my assistance was no longer needed.','null'),(1231,'Bill Picardi','KandyLink/SPiDR-OAM','2016-05-20','160520-582152','TriMil - Toy Genius','ER called SPiDR OAM GPS for a Kandy customer with clients that could not log in.  The issue was no longer seen by the customer by the time GPS joined the call.  The SPiDR hosts were checked for status, and other Support resources were summoned. Santhana Krishnasamy later joined the call and commented that SkyWriter testing caused the outage, from a burst of requests of the presence API \"GetLastSeen\", sent every 2 seconds by the application.  This resulted in a denial-of-service event upon the system, breaking Kandy functionality.  The issue is known to the team, who are now attempting to fix the problem, and steps are taken to remediate the issue from future outages.','null'),(1232,'Tugkan INCE','KandyLink/SPiDR-OAM','2016-05-16','160516-581281','Telstra Belong','I was paged by ER regarding the E1 outage experienced in Telstra Belong site to join the bridge for the recovery operations.\n\r\nBy the time I join the bridge, Stephen Brown and Phil Karam analysed that the issue was completely related with the network problems observed in the site. After I verify with participants that the issue was not a SPiDR problem, I dropped the call.','null'),(1233,'Cigdem Vural','KandyLink/SPiDR-OAM','2016-05-14','160513-581000','Telstra Belong','We had a outage on May 13 at Telstra Belong and freshly installed DB, then everything got stabil.\r\nAgain today Device count increased around 33000~, and we suspect that we can go into the outage situation again.\n\r\nSo worked on the system to decrease device count and make the system stabil.\r\nWhile trying to delete about 20000 device DB got unresponsive and even one select took 1,5 minutes and free memory went down.\n\r\nChanged the audit configuration as 3 minutes and 150 device per app tier.\r\nWaited for another KOL registration time period and everthing was OK again.\n\r\nDevice table has ~2500 count and audit is deleting devices at the scheduled time period.\n\r\nSince everything is OK, we dropped off.','null'),(1234,'Tugkan INCE','KandyLink/SPiDR-OAM','2016-05-13','160513-581000','Telstra Belong','We were paged by SPiDR CallP GPS due to database instances going down in Telstra Belong Kandy system during the investigation of registration failures.\n\r\nWe have connected to the site and checked the status of db instances. They were both down and design team was engaged with the problem since they were already working on the Callp outage with Callp GPS team.\n\r\nOur investigation showed that the disk IO rate was much higher than expected values and mysql database was behaving abnormally causing system to be unsta ble in terms of service.\n\r\nAs a recovery action, we tried plenty of actions which are listed as below with initiation order, yet none of them worked out to recover the site:\n\r\n1-) Restart Application Tier nodes\r\n2-) Restart Admin Tier nodes\r\n3-) Reboot Admin Tears nodes\r\n4-) Multiple times of db swacts\r\n5-) Multiple times of db stop/start operations\r\n6-) Primary Hypervisor reboot\n\r\nThen we saw that mysql database was generating temporary tablespaces in the filesystem due to not being able to process some querries called in the database. Those files were around 1-3GB sized files and generated continiously. \n\r\nAdditionally, these files were tried to be transfered/regenarated on secondary database instance as soon as it gets active which was causing enormous IO rates and cripples the system. \n\r\nWith the involvement of SPiDR architects the root cause of the possbility of the generation of these files were investigated co-operatively. Moreover to that a support ticket was created to oracle for further investigation in parallel. \n\r\nThe trigger of this event could not be identified and since the outage level was E1, we decided to wipe the database and fresh install it with dbinit.sh script after collecting all the data requested by oracle for further investigation.\n\r\nWe have wiped out the database completely to avoid any corrupted data being present in the tables with dbinit.sh script and created mwi tables manually on the database as part of the recovery.\n\r\nFinally, after adding external providers and configuring the additional items in SPiDR admin GUI, registrations started to be successful and calls were established without any errors.\n\r\nWe have verified that all the subscribers in the KOL database were registered in the system and the system was healthy in terms of both OAM and CallP perspective.\n\r\nAll the data required for mysql db problem was collected along with the problematic database backup of customer. We will be trying to reproduce the problem in our labs and drive support process with oracle to find out the root cause.\n\r\nAdditionally, the initial performance tuning recommendations by oracle will also be tested in the lab before implementation. \n\r\nWe will also be investigating the problem observed on SPiDR system regarding devices not being expired within the time interval specified.\n\r\nAfter agreeing on the call that the outage was over and system was healthy, we left the pager call.','null'),(1235,'Tugkan INCE','KandyLink/SPiDR-OAM','2016-05-11','160511-580640','Telstra Belong','Dean Gilbert from ER paged me to report the registration failures on Telstra Kandy system.\n\r\nDean reported that, during their investigation Stephen Brown and the team, they suspected of multiple devices registration exceeding in telstra system. It is analysed that SPiDR system was not logging out or timing out devices within the time period it should.\n\r\nI have connected to the site and checked the situation. The analysis regarding devices not being timed out after their session is expired was correct. However the registration failures were not related with this problem since we were receiving registration failures for the users whose device limits were not exceeded.\n\r\nWe have also checked the SPiDR logs from OAM perspective and everything seemed to be okay OAM-wise(except for the device time-out issue which should be investigated with a separate case).\n\r\nIn the meantime I asked Dean to involve CallP GPS to for investigation of the registration problem. They have also worked on the site for a while and analysed that the problem was related with KOL and unpredictable/different behaviour of KOL for the same requests/calls from SPiDR.\n\r\nAfter analysing that the problem is a KOL issue, we agreed to leave the pager call.','null'),(1236,'Tugkan INCE','KandyLink/SPiDR-OAM','2016-05-09','TBD','Kandy - China','Customer Spidr load -> Simplex 8.4.ai86\n\r\nSabri from Callp GPS paged me to check the status of the Broker since according to the wae-status output, Broker was failing in terms of functionality.\n\r\nCallp team suspected of the problem observed, was the same with the previously experienced broker crash issue which is already fixed in the latest version of 3.1.2 and above (ABE-8619).\n\r\nWe have contacted with design team to see if there is a procedure to upgrade only the Broker instance in the system to recover its functionality. Yet design team reported that there is no such procedure for simplex systems.\n\r\nSo either we had to upgrade the whole system or try to recover the broker instance in the current release.\n\r\nHowever considering that by that time, site was inoperable for remote activities due to intermittent networking problems -even getting a response for type operations was taking more than 5 minutes-, we have tried recovering Broker instance before attempting to upgrade due to upgrade procedure being fully manual and its quite risky to start an upgrade in such networking status.\n\r\nWe have tried restarting the broker instance with the command: \n\r\nservice wae restart broker\n\r\nThe restart operation recovered the service availability and test calls succeeded. We agreed on the bridge to monitor the site for a couple of hours to see if broker instance crashes again or not. \n\r\nWe specified that if broker instance gets crashed again, they should contact with software delivery teams to perform the upgrade to SPiDR 3.1.2 release. \n\r\nAfter agreeing on the bridge, we left the pager call.\n\r\nAccording to our monitoring results, brokers are performing successfully for the past 4 hours.','null'),(1237,'Burak Biyik','KandyLink/SPiDR-OAM','2016-05-08','TBD','Kandy- China','Ozgur from Callp GPS paged me to check if there is something wrong in China multiplex SpiDR tiers since the commands are responding too late. I also joined an existing bridge and was informed about the history.\n\r\nThere were two separated setups in China:\n\r\n1. Simplex SPiDR (running 8.4.ai86 - 3.0 MR)+ production sip proxy\r\n2. New MultiSPiDR (running 8.5.2.am28 - 3.1.2 MR) + test sip proxy\n\r\nAfter migration from Simplex to MultiSpiDR, there were call failures that CallP GPS had already been engaged (SPiDR was responding 403 Forbidden to incoming requests)\n\r\nI first check CPU, memory usage of the SPiDR tiers and ping each of the tiers\' default GW and saw that ping times were under 1 ms which is quite normal. I also pinged each tier from different hosts and the results were fine as well. Even though LAN statistics looked fine, there was an obvious issue while accessing from external networks. Not only me but also all of the people in the call confirmed that commands are returning the results very late and GUI was being launched very slowly. It was said that access from external network is expected to be slow, so there was nothing we could do.\n\r\nThe second issue was having no record in Multiplex SPiDR database for the subscribers already registered. Then, it was found that the requests for the domain that we were working on are still routed to Simplex SPiDR. Even though this domain was under External Provider, there was a configuration missing in KOL.\n\r\nThe last problem was with the one of the broker tiers (out of three). Admin tier was not able to reach this broker via OAM interface so it was shown as \"Unable to connect\" in \"wae-status\" output. While trying to understand if the RCA of the call failures could be unreachable broker, the second broker was dropped to unreachable status too. Looking at the APP tier logs, design noticed the following error and wanted us to make all brokers up.\n\r\n2016-05-08 16:47:17,052 DEBUG [com.genband.wae.apps.mpcp.service.MpcpServiceBean] [getAvailableResource] Available Broker Result-> null\r\n2016-05-08 16:47:17,052 ERROR [com.genband.wae.apps.mpcp.service.MpcpServiceBean] [sendCreateMessage] There is no registered Broker on the system..call will be release..\n\r\nWe asked Stephen to restart broker VMs from Amazon console. While restarting he also realized some misconfiguration in the console and told that this might have led to unstable conditions of broker tiers.\n\r\nWhile waiting for all brokers to be up and running, we also added following two parms into system.conf file in APP tiers as recommended by design.\n\r\n----------------------\r\n# global variables\r\ndualBrokerUsageEnabled=true\r\nallowCallsOnOneBroker=true\r\n-----------------------\n\r\nThis did not help improving call failures.\n\r\nThe network slowness was to high and blocking the investigation. Considering the existing call failures (to be analyzed later on) and network handicap, we decided to rollback the system to Simplex mode.\n\r\nAfter running few successfull test calls on Simplex system, we agreed to drop the call and migrate the system whenever the network speed is better.','null'),(1238,'Yunus Ozturk','KandyLink/SPiDR-OAM','2016-05-03','160503-579710	','Telstra Belong','Problem Description:\r\n====================\n\r\nSPiDR Upgrade Path : From 8.3.ai70.12 (Customized 3.0 MR) to 8.5.2.am28 (Official 3.1.2 MR)\n\r\nAfter performing a SPiDR + KOL (Kandy Orchestration Layer) upgrades on customer site, we have observed high volume of registration failures. \n\r\nActions Taken:\r\n===============\n\r\n- The logs on both KOL and SPiDR Products have been collected and investigated while the problem was happening\r\n- KOL was sending registerHttpUserToSpidr messages to SPIDR but SPIDR was responding with 500 response code. \r\n- The initial problem was found to be a bug in the KOL layer sending the wrong authentication user. \r\n- Once this was corrected, we started seeing 403\'s upon subscription/registration attempts. \r\n- We then restarted Admin/App services on SPiDR. This helped for a short period but did not fix the issue. \r\n- Then we performed a config change on SPiDR Admin GUI to address a locking problem and then performed App services restart on each App Tier. This helped for some time but again did not fix the issue completely. \r\n- We then restarted the KOL and saw a burst of registrations for a short period. \r\n- After further investigations on SPiDR Admin/App Tiers, we found out the following exceptions which were related to MWI feature;\n\r\nERROR [com.genband.wae.client.restclient.SubscriptionResource] Internal error: \r\njava.lang.ClassCastException: com.genband.wae.model.entities.EndUserPreferenceImpl cannot be cast to com.genband.wae.model.entities.MWIEndUserPreference \r\nat com.genband.wae.client.restclient.SubscriptionResource.getMWINotificationTimeoutValue(SubscriptionResource.java:880) \r\nat com.genband.wae.client.restclient.SubscriptionResource.triggerMWI(SubscriptionResource.java:794) \r\nat com.genband.wae.client.restclient.SubscriptionResource.newSubscriptionLogic(SubscriptionResource.java:724) \r\nat com.genband.wae.client.restclient.SubscriptionResource.newSubscription(SubscriptionResource.java:122) \r\nat sun.reflect.GeneratedMethodAccessor614.invoke(Unknown Source)\n\r\n- These exceptions were indicating that there was a corruption related to MWI feature within database.\r\n- SPiDR Design Team has noticed that some MWI related tables and data were somehow corrupted for some reason on DB and the expected MWI Data was not there. \r\n- Due to this problem on SPiDR side, MWI feature has been turned off on KOL side as a workaround solution and subscription/registration attempts from KOL to SPiDR have been forced without MWI related parameters.\r\n- This workaround solution seemed to stabilize the user registrations\r\n- As a permanent solution, the required MWI tables and data has been added manually to the Database by SPiDR Design Team as customer wanted to use the MWI feature.\r\n- After manually adding the required MWI data to the SPiDR Database and turning on the MWI feature back on KOL side, subscription/registration attempts started work fine and stabilize..\n\r\n- Further investigations on the DB side showed us that some MWI related DB entries were somehow corrupted by the DB upgrade scripts and the DB/Schema versions;\n\r\nwae-upgrade --db --fromver $ver\r\nwae-upgrade --schema --fromver $ver\n\r\n- When we have checked the logs, we noticed the following errors related to this MWI data\n\r\nMay 03 17:08:08 wae-mwi-adm_install schema /opt/wae/mwi/conf/mysql/populate.sql\r\nError output from /usr/bin/mysql is in /var/log/wae/wae-mwi-adm_install.results\r\nMay 03 17:08:08 running: /opt/wae/components/wae-nodemonitor-adm_install.sh schema\n\r\nERROR 1452 (23000) at line 67: Cannot add or update a child row: a foreign key constraint fails (`aspspdr2`.`#sql-69ae_24d9`, CONSTRAINT `mwi_ibfk_1` FOREIGN KEY (`accountid`) REFERENCES `account` (`id`) ON DELETE CASCADE)\r\nMay 03 17:08:08 done: /usr/bin/mysql -f aspspdr2\n\r\n- As an initial RCA, Design Team is suspicious about a wrong DB/Schema version issue on the un-official SPiDR Release (8.3.ai70.12) that was being used on the customer site.','null'),(1239,'Kemal AYDEMIR (NETAS External)','KandyLink/SPiDR-OAM','2016-04-28','160428-578881','Telstra','Problem Description :\r\n=====================\n\r\nI have been paged by ER due to approx. 20% registrations on the SPiDR were failing with a 500 response.\r\nWhen I involved to the issue APP tiers has restarted and the active DB has swacted but registrations are still failing.\n\r\nActions Taken :\r\n==============\n\r\nAndy informed me about the issue here is related about MySQL database since he has seen this log below on the error.log.\n\r\n2016-04-28 03:56:31,180 WARN [org.hibernate.util.JDBCExceptionReporter] SQL Error: 0, SQLState: 08S01\r\n2016-04-28 03:56:31,192 ERROR [org.hibernate.util.JDBCExceptionReporter] Communications link failure\r\n2016-04-28 03:56:31,193 INFO [com.genband.wae.client.restclient.SubscriptionAllExceptionMapper] Building Error Response: HTTP STATUS:Internal Server Error STATUS CODE:REST_INTERNAL_SERVER_ERROR\n\r\nI have investigated the APP tiers logs but could not see any issue. Requested to restart ADM tier one by one but despite that GTS reported that the issue continued.\r\nWe collected a call trace on the test user and call scenario showed that failures coming from IMS side.\r\nThen suddenly GTS reported that errors are decreased to normal state.\r\nPossibly ADM restarts has solved the registration issue here but yet we still not sure about that.','null'),(1240,'Cigdem Vural','KandyLink/SPiDR-OAM','2016-04-25','160425-578351','Nuvia UCC','Upgrade path during from 3.1.2 to 4.0.\r\nAfter upgrade to 4.0 all registrations were down. And SpiDR got 403 forbidden for the registration.\n\r\nAfter checking with architects they told tehre is a validation for the parameter \"EXT_TELEPHONY_SIP_PROXY_VIP\" on the system.conf file on app tiers.\n\r\nsystem.conf was supported for both  with Quotation Mark or  without Quotation Mark  before 4.0 release of SpiDR,yet with the changes of Spidr 4.0 Quotation Mark has become unsupported which caused outage for the parameter \"EXT_TELEPHONY_SIP_PROXY_VIP\".\n\r\nActions taken to recover:\n\r\nRemoved the quotation mark for the parameter EXT_TELEPHONY_SIP_PROXY_VIP and restarted both app tiers. Then all users could login to the system without any issues.\n\n\r\nNext Action-RCA : Wae-config-validate script will be updated to catch and notify if the parameter is set with quotation mark','null'),(1241,'Oktay Esgul','KandyLink/SPiDR-OAM','2016-04-21','160421-578004','NUVIA','Gerry from ER team paged me out to report that NUVIA UCC down intermittently, and calls were failing.\n\r\nWe have attended to conf call with ER/Nuvia/SPIDR GPS to discuss the issue.\n\r\nConnected admin servers and checked the status, all applications looked good at first check, yet when we checked the app tiers logs observed many database connection error even active/standby dbs were fine.\n\r\nIn order to recover the service outage,firstly we tried to restart apps yet it did not help,btw notice that memory of active admin overloaded. \n\r\nI have stop database at admin2 and switchover to admin1,this seems solved issue yet after 20 minutes somehow memory usage increased and started to surfaces same issue.\n\r\nWhen we run top command to check memory usage of process ,figure out that there is unexpected memory usage of mysql of admin servers which causes memory usage peak suddenly.\n\r\n[root@wvm82-1 wae]# top\r\ntop - 17:45:59 up  3:18,  8 users,  load average: 22.39, 22.95, 22.05\r\nTasks: 177 total,   1 running, 176 sleeping,   0 stopped,   0 zombie\r\nCpu(s):  3.5%us,  3.4%sy,  0.0%ni,  9.6%id, 82.6%wa,  0.3%hi,  0.5%si,  0.0%st\r\nMem:  16443132k total, 16360808k used,    82324k free,    26664k buffers\r\nSwap:  2096440k total,      152k used,  2096288k free, 11115136k cached\n\r\nContinued investigation and tried several server wae restart at app tiers to clean up stuck db connection ,yet it did not help.\n\r\nThen, rebooted the virtuls servers yet it did not help either,servers booted up properly, application started without any issue after reboot, yet in 15-20 minutes memory issue surfaced.\n\r\nWhen the memory peak occured, all connection admin guis or new call attempt or login from web client were failing.\n\r\nContinued investigation,yet could not find any clue which may trigger the memory cache issue.\n\r\nWe tried to modify /etc/my.cnf file and increase corresponding memory values to handle memory issues yet it did not help.\n\r\nSomehow ,cache memory increase suddenly which cause all application go down.\n\r\nWe went through investigation and notice slow queries numbers are high out of normal,tried to run several basic sql queries like select ,observed that mysql respond any query almost 2 minutes which is extraordinary (while the memory stable this queries just takes 1 or 2 sec). Focused on mysql investigation yet could not find any helpful.\n\r\nAs a last attempt, I recommend to reboot entire physical servers of the system to clean up if there is hung process causing the issue.\n\r\nWhen I connect to host servers server, noticed that servers were up allmost 675 days which really long time and server platform are old and not patch after fresh installation.\n\n\r\n[root@sap-spidr-s1 ~]# uptime\r\n                   22:46:41 up 675 days, 13:23,  3 users,  load average: 0.50, 0.59, 0.60\n\n\r\n[root@sap-spidr-s1 ~]# mcpRelease.pl\n\r\n                           *** MCP Platform Release ***\n\r\n                   System Type:     mcp_core_linux_ple4\r\n                   Release Level:   17.0.14 (via install)\r\n                   Hardware Env:    Other\n\n\n\r\nI have rebooted all of 4 host servers one by one (we had issues to recover host1 which hosting admin1 and app1) .\n\r\nAfter reboots completed, system became stable.\n\r\nWe started monitoring and monitored the system almost 2 hours and memory issue is not seen again.\n\r\nRca investigation will be continued to figure out what caused the memory issue at servers.','null'),(1242,'Oktay Esgul','KandyLink/SPiDR-OAM','2016-03-30','160330-574750','INTELECOM','David Berry paged me to report call failures at Intelecom since the customer see ntpd degraded status for broker1.\n\r\nI have explained that ntpd should not cause call failures, even more if the broker1 was not functioning, broker2 should handle the calls.\n\r\nIn order to clear ntpd problem, we tried to restart ntpd after connecting host5 yet it was too slow,then the server stucked while restaring the ntpd. Since the guest servers are located at cloud, we asked to customer to reboot the server since we do not have access.\n\r\nThey rebooted the servers and broker1 became up/running with all services without any issue(ntp included). In the meantime, customer informed that call failures had been still continued, so that as all component of spidr seems up and running properly, I handed over the case to SPIDR CallP  gps  to continue investigation with CALLP perspective.\n\r\nThen, dropped the call.','null'),(1243,'Burak Biyik','KandyLink/SPiDR-OAM','2016-03-15','160315-572693','Kandy- US Prod','I was paged by ER team to get engaged with KANDY/SPIDR system outage where subscribers intermittently were unable to log into system. Once logged in, they were able to make and receive calls successfully.\n\r\nI had been involved this outage earlier with CallP GPS to take some actions. In addition to login inability, customer reported that login process was taking too long (more than 10 sec) and we were able to see it with provided test users. Looking at the logs taken from APP tiers, there was a problem during service retrieval from Personal Agent.Sample output from failing scenario is below. APP tiers were sending SOPI request to get user services, yet failing with an exception:\n\r\n----------------------------------------------------------\n\r\n2016-03-15 09:49:26,896 DEBUG [com.genband.wae.a2.ws.sopi.A2SopiBean] Sending pa request: https://54.84.188.11:8043/sopi/services/ServiceUserService\n\r\n......\n\r\norg.apache.axis.ConfigurationException: サービス名ServiceUserServiceは利用できません / [en]-(No service named ServiceUserService is available)\n\r\n-------------------------------------------------------------\n\r\nWe had realized that EM Servers (that PROV/PA are deployed on) were up for 500 days. After the reboot of both EM Servers, we were unable to reproduce the issue for a while and there was a little improvement in login speed.\n\r\nThen, it was reported that the initial problems (login failures and slowness) came back. Then we set up an internal conference bridge to discuss further steps.\n\r\nThe first test performed was trying to login to PA directly several times with test users and sending the same SOPI requests to PA for service retrieval. None of the attempts failed when we isolated PA from SPiDR environment. Considering that EM Servers (hosting PA) were rebooted already, then we focused on SPiDR side.\n\r\nWhen we refreshed GCFW client web page, we faced two different behaviors. One was failed logins and the other was getting \"400 Bad Request\" and \"500 Server Internal Error\" while retrieving some of the services such as \"addressbook, routelist etc.\"\n\n\r\nREST, WebSocket and SOPI packages were captured on each App tier while refreshing the page on GCFW by using following commands (PA1:54.84.188.11, PA2 :54.84.188.12, 8580: client interface https port, 8581: client interface wss/ws port)\n\n\r\ntshark -i any -f \"host 54.84.188.11 or host 54.84.188.12 or port 8580 or port 8581\" -w host3.restsopi1.cap\r\ntshark -i any -f \"host 54.84.188.11 or host 54.84.188.12 or port 8580 or port 8581\" -w host4.restsopi1.cap\n\r\nBy using following wireshark filter, we noticed that the SOPI TCP connections on host4 were very short, indicating that the connection was being terminated prematurely.\n\r\nhttp contains deneme.netas.com.tr or http.response.code == 500 or http.response.code == 400 or ip.addr==54.84.188.11 or ip.addr==54.84.188.12\n\r\nSOPI messages were decoded to SSL and it was found that SPiDR host4 App tier (54.84.190.41) is aborting SSL/TLS negotiation with the PA. One sample flow is below:\n\r\n54.84.190.41 (host4) --> 54.84.188.11  (SSLv2)    \"Client Hello\"\r\n54.84.188.11 (PA1)   --> 54.84.190.41  (TLSv1)    \"Server Hello, Certificate, Server Hello Done\"\r\n54.84.190.41 (host4) --> 54.84.188.11  (TLSv1)    \"Fatal, Internal Error\"\n\r\nAfter finding out faulty node, host4 was removed from Amazon ELB so that all client connections from Internet will be routed to host3. SOPI connections to PA looked fine with only host3 and registration issue was ended.\n\r\nThe issue on host4 was resolved after rebuilding server certificates under /etc/cert directory by isseuing following commands respectively:\n\r\n- service wae stop\r\n- wae-assetup --rebuild\r\n- service wae start\n\r\nIn addition to taken actions to make site outage-free, config.json file was also modified by changing \"websocketProtocol\" to \"wss\" instead of \"ws\" to allow secure connections. It was observed that establishing secure web socket connections made an improvement on login duration as well. \n\n\r\nThe follow-up case will be used for RCA tracking and GPS will be analyzing logs to determine how host4 fell into this state breaking TLS/SSL negotiation with PA.','null'),(1244,'Tugkan INCE','KandyLink/SPiDR-OAM','2016-03-03','160303-571054','Netfortris','Turn servers were failing to come up after the upgrade from 7.0.ag49 to 8.5.2.am28 on Netfortris site.\n\r\nWe have investigated the site and the log files and analysed that servers were failing to come up due to missing turnserver certificates under /etc/cert directory. We waited until customer provides their certificates from an accessible link(the only link they provided was not able to be reached due to permission errors)\n\r\nAfter being able to download the customer certificates from an accessible link, turn servers come up successfully. We agreed on leaving the pager call.','null'),(1245,'Cigdem Vural','KandyLink/SPiDR-OAM','2016-03-01','160229-570655','Kandy-US Prod','Tom paged SpiDR OAM with the request of Kandy team.\r\nThey were having CallP outages and at same time has CPU usage over %80 on app tiers.\n\r\nWhen I connected to the system the CPU usage was shown as %2 / %20 /%50\r\nThey  also confirmed the CPU is going down it is not high, but still has CallP outage with some users and there is a high traffic.\n\r\nI collected jstat and jstack outputs, in case of any need.\r\nThe app tiers were up for 292 days so requested a reboot of these guest servers as well.\n\r\nWe rebooted them one by one. And there is one other person joined to call( I guess from Fring side) and told there is one user trying to login continuosly so may causing traffic. At the bridge conference there were several discussions on different problems.\n\r\nAfter a while we performed the reboot on app tiers, they said calls are passing and tehre is no issue left.\n\r\nI dropped off the call.','null'),(1246,'Yunus Ozturk','KandyLink/SPiDR-OAM','2016-02-14','160214-568307','Genband Nuvia','Problem Description:\n\r\nWhile upgrading one of the Brokers (host5), customer has faced with the following error;\n\r\n[root@apn1spr1-1 ~]# runon 5 \"service wae stop;yum clean all;yum -y upgrade\"\r\n52.69.189.15:\r\nStopping ALL WAE services...\r\nWebRTC Broker is stopped\r\n/opt/wae/webrtcBroker/bin/webrtcBroker.sh: line 185: /opt/wae/webrtcBroker/log/webrtcBroker.sh.log: Read-only file system\r\nOK\r\nCleaning up Everything\r\nSetting up Upgrade Process\r\nResolving Dependencies\r\n--> Running transaction check\r\n---> Package bind-libs.x86_64 30:9.3.6-25.P1.el5_11.4 set to be updated\r\n---> Package bind-utils.x86_64 30:9.3.6-25.P1.el5_11.4 set to be updated\r\n---> Package device-mapper-multipath.x86_64 0:0.4.7-64.el5_11 set to be updated\r\n---> Package glibc.i686 0:2.5-123.el5_11.3 set to be updated\r\n---> Package glibc.x86_64 0:2.5-123.el5_11.3 set to be updated\r\n---> Package glibc-common.x86_64 0:2.5-123.el5_11.3 set to be updated\r\n---> Package glibc-devel.x86_64 0:2.5-123.el5_11.3 set to be updated\r\n---> Package glibc-headers.x86_64 0:2.5-123.el5_11.3 set to be updated\r\n---> Package kernel.x86_64 0:2.6.18-406.el5 set to be installed\r\n---> Package kpartx.x86_64 0:0.4.7-64.el5_11 set to be updated\r\n---> Package libvolume_id.x86_64 0:095-14.33.el5_11 set to be updated\r\n---> Package nscd.x86_64 0:2.5-123.el5_11.3 set to be updated\r\n---> Package nspr.x86_64 0:4.10.8-2.el5_11 set to be updated\r\n---> Package nss.x86_64 0:3.19.1-2.el5_11 set to be updated\r\n---> Package openldap.x86_64 0:2.3.43-29.el5_11 set to be updated\r\n---> Package tzdata.x86_64 0:2015g-1.el5 set to be updated\r\n---> Package wae-platform.noarch 0:8.5.0.al84-1 set to be updated\r\n---> Package wae-tdk.noarch 0:8.5.0.al84-1 set to be updated\r\n--> Finished Dependency Resolution\n\r\nDependencies Resolved\n\r\n================================================================================\r\nPackage                 Arch   Version                 Repository         Size\r\n================================================================================\r\nInstalling:\r\nkernel                  x86_64 2.6.18-406.el5          rh5-64-x86-Server  22 M\r\nUpdating:\r\nbind-libs               x86_64 30:9.3.6-25.P1.el5_11.4 rh5-64-x86-Server 901 k\r\nbind-utils              x86_64 30:9.3.6-25.P1.el5_11.4 rh5-64-x86-Server 180 k\r\ndevice-mapper-multipath x86_64 0.4.7-64.el5_11         rh5-64-x86-Server 3.0 M\r\nglibc                   i686   2.5-123.el5_11.3        rh5-64-x86-Server 5.4 M\r\nglibc                   x86_64 2.5-123.el5_11.3        rh5-64-x86-Server 4.8 M\r\nglibc-common            x86_64 2.5-123.el5_11.3        rh5-64-x86-Server  16 M\r\nglibc-devel             x86_64 2.5-123.el5_11.3        rh5-64-x86-Server 2.4 M\r\nglibc-headers           x86_64 2.5-123.el5_11.3        rh5-64-x86-Server 602 k\r\nkpartx                  x86_64 0.4.7-64.el5_11         rh5-64-x86-Server 446 k\r\nlibvolume_id            x86_64 095-14.33.el5_11        rh5-64-x86-Server  43 k\r\nnscd                    x86_64 2.5-123.el5_11.3        rh5-64-x86-Server 178 k\r\nnspr                    x86_64 4.10.8-2.el5_11         rh5-64-x86-Server 123 k\r\nnss                     x86_64 3.19.1-2.el5_11         rh5-64-x86-Server 1.3 M\r\nopenldap                x86_64 2.3.43-29.el5_11        rh5-64-x86-Server 306 k\r\ntzdata                  x86_64 2015g-1.el5             rh5-64-x86-Server 734 k\r\nwae-platform            noarch 8.5.0.al84-1            wae9500            23 M\r\nwae-tdk                 noarch 8.5.0.al84-1            wae9500            28 M\n\r\nTransaction Summary\r\n================================================================================\r\nInstall       1 Package(s)\r\nUpgrade      17 Package(s)\n\r\nTotal download size: 110 M\r\nDownloading Packages:\r\n--------------------------------------------------------------------------------\r\nTotal                                            77 MB/s | 110 MB     00:01     \r\nRunning rpm_check_debug\r\nRunning Transaction Test\r\nFinished Transaction Test\r\nTransaction Test Succeeded\r\nRunning Transaction\r\n  Updating       : tzdata                                                  1/35WAE preinstall: old: 8.4.aj52 new: 8.5.0.al84\n\r\n  Updating       : wae-platform                                            2/35Error unpacking rpm package wae-platform-8.5.0.al84-1.noarch\n\r\nerror: unpacking of archive failed on file /opt/wae: cpio: chown\r\n  Updating       : glibc-common                                            3/35 \r\n  Updating       : glibc                                                   4/35 \r\n  Updating       : nspr                                                    5/35 \r\n  Updating       : bind-libs                                               6/35 \r\n  Updating       : nss                                                     7/35 \r\n  Updating       : openldap                                                8/35 \r\n  Updating       : kpartx                                                  9/35 \r\n  Updating       : libvolume_id                                           10/35 \r\n  Updating       : device-mapper-multipath                                11/35 \r\n  Updating       : bind-utils                                             12/35 \r\n  Updating       : nscd                                                   13/35 \r\n  Updating       : glibc-headers                                          14/35 \r\n  Updating       : glibc-devel                                            15/35Installing version: 8.5.0.al84\r\n(upgrading from version: 8.4.aj52)\n\r\n  Updating       : wae-tdk                                                16/35Error unpacking rpm package wae-tdk-8.5.0.al84-1.noarch\n\r\nerror: unpacking of archive failed on file /opt/wae/components: cpio: chown\r\n  Installing     : kernel                                                 17/35 \r\n  Updating       : glibc                                                  18/35 \r\n  Cleanup        : glibc-devel                                            19/35 \r\n  Cleanup        : nspr                                                   20/35 \r\n  Cleanup        : openldap                                               21/35 \r\n  Cleanup        : bind-libs                                              22/35 \r\n  Cleanup        : tzdata                                                 23/35 \r\n  Cleanup        : nss                                                    24/35 \r\n  Cleanup        : nscd                                                   25/35 \r\n  Cleanup        : bind-utils                                             26/35 \r\n  Cleanup        : kpartx                                                 27/35 \r\n  Cleanup        : glibc                                                  28/35 \r\n  Cleanup        : libvolume_id                                           29/35 \r\n  Cleanup        : glibc                                                  30/35 \r\n  Cleanup        : glibc-headers                                          31/35 \r\n  Cleanup        : device-mapper-multipath                                32/35 \r\n  Cleanup        : glibc-common                                           33/35 \n\r\nInstalled:\r\n  kernel.x86_64 0:2.6.18-406.el5                                                \n\r\nUpdated:\r\n  bind-libs.x86_64 30:9.3.6-25.P1.el5_11.4                                      \r\n  bind-utils.x86_64 30:9.3.6-25.P1.el5_11.4                                     \r\n  device-mapper-multipath.x86_64 0:0.4.7-64.el5_11                              \r\n  glibc.i686 0:2.5-123.el5_11.3                                                 \r\n  glibc.x86_64 0:2.5-123.el5_11.3                                               \r\n  glibc-common.x86_64 0:2.5-123.el5_11.3                                        \r\n  glibc-devel.x86_64 0:2.5-123.el5_11.3                                         \r\n  glibc-headers.x86_64 0:2.5-123.el5_11.3                                       \r\n  kpartx.x86_64 0:0.4.7-64.el5_11                                               \r\n  libvolume_id.x86_64 0:095-14.33.el5_11                                        \r\n  nscd.x86_64 0:2.5-123.el5_11.3                                                \r\n  nspr.x86_64 0:4.10.8-2.el5_11                                                 \r\n  nss.x86_64 0:3.19.1-2.el5_11                                                  \r\n  openldap.x86_64 0:2.3.43-29.el5_11                                            \r\n  tzdata.x86_64 0:2015g-1.el5                                                   \n\r\nFailed:\r\n  wae-platform.noarch 0:8.5.0.al84-1        wae-tdk.noarch 0:8.5.0.al84-1       \n\r\nComplete!\n\r\nActions Taken:\n\r\nSince the same problem was not observed on the other Broker Tiers (host7 / host8) and host5 was already at stopped status before the upgrade, we agreed to work on this problem with a Technical Support case (160214-568307) during business hours.','null'),(1247,'Yunus Ozturk','KandyLink/SPiDR-OAM','2015-12-06','160106-562618','Genband','Problem Description:\n\r\nCustomer reported that they have no Callp for the new users and any new customers or provisioned domains are not working.\n\r\nCustomer also informed that Spidr Admin Tiers were un-responsive on Amazon LB side and they were not able to launch the Spidr Admin GUI. \n\r\nActions Taken:\n\r\nChecked the Spidr Admin Tiers and they were running properly even they were seen as un-responsive on the far end LB side. Checked the Admin Tier Debug logs but there were no indication about this kind of failure. No logs were reported during the incident time interval. \n\r\nAsked the customer to restart the Admin Tiers to figure out this issue. When the Admin Tiers were restarted, we were able to launch the Spidr Admin GUI. However, customer stated that they were still seeing the Admin1 Tier as Down status on the far end Amazon LB side. \n\r\nAfter making some further investigation, we noticed that CPU utilization for the Java process on Admin1 Tier was running over 100%. Therefore, we decided to kill this Java process and restart the Admin1 Tier again. Afterwards, the issue is resolved. \n\r\nSo, we had 3 problems here. 1st one was that the Admin tiers went insane and could not be connected to properly. And this caused the provisioning failed as a result and we had to kill the processes and restart them to recover. 2nd problem was, wae-status output showed us that everything was up and running even it was not based on the far end LB side. And the 3rd problem was the slowness on the system as the Kandy Mediator was not able make domain provisioning to Spidr. When the domain provisioning is performed manually through the Spidr Admin GUI, there is no issue.\n\r\nFor the slowness issue, we have disabled the Admin Tier Debug logs but that did not fix the Kandy Mediator provisioning issue and the logs on the Spidr Admin Tier side was not indicating what was happening about this problem. \n\r\nCustomer collected logs from the Kandy Mediator. Based on these logs, it seems like Kandy Mediator is sending out 600,000 character SOAP request to Spidr to set and provision the domains for a provider and Spidr is responding back 400 Bad Request errors for these requests which make us think that it is too long for Spidr DB. We need to verify this behavious with Spidr Design to see if we have any limitation regarding this SOAP request characters. If that is the case, customer needs to check their tool and should not send out longer SOAP requests to Spidr DB. \n\r\nWhen new domain and users are added at Kandy Portal, this request goes to Mediator and it independently adds domains and users to EXPERiUS and adds domain to External Provider on SPiDR.\n\r\nAs part of the Domain add to SPiDR, Mediator:\n\r\n-Queries SPiDR for list of domains associated with the External Provider\r\n-Constructs request to SPiDR Admin that includes list of domains it received from SPiDR plus the new domain.\n\r\nIt also follows that once a SPiDR domain add request fails  the mediator will never try again. So, this can also be a problem on the Mediator side. Customer will discuss this with the Fring Team as well.','null'),(1248,'Cigdem Vural','KandyLink/SPiDR-OAM','2015-11-30','151130-558284','Kandy-Fring-AT&T Mobility','**********Problem Description**********\r\nER paged and  told Fring has a provisioning problme.\r\nThere was no detail on the case notes so I asked for a bridge.\n\r\nAnd Fring guys told they cannot login to SpiDR admin GUI and they cannot do any provisioning for external providers.\n\r\nLoad information: 8.4.ai86  SpiDR 3.0\n\n\r\n**********Solution*********\r\nIt seemed as related with the SSL certificates. Cause I had worked a similar issue before. So I modified the server.xml file under directory  /opt/wae/profiles/adm/deploy/jbossweb.sar & run a script that was provided to me before by design and restarted admin tier.\n\r\nAfter that I could open the Admin GUI via IE. Then Fring guys also tested and told everything is OK. With my previous case, Design team had informed me that TLS version issue has been fixed with SpiDR3.1 release. \n\r\nI have configured that site and since it is fixed on upper releases, that problem will not happen again with same reason.\n\r\n******Solution details:********\n\r\nGo to the below directory on both admin nodes:\n\r\n#cd /opt/wae/profiles/adm/deploy/jbossweb.sar\n\r\nAnd add the changes on server.xml file which I wrote in red below:\n\r\n/opt/wae/profiles/adm/deploy/jbossweb.sar/server.xml :\n\n\n\r\n           port=\"10077\" address=\"${wae.web.address,jboss.bind.address}\"\r\n           scheme=\"https\" secure=\"true\" clientAuth=\"false\"\r\n           keystoreFile=\"${jboss.server.home.dir}/conf/chap8.keystore\"\r\n           ciphers=\"SSL_RSA_WITH_RC4_128_MD5, SSL_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, SSL_RSA_WITH_3DES_EDE_CBC_SHA, SSL_RSA_WITH_DES_CBC_SHA, TLS_EMPTY_RENEGOTIATION_INFO_SCSV\"\r\n           keystorePass=\"changeit\" sslProtocol = \"TLSv1\" />\n\r\nAfter making that change on server.xml file  please put the script inside of the zip file that I attached to this email on the admin node1.\r\nGo to the directory where you put the script certsetup and run the following commands:\n\n\r\n#chmod 755 certsetup\r\n#./certsetup --rebuild\r\n#runon --admin \"service wae stop adm ; service wae start adm\"','null'),(1249,'Kemal AYDEMIR (NETAS External)','KandyLink/SPiDR-OAM','2015-11-17','151117-556505','Telstra','I have been paged by Tom Draper due to Telsta cannot connect to SPiDR.\r\nWe do not have any info in the case. Tom said to me call Omer  Chone for further details. I have spoken with Omer Chone and he routed me to discuss this issue with Stephen Brown. \n\r\nProblem Description :\r\n=====================\n\r\nStephen told me as problem statement both App tiers stopped responding to SIP OPTIONS messages and were marked offline by load balancer. They were running on 8.3.ai70.12 load.\n\r\nActions Taken :\r\n==============\n\r\nAs a first action, We have wanted wae-status output on admins to sure tiers are running.\n\r\n[root@aspspdr2-1 ~]# wae-status \r\nHost VERSION           ADMIN DHCP DNS REPO APP PROXY PRES DB  NTPD STUN TURN BROKER  \r\n1   8.3.ai70.12       run   run  run run  -   -     -    STA run  -    -    -       \r\n2   8.3.ai70.12       run   OFF  run run  -   -     -    ACT run  -    -    -       \r\n3   8.3.ai70.12       -     -    -   -    run run   run  -   run  -    -    -       \r\n4   8.3.ai70.12       -     -    -   -    run run   run  -   run  -    -    -     \r\n5   8.3.ai70.12       -     -    -   -    -   -     -    -   run  -    - run     \r\n6   8.3.ai70.12       -     -    -   -    -   -     -    -   run  -    -    run     \r\n7   8.3.ai70.12       -     -    -   -    -   -     -    -   run  -    run  -      \r\n8   8.3.ai70.12       -     -    -   -    -   -     -    -   run  -    run  -  \n\n\r\nIn addition to that we wanted to execute service wae diag command on admins.\n\r\n[root@aspspdr2-1 ~]# service wae diag \r\ndnsmasq  :running: pid=17112 \r\n     dns........OK \r\n     dhcp.......OK (configured) \r\n     tftp(pxe)..OK \r\nhttprepo :running: pid=7848 \r\n  Yum repo: ERR port 80 on pid: 6471 \r\n  pxe prompt? yes \r\nntpd     :running: pid=7226 \r\n  server: OK \r\nmysql    :running: pid=3709 \r\n  SQL: OK \r\nubsvc    : \r\nlocal node status=STANDBY \r\n    reason=\"\" \r\n    action=\"\" \n\r\npeer node status=ACTIVE \r\n    reason=\"\" \r\n    action=\"\" \r\nsyslog   :tbd \r\nadm      :running: pid=1849 \r\n  admin GUI: OK\n\r\nThen I have asked to sure if they tried to execute service wae restart  command or not.\r\nWe recommend executing service wae restart on app tiers.\r\nStephen has informed  me after shutting down host4  app tier , outage situation has been eliminated.\r\nThen he started host4 but the issue still exists. We noticed that was not a Load Balancer issue.When both App tiers are up, HTTP Subscriptions from Kandy orchestration are rejected by SPiDR.\n\r\nStephen has informed me about also this is not the first time they have appeared with this issue. There are known issues being followed by JIRA  ABE-7844.\n\r\nUgur Acar involved and updated Stephen about a jar was missing on host4 so it causes outage situation.\r\nThe symptoms of this incident we paged out are quite same yet the server was rebooted instead of service wae restart ,issue  is resolved after re-deploying the  wae-base-sipt application .\n\r\nWe will follow up this incident by 2 separate cases :\n\r\n1st one ; to investigate why wae-base-sipt app is not deployed after VM reboot.\r\n2nd one ;  for the slb initialization issue.\n\r\nWe agreed and dropped the call.','null'),(1250,'Senem Gultekin','KandyLink/SPiDR-OAM','2015-11-06','151106-555056','Nuvia','Updated the pager product as SPiDR OAM.\n\r\nDonnell Williamson paged me out since Broker was down after completing SPIDR upgrade.\n\r\nUpgrade Path:\n\r\nFrom: 8.4.ai72\r\nTo: 8.5.0.al58\n\r\nKen and Phil had already involved when they paged me out and they said that NTP ips were modified after server upgrade which cause NTP sync problem in the system that may trigger the failures and problem.\n\r\nWe discussed and modified /etc/ntp.conf file manually by below steps .\n\r\n-->vi /etc/ntp.conf\r\n-->Edit the ip adress with correct one.\r\n-->Service ntpd stop\r\n-->ntp \r\n-->Service ntp start\r\n-->reboot the broker\n\r\nAfterward above actions completed, Broker became up and running and problem resolved.\n\r\nWe have raised a follow up case in order to find RCA of ntp.conf file modification  after upgrade.\n\r\nThanks','null'),(1251,'Nuri SELCUK','CPaaS','2019-05-10','190510-256970      ','','Kandy wrappers for KBS (https://kbs-na-kl.kandy.io websocket was down). As per Stephen pointed out that websocket requests arrived to CPaaS gw 1-1 and they hang up (10.234.18.23) NA - old system, meanwhile requests arriving at gw-2-1 get response (10.234.18.24). So traffic is disabled sending to gw-1-1 but it didn\'t makes resolve. Problem keeps happening.\n\nSo, Dave connected to the site and investigated it. It shows that one of the websocketadapters have errors since other one was fine. Already, Scott\'s test indicated that issue occured sproadically. One time works then doesn\'t not work. So, he looked into the websocketms logs.\n\nHere is the notification instances working on system.\n\n>>klgpno\n\nNAME                              READY     STATUS    RESTARTS   AGE\ncimadapterms-345704244-fl6kz      1/1       Running   0          27d\ncimadapterms-345704244-qk12n      1/1       Running   0          27d\nnotificationms-3962702805-66c95   1/1       Running   0          27d\nnotificationms-3962702805-bxv7j   1/1       Running   0          27d\npushms-1120329254-3vh5j           1/1       Running   0          27d\npushms-1120329254-d293j           1/1       Running   0          27d\nspidradapterms-244452332-9tpzm    1/1       Running   0          27d\nspidradapterms-244452332-xzwjv    1/1       Running   0          27d\nwebsocketms-561653187-g3j05       1/1       Running   0          27d\nwebsocketms-561653187-kg5bw       1/1       Running   0          27d\n\nWe care about websockets and notifications.\n\nIn order to check the logs;\n\n>>kllono websocketms-561653187-g3j05\n>>kllono websocketms-561653187-kg5bw\n\nThat showed that\n\nwebsocketms-561653187-g3j05 is appeared to be problematic.\nwebsocketms-561653187-kg5bw is fine. \n\nErrors on problematic instance;\n\nCaused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method(reply-code=404, reply-text=NOT_FOUND - no queue \'websocketms-websocketms-561653187-g3j05\' in vhost \'/\', class-id=60, method-id=20)\n        at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:66) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:32) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:366) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.ChannelN.basicConsume(ChannelN.java:1253) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.recovery.RecordedConsumer.recover(RecordedConsumer.java:60) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.recoverConsumers(AutorecoveringConnection.java:657) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        ... 7 more\nCaused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method(reply-code=404, reply-text=NOT_FOUND - no queue \'websocketms-websocketms-561653187-g3j05\' in vhost \'/\', class-id=60, method-id=20)\n        at com.rabbitmq.client.impl.ChannelN.asyncShutdown(ChannelN.java:505) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.ChannelN.processAsync(ChannelN.java:336) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.AMQChannel.handleCompleteInboundCommand(AMQChannel.java:143) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.AMQChannel.handleFrame(AMQChannel.java:90) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.AMQConnection.readFrame(AMQConnection.java:634) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.AMQConnection.access$300(AMQConnection.java:47) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:572) ~[amqp-client-4.0.0.jar!/:4.0.0]\n        ... 1 more\n\nSo, restarted it;\n\n>>kldlpno websocketms-561653187-g3j05\n\nthen this instance terminated and brand new created. Test the wrappers. It started to work.\n\nSpecial thanks to Dave to this on-action training and rapidly resolution of the outage.','null'),(1252,'Berat TOPCU','CPaaS','2019-05-05','190505-247186','Kandy Business Solutions','Paged out by ER and reported there is SMS issue on CpaaS prod site that similar issue with the case 190503-247027.The clients can send SMS but incoming SMS cannot received.\n\r\nContacted with the SMS provide Intelepeer, they started to working on the issue.\n\r\nValidated the webhook for our users, validated our ability to send SMS.\r\nNoted immediately that some tests were not being delivered in addition to inbound SMS failures.  After extensive testing and review we isolated the issue to the following:  When running a TCPDump we can see messages arrive but 85% of the time they are delayed.  letting the TCPDump run we can see test SMS from 25 minutes earlier arrive.\n\r\nAt some pint during the day we began to receive inbound messages again with regularity.  We dropped from a 85% failure rate to a 2% failure rate.  Additional testing at this point determined that CPaaS was unable to process the inbound SMS and deliver it to the user.  The test user ctab2\'s state machine was reset and the notification brokers reset. Following this we were able to receive messages, but sending still had a 50% failure rate.We continue to work with Intelepeer, our 3rd party SMS provider, to resolve their outbound SMS issues.\n\r\nThe problem has been resolved by SMS provide about 08:00 (GMT +3), will follow up the RCA with Intelepeer from the SFDC case.','null'),(1253,'Ozan KAYA','CPaaS','2019-04-11','190412-182076      ','Kandy Business Solutions','Batuhan called me and reported that few customers who use the Smart office app, had an issue where when their extension is called it didn\'t ring the mobile phone when the phone is locked. it only rings when they were on the app itself. \n\n\nThe problem has occurred between Spidr and Cpaas so Cpaas doesn\'t receive any incoming call or IM notification for a push from EMEA Spidr, in the end client didn\'t receive any call incoming call or IM notification too.\n\nI collected Cpaas and SPIDR logs simultaneously and realized that HTTP direct message had been sent by Cpaas to SPIDR as seen below \n\n{\"subscribeRequest\":{\"expires\":0,\"service\":[\"call\",\"IM\"],\"notificationType\":\"HttpDirect\",\"serverIP\":\"kbs-na-token-auth.kandy.io\",\"serverPort\":\"8080\",\"serverUrl\":\"/rest/version/1/user/bonay@emnuvia.com/notification\",\"httpDirectProtocol\":\"HTTP\",\"httpDirectMethod\":\"POST\",\"clientCorrelator\":\"cpaas\"}}\n\nBut incoming ,\"serverIP\":\"kbs-na-token-auth.kandy.io\" was not able to resolve by SPIDR and SPIDR was not able to send PUT message to this serverIP for push notification;\n\n[processMessage] Url Connection Failed for url:http://kbs-na-token-auth.kandy.io:8080/rest/version/1/user/bonay@emnuvia.com/notification\n\nI already inform Cpaas Design team to change this ip to make reachable for SPIDR. We are waiting for them for next action.','null'),(1254,'Ozan KAYA','CPaaS','2019-03-25','190324-178919      ','RIBBON COMMUNICATIONS OPERATING COMPANY, INC.','MArk called me and reported that Customer were unable to access Kandy Chat using the link: https://kbs-na-wrappers-agent.kandy.io/VaOx12Zo/login.They were able to login with our credentials but the screen is stuck and didn\'t move to the default homepage\n\nKen provided test username and password to investigate and I saw that the websocket is broken that\'s why ı got \"Service subscription failed\" error when ı tried to go to home page.\n\nWe restored service by switching the WebSocket node from usec01-app-1-1 to usec01-app-2-1, and rebooting usec-01-app-1-1.Additionally the spidr adapter ms was also rebooted and issue was resolved\n\nFollow-up case 190325-178943 is created for RCA.','null');
/*!40000 ALTER TABLE `generalinfo` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `problemfields`
--

DROP TABLE IF EXISTS `problemfields`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
 SET character_set_client = utf8mb4 ;
CREATE TABLE `problemfields` (
  `pname` varchar(255) NOT NULL,
  `field1` varchar(255) DEFAULT NULL,
  `field2` varchar(255) DEFAULT NULL,
  `field3` varchar(255) DEFAULT NULL,
  `field4` varchar(255) DEFAULT NULL,
  `field5` varchar(255) DEFAULT NULL,
  `field6` varchar(255) DEFAULT NULL,
  `field7` varchar(255) DEFAULT NULL,
  `field8` varchar(255) DEFAULT NULL,
  `field9` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`pname`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `problemfields`
--

LOCK TABLES `problemfields` WRITE;
/*!40000 ALTER TABLE `problemfields` DISABLE KEYS */;
INSERT INTO `problemfields` VALUES ('elmaupgrade','current load','sada','dad',NULL,NULL,NULL,NULL,NULL,NULL),('hardware','from','to',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('local','local1','local2','local3',NULL,NULL,NULL,NULL,NULL,NULL),('p2','f2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('Software','from','to','date','place','person','time',NULL,NULL,NULL),('upgrade','fromload','to load',NULL,NULL,NULL,NULL,NULL,NULL,NULL);
/*!40000 ALTER TABLE `problemfields` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `problems`
--

DROP TABLE IF EXISTS `problems`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
 SET character_set_client = utf8mb4 ;
CREATE TABLE `problems` (
  `problemname` varchar(255) NOT NULL,
  PRIMARY KEY (`problemname`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `problems`
--

LOCK TABLES `problems` WRITE;
/*!40000 ALTER TABLE `problems` DISABLE KEYS */;
INSERT INTO `problems` VALUES ('hardware'),('local'),('Software'),('upgrade');
/*!40000 ALTER TABLE `problems` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `products`
--

DROP TABLE IF EXISTS `products`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
 SET character_set_client = utf8mb4 ;
CREATE TABLE `products` (
  `productname` varchar(255) NOT NULL,
  UNIQUE KEY `productname` (`productname`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `products`
--

LOCK TABLES `products` WRITE;
/*!40000 ALTER TABLE `products` DISABLE KEYS */;
INSERT INTO `products` VALUES ('AS-GW'),('AS-OAM'),('CPaaS'),('KANDY'),('KandyLink/SPiDR-OAM');
/*!40000 ALTER TABLE `products` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `users`
--

DROP TABLE IF EXISTS `users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
 SET character_set_client = utf8mb4 ;
CREATE TABLE `users` (
  `adminname` varchar(255) NOT NULL,
  `username` varchar(255) NOT NULL,
  `userpassword` varchar(255) NOT NULL,
  PRIMARY KEY (`username`),
  KEY `adminname` (`adminname`),
  CONSTRAINT `users_ibfk_1` FOREIGN KEY (`adminname`) REFERENCES `admins` (`adminname`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `users`
--

LOCK TABLES `users` WRITE;
/*!40000 ALTER TABLE `users` DISABLE KEYS */;
INSERT INTO `users` VALUES ('admin','34324','32432432'),('admin','admin','netas**'),('admin','kaanb','123'),('admin','omer20','omer123'),('admin','user90','123');
/*!40000 ALTER TABLE `users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Dumping events for database 'reportengine'
--
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2019-05-28  8:50:38
